ERDDAP: Serving Datasets in the Geosciences
==========================================
Steven K. Baum
v0.1, 2012-07-22
:doctype: book
:toc:
:icons:
:threddshome: http://www.unidata.ucar.edu/projects/THREDDS/
:cdm: http://www.unidata.ucar.edu/software/netcdf-java/CDM/index.html
:ncml: http://www.unidata.ucar.edu/software/netcdf/ncml/
:netcdf: http://www.unidata.ucar.edu/software/netcdf/
:hdf: http://www.hdfgroup.org/
:grib: http://www.grib.us/
:nexrad: http://en.wikipedia.org/wiki/NEXRAD
:wms: http://en.wikipedia.org/wiki/Web_Map_Service
:wcs: http://en.wikipedia.org/wiki/Web_Coverage_Service
:http: http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol
:java: http://www.java.com/en/
:tomcat: http://tomcat.apache.org/

:numbered!:

[preface]
Preface
-------

This document should explain:

* what ERDDAP is and how it might be useful to you;
* how to obtain and install ERDDAP; and
* how to create and configure datasets that can be served by ERDDAP.

The creation and configuration of datasets will take up by far the most
of your time and effort.  As of this writing, the types and formats of
datasets that ERDDAP
can serve include:

* gridded and tabular data from OPeNDAP servers;
* gridded and tabular data from THREDDS servers;
* gridded and tabular data from local NetCDF files;
* gridded and tabular data from another ERDDAP server;
* tabular data from local or remote MySQL, PostgreSQL, etc. database servers;
* tabular data from NOAA NOS web services;
* tabular data from Automatic Weather Station XML files;
* tabular data from NOS XML servers;
* tabular data from OBIS servers; and
* tabular data from SOS servers.

It can also:

* aggregate data from many of these data sources; and
* automatically create local copies of remote datasets.

:numbered:

Introduction to ERDDAP
----------------------

Motivation and Overview
~~~~~~~~~~~~~~~~~~~~~~~

The Environmental Research Division's Data Access Program (*ERDDAP*) is a data
server that provides a simple, consistent way to
download and create maps and graphs of entire or subsets of scientific
datasets.  An example of a working ERDDAP server can be found at:

http://pacioos-mapserver2.ancl.hawaii.edu/erddap/info/index.html?page=1&itemsPerPage=1000[+http://pacioos-mapserver2.ancl.hawaii.edu/erddap/info/index.html?page=1&itemsPerPage=1000+]

Once you have properly configured the datasets served by ERDDAP, the
web interface provides the end user with a wide range of options for finding, exploring
and obtaining the data including:

* full text search for datasets;
* searching for datasets by category;
* obtaining full or partial datasets in a wide range of formats;
* interactively creating graphs and maps that can be downloaded in several
formats;
* access to gridded datasets via the
http://en.wikipedia.org/wiki/Web_Map_Service[Web Map Service (WMS)]; and
* http://en.wikipedia.org/wiki/Representational_state_transfer[RESTful] access to the data.

Roadmap
~~~~~~~

In this document you will learn how to:

* xref:Installation[install] the ERDDAP server and its software prerequisites;
* generally xref:Configuring_the_Server[configure the ERDDAP server] for serving datasets from your
location;
* xref:Automatic_Dataset_Configuration[automatically create configuration
files] for a wide range of specific dataset types;
* xref:Configuration_of_Datasets[modify and supplement the configuration files] for
those datasets to make them even more useful; and
* create or modify datasets to be better compliant with and therefore
useful in ERDDAP.

Features and Capabilities
~~~~~~~~~~~~~~~~~~~~~~~~~

Push and Pull Technology
^^^^^^^^^^^^^^^^^^^^^^^^

The normal mode of ERDDAP is to act as an intermediary, i.e. it takes a
request from a user, gets data from a remote source, reformats the
data, and sends it to the user.
ERDDAP can also also use what is known and push and pull technology.

* http://en.wikipedia.org/wiki/Pull_technology[*Pull Technology*] - the
ability to actively get all of the available data from a remote
source and xref:EDDGridCopy[store a local copy of the data].
* http://en.wikipedia.org/wiki/Push_technology[*Push Technology*] - the use
of subscription services to notify other data servers when new data
is available so they can request or pull the data.

The xref:EDDGridFromErddap[+EDDGridFromErddap+]
and xref:EDDTableFromErddap[+EDDTableFromErddap+] ERDDAP
dataset types use the subscription services and
xref:flag_system[flag system] to make immediate notifications when new data
is available.

[[flag_system]]
Flag System
^^^^^^^^^^^

The use of a flag file

[[subscription_service]]
Subscription Services
^^^^^^^^^^^^^^^^^^^^^

The

Metadata Standards
~~~~~~~~~~~~~~~~~~

The metadata will be implemented via the IOOS Biological Data Terminology at:

http://www.ioos.gov/schema/ioosbiology/1_0/ioos_biological_terminology20120608v1point0.xml[+http://www.ioos.gov/schema/ioosbiology/1_0/ioos_biological_terminology20120608v1point0.xml+]

[[Installation]]
Installation
------------

Related pages with additional installation information:

*Set Up Your Own ERDDAP*:
http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html[+http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html+]

Installing the Prerequisite Software
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The two key prerequisites for installing ERDDAP are
{java}[Java] and {tomcat}[Apache Tomcat].
Many operating systems are shipped with a recent version of Java, while Tomcat
usually has
to be installed separately.  Be sure to note which versions of Java
and Tomcat your version of ERDDAP requires.

UNIX/Linux
^^^^^^^^^^

A UNIX platform - preferably Linux - is a highly recommended choice
on which to install and run your ERDDAP server.
In addition to being a stable platform on which to run the server,
the ERDDAP software was and is still being developing on a
Linux platform.
Also, the vast majority of the documentation - including this document - has
been written by those employing Linux and contains many examples specific
to that platform.

While the ERDDAP server can be installed and run in a production mode
on a Windows or OS X platform, it is not recommended and certainly
not supported herein.
Those platforms are better employed in a client rather
than server capacity.

Java
^^^^

Java is a programming language designed to have minimal implementation
dependencies,
which enables developers to write a program once that will run on any device
that
includes a Java installation.  To accomplish this goal, Java programs are
compiled to run on so-called virtual
machines, which are basically a software version of a computer's hardware CPU.
Once a virtual machine is create for a specific hardware architecture, any
Java program
should run on that architecture.  Java is especially useful for client-server
web
applications such as ERDDAP.

Although it is unusual for a computing platform to not come with Java already
installed,
if the need arises the latest version of Java can be obtained at the download
site at:

* http://www.java.com/en/download/manual.jsp[+http://www.java.com/en/download/manual.jsp+]

which has virtual machine packages for Windows, Linux and Solaris machines for
both
32- and 64-bit architectures.
Apple supplies their own version of Java for the OS X operating system.

Once you have figured out where the Java installation is located, you need to
specify it via a global environment variable.  An example, for a Java
installation
located at */opt/java*, would be:

--------------------------------------
export JAVA_HOME=/opt/java
--------------------------------------

The location is highly variable by platform, so you may have to consult your
sysadmin about this.

Apache Tomcat
^^^^^^^^^^^^^

Relevant Documentation
++++++++++++++++++++++

Tomcat - http://tomcat.apache.org/tomcat-6.0-doc/index.html[+http://tomcat.apache.org/tomcat-6.0-doc/index.html+]

Tomcat Installation - http://tomcat.apache.org/tomcat-6.0-doc/setup.html[+http://tomcat.apache.org/tomcat-6.0-doc/setup.html+]

Tomcat and TDS Security Tutorial -
http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html[+http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html+]

Introduction
++++++++++++

Apache Tomcat is an open source implementation of Java Servlet and JavaServer
Pages
techologies.  A Java servlet is a Java programming language class used to
extend the
capabilities of servers that host web applications accessed by a
request-response
model.  Servlets provide component-based, platform-independent methods for
building
web-based applications such as ERDDAP.  JavaServer Pages (JSP) is a
technology
for creating dynamically generated web pages based on HTML and XML.  It is
similar
to the PHP web programming language but built on top of Java.
Basically, Tomcat is an HTTP server that leverages the platform-independence
of Java all the way up to the server level, enabling and allowing the
portability of such web-based applications as ERDDAP.

It should be noted that implementations of Java Servlet and JavaServer Pages
other
than Tomcat are available and can also be used with ERDDAP.  We have chosen
to
document Tomcat for the simple reason that it works well with ERDDAP and that
most of the available documentations is for that combination.

Tomcat is usually not included in standard operating system distributions and
must be obtained from the Tomcat home site at:

* http://tomcat.apache.org/[+http://tomcat.apache.org/+]

Tomcat can be obtained in source code format and compiled for your specific
platform, but it is recommended that you skip that chore and simply
download a binary distribution appropriate to your platform.

An Installation Warning
+++++++++++++++++++++++

*WARNING*: This section contains a quick and easy method for installing Apache Tomcat.
It is also a dangerous and insecure method, and as such anything in this
section that contradicts anything found in the *Tomcat and TDS Security
Tutorial* at:

http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html[+http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html+]

should be ignored in favor of the instructions in the latter.

Please do follow the instructions in the latter on how to:

* create a dedicated, non-root user and group under which you can run Tomcat;
* remove unused web applications in the +webapps+ directory;
* use digested passwords;
* enable SSL encryption;
* block non-essential port access at the firewall;
* restrict access to the TDS by remote IP address or host;
* secure the Tomcat +manager+ application;
* run the TDS behind a proxy server; and
* run Tomcat with a security manager.

Performing any of these tasks essentially creates another layer of
security to frustrate hackers, and the more layers you have
the more secure your installation will be.

That being said, if you're chomping at the bit to give THREDDS a try
and you're comfortably and safely behind about 20 firewalls, then
you can have THREDDS up and running in probably about 15 minutes
using the instructions in this section.  That being said, please
do follow all or most of the security suggestions if you're
planning to move on and install a production server that will
be open to the public, including every bored hacker on the
planet.

A Quickie Installation Procedure
++++++++++++++++++++++++++++++++

The download page for Apache Tomcat version 6.x is at:

http://tomcat.apache.org/download-60.cgi[+http://tomcat.apache.org/download-60.cgi+]

A basic installation procedure begins with downloading a compressed version of
the distribution,
in this case +apache-tomcat-6.0.35.tar.gz+.   We will be installing
Tomcat in the
+/opt+ directory, but that can vary depending on how your local
installation is
configured.  
If you install in a different location, just replace +/opt+ with the root path
of your installation in all that follows.
After we have downloaded the distribution, we enter the following
commands (with
root privileges required for this particular location as indicated by
the +su+ command below):

------------------------------------------------------
su
mv apache-tomcat-6.0.35.tar.gz /opt
cd /opt
tar xzvf apache-tomcat-6.0.35.tar.gz
mv apache-tomcat-6.0.35 tomcat
------------------------------------------------------

This uncompresses and unarchives the files contained in the distribution and
creates a hierachy of subdirectories in the +tomcat+ directory that, upon
entering the commands:

-------------------------
cd tomcat
ls -l
-------------------------

should look something like this:

--------------------------------------------------------------------------
drwxr-xr-x. 2 root root  4096 Apr 24 14:24 bin
drwxr-xr-x. 3 root root  4096 Apr 24 14:24 conf
drwxr-xr-x. 2 root root  4096 Apr 24 14:08 lib
-rw-r--r--. 1 root root 37951 Nov 28 04:22 LICENSE
drwxr-xr-x. 2 root root  4096 Apr 24 14:24 logs
-rw-r--r--. 1 root root   558 Nov 28 04:22 NOTICE
-rw-r--r--. 1 root root  8680 Nov 28 04:20 RELEASE-NOTES
-rw-r--r--. 1 root root  6670 Nov 28 04:22 RUNNING.txt
drwxr-xr-x. 2 root root  4096 Apr 24 14:08 temp
drwxr-xr-x. 7 root root  4096 Nov 28 04:20 webapps
drwxr-xr-x. 3 root root  4096 Apr 24 14:24 work
--------------------------------------------------------------------------


Setting the Environment
+++++++++++++++++++++++

To save time, frustration and grief in the long run, it is strongly
recommended that a +setenv.sh+ file be created in the
directory +/opt/tomcat/bin+ that is created during installation.
To do this, execute the following command:

------------------------------
cd /opt/tomcat/bin
------------------------------

and use a text editor to create a file containing the following commands.  For
example, if we use the +vi+ text editor we would issue the command:

---------------------------------
vi setenv.sh
---------------------------------

and, upon entering editing mode, enter the following lines:

------------------------------------------------------------------------------------------------
#!/bin/sh
#
# ENVARS for Tomcat and TDS environment
#
JAVA_HOME="/opt/java"
export JAVA_HOME

JAVA_OPTS="-Xmx4096m -Xms512m -server -Djava.awt.headless=true
-Djava.util.prefs.systemRoot=$CATALINA_HOME/content/thredds/javaUtilPrefs"
export JAVA_OPTS

CATALINA_HOME="/opt/tomcat"
export CATALINA_HOME
------------------------------------------------------------------------------------------------

On 32-bit platforms where RAM size may be smaller than 4 Gb, we can
swap +-Xmx4096m+ for +-Xmx1500m+ in the above.

See the *Security Measures* section below for additional steps recommended for
production installations of ERDDAP.

Starting the Server
+++++++++++++++++++

There are a couple of options for starting Tomcat, with extensive details
available at:

http://www.mulesoft.com/tomcat-start[+http://www.mulesoft.com/tomcat-start+]

Basically, Tomcat can be started manually or automatically.
A manual start - given the +/opt+ location into which we have installed the
package - would be performed via:

------------------------------------------------------
/opt/tomcat/bin/startup.sh
------------------------------------------------------

with a shutdown performed similarly via:

------------------------------------------------------
/opt/tomcat/bin/shutdown.sh
------------------------------------------------------

If these commands do not work, check the commands you used to set up the
environment via creating the +setenv.sh+ file.

Tomcat can also be run automatically as a
http://en.wikipedia.org/wiki/Daemon_%28computing%29[UNIX daemon]
using a program called +jsvc+ that is included in Tomcat binary
distributions.  The following commands will compile and install this program:

---------------------------------------------------------
cd /opt/tomcat/bin
tar xzvf commons-daemon-native.tar.gz
cd commons-daemon-1.0.x-native-src/unix
./configure
make
cp jsvc ../..
----------------------------------------------------------

This procedure puts the +jsvc+ binary in the +/opt/tomcat/bin+ directory and
allows you to run it as a daemon via:

----------
/opt/tomcat/bin/jsvc -cp ./bin/bootstrap.jar -outfile ./logs/catalina.out
            -errfile ./logs/catalina.err org.apache.catalina.startup.Bootstrap
----------

Additional information about this procedure including several additional
options can be found at:

http://tomcat.apache.org/tomcat-6.0-doc/setup.html[+http://tomcat.apache.org/tomcat-6.0-doc/setup.html+]

Checking for a Running Server
+++++++++++++++++++++++++++++

Once you have started the Tomcat server via one of the procedures above, you
can verify that it
is running either via the command line with:

--------------------------------
ps -ef | grep tomcat
--------------------------------

where, if the server is running, you'll see something like the following
confusing mess:

--------------------------------
baum     10781 23963  0 13:40 pts/26   00:00:00 grep tomcat
root     18619     1  0 Apr24 pts/32   00:04:13 /usr/bin/java
   -Djava.util.logging.config.file=/opt/tomcat/conf/logging.properties -Xmx4096m
   -Xms512m -XX:MaxPermSize=180m -server -Djava.awt.headless=true
   -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager
   -Djava.endorsed.dirs=/opt/tomcat/endorsed -classpath
   /opt/tomcat/bin/bootstrap.jar -Dcatalina.base=/opt/tomcat
   -Dcatalina.home=/opt/tomcat -Djava.io.tmpdir=/opt/tomcat/temp
   org.apache.catalina.startup.Bootstrap start
---------------------------------

or, if it's not running, you'll see just the first line containing +grep
tomcat+.

You can also check for a running server by opening a browser window or tab and going to:

http://localhost:8080/[+http://localhost:8080/+]

where you should see the welcome page for Apache Tomcat looking something like
this:

.Tomcat Welcome Page
image::thredds/tomcat.png[height=700]

Troubleshooting
+++++++++++++++

Tomcat troubleshooting starts with checking the logs in the directory:

--------------------------------
/opt/tomcat/logs
--------------------------------

with the most useful messages usually ending up in the main log file, that is:

---------------------------------------------
/opt/tomcat/logs/catalina.out
---------------------------------------------

although you may not be able to make out hide nor hair of what's happening in
there due to the recondite nature of Java and its error messages.  Try Google
or you local sysadmin if you're hopelessly confused on the matter.

Security Measures
+++++++++++++++++

The *Tomcat and TDS Security Tutorial* at:

http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html[+http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Security.html+]

is an excellent, detailed look at how to secure your Tomcat and TDS
installation.

The THREDDS site has a checklist for production installation available at:

http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Checklist.html[+http://www.unidata.ucar.edu/projects/THREDDS/tech/tds4.2/tutorial/Checklist.html+]

that details several additional installation steps recommended for securing a
production Tomcat installation.  Basically, if you do this now you'll be
dealing much less with the predations of Romanian hackers later.

Performance Tuning
++++++++++++++++++

If you are having performance issues with Tomcat, for instance, it's running
very slowly, then you have some options to tune it for optimum performance.  A
good overview of these options can be found at:

http://www.mulesoft.com/tomcat-performance[+http://www.mulesoft.com/tomcat-performance+]

although they fall outside of the purview of our basic installation procedure.

Installing ERDDAP
~~~~~~~~~~~~~~~~~

Relevant Documentation
^^^^^^^^^^^^^^^^^^^^^^

Set Up Your Own ERDDAP -
http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html[+http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html+]

Obtaining ERDDAP
^^^^^^^^^^^^^^^^

The latest ERDDAP release can always be found on this page:

http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html[+http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html+]

with the actual file link being:

http://coastwatch.pfeg.noaa.gov/erddap/download/erddap.war[+http://coastwatch.pfeg.noaa.gov/erddap/download/erddap.war+]

ERDDAP configuration files are supplied in a separate file:

http://coastwatch.pfeg.noaa.gov/erddap/download/erddapContent.zip[+http://coastwatch.pfeg.noaa.gov/erddap/download/erddapContent.zip+]

and should be downloaded and installed first.

Installing the Configuration Files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once the +erddapContent.zip+
file has been downloaded, you move it from where you downloaded it to the
Tomcat base directory, e.g.

------
mv erddapContent.zip /opt/tomcat
------

and then unzip it.  This will unpack the files therein and create and
populate the directory:

------
/opt/tomcat/content/erddap
------

You must make some changes to the +setup.xml+ configuration file before you
can attempt to install the +erddap.war+ file.

Startup Configuration of +setup.xml+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A +bigParentDirectory+ must be configured wherein various logging,
cache and temporary files will be stored.  In our case we create a
+/raid/erddap+ directory and set it as:

-----
<bigParentDirectory>/raid/erddap/</bigParentDirectory>
-----

Email addresses where daily and other reports can be sent must be
set in the lines:

------
<emailEverythingTo>your.email@yourInstitution.edu</emailEverythingTo>
<emailDailyReportsTo>your.boss@yourInstitution.edu</emailDailyReportsTo>
------

A base URL must be set that is the start of the public URL for
your Tomcat server.  This is set in the line:

-----
<baseUrl>http://127.0.0.1:8080</baseUrl>
-----

and should be changed to something like:

-----
<baseUrl>http://barataria.tamu.edu</baseUrl>
-----

These are the changes needed to initially start ERDDAP.

Installing ERDDAP
^^^^^^^^^^^^^^^^^

The ERDDAP Java web archive file or *war* file is installed in the +webapps+
subdirectory
of the main Tomcat installation which, in our example, is:

---------------------------------
/opt/tomcat/webapps
---------------------------------

The installation is performed simply by copying or moving the
+thredds.war+ into that directory,
after which a running Tomcat server will automatically unpack and start the
programs
therein.

You can tell that the process has at least started if you can see
that
an +erddap+ subdirectory has been created under
+/opt/tomcat/webapps+, i.e.

------------------------------
/opt/tomcat/webapps/erddap
------------------------------

If nothing happens, first check to see if you have successfully started you
Tomcat server, and then check the logs.

Checking for a Running ERDDAP
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the installation has proceeded correctly, you can point your browser at:

http://localhost:8080/thredds/[+http://localhost:8080/thredds/+]

and, if you have been successful, the dynamically generated default ERDDAP home
page appears something like this:

.ERDDAP Default Welcome Page
image::erddap/erddap_home.png[height=700]

You can also check on the page listing the default datasets by clicking
on *View a List of Datasets* in the upper right of the home page to obtain:

.ERDDAP Default Data Page
image::erddap/erddap_data_default.png[width=1000]

[[setup]]
[[Configuring_the_Server]]
Configuring the Server
----------------------

Overview
~~~~~~~~

You will be creating or editing configuration files for both
your server and your datasets.  First,
you will be configuring the server itself.  There is an XML file that
contains information about the server itself, e.g. its name, who owns
and runs it, what organization is involved, etc.  It also allows you
to configure how your HTML pages will appear to users, how large your
caches will be, whether or not various services will be enabled, etc.
Basically, you establish the way your server will look and perform
via configuring this file.
This is something you do when you first install the TDS, and then
perhaps tweak occasionally as your situation changes and you might
need to offer more services, establish larger caches, etc.

The second type of configuration is done within data catalogs.
That is where you configure how your datasets are served to
your users.  This is significantly more complicated and involved
than configuring the server itself, and will be more or less an
ongoing process as you create or acquire more datasets and gradually
learn the full capabilities of the TDS and apply them to your
datasets.

Both the server and data configuration files are configured
using a human- and machine-readable file written in a dialect
of XML.  Since XML is a fairly abstract concept, we'll now take
a brief look at what it is. 

XML
^^^

The eXtensible Markup Language or XML is a markup language that defines a set
of rules
for encoding documents in a format that is both human- and machine-readable.
An XML
document is immediately recognizable by the number of angle brackets contained
therein, and
sort of resembles HTML markup although the two markup languages have different
purposes.
A good way of looking at this is that HTML is for form and XML is for content.

XML documents consist mainly of *elements* and
*attributes*.
An element is a component
that begins with a start-tag and ends with a matching end-tag, with an example
being:

---------------------------
<serviceName>odap</serviceName>
---------------------------

where +serviceName+ is the element name.
An attribute is a name/value pair within a start-tag, and example being:

---------------------------
<dataset name="TDS Tutorial"/>
---------------------------

where within the element +dataset+ the attribute name is +name+
and the value is +TDS Tutorial+.

An XML schema or grammar is a description of a specific type of XML document,
usually
expressed in terms of constraints on the structure and content of documents of
that
type.  Typically a schema constrains the set of elements that may be used in a
document,
which attributes
can be applied to them, the order in which they may appear,
and
the allowable parent/child relationships.  Basically, a schema allows you to
specify
what you really need in a document while excluding extraneous material.  The

ERDDAP Configuration Directory
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All the configuration files are located in a single directory
in the standard, default ERDDAP distribution.
In our installation example, this  directory is located at:

---------------------------------
/opt/tomcat/content/erddap
---------------------------------

wherein all the configuration files are located.  They are located in a
directory separate from the +webapp+ directory into which the software is
installed to separate the software from the configuration files and thus
allow the former to be upgraded without disturbing the latter.  This
subdirectory should look something like this:

-------------------------------------------------------
-rw-r--r-- 1 root root  49629 May  2 16:00 datasets.xml
drwxr-xr-x 2 root root   4096 Jul 24 17:49 images
-rw-r--r-- 1 root root 169596 May  2 16:00 messages.xml
-rw-r--r-- 1 root root  33829 Jul 24 18:09 setup.xml
-------------------------------------------------------

The initial and basic configuration is performed within the +setup.xml+
file.  The content configuration is performed
within the +datasets.xml+ file.

Mandatory Server Configuration Changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ERDDAP server configuration file is at:

+/opt/tomcat/content/erddap/setup.xml+

and allows the ERDDAP administrator to set parameters that control the general
behavior of ERDDAP.  Most are set to reasonable parameters, although there
is a group of parameters that [red]*must be changed* for your specific
server.

[[bigParentDirectory]]
+bigParentDirectory+
^^^^^^^^^^^^^^^^^^^^

The +bigParentDirectory+ is the absolute path on the serve (with a trailing
slash) to a directory with a large amount of space.  This should be a newly
created directory *outside* of the Tomcat directory, for instance somewhere
on your huge data disc array.
The user that runs Tomcat must have read and write privileges for this
directory.  ERDDAP will create several subdirectories under this:

* +datasetInfo+ - which is used by some datasets to cache information
about the dataset to speed up reloading;
* +flag+ - wherein you can place a file with the name of a
xref:datasetID[+datasetID+] to force the reloading of that dataset;
* +cache+ - which is used to hold cached data files (and wherein
files are removed that are older than +cacheMinutes+).

The ERDDAP +log.txt+ file will also be placed in this directory. An
example is:
[source,xml]
-----
<bigParentDirectory>/data/erddap/</bigParentDirectory>
-----

[[emailEverythingTo]]
[[emailDailyReportsTo]]
+emailEverythingTo+ and +emailDailyReportsTo+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Status and change reports are emailed to the email addresses
contained within +emailEverythingTo+ and +emailDailyReportsTo+.
Either of these can be not specified, specified but with no value (i.e.
blank), and specified with either a single email address or
multiple, comma-separated email addresses.

Daily status reports are emailed to the email address contained
within +emailDailyReportsTo+.
The first +emailEverythingTo+ address is more important than
the others if multiple addresses are specified.
It is used for subscriptions to +EDDXxxxFromErddap+
datasets.  
An example of their usage is:
[source,xml]
-----
<emailEverythingTo>your.email@yourInstitution.edu</emailEverythingTo>
<emailDailyReportsTo>your.boss@yourInstitution.edu</emailDailyReportsTo>
-----
Even if you do not set up the email system, all potential
email messages are logged to an +emailLogYEAR-MONTH.txt+ file
in xref:bigParentDirectory[+bigParentDirectory+].

[[baseUrl]]
+baseUrl+
^^^^^^^^^

The +baseUrl+ sets the start of the public URL to which +/[warName]+
is appended.  This must be changed when you install ERDDAP.  For
example, if you're running and testing ERDDAP on your own computer
you might specify this as:
[source,xml]
-----
<baseUrl>http://127.0.0.1:8080</baseUrl>
-----
and if you're finished testing and want to go into production mode
you might specify it as:
[source,xml]
-----
<baseUrl>http://erddap.university.edu:8080</baseUrl>
-----

Email Account Information
^^^^^^^^^^^^^^^^^^^^^^^^^

The email account information is used for sending emails to the
xref:emailEverythingTo[+emailEverythingTo+] and
xref:emailDailyReportsTo[+emailDailyReportsTo+] addresses.  If either
has been specified, this information must be changed.
If you do not wish to send emails, then specify the
+emailSmtpHost+ tag to be blank.

The +emailPassword+ tag is optional, although if it is absent
emails cannot be sent to non-local addresses.

The +emailProperties+ tag is a list of additional properties of the form:
-----
prop1|value1|prop2|value2
-----
with an example for the requirements of GMail accounts being:
-----
mail.smtp.starttls.enable|true
-----
(with the +emailSmtpPort+ needed for GMail being +587+).
The default is a blank.

An example of the email account information tags is:
[source,xml]
-----
<emailFromAddress>your.email@yourCompany.com</emailFromAddress>
<emailUserName>your.email</emailUserName>
<emailPassword>yourPassword</emailPassword>
<emailProperties></emailProperties>
<emailSmtpHost>your.smtp.host.edu</emailSmtpHost>
<emailSmtpPort>25</emailSmtpPort>
-----

Even if you do not set up the email system, all potential email
messages are logged to an +emailLogYEAR-MONTH.txt+ file in the
xref:bigParentDirectory[+bigParentDirectory+].

If you cannot get ERDDAP to send email, receive the error
message +Connection refused+, and are using McAfee anti-virus
software, then uncheck:
-----
Virus Scan Console : Access Protection Properties : Anti Virus Standard Protections : Prevent mass mailing worms from sending mail
-----

[[adminInstitution]]
[[adminIndividualName]]
[[adminPosition]]
[[adminPhone]]
[[adminCountry]]
[[adminEmail]]
ERDDAP Administrator Information
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Information about the ERDDAP administrator is used for the SOS and WMS
servers.  These tags [red]*must* be changed to describe your installation.
An example is:
[source,xml]
-----
<adminInstitution>Texas A&amp;M University, Dept. of Oceanography</adminInstitution>
<adminIndividualName>Steven Baum</adminIndividualName>
<adminPosition>ERDDAP administrator</adminPosition>
<adminPhone>979-867-5309</adminPhone>
<adminAddress>D. G. Eller Bldg.</adminAddress>
<adminCity>College Station</adminCity>
<adminStateOrProvince>TX</adminStateOrProvince>
<adminPostalCode>77843</adminPostalCode>
<adminCountry>USA</adminCountry>
<adminEmail>baum@stommel.tamu.edu</adminEmail>
-----

OGC Services Requirements
^^^^^^^^^^^^^^^^^^^^^^^^^
The +accessConstraint+, +accessRequiresAuthorization+, +fees+ and
+keywords+ tags are tags used for
default access constraints, fees and keywords that need to be supplied
for the ERDDAP SOS, WCS and WMS services.

* If a dataset does not have an xref:accessibleTo[+accessibleTo+] tag, then
+accessConstraints+ and +fees+ are the defaults.
* If a dataset has an xref:accessibleTo[+accessibleTo+] tag, then
+accessRequiresAuthorization+ is the default.

The comma-separted list of +keywords+ should describe the dataset in a general way.
The +accessConstraints+, +fees+ and +keywords+ tags can be overwritten
by attributes of the same name in a dataset's global attributes
within the +datasets.xml+ file.
Examples of their use are:
[source,xml]
-----
<accessConstraints>NONE</accessConstraints>
<accessRequiresAuthorization>only accessible to authorized users</accessRequiresAuthorization>
<fees>NONE</fees>
<keywords>earth science, atmosphere, ocean, biosphere, biology, environment</keywords>
-----

Legal Boilerplate
^^^^^^^^^^^^^^^^^

The following XML fragment appears on the +erddap/legal.html+ web page
after the general disclaimer.
[source,xml]
-----
<legal><![CDATA[
[standardDisclaimerOfEndorsement]
[standardDisclaimerOfExternalLinks]
[standardPrivacyPolicy]
[standardDataLicenses]
[standardContact]
]]></legal>
-----
Any of the +standard*+ parts can be replaced with local information.
The +standardContact+ tag refers to the
xref:adminEmail[+adminEmail+] address.

Optional Server Configuration Changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[logLevel]]
+logLevel+
^^^^^^^^^^

The +logLevel+ tag determines how many diagnostic messages will be sent
to the +log.txt+ file.  It can be set to:

* +warning+ - for the fewest messages;
* +info+ - the default value with more messages;
* +all+ - for the most messages.

An example is:
[source,xml]
-----
<logLevel>info</logLevel>
-----

Diagnostic messages are displayed on some HTML pages if
xref:displayDiagnosticInfo[+displayDiagnosticInfo+] is
set to +true+.  The diagnostic messages are written to
the +log.txt+ in the xref:bigParentDirectory[+bigParentDirectory+].
Leaving this at the default value of +info+ is a good idea
if you're not doing any active development on the software.

[[warName]]
+warName+
^^^^^^^^^

This should always be +erddap+.  If you desire to install a second
ERDDAP for testing and development, see:

http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html#secondErddap[http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html#secondErddap]

instead of changing +warName+ to +erddap2+ or something else.

[[datasetsRegex]]
+datasetsRegex+
^^^^^^^^^^^^^^^

The +datasetsRegex+ tag lets you specify a regular expression
that will determine which +datasetID+ tags in the +datasets.xml+
file should be loaded.  The default is +.*+ which loads all
available datasets.  It might be useful to set this to be more
restrictive during development.  For example, if you have a large
number of datasets you might want to limit the files being loaded
to a much smaller subset to speed up turnaround time on server reboots.
An example of a restrictive value is:
[source,xml]
-----
<datasetsRegex>etopo.*|erdQSu101day|erdMHchla8day|erdGlobecBottle|erdBAssta5day|pmelTaoDySst</datasetsRegex>
-----
as opposed to the default:
[source,xml]
-----
<datasetsRegex>.*</datasetsRegex>
-----

[[quickRestart]]
+quickRestart+
^^^^^^^^^^^^^^

The +quickRestart+ tag can be used to have ERDDAP reload information
from a local cache rather than attempt to gather it again from a remote
server.  This works with some types of datasets such as
xref:EDDGridFromDap[+EDDGridFromDap+] which can have cached information
in the form of +.dds+ and +.das+ files.  The age of the dataset
will be based on when it was last reloaded.
The should be left at the default value of +true+ unless you want
to bypass the cached information for some reason.
An example is:
[source,xml]
-----
<quickRestart>true</quickRestart>
-----

[[authentication]]
+authentication+
^^^^^^^^^^^^^^^^

The +authentication+ tag is used if you want to restrict access to
some datasets.  You specify the authentication method to be used
for logging in with this.
See the xref:user[+user+] section for details about the available
values, which are presently:

* nothing - the default where logins are not supported;
* +custom+
* +openid+

An example of the default is:
[source,xml]
-----
<authentication></authentication>
-----
Note that +openid+ logins will not work when testing with the
localhost (i.e. +https://127.0.0.1:8443+).

[[passwordEncoding]]
+passwordEncoding+
^^^^^^^^^^^^^^^^^^

The +passwordEncoding+ tag specifies how you have stored passwords
if you have set +authentication+ to allow logins.  See
xref:user[+user+] section for details about the available
methods.  The available values (in order of increasing security) are:

* +plaintext+
* +MD5+
* +UEPMD5+

The default is +EUPMD5+, and the other two options should only
be used if you need to match values stored that way in an
external password database.

An example showing the default +EUPMD5+ is:
[source,xml]
-----
<passwordEncoding>UEPMD5</passwordEncoding>
-----

[[listPrivateDatasets]]
+listPrivateDatasets+
^^^^^^^^^^^^^^^^^^^^^

The +listPrivateDatasets+ tag determines whether datasets to which
the user currently does not have access - because he isn't logged
in or because his xref:roles[+roles+] don't allow access - should be shown on lists
of datasets, e.g. from full text search, categories, view all datasets, etc.
The options are +true+ and +false+, with the latter the default value.
If this is +false+, no information about the dataset is shown to users
without access to it.  If this is true, some information about the dataset
(e.g. title, summary, etc.) is shown to users without access to it.
If a user clicks on a link to a dataset to which he does not have
access, he will receive and error message and be prompted to log in.
An example showing the default value is:
[source,xml]
-----
<listPrivateDatasets>false</listPrivateDatasets>
-----

[[baseHttpsUrl]]
+baseHttpsUrl+
^^^^^^^^^^^^^^

The +baseHttpsUrl+ tag is a varient of xref:baseUrl[+baseUrl+] that
is used when authentication is active and the user is logged in.
In general is is the +baseUrl+ with +http+ replaced with +https+
and the addition of +:8443+ (although if you create a proxy to
dispense with the +:8443+ then it is not needed here, either).
Examples of this are:
[source,xml]
-----
<baseHttpsUrl>https://127.0.0.1:8443</baseHttpsUrl>
<baseHttpsUrl>https://coastwatch.pfeg.noaa.gov:8443</baseHttpsUrl>
-----
with the first for running or testing on a personal computer,
and the second for a production server.

[[displayDiagnosticInfo]]
+displayDiagnosticInfo+
^^^^^^^^^^^^^^^^^^^^^^^

The +displayDiagnosticInfo+ tag is used to display diagnostic
information on almost all HTML pages generated by ERDDAP.  The information
displayed is the same as is written to the +[bigParentDirectory]logs/log.txt+
log file.  It is [red]*recommended* to always leave this false, e.g.
[source,xml]
-----
<displayDiagnosticInfo>false</displayDiagnosticInfo>
-----
which will cause diagnostic information to not be displayed.

[[searchEngine]]
+searchEngine+
^^^^^^^^^^^^^^

The +searchEngine+ tag allows you to choose between two search engines
for full text searches.  The choices are:

* +original+ - the best choice if you have fewer than about 10,000
datasets; and
* +lucene+ - the choice for more than 10,000 datasets.

The +lucene+ engine works fast and uses very little memory for a
very large number of datasets, although it has some quirks that
don't happen with the other search engine.  It's behavior, though,
is mostly identical to that of the +original+ search engine.
An example with the default value is:
[source,xml]
-----
<searchEngine>original</searchEngine>
-----

[[units_standard]]
+units_standard+
^^^^^^^^^^^^^^^^

The +units+ tag specifies the default units standard used
to specify units, which can
be either +UDUNITS+ (default) or +UCUM+.  The value is
case-sensitive.
This is used the the ERDDAP SOS server to determine if
the units need to be converted to UCUM units for WMS and
SOS +GetCapabilities+ responses.
An example of the default is:
[source,xml]
-----
<units_standard>UDUNITS</units_standard>
-----

[[fgdcActive]]
[[iso19115active]]
Metadata Generation
^^^^^^^^^^^^^^^^^^^

The +fgdcActive+ and +iso19115Active+ tags enable ERDDAP to
automatically generate FGDC and ISO19115 metadata for all
relvant datasets using information from the metadata of the
datasets.  These services are on - +true+ - by default, e.g.
[source,xml]
-----
<fgdcActive>true</fgdcActive>
<iso19115Active>true</iso19115Active>
-----

WMS Example Dataset
^^^^^^^^^^^^^^^^^^^

The +wmsSampleDataset+, +wmsSampleVariable+ and +wmsSampleBBox+
tags are used to establish a sample for the WMS server.
They are:

* +wmsSampleDataset+ - a grid dataset that has longitude and
latitude axes;
* +wmsSampleVariable+ - a variable within the sample grid
dataset; and
* +wmsSampleBBox+ - a comma-separated list of the bounding
box values +minx+, +miny+, +maxx+ and +maxy+ (with longitude
values within either -180 to 18 or 0 to 360 both valid).

Samples of these tags are:
[source,xml]
-----
<wmsSampleDatasetID>erdBAssta5day</wmsSampleDatasetID>
<wmsSampleVariable>sst</wmsSampleVariable>
<wmsSampleBBox>0,-75,360,75</wmsSampleBBox>
-----

[[flagKeyKey]]
+flagKeyKey+
^^^^^^^^^^^^

The +flagKeyKey+ tag is used by a service that lets remove users set
a flag to notify ERDDAP to reload a dataset.  These requests use a
key generated based on +baseUrl/warName+, a +datasetID+ and
+flagKeyKey+.
Change this value and supply users who might need to use it with
the +flagKeyKey+ value.
If this flag system is being abused, just change the value,
restart ERDDAP, and send the users the relevant new keys.
A list is available in the xref:daily_report[Daily Report].
An example of this is:
[source,xml]
-----
<flagKeyKey>We don't need no stinking badges.</flagKeyKey>
-----

[[subscriptionSystemActive]]
+subscriptionSystemActive+
^^^^^^^^^^^^^^^^^^^^^^^^^^

The +subscriptionSystemActive+ tag activates an email/URL
subscription system that sends a user an email or pings a
URL whenever a dataset of interest changes.
The systems relies on the server being able to send emails
to people to validate their subscription requests, and
the email come from the xref:emailFromAddress[+emailFromAddress+]
address.

If your server cannot send emails, do not activate this system.
The valid values are +true+ and +false+.
If you change this and restart ERDDAP, the list of subscriptions
in +[bigParentDirectory]/subscriptionsV1.txt+ is unaffected.
See also xref:subscriptionEmailBlacklist[+subscriptionEmailBlacklist+].
An example is:
[source,xml]
-----
<subscriptionSystemActive>true</subscriptionSystemActive>
-----

[[cacheMinutes]]
+cacheMinutes+
^^^^^^^^^^^^^^

The +cacheMinutes+ tag establishes the age of files to be
deleted every xref:loadDatasetsNMinutes[+loadDatasetsNMinutes+].
The appropriate files in +[bigParentDirectory]cache+ and
in the public directory which are more than
+cacheMinutes+ old will be deleted.
Note that when a dataset is reloaded, all files in the
appropriate +[bigParentDirectory]cache/[datasetID]+ directory
are deleted.

In general, only image files are cached since the same
images are often requested repeatedly.  Removing files in the
cache based on age - as opposed to least recently used - ensures
that files won't stay in the cache for too long.
Although it seems like a given request should always return
the same response, if new data arrives for a dataset the
response to a request identical to one made before the arrival
of the new data will change.
An example of this is:
[source,xml]
-----
<cacheMinutes>60</cacheMinutes>
-----

[[loadDatasetsMinMinutes]]
+loadDatasetsMinMinutes+
^^^^^^^^^^^^^^^^^^^^^^^^

The +loadDatasetsMinMinutes+ tag specifies the fewest minutes
between checks if the datasets need to be reloaded.
If a given run of loadDatasets takes less than this time, the
loader just looks at the flag directory and/or sleeps until
the remaining time has passed.
The default value is 15 minutes, which should work well for
almost everyone.
No matter what, each dataset will not be reloaded more often
than its xref:reloadEveryNMinutes[+reloadEveryNMinutes+] value
as specified in +datasets.xml+.

A disadvantage to setting this to a smaller number is that
it will increase the frequency that ERDDAP retries datasets that
aren't loading.  If there are many such datasets and they are
frequently retested, the data source might consider it
pestering or aggressive behavior and take some sort of
defensive action.
An example of the default value is:
[source,xml]
-----
<loadDatasetsMinMinutes>15</loadDatasetsMinMinutes>
-----

[[loadDatasetsMaxMinutes]]
+loadDatasetsMaxMinutes+
^^^^^^^^^^^^^^^^^^^^^^^^

The +loadDatasetsMaxMinutes+ tag specifies the most minutes that
reloading datasets is allowed to take before the thread is considered
stalled and interrupted.
In general, this should be set to at least twice as long as you
reasonably think that reloading all the datasets cumulatively should
take, since computers and networks can occasionally be much slower
than expected.
This should always be much longer than
xref:loadDatasetsMinMinutes[+loadDatasetsMinMinutes+].
The default is 60 minutes, although some will want to set it to
a longer value.  An example of the default is:
[source,xml]
-----
<loadDatasetsMaxMinutes>60</loadDatasetsMaxMinutes>
-----

[[unusualActivity]]
+unusualActivity+
^^^^^^^^^^^^^^^^^

The +unusualActivity+ sets an upper limit to the number
of requests between two runs of loading the datasets.  If this
number is exceeded, an email is sent to
xref:emailEverythingTo[+emailEverythingTo+].
The default is 10000, and an example with this default is:
[source,xml]
-----
<unusualActivity>10000</unusualActivity>
-----

Map Legends
^^^^^^^^^^^

The +legendTitle1+ and +legendTitle2+ tags are used to
specify titles for the legend of the maps.  Use them or
not at your discretion.  Examples are:
[source,xml]
-----
<legendTitle1>NOAA ERD's</legendTitle1>
<legendTitle2>ERDDAP</legendTitle2>
-----

Image Files for Graph and Map Legends
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +highResLogoImageFile+ and +lowResLogoImageFile+
tags specify image files to be used for map and graph
legends.  The high resolution images are 40 or 80 pixels
square, and the low resolution images 20 pixels square.
The +googleEarthLogoFile+ tag contains an image that
is displayed in the lower left corner of the Google
Earth map if the user selects a +.kml+ file type.
All of these images are also used by OpenSearch.
The +questionMarkImage+ tag contains an image that
identifies the places on HTML pages where a user can
mouse-over to get more information.

The files [red]*must* be located in the
-----
[tomcat]/content/erddap/images/
-----
directory, and will be copied to:
-----
[tomcat]/webapps/erddap/images
-----
and made available for downloading by any client.
Any text files in this directory must be stored
as plain ASCII (7 bit) or with UTF-8 encoding.
At present, the image files must be of PNG, GIF, JPG
or BMP format.  If you are going to substitute other
image files for these, it is best to make them a similar
number of pixels wide and high.
An example of the default values is:
[source,xml]
-----
<highResLogoImageFile>noaa_simple.gif</highResLogoImageFile>
<lowResLogoImageFile>noaa20.gif</lowResLogoImageFile>
<googleEarthLogoFile>nlogo.gif</googleEarthLogoFile>
<questionMarkImageFile>QuestionMark.jpg</questionMarkImageFile>
-----

[[fontFamily]]
+fontFamily+
^^^^^^^^^^^^

The +fontFamily+ tag specifies the font family to be used for
all of the text in images.  The default is +Bitstream Vera Sans+,
a sans serif font that is a commonly used for such purposes.

On Windows machines, this translates to Arial.  The +SansSerif+
font that is available in most Linux distributions is less than
desirable, so the open source +Bitstream Vera Sans+ font
is used instead.  This can be downloaded from:

http://www.gnome.org/fonts/[+http://www.gnome.org/fonts/+]

or, more straightforwardly, from:

http://coastwatch.pfeg.noaa.gov/erddap/download/BitstreamVeraSans.zip[+http://coastwatch.pfeg.noaa.gov/erddap/download/BitstreamVeraSans.zip+]

after which you unzip them and place them into:
-----
[javaHome]/jre/lib/fonts
-----
so your Java distribution can find them.
An example with this default is:
[source,xml]
-----
<fontFamily>Bitstream Vera Sans</fontFamily>
-----

Chunk Sizes
^^^^^^^^^^^

If possible, ERDDAP breaks source data requests into chunks to
conserve memory.
With 32-bit Java - in a simplistic sense - the maximum number
of simultaneous large requests is roughly 3/4 of the
available memory - the +-Xmx+ value passed to Tomcat
on startup - divided by the chunk size, e.g. 1200 MB/100 MB = 12 requests.
But since other processes require memory, the actual number of
requests will be less.
And sometimes, in practice, chunking isn't possible at all.
On the 32-bit platform one huge or a few very large simultaneous
non-chunkable requests could cause problems.
This is much less of a problem on 64-bit platforms since the
+-Xmx+ passed to Tomcat can be much larger.

The +partialRequestMaxBytes+ tag contains the preferred maximum
number of bytes for a partial grid data request, i.e. a chunk
of the total request.  The recommended value is +100000000+,
and values can be specified up to 500 MB, which is the THREDDS
limit for DAP responses.
A larger size than the default may require fewer accesses 
of many files - e.g. ERD satellite data wherein each time point
is in a separate file - and it is better to get more data from
each file in each partial request.

The +partialRequestMaxCells+ tag contains the preferred maximum
number of cells - i.e. the product of rows and columns in the
data table - for a partial table data request.  The default
is +100000+, and larger sizes than this will result in a longer
wait for the initial batch of data from the source.

An example showing the default sizes of these chunks for
grids (+Bytes+) and tables (+Cells+) is:
[source,xml]
-----
<partialRequestMaxBytes>100000000</partialRequestMaxBytes>
<partialRequestMaxCells>100000</partialRequestMaxCells>
-----

[[variablesMustHaveIoosCategory]]
+variablesMustHaveIoosCategory+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +variablesMustHaveIoosCategory+ tag is used to specify
whether all variables for all datasets must have an
+ioos_category+ attribute defined in
+sourceAttributes+ or
+addAttributes+, with the value obtained from:

http://code.google.com/p/ioostech/wiki/ControlledVocabularies[+http://code.google.com/p/ioostech/wiki/ControlledVocabularies+]

If you are affiliated with NOAA, this should be set to
+true+ because the NOAA IOOS office wants to be able to
categorize variables and datasets in this way.
For non-NOAA institutions, there is no downside to
setting this to +false+.  An example for a NOAA affiliate is:
[source,xml]
-----
<variablesMustHaveIoosCategory>true</variablesMustHaveIoosCategory>
-----

[[categoryAttributes]]
+categoryAttributes+
^^^^^^^^^^^^^^^^^^^^

The +categoryAttributes+ tag contains a comma-separated list - preferably in
alphabetical order - of the global attribute and variable attribute names
which will be used to categorize the datasets and shown to clients at URLs
like:
-----
.../erddap/categorize/ioos_category/index.html
-----
If an attribute is global, it should be identified by prefixing it with
+global:+.  The value +variableName+ is special case as it categorizes
the +dataVariable+ +destinationNames+.
An example is:
[source,xml]
-----
<categoryAttributes>global:cdm_data_type, global:institution, ioos_category, global:keywords, long_name, standard_name, variableName</categoryAttributes>
-----

Griddap Examples
^^^^^^^^^^^^^^^^

This is a group of tags providing settings used to make examples for the
GRIDDAP documentation that appears at:
-----
[baseUrl]/erddap/griddap/documentation.html
-----
and elsewhere.  If you include the +erdBAssta5day+ dataset
in +datasets.xml+ - which is highly recommended - then you
won't need to change these.  This +dataset+ entry is:
-----
<dataset type="EDDGridFromErddap" datasetID="erdBAssta5day" active="true">
    <sourceUrl>http://coastwatch.pfeg.noaa.gov/erddap/griddap/erdBAssta5day</sourceUrl>
</dataset>
-----
If you don't include this dataset, then you [red]*must*
change these settings before you bring your ERDDAP on line
or none of the examples will work.
Any new settings should be very similar to the defaults shown
below.
If your ERDDAP will not serve any tabular datasets, then use
+NOT_APPLICABLE+ for all of the tags.
Recall that in XML files the ampersand, less than and greater than
characters have to be HTML encoded as +&amp;+, +&lt;+ and +&gt;+,
respectively.
The defaults are:
[source,xml]
-----
<!-- Griddap Examples
This group of settings is used to make examples for the griddap documentation
that appears at [baseUrl]/erddap/griddap/documentation.html and elsewhere.
If you include the erdBAssta5day dataset in your datasets.xml (recommended),
you don't need to change these.
If you don't, you MUST change these before you make your ERDDAP public;
otherwise, none of the examples will work!
The new settings should be very similar to the defaults.
If your ERDDAP won't serve any tabular datasets, use "NOT_APPLICABLE" for all
of the enties.
In .xml files like this, ampersand, lessThan, and greaterThan have to be
HTML encoded as "&amp;", "&lt;", "&gt;".
-->
<!-- This is the datasetID for an EDDGrid dataset that is served by your ERDDAP.
     This dataset is used as the basis for all of the EDDGrid examples below.
     Ideally, it is a dataset that (at least) has time, latitude, and longitude dimensions.
     ('time' allows for making a time series graph. 'latitude' and 'longitude' allow for making a map.)
     The longitude values used in the examples below must not span 180 (to work with .esriAscii).
     (It's okay if the dataset is 0 to 360 - just use the 0 to 180 (or 180 to 360) portion.) -->
<EDDGridIdExample>erdBAssta5day</EDDGridIdExample>
<!-- This is a complete query string for getting some axis data from the example grid dataset. -->
<EDDGridDimensionExample>latitude[0:10:100]</EDDGridDimensionExample>
<!-- This is the name of a data variable in the example grid dataset. -->
<EDDGridNoHyperExample>sst</EDDGridNoHyperExample>
<!-- This is used to name/explain/describe all of the dimensions in the example grid dataset. -->
<EDDGridDimNamesExample>[time][altitude][latitude][longitude]</EDDGridDimNamesExample>
<!-- This is an example data query using ERDDAP's () notation and ISO-formatted time.
     t is nice (not required) that this subset be suitable for making a map (see EDDGridMapExample below).
     The longitude values used must not span 180 (to work with .esriAscii).
     The use of ":100:" avoids sending the user a ton of data.
     You could generate your example via your dataset's Data Access Form in ERDDAP.  -->
<EDDGridDataTimeExample>sst[(2007-10-21T00:00:00)][0][(-75):100:(75)][(180):100:(360)]</EDDGridDataTimeExample>
<!-- This is an equivalent example data query, but which specifies time as seconds-since-1970-01-01.
     If you need to convert a date/time to seconds-since-1970-01-01, email bob dot simons at noaa dot gov. -->
<EDDGridDataValueExample>sst[(1192924800)][0][(-75):100:(75)][(180):100:(360)]</EDDGridDataValueExample>
<!-- This is an equivalent example data query, but which uses the traditional opendap indices-style notation only. -->
<EDDGridDataIndexExample>sst[656][0][0:100:1500][1800:100:3600]</EDDGridDataIndexExample>
<!-- This is an example query which generates a graph.
     You could generate your example via your dataset's Make A Graph form in ERDDAP.  -->
<EDDGridGraphExample>sst[(2007-07-01):(2007-10-21)][0][(29)][(225)]&amp;.draw=linesAndMarkers&amp;.vars=time|sst|&amp;.marker=1|3&amp;.color=0xFF9900&amp;.colorBar=|C|Linear|||</EDDGridGraphExample>
<!-- This is an example query which generates a map.
     The "map" example must have longitude which doesn't span 180 for .esriAscii.
     The use of ":50:" avoids sending the user a ton of data.
     ":50:" is irrelevant when requesting a .png, but relevant if the user gets the actual data (e.g., .csv). -->
<EDDGridMapExample>sst[(2007-10-21)][0][(-75):50:(75)][(180):50:(360)]&amp;.draw=surface&amp;.vars=longitude|latitude|sst&amp;.colorBar=Rainbow|C|Linear|0|32|</EDDGridMapExample>
<!-- This is a Matlab example which uses data from the EDDGridMapExample.
     Hopefully, you can just substitute your datasetID for "erdBAssta5day"
     and modify "[0 32]" (the data range for the color bar).  -->
<EDDGridMatlabPlotExample>imagesc(erdBAssta5day.longitude, erdBAssta5day.latitude, squeeze(erdBAssta5day.sst), [0 32])
set(gca, 'YDir', 'normal')</EDDGridMatlabPlotExample>
-----

Tabledap Examples
^^^^^^^^^^^^^^^^^

This is a group of tags providing settings used to make examples for
the tabledap documentation that appears at:
-----
[baseUrl]/erddap/tabledap/documentation.html
-----
and elsewhere.  If you included the +erdGlobecBottle+ dataset
in +datasets.xml+ - which is [red]*recommended* - you will not
need to change any of these.
If you do not include it, then you [red]*must* change these before
you bring your ERDDAP publicly online or the examples will not
work.
Any new settings should be very similar to the defaults shown below.
If your ERDDAP will not serve any tabular datasets, use
+NOT_APPLICABLE+ as the value for all the tags.  In .xml files like this,
ampersand, lessThan, and greaterThan have to be
HTML encoded as "&amp;", "&lt;", "&gt;".
The default values are:
[source,xml]
-----
<!-- This is the datasetID for an EDDTable dataset that is served by your ERDDAP.
     This dataset is used as the basis for all of the EDDGrid examples below.
     Ideally, it is a dataset that has longitude, latitude, and time variables
     Ideally, it is a dataset that has longitude, latitude, and time variables (among others).
     ('time' allows for making a time series graph. 'latitude' and 'longitude' allow for making a map.)
     The dataset can have longitude values -180 to 180, or 0 to 360. -->
<EDDTableIdExample>erdGlobecBottle</EDDTableIdExample>
<!-- This is a comma-separated list of variables from the dataset.
     It is useful if it is "longitude,latitude,time," plus a data variable name. -->
<EDDTableVariablesExample>longitude,latitude,time,bottle_posn,temperature1</EDDTableVariablesExample>
<!-- This is the constraints example which is appended to EDDTableVariablesExample. -->
<EDDTableConstraintsExample>&amp;time&gt;=2002-08-17T00:00:00Z&amp;time&lt;=2002-08-19T20:18:00Z</EDDTableConstraintsExample>
<!-- This is an example data query using an ISO-formatted time.
     You could generate your example via your dataset's Data Access Form in ERDDAP.  -->
<EDDTableDataTimeExample>longitude,latitude,time,bottle_posn,temperature1&amp;time&gt;=2002-08-17T00:00:00Z&amp;time&lt;=2002-08-19T20:18:00Z</EDDTableDataTimeExample>
<!-- This is an equivalent example data query, but which specifies time as seconds-since-1970-01-01.
     If you need to convert a date/time to "seconds since 1970-01-01", use
     http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html -->
<EDDTableDataValueExample>longitude,latitude,time,bottle_posn,temperature1&amp;time&gt;=1029542400&amp;time&lt;=1029788280</EDDTableDataValueExample>
<!-- This is an example query which generates a graph.
     You could generate your example via your dataset's Make A Graph form in
     You could generate your example via your dataset's Make A Graph form in ERDDAP.  -->
<EDDTableGraphExample>bottle_posn,temperature1&amp;time=2002-08-19T10:06:00Z&amp;.draw=lines</EDDTableGraphExample>
<!-- This is an example query which generates a map.
     In the default mapExample, temperature1, time, bottle_posn are useful
     because they appear in GoogleEarth with the .kml example
     and are ignored by the other image file types. -->
<EDDTableMapExample>longitude,latitude,temperature1,time,bottle_posn&amp;time&gt;=2002-08-13T00:00:00Z&amp;time&lt;=2002-08-20T00:00:00Z&amp;bottle_posn=1&amp;.draw=markers&amp;.marker=5|5</EDDTableMapExample>
<!-- This is a Matlab example which uses data from the EDDTableGraphExample.
     Note the Matlab notation datasetName.variableName.  -->
<EDDTableMatlabPlotExample>plot(erdGlobecBottle.bottle_posn,erdGlobecBottle.temperature1)</EDDTableMatlabPlotExample>
-----

[[drawLandMask1]]
+drawLandMask+
^^^^^^^^^^^^^^

The +drawLandMask+ tag value specifies the default +Make a Graph+
page setting for whether the landmask should be drawn +over+ or
+under+ surface data on maps.  It is [green]*recommended* that
this be set to +over+ for primarily oceanography data so grid data
over land is obscured by the landmask, and
to +under+ for all other data
An example for oceanographic data is:
[source,xml]
-----
<drawLandMask>over</drawLandMask>
-----

[[startHeadHtml]]
+startHeadHtml+
^^^^^^^^^^^^^^^

The +startHeadHtml+ tag contains the values for the start of
the HTML document and the head tags - starting at +<!DOCTYPE>+ but
not including +</head>+ for all HTML pages.
This value may include +&erddapUrl;+, which is expanded to be
-----
[baseUrl]/erddap
-----
or
-----
[baseUttpsUrl]/erddap
-----
if the user is logged in.  If your ERDDAP allows users to
log in, all referenced image files, CSS files, etc. 
[red]*must* be in:
-----
[tomcat]/content/erddap/images
-----
or a subdirectory therein, and must be referenced here with:
-----
&erddapUrl;/images/[fileName]
-----
The +favicon.ico+ icon is the image that browsers associate
with your website, and you can supply your own by putting it in:
-----
(tomcat)/content/erddap/images
-----
See

http://en.wikipedia.org/wiki/Favicon[http://en.wikipedia.org/wiki/Favicon]

for further details about this.  You can also change the appearance
of all of your ERDDAP HTML pages by change the CSS style settings.
An example of a significantly different style can be seen by changing
the import reference below to:
-----
[tomcat]/content/erddap/images/erddapAlt.css
-----

If your CSS style file includes links to files (e.g. images), that
information must be inline in the style tag below, after the
+import+ line, and not in the +.css+ file.
Put all the image files in the
-----
[tomcat]/content/erddap/images
-----
directory - or a subdirectory - and reference them starting
with +&erddapUrl;+.
This is required because on ERDDAP +https:+ web pages, all links
should use +https:+ rather than +http:+ or browsers will not
consider the web page fully secure.
Since ERDDAP uses the same +.css+ file for +https:+ and +http:+
web pages, the links within the +.css+ file would not switch
between +https:+ and +http:+.  There is no way around this problem
other than using inline style information.

The default values are:
[source,xml]
-----
<startHeadHtml><![CDATA[
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
  "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>ERDDAP</title>
<link rel="shortcut icon" href="&erddapUrl;/images/favicon.ico">
<link href="&erddapUrl;/images/erddap.css" rel="stylesheet" type="text/css">
</style>
]]></startHeadHtml>
-----

[[startBodyHtml]]
+startBodyHtml+
^^^^^^^^^^^^^^^

The +startBodyHtml+ tag contains the start of the body for all HTML
web pages, starting with the +body+ tag.
This may include +&erddapUrl;+ which is expanded to be
+[baseUrl]/erddap+ (or +[baseUttpsUrl]/erddap+ if the user
is logged in).
If your ERDDAP allows users to log in, all referenced image files, etc.
[red]*must* be in
-----
[tomcat]/content/erddap/images
-----
and a subdirectory therein, and must be referenced in this tag as:
-----
&erddapUrl;/images/[fileName]
-----
If you want to use +https:+ and have users log in, see the instructions
in the xref:Security_Authorization:[+Security Authorization+] section,
and include +&loginInfo;+ in +startBodyHtml+ to include the user's
login status and a link to log in/out.
If +authentication+ is set to +""+, then +&loginInfo;+ disappears.
The default for this is:
[source,xml]
-----
<startBodyHtml><![CDATA[
<body>
<table bgcolor="#128CB5" border="0" cellpadding="2" cellspacing="0" width="100%">
  <tr>
    <td align="center" style="width:100px;"> &nbsp;
      <img src="&erddapUrl;/images/noaab.png" alt="NOAA" align="middle">&nbsp;
      </td>
    <td align="left">
      <font color="#FFFFFF">
      <font size="+2"><b>ERDDAP</b></font>
      <br>Easier access to scientific data</font>
      </td>
    <td align="right"><font size="-1">
      &loginInfo; &nbsp; &nbsp;
      <br>Brought to you by
      <a title="National Oceanic and Atmospheric Administration" rel="bookmark" href="http://www.noaa.gov">NOAA</a>
      <a title="National Marine Fisheries Service" rel="bookmark" href="http://www.nmfs.noaa.gov">NMFS</a>
      <a title="Southwest Fisheries Science Center" rel="bookmark" href="http://swfsc.noaa.gov">SWFSC</a>
      <a title="Environmental Research Division" rel="bookmark" href="http://swfsc.noaa.gov/textblock.aspx?Division=ERD&amp;id=1315&amp;ParentMenuId=200">ERD</a>
      &nbsp; &nbsp;
      </font></td>
  </tr>
</table>
]]></startBodyHtml>
-----

[[endBodyHtml]]
+endBodyHtml+
^^^^^^^^^^^^^

The +endBodyHtml+ tag contains the end of the body of the HTML
code for all HTML web pages (with +</body>+ at the end).
This may include +&erddapUrl;+ which will be expanded to be
+[baseUrl]/erddap+ (or +[baseUttpsUrl]/erddap+ if the user is
logged in).
If your ERDDAP allows users to log in, all referenced image files, etc.
[red]*must* be in
-----
[tomcat]/content/erddap/images
-----
and a subdirectory therein, and must be referenced in this tag as:
-----
&erddapUrl;/images/[fileName]
-----
The email address should be changed, but keep
+ERDDAP, Version &erddapVersion;+ and the references
to disclaimers and the privacy policy.  Other things
besides these can be changed.
The default is:
[source,xml]
-----
<endBodyHtml><![CDATA[
<br>&nbsp;
<hr>
ERDDAP, Version &erddapVersion;
<br><a rel="copyright" href="&erddapUrl;/legal.html">Disclaimers</a> |
    <a rel="bookmark" href="&erddapUrl;/legal.html#privacyPolicy">Privacy Policy</a> |
    <a rel="bookmark" href="&erddapUrl;/legal.html#contact">Contact</a>
</body>
]]></endBodyHtml>
-----

[[theShortDescription]]
+theShortDescription+
^^^^^^^^^^^^^^^^^^^^

The +theLongDescription+ tag contains a short desciption of ERDDAP that
is used in the middle of the left side of the home page.
This can refer to +&erddapUrl;+, +&requestFormatExamplesHtml;+,
+&resultsFormatExamplesHtml;+ and +[standardShortDescriptionHtml]+ (see
+messages.xml+.
If your ERDDAP allows users to log in, all referenced image files, etc.
[red]*must* be in
-----
[tomcat]/content/erddap/images
-----
and a subdirectory therein, and must be referenced in this tag as:
-----
&erddapUrl;/images/[fileName]
-----

If you do not use +[standardShortDescriptionHtml]+, then include a
link like:
-----
<a href="&erddapUrl;/information.html">More detailed information about ERDDAP</a>
-----
All of the information in +[standardShortDescriptionHtml]+ is also
contained in +<theLongDescriptionHtml>+ in +messages.html+.
The default values are:
[source,xml]
-----
<theShortDescriptionHtml><![CDATA[
<h1>ERDDAP</h1>
ERDDAP (the Environmental Research Division's Data Access Program)
is a data server that gives you a simple, consistent way to download
subsets of scientific datasets in common file formats and make graphs and
maps.
This particular ERDDAP installation has oceanographic data
(for example, data from satellites and buoys).

[standardShortDescriptionHtml]

]]></theShortDescriptionHtml>
-----

Dataset Reloading
~~~~~~~~~~~~~~~~~

In this section we take a detailed look at how the various dataset reloading mechanisms
work and interact.

An ERDDAP thread called +RunLoadDatasets+ is the master thread controlling
when datasets are reloaded.  This thread loops forever, and at the
start of each loop it notes the current time and
starts a +LoadDatasets+ thread to do a +majorLoad+.
Information about the current or most recent +majorLoad+ can be
found at the top of the ERDDAP status page at:

+http://www.yourSite.org/erddap/status.html+

with an example status page available for viewing
http://coastwatch.pfeg.noaa.gov/erddap/status.html[here].
The actions of +LoadDatasets+ are:

* to make a copy of +datasets.xml+;
* to read through the copy of +datasets.xml+ to check each
dataset to see if it needs to be loaded, reloaded or removed as
per the following constraints:
** if a xref:flag[+flag+] file exists for a dataset, the file is
deleted and the dataset removed if +active="false"+, or
(re)loaded if +active="true"+ (regardless of the age of the
dataset);
** if the XML code portion in +datasets.xml+ for a dataset contains
+active="false"+ and dataset is currently loaded (active), it is unloaded
(removed);
** if the dataset has +active="true"+ and the dataset is not already
laoded, it is loaded;
** if the dataset has +active="true"+ and the data is already loaded,
the dataset is reloaded if its age - i.e. the time since the last
(re)load - is greater than the value within the
xref:reloadEveryNMinutes[+reloadEveryNMInutes+] tag, otherwise it
is left alone.
* to finish and return control to +RunLoadDatasets+.

The +RunLoadDatasets+ thread waits for the +LoadDatasets+ thread to 
finish.  If this takes longer than the value of
xref:loadDatasetsMinMinutes[+loadDatasetsMinMinutes+],
+RunLoadDatasets+ interrupts the +LoadDatasets+ thread.
If +LoadDatasets+ notices the interrupt it will finish, but
if it does not notice it within a minute +RunLoadDatasets+
will call +loadDatasets.stop()+, which is a much less
desirable outcome.

During the interval when the time since the start of the last
+majorLoad+ is less than the value specified via the
xref:loadDatasetsMinMinutes:[+loadDatasetsMinMinutes+] tag,
+RunLoadDatasets+ repeatedly looks for xref:flag[+flag+]
files in the

+xref:bigParentDirectory:[bigParentDirectory]/flag+

directory.  If one or more flag files are found, they
are deleted, and +runLoadDatasets+ will start a
+LoadDatasets+ thread to do a +minorLoad+ to reload
just the flagged datasets.  The actions of
+LoadDatasets+ under +minorLoad+ status are:

* to make a copy of +datasets.xml+;
* to read through the copy of +datasets.xml+ and, for
each dataset for which there is a flag file:
** if the dataset's XML code has +active="false"+ and
the dataset is currently loaded (active), is is unloaded
(removed);
** if the dataset has +active="true"+, the dataset is
(re)loaded, regardless of its age;
** if a dataset is not flagged, it is ignored.
* to finish and return control to +RunLoadDatasets+.

After this has been acomplished, +RunLoadDatasets+ starts 
another loop.

Additional notes about the reloading processes include:

* Every dataset with +active="true"+ is loaded when you
restart ERDDAP.
* When a dataset is (re)loaded, its cache - including any data
response files and/or image files - is emptied.
* If there are many datasets and/or one or more datasets are
slow to (re)load, a +LoadDatasets+ thread may take a long time
to finish, perhaps even longer than the time specified in
xref:loadDatasetsMinMinutes[+loadDatasetsMinMinutes+].
* There will never be more than one +LoadDatasets+ thread
running at once.  If a flag is set while +LoadDatasets+ is
running, it will probably not be noticed or acted upon until
the thread finishes.  There are rapidly diminishing returns
from having more than one +LoadDatasets+ thread running at a
time, e.g. even one thread can put substantial stress on
a remote server or a RAID array.
* Setting a flag signals that a dataset should be reloaded
as soon as possible, which is not necessarily immediately.
If a +LoadDatasets+ thread is not running, then the
dataset will be reloaded within a few seconds, but if a
thread is running it probably won't be reloaded until
the thread finishes.
* See the xref:setting_flags[Setting Flags] section for information
about how the flag system can be used to create a more
responsive and efficient reloading process than by
setting xref:reloadEveryNMinutes[+reloadEveryNMinutes+]
to every smaller numbers.

[[setting_flags]]
[[flag_file]]
Setting Flags
~~~~~~~~~~~~~

A *flag* is a mechanism for telling ERDDAP to immediately reload
a dataset.  This can be done via either creating a flag file or
using a flag URL.

The flag system can be used as the basis of a more efficient
method for telling ERDDAP to reload a dataset than via the
xref:reloadEveryNMinutes[+reloadEveryNMinutes+] mechanism.
For example, you could set the xref:reloadEveryNMinutes[+reloadEveryNMinutes+]
value for a dataset to a large number, e.g. 10080 or one week.  Then,
when you know that the dataset has changed - perhaps because
you added a file to the dataset's data directory - you can set a
flag (by either of the mechanisms described below) so the dataset
is reloaded as soon as possible.
Flags are typically noticed quickly by ERDDAP, although if
the LoadDatasets thread is already busy it may take a while before
the flag can be acted upon.  The flag system is in general more
responsive and much more efficient than setting
xref:reloadEveryNMinutes[+reloadEveryNMinutes+] to a small
number.

Flag Files
^^^^^^^^^^

A flag file tells ERDDAP to attempt to reload a dataset as soon
as possible.  Normally ERDDAP will not notice any changes to the
parameters of a dataset in +datasets.xml+ until it reloads the
dataset.
If you want to force ERDDAP to reload the dataset before its
xref:reloadEveryNMinutes[+reloadEveryNMinutes+] value would
cause it to be reloaded, put a file in the

+xref:bigParentDirectory[bigParentDirectory]/flag+ 

directory with the
same name as the xref:datasetID[+datasetID+].
This file can be empty or not, since only the presence
of the file is relevant.  ERDDAP continuously looks for
these flag files, and when it finds one it deletes it and
attempts to reload the dataset as soon as possible (which
pragmatically means usually within seconds).

Additional consequences and considerations of using flag files include:

* when a dataset is reloaded,
all the files in the
+xref:bigParentDirectory[bigParentDirectory]/xref:cache[cache]/xref:datasetID[datasetID]+
directory are deleted; and
* if the dataset's configuration XML code includes +active="false"+, a flag
will cause the dataset to be made inactive (if it is active) and not reloaded.

[[setDatasetFlag]]
Flag URLs
^^^^^^^^^

ERDDAP also has a web service that enables flags to be set via URLs.  An
example is:
-----
http://coastwatch.pfeg.noaa.gov/erddap/setDatasetFlag.txt?datasetID=rPmelTao&flagKey=31415926 
-----
which will set a flag for the dataset named +rPmelTao+.
There is a different +flagKey+ for each 
xref:datasetID[+datasetID+].
Administrators will find a list of flag URLs for all datasets at the bottom
of their xref:daily_report[Daily Report] email, and they should be treated as confidential and
only given to those who require them for specific datasets.

If you wish to change the +flagKey+ values for security purposes or any other
reason, you can do so by changing the xref:flagKeyKey[+flagKeyKey+] value
in xref:setup[+setup.xml+].
If you do change xref:flagKeyKey[+flagKeyKey+], then be sure to
delete all of the old subscriptions (which can be found in the
xref:daily_report[Daily Report]) and send the new URLs to those who you want to
have them.

Security, Authorization and Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Overview
^^^^^^^^

ERDDAP runs as an entirely public server (using +http:+)
by default, with no login system
http://en.wikipedia.org/wiki/Authentication[*authentication*] and
no restrictions to data access
http://en.wikipedia.org/wiki/Authorization[*authorization*].
If you wish to restrict access to some or all datasets to some
users, you can use the built-in ERDDAP security system.

When the security system is employed ERDDAP uses
http://en.wikipedia.org/wiki/Role-based_access_control[*role-based
access control*] wherein the ERDDAP administrator:

* defines users with the
xref:user[+user+] tag in +datasets.xml+, with
each user having a username, password, and one or more roles; and
* defines which roles have access to which dataset via the
xref:accessibleTo[+accessibleTo+] tag in +datasets.xml+ for any
dataset that should not be publicly accessible.

The various features of an enabled security system include:

* the user's log in status - and a link to log in/out - is
shown at the top of every web page;
* blocking a user from trying to log in for 15 minutes if the user
tries and fails to log in 3 consecutive times;
* the use of +http+-prefaced URLS for users who are not logged in,
and +https+-prefaced URLS for those who are to prevent
http://en.wikipedia.org/wiki/Session_hijacking[*session hijacking and
sidejacking*];
* the access and use of publicly available datasets by those who
are not logged in, although private datasets are by default not
included in any listing of datasets available to such users (although
this behavior can be modified via
xref:listPrivateDatasets[+listPrivateDatasets+]);
* the ability to see and request data from any public or private
dataset for which their role allows access to anyone who is logged
in;
* the availability of the RSS information for all datasets to
anyone, logged in or not, except for private datasets; and
* the ability to set up email subscriptions when a user has
access to a dataset.

Configuring the Security System
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After the initial ERDDAP setup the security, authentication and
authorization system is set up by:

* adding/changing the 
xref:authentication[+authentication+] value from +nothing+ to
+custom+ or +openid+ in +setup.xml+;
* adding/changing the xref:baseHttpsUrl[+baseHttpsUrl+] value in +setup.xml+;
* inserting/uncommenting +&loginInfo;+ in
xref:startBodyHtml[+startBodyHtml+] in +setup.xml+;
* http://tomcat.apache.org/tomcat-7.0-doc/ssl-howto.html[configuring Tomcat to support SSL] - the
basis for +https+ connections - by creating a keystore with a
http://en.wikipedia.org/wiki/Public_key_certificate[digital certificate] (see
xref:digital_certification[Obtaining a Digital Certificate];
* modifying +[tomcat]/conf/server.xml+ by uncommenting the
connector to port 8443;
* modifying the Apache +/etc/httpd/conf/httpd.conf+ file - if Tomcat is
running in Apache - to allow HTTPS traffice to/from ERDDAP by adding
a couple of lines to the +VirtualHost+ tag;
* creating a xref:user[+user+] tag in +datasets.xml+ for each user
with a username, password and one or more roles, i.e. authorization;
* adding an xref:accessibleTo[+accessibleTo+] tag to each dataset
that should not have public access, with the tag values specifying
which roles have access to the dataset;
* restarting Tomcat, and going through the debugging cycle if necessary.

Additional considerations/cautions/checks include:

* checking and double-checking each step since any mistake can lead
to a security flaw;
* checking that the login page uses +https+ and not +http+, with the
correct behavior being any attempts to connect via the latter automatically
redirected to +https+ and port 8443 (with external sysadmin help probably
required to open port 8443 on your server);
* the capability of changing the xref:user[+user+] and
xref:accessibleTo[+accessibleTo+] tags at any time, with the
changes applied at the next regular reload of any dataset;
* checking to see if your +.keystore+ certificate has expired
if users suddenly can't access the log in page.

Authentication Types
^^^^^^^^^^^^^^^^^^^^

ERDDAP supports +custom+ and
http://openid.net/[+openid+] authentication types, with
the latter [green]*strongly recommended* since it
frees you from the responsibility of storing and
handling user passwords.  Users tend to use the same
password on multiple sites, and if you have a password
database that is compromised an incautious user's
password might be used to access something a lot more
tangible than data like, for instance, their bank
account.

[[custom]]
+custom+
++++++++

The +custom+ authentication type is a system for enabling
users to login in by entering a username and a password via
a form on a web page.
This is secure in the sense that both are transmitted
in encoded format via +https+ rather than in plaintext
via +http+, although you still have to deal with the security
problems of collecting, storing and protecting a password
database.
If you must use this option, then consider collecting
usernames and passwords via phone rather than email.
After they are collected, you can place them in
xref:user[+user+] tags in +datasets.xml+.

[[openid]]
+openid+
++++++++

The +openid+ option uses OpenID, an open standard that lets
users log in with a password at one web site, and then log in
without that password at any other web site that employs
OpenID authentication, e.g. ERDDAP.  Not to belabor the
issue, but this absolves you from having to deal with
a password database, the holy grail of every malicious hacker
on the planet.  All that is needed with this option is a
user's OpenID URL.

OpenID does use and require a cookie on the user's
computer, so the user's browser must be set to allow
cookies.  An unfortunate side effect of this is that
it is hard to automatically request data using a program
or script with cookies.

Secure Data Sources
^^^^^^^^^^^^^^^^^^^

If the access to a dataset is to be restricted to ERDDAP
users, then the data source from where ERDDAP gets the
data should not be publicly accessible.  Some options
for accomplishing this include:

* serving data from local files via +EDDTableFromFiles+
or +EDDGridFromFiles+;
* placing ERDDAP in a
http://en.wikipedia.org/wiki/Demilitarized_zone_%28computing%29[DMZ]
with the data source - e.g. an OPeNDAP server or database - behind a
firewall where it is accessible to ERDDAP but not to the public;
* having the data source on a public web site that
requires a log in to get the data (with this already done with
the +EDDTableFromDatabase+ dataset type that requires a username
and password for the remote database server).

In general, though, ERDDAP cannot deal with data sources other
than those accessed via +EDDTableFromDatabase+ because it has
no provisions for logging on to the data source.  This is why
access to +EDDGridFromErddap+ and +EDDTableFromErddap+ datasets
cannot be restricted.  The local ERDDAP has no way to log in and
access metadata information from the remote ERDDAP.  Putting
the remote ERDDAP behind your firewall and removing its dataset
+accessibleTo+ restrictions does not solve the problem, since
user requests for +EDDXxxFromErddap+ have to be redirected to
the remote ERDDAP, which can't be done if it is behind a firewall.

Questions? Suggestions? If you have any questions about ERDDAP's security
system or have any questions, doubts, concerns, or suggestions about how it is
set up, please email +bob dot simons at noaa dot gov+.


[[digital_certificate]]
Obtaining a Digital Certificate
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is [green]*green* recommended that you obatin a digital
certificate from a
http://en.wikipedia.org/wiki/Certificate_authority[*certificate authority*]
rather than making a
http://en.wikipedia.org/wiki/Self-signed_certificate[*self-signed
certificate*].
This will give your clients more assurance that they are connecting
to your ERDDAP and not some imposter's website.
There are many vendors who sell digital certificates, and they
are generally inexpensive.

Diagnostics and Troubleshooting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[daily_report]]
Daily Report
^^^^^^^^^^^^

The daily report contains a great deal more useful information than the
Status Page.  It is the most complete summary of the status of your
ERDDAP.  Among the most important information within the daily
report is a list of datasets that did not load and the
exceptions they generated.  This is an excellent place to start
debugging datasets that do not load.

The daily reports are found in the

+xref:bigParentDirectory[bigParentDirectory]/logs+

directory, and is contained within a file with a name of the
form +emailLog2013-06-13.txt+.

The start of a typical daily report looks like:
-----
ERDDAP Daily Report 2013-06-13T13:11:48

Current time is 2013-06-13T13:11:49 local time
Startup was at  2013-06-13T13:11:43 local time
Last major LoadDatasets started 4.000 s ago and is still running.
nGridDatasets  = 3
nTableDatasets = 3
nTotalDatasets = 6
n Datasets Failed To Load (in the last major LoadDatasets) = 1
    erdGlobecBottle, (end)
Response Failed    Time (since last major LoadDatasets) n =        0
Response Failed    Time (since last Daily Report)       n =        0
Response Failed    Time (since startup)                 n =        0
Response Succeeded Time (since last major LoadDatasets) n =        3,  median ~=        1 ms
Response Succeeded Time (since last Daily Report)       n =        3,  median ~=        1 ms
Response Succeeded Time (since startup)                 n =        3,  median ~=        1 ms
TaskThread has finished 0 out of 0 tasks.  Currently, no task is running.
TaskThread Failed    Time (since last Daily Report)     n =        0
TaskThread Failed    Time (since startup)               n =        0
TaskThread Succeeded Time (since last Daily Report)     n =        0
TaskThread Succeeded Time (since startup)               n =        0
Number of non-Tomcat-waiting threads in this JVM = 17
Memory: currently using      25 MB (high water mark =     143 MB) (Xmx ~= 11500 MB)
-----


Dataset Types
-------------

There are just two basic data structures for datasets used within ERDDAP.
These are:

. a gridded data structure such as satellite data or numerical model output;
and
. a tabular data structure such as in-situ buoy, station or trajectory data.

Not all possible data can be expressed within the confines of these two
structures, but most of the data created within the geosciences can be
expressed within gridded or tabular data structures.

Advantages to having just two data structures include:

. data queries are easier to construct;
. data responses have a simple structure which makes it easier to serve
the data in a wider variety of standard file types (which often support
simple data structures themselves);
. client software which works with all ERDDAP datasets is easier
to construct; and, ultimately,
. comparing data from different sources is easier for the end user.

The ERDDAP approach requires a bit of tedious work to create
configuration files for your datasets, and perhaps even a bit more tedious work
to slightly reconfigure your datasets so they're amenable to being
served by ERDDAP.  Once this is finished, though, all of your datasets
will be permanently available within an interactive system that enables the flexible
and easy comparison of datasets that used to be in widely varying formats
that made data comparison a new and frustrating adventure every time.
You will also be able to download entire datasets or arbitrary subsets thereof from
ERDDAP
in a wide range of formats once they've been properly configured.
Think of ERDDAP as an ounce of prevention now to prevent many pounds of cure
later.

[[EDDGrid]]
Gridded Datasets
~~~~~~~~~~~~~~~~

The gridded or
*EDDGrid* ERDDAP dataset
types or classes are:

. xref:EDDGridFromFiles[+EDDGridFromFiles+] - a superclass of all
+EDDGridFrom*Files+ classes, of which there is currently just one.
This is not used directly, but its properties are inherited and used
by all of its subclasses.

.. xref:EDDGridFromNcFiles[+EDDGridFromNcFiles+] - which aggregates data
from a
group of local
http://en.wikipedia.org/wiki/NetCDF[NetCDF] files;

. xref:EDDGridFromDap[+EDDGridFromDap+] - which handles gridded data from
http://en.wikipedia.org/wiki/OPeNDAP[DAP] servers;

. xref:EDDGridFromErddap[+EDDGridFromErddap+] - which handles gridded data
from a remote ERDDAP
server;

. xref:EDDGridFromEtopo[+EDDGridFromEtopo+] - which handles the built-in
http://www.ngdc.noaa.gov/mgg/global/[ETOPO] gridded global relief data;

. xref:EDDGridSideBySide[+EDDGridSideBySide+] - which aggregates two or more
EDDGrid datasets
side by side;

. xref:EDDGridAggregateExistingDimension[+EDDGridAggregateExistingDimension+]
- which aggregates two or more
EDDGrid datasets based on different values of the first dimension; and

. xref:EDDGridCopy[+EDDGridCopy+] - which creates a local copy of any EDDGrid's data and
serves data from that local copy.

[[EDDTable]]
Tabular Datasets
~~~~~~~~~~~~~~~~

The tabular or *EDDTable* dataset types are:

. xref:EDDTableFromFiles[+EDDTableFromFiles+] - a superclass of all
+EDDTableFrom*Files+ classes.  This is not used directly, but its properties
are inherited and used by all of its subclasses.

.. xref:EDDTableFromAsciiFiles[+EDDTableFromAsciiFiles+] - which aggregates data from comma-, tab- or
space-separated tabular ASCII data files;

.. xref:EDDTableFromAwsXmlFiles[+EDDTableFromAwsXmlFiles+] - which aggregates
data from a set of
Automatic Weather Station (AWS)
http://developer.weatherbug.com/docs/read/WeatherBug_Rest_XML_API[XML files];

.. xref:EDDTableFromHyraxFiles[+EDDTableFromHyraxFiles+] - which aggregates
data from files with several
variables with shared dimensions served by a
http://www.opendap.org/download/hyrax[Hyrax OPeNDAP server];

.. xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] - which aggregates data from
NetCDF files with several
variables with shared dimensions;

.. xref:EDDTableFromNcCFFiles[+EDDTableFromNcCFFiles+] - whihc aggreates data
from NetCDF files which use
one of the file formats specified by the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#discrete-sampling-geometries[CF
discrete sampling geometries conventions];

.. xref:EDDTableFromThreddsFiles[+EDDTableFromThreddsFiles+] - which aggreates
data from files with several
variables with shared dimensions served by a
http://www.unidata.ucar.edu/projects/THREDDS/[THREDDS OPeNDAP server];

. xref:EDDTableFromAsciiService[+EDDTableFromAsciiService+] - a superclass of
all +EDDTableFromAsciiService*+ classes, which is not used directly although
its properties are inherited and used by all of its subclasses;

.. xref:EDDTableFromAsciiServiceNOS[+EDDTableFromAsciiServiceNOS+] - which handles data from some of the
http://opendap.co-ops.nos.noaa.gov/axis/[NOAA NOS web services];

. xref:EDDTableFromDapSequence[+EDDTableFromDapSequence+] - which handles tabular data from
DAP sequence servers;

. xref:EDDTableFromDatabase[+EDDTableFromDatabase+] - which handles tabular data from one
database table;

. xref:EDDTableFromErddap[+EDDTableFromErddap+] - which handles tabular data from a remote ERDDAP;

. xref:EDDTableFromNOS[+EDDTableFromNOS+] - which handles tabular data from
http://opendap.co-ops.nos.noaa.gov/axis/[NOS XML servers];

. xref:EDDTableFromOBIS[+EDDTableFromOBIS+] - which handles tabular data from
http://www.iobis.org/[OBIS] servers;

. xref:EDDTableFromSOS[+EDDTableFromSOS+] - which handles tabular data from
http://www.opengeospatial.org/projects/groups/sensorwebdwg[SOS] servers; and

. xref:EDDTableCopy[+EDDTableCopy+] - which makes a local copy of any EDDTable's data and serves
data from the local copy.

[[Configuration_of_Datasets]]
Configuring Datasets, or, Creating New +dataset+ Tags for +datasets.xml+
------------------------------------------------------------------------

Overview
~~~~~~~~

In the context of ERDDAP, configuring a dataset means creating
a xref:dataset[+dataset+] XML tag for a specific dataset, and
then adding that tag to the +datasets.xml+ configuration file.
This is usually a nontrivial and tedious task, although a couple of tools
have been created to bypass most of the tedium and enable you to automatically create,
debug and refine your +dataset+ XML tags.  They are:

* xref:GenerateDatasetsXml[+GenerateDatasetsXml+] - for automatically
creating a +dataset+ XML tag for a dataset; and

* xref:DasDds[+DasDds+] - for interactively debugging and
refining +dataset+ XML tags.

In this chapter we will give a brief and hopefully motivational description of how and why
these command-line tools should preferably be used.  The details
of their use, however, are omitted as they are available in the
separate chapter for each tool.

In this chapter will will proceed as if we are creating the
entirety of a +dataset+ XML tag by hand, going into
excruciating detail about the options, requirements, recommendations,
etc. for each and every available XML attribute
and tag.  Why?  Because the tools can be used to create at most
a minimal skeleton +dataset+ tag that may be immediately
usable in the sense that it is error-free and ERDDAP will show it on
the +List of All Datasets+ web page, but is almost certainly not the final tag you'll want to
be adding to +datasets.xml+.

This chapter will show you how to modify existing tags and attributes
and add new ones to a minimally functional +dataset+ tag that will:

* enable the end user to employ the full functionality of ERDDAP on all
of your datasets; and

* supply the dataset with the sort of FGDC and ISO 19115 metadata
that will make it easier for interested users to find and use it,
and thus greatly increase its shelf life.

General Procedure for Automatically Creating Dataset Configurations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The suggested and probably inevitable route you will take from spin-up through
zen-like expertise in creating and using ERDDAP dataset configuration files
is:

* use the xref:GenerateDatasetsXml[+GenerateDatasetsXml+] tool to create a raw
but probably usable +dataset+ XML tag for a specific type of dataset;

* add the XML tag to the +datasets.xml+ file and restart the server;

* observe the server interface and check the error log to see if your
+dataset+ XML tag has made it through the ERDDAP gauntlet and is minimally
sufficient to be displayed on the +List of All Datasets+ web page;

* if the dataset is not displayed, check the error log for messages
about what went wrong and suggestions about how to resolve the problem;

* make appropriate changes to your dataset and/or the +dataset+ XML tag you created
in the first step, and restart the server again;

* after all errors have been corrected, check the +Data Access Form+ and
+Create a Graph+ pages to see if everything is functioning properly because
even an error-free +dataset+ XML tag can contain problems that will limit
functionality, e.g. improperly configured xref:LLAT[LLAT] variables that severely
limit the usefulness of the +Create a Graph+ page;

* repeat these steps as needed until you have a dataset and a
+dataset+ XML tag that enable ERDDAP to provide the end user with
the full range of its available functionality.

Alternatively, you can use xref:DasDds[+DasDds+] to debug and
refine your +dataset+ XML tag.

The usage of both
xref:GenerateDatasetsXml[+GenerateDatasetsXml+] and
xref:DasDds[+DasDds+] along with specific examples for
various dataset types is extensively detailed in their
respective chapters.

The +datasets.xml+ Configuration File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Datasets are configured by adding
+dataset+ XML tags to the +datasets.xml+ file, the
basic structure of which is:
[source,xml]
-----
<?xml version="1.0" encoding="ISO-8859-1" ?>
<erddapDatasets>
  <convertToPublicSourceUrl /> <!-- 0 or more -->
  <requestBlacklist>...</requestBlacklist> <!-- 0 or 1 -->
  <subscriptionEmailBlacklist>...</subscriptionEmailBlacklist> <!-- 0 or 1 -->
  <user username="..." password="..." roles="..." /> <!-- 0 or more -->
  <dataset>
    ...
  </dataset>
  <dataset>
    ...
  </dataset>
    ...
</erddapDatasets>
-----
The outermost tag level is the +erddapDatasets+ tag,
which is outside the scope of the command-line tools and
all the attributes and tags of which - excepting the
+dataset+ tag - are configured by hand by the ERDDAP
administrator.  These non-+dataset+ tags and attributes
apply to all of the datasets configured by the separate
+dataset+ tags within the +datasets.xml+ file.

We now supply the details need to properly configure
the +erddapDatasets+ tag.

Configuring the +erddapDatasets+ Tag
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The basic structure of the +erddapDatasets+ tag - omitting
all of the +dataset+ tags - is:
[source,xml]
-----
<erddapDatasets>
  <convertToPublicSourceUrl /> <!-- 0 or more -->
  <requestBlacklist>...</requestBlacklist> <!-- 0 or 1 -->
  <subscriptionEmailBlacklist>...</subscriptionEmailBlacklist> <!-- 0 or 1 -->
  <user username="..." password="..." roles="..." /> <!-- 0 or more -->
...
</erddapDatasets>
-----

The first line:
-----
<?xml version="1.0" encoding="ISO-8859-1" ?>
-----
is standard, generic XML boilerplate.  ERDDAP presently allows only the
http://en.wikipedia.org/wiki/ISO/IEC_8859-1[+ISO-8859-1+] encoding.


The +erddapDatasets+ tag encloses the entirety of the configuration file.
Common tags and attributes that can be contained within the
+erddapDatasets+ tag will now be described.

+convertToPublicSourceUrl+
^^^^^^^^^^^^^^^^^^^^^^^^^^

This is an [green]*optional* tag containing +from+ and +to+ attributes that specify
how to convert a matching local source URL into a public source URL.  An
example is:
-----
<convertToPublicSourceUrl from="http://192.168.31.18/" to="http://oceanwatch.pfeg.noaa.gov/" /> 
-----
which will convert a local source URL, e.g.
-----
http://192.168.31.18/thredds/dodsC/satellite/BA/ssta/5day
-----
into a public source URL, e.g.
-----
http://oceanwatch.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day
-----

+requestBlacklist+
^^^^^^^^^^^^^^^^^^

This is an [green]*optional* tag containing a comma-separated list of numeric
IP addresses that are to be blacklisted.  This is typically used to fend off 
the notorious http://en.wikipedia.org/wiki/Denial_of_service[denial of service
attack] or a hyperzealous http://en.wikipedia.org/wiki/Internet_bot[web
robot].  An example of this is:
-----
<requestBlacklist>98.76.54.321, 123.45.68.*</requestBlacklist>
-----
in which it is seen that one can specify either the entire IP number
(+98.76.54.321+) or a wild card character (+123.45.68.*+) to block all the possible
numbers from a given site.
A list of possibly abusive IP numbers that might have to be blocked can be found in the ERDDAP daily
report.  This will cause all of the requests from blacklisted IP numbers
to receive the reply:
-----
HTTP Error 403: Forbidden
-----

+subscriptionEmailBlacklist+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The subscription system can (and will) be abused.  The system can be seen at:

http://coastwatch.pfeg.noaa.gov/erddap/subscriptions/index.html[+http://coastwatch.pfeg.noaa.gov/erddap/subscriptions/index.html+]

and allows you to add subscriptions that will enable you to be notified
whenever a dataset changes.  This [green]*optional* tag contains a comma-separated
list of email addresses that are to be blacklisted from the subscription
system. An example is:
-----
<subscriptionEmailBlacklist>bob@badguy.com,john@badguy.com</subscriptionEmailBlacklist>
-----
If an email address is added to this and it already has subscriptions, the
subscriptions will be cancelled.  If a blacklisted address attempts to
create a new subscription, the request will be denied.

[[roles]]
[[user]]
+user+
^^^^^^

This [green]*optional* tag identifies a user's username, password, and a
comma-separated list of their roles.  This is a part of the security
system that enables restricting access to some datasets to
specific users.  An example of this tag is:
-----
<user username="jsmith" password="57AB7ACCEB545E0BEB46C4C75CEC3C30" roles="role1, role2" /> 
<user username="http://jsmith.myopenid.com/" roles="role3, role4" />
-----
where we see that a separate +user+ tag is required and created for each user.

The form of and presence of the +password+ attribute is predicated upon
how the xref:authentication[+authentication+] tag within +setup.xml+ is handled.

. If the xref:authentication[+authentication+] is specified is +custom+, then both the
+user+ and +password+ attributes need to specified, with the latter
having to be at least 7 characters in length and case-sensitive.
The specification of the xref:passwordEncoding[+passwordEncoding+] tag in the +setup.xml+
files determines how passwords are stored within the +user+ tags.
In the first +user+ example above, the password was created using
the UEPMD5 option.

. If the xref:authentication[+authentication+] is specified as +openid+, then the user's 
http://openid.net/[OpenID] URL is the +username+ and the +password+
isn't specified, as seen in the second of the +user+ examples above.

The password options are (in order of increasing security):

. plaintext, which is *not at all recommended*;

. http://en.wikipedia.org/wiki/MD5[MD5], wherein the form of the password is
derived from using an appropriate MD5 password generator to create an encoded
string from a chosen +password+; and

. UEPMD5, wherein the form of the password is derived from using the MD5
password generator to created an encoded string from
+username:ERDDAP:password+.

The *roles* are part of a system called
http://en.wikipedia.org/wiki/Role-based_access_control[role-based access
control], wherein each user in a group that one wishes to grant access to a specific
dataset are assigned a role corresponding to that dataset.  In practice, the values
assigned to a given user within the +roles+ attribute will be compared to the
roles listed in the dataset's +accessibleTo+ tag.  If there is a match, then
that user can access that dataset.

Configuring the +dataset+ Attributes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The +dataset+ tag is used to supply specifications for each separate
dataset.  Within the +dataset+ tag itself, three attributes can appear, e.g.

-----
<dataset type="EDDGridFromDap" datasetID="etopo180" active="true" >
-----

[[type]]
+type+
^^^^^^

ERDDAP datasets are classified into two categories.  There are datasets that
handle gridded data, and datasets that handle tabular data.  Within each
category there are several different types available, each of which is
briefly described in the xref:Dataset_Types[Dataset Types] chapter, and
each of which also has its own section further on that details how it
can be used.

[[datasetID]]
+datasetID+
^^^^^^^^^^^

The +datasetID+ attribute for the +Dataset+ tag
is a [red]*required* attribute which assigns a short, unique, identifying
name to the dataset.  The valid characters are +A-Z+, +a-z+ (upper- and lower-case alphabet), +0-9+,
an underscore and a dash, i.e. +\_+ and +-+.
It is [red]*recommended* to start with a letter and follow up with just +A-Z+,
+a-z+, +0-9+ and +_+.
The +datasetID+ is case sensitive, but it is [red]*not recommended* to create two
IDs that differ only in upper- and lower-case letters, e.g.
-----
dataset15
dataSet15
-----
as this will cause
problems on Windows systems.

It is [red]*recommended* to use
http://en.wikipedia.org/wiki/CamelCase[*camelCase*] as a best practice.
This is the practice of writing compound words where the first letter
of an identifier is lowercase, and the first letter of each subsequent
concatenated word is capitalized.  Some examples of this are +oldHtmlFile+,
+parseDbmXml+ and +sqlServer+.

It is additionally [red]*recommended* that the first part of the +datasetID+
be an acronym of the source institution's name, and the second part be an
acronym or abbreviation of the dataset's name.  An example of a +datasetID+
following these recommendations that is
derived from the source institution:
-----
NOAA NMFS SWFSC Environmental Research Division
-----
and from a dataset designated at the source as:
-----
satellite/PH/ssta/8day
-----
would be:
-----
datasetID="erdPHssta8day"
-----

[[active]]
+active+
^^^^^^^^

The +active+ attribute for the +Dataset+ tag is [green]*optional*
and indicates if a dataset is active or not.  The valid values
are +true+ and +false+, with +true+ the default.  Practically,
this isn't really needed unless you want to specify it as +false+
to force a dataset to be removed as soon as possible.

[[LLAT]]
The Importance of the Time and Space Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before we get into the details of the various +dataset+ tags
and attributes, we will supply a bit of motivation about the
special nature of certain variables and how important it is
to configure them correctly.
The time and space variables - +longitude+,
+latitude+, +altitude+ and +time+ a.k.a. LLAT - are special within ERDDAP.  
These are automatically made known to ERDDAP if:

* the xref:destinationName[+destinationName+] values for the xref:axisVariable[+axisVariable+] tags are LLAT for
xref:EDDGrid[+EDDGrid+] datasets; or

* the xref:destinationName[+destinationName+] values for the xref:dataVariable[+dataVariable+] tags are LLAT for
xref:EDDTable[+EDDTable+] datasets.

It is [red]*strongly recommended* that you use the standard names
+longitude+,
+latitude+, +altitude+ and +time+ for the
time and space variables in your datasets if they are xref:appropriate[appropriate]. If you do not use these, then
ERDDAP will fail to recognize their significance and, for example, will
make a graph instead of a map if the x-axis variable is +lon+ instead of
+longitude+ and
the y-axis variable is +lat+ instead of +latitude+.

As a motivation for striving to specify them properly if xref:appropriate[appropriate], the
following good things happen in the LLAT variables are specified correctly:

* ERDDAP will automatically add extra metadata to the LLAT variables, e.g.
+ioos_category+, +units+ and several standards-related attributes
like +_CoordinateAxisType+;

* ERDDAP will automatically add extra global metadata related to the LLAT
values of the selected data subset, e.g. +geospatial_lon_min+;

* the +Make a Graph+ page will create a map - using a standard projection
and with a land mask, political boundaries, and other features - instead of a graph if the x-axis
variable is +longitude+ and the y-axis variable is +latitude+.

* clients that support these same standards will easily be able to take
advantage of the additional metadata to position the data in space and time;

* clients will find it easier to generate queries that include LLAT
variables because the names will be the same in all relevant datasets.

* the +time+ variable and xref:units[related timestamp variables] will convert
data values from the source's time format into either a numeric value - e.g.
+seconds since 1970-01-01T00:00:00Z+ - or a string values - e.g.
ISO 8601:2004(E) format - depending on the requirements of the situation.

[[appropriate]]
Notes on the appropriate use of the LLAT variables:

* The use of +longitude+ and +latitude+ as +destinationName+ values is appropriate only if their
respective units are +degrees_east+ and +degrees_north+.  If this is not
true for your data, then use different names, e.g. +lonRadians+
and +latRadians+.

* The use of +altitude+ as a +destinationName+ value is only appropriate if it is being used to
describe a variable that is the distance above or below sea level.
Use the +altitudeMetersPerSourceUnit+ tag - which has a default
value of ++1+ - to convert the data into
meters above sea level if it is not already expressed in meters.
Examples of this include using a value
+-1+ if the data is depth in meters below sea level, or a value of +0.001+ if the source
data is measured in km above sea level.  If the
http://www.oostethys.org/best-practices/verticalcrs[vertical datum]
is known, it should be specified in the metadata.  If your dataset
does not fit these requirements, use a different +destinationName+
such as +aboveGround+, +depth+ or +distanceToBottom+.

* The use of +time+ as a +destinationName+ value is only appropriate for
variables that include the entire data+time (or date, if that is the extent
of the available time information). For example, if there are separate
columns for both the +date+ and +timeOfDay+, do not use the variable
name +time+.  See the section describing the xref:units[+units+] attribute
for further details.

*Note*: ERDDAP does [red]*not* follow the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#time-coordinate[CF
time standard] when converting +years since+ and +months since+ time values to
+seconds since+.  The CF standard defines a year as a fixed, single value of
3.15569259747e7 seconds, and a month as 1/12 of a year.  Most datasets that
use +years since+ and +months since+ clearly intend the values to be
calendar years or months, e.g. +3 months since 1970-01-01+ is clearly
intended to mean 1970-04-01.  This intent will not be met by the CF standard,
so ERDDAP instead interprets +years since+ and +months since+ as calendar
years
and months.

Much more about the inherently complicated time variable can be found in
the xref:units[+units+] and xref:how_erddap_handles_time[+How ERDDAP Handles
Time+] sections. 

Configuring the +dataset+ Tags
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We now describe the use of the various tags available within the
+dataset+ tag.

[[sourceUrl]]
+sourceUrl+
^^^^^^^^^^^

The +sourceUrl+ tag is [red]*required* 
within the +dataset+ tag, and specifies the URL
source of the data.

[[axisVariable]]
+axisVariable+
^^^^^^^^^^^^^^

The +axisVariable+ tag is [red]*required* within an EDDGrid +dataset+ tag, and 
is used to describe a dimension or axis shared by the data variables in
the EDDGrid dataset.  This is [red]*not allowed* for EDDTable datasets.
All data variables in an EDDGrid dataset [red]*must* share all of the
axis variables.  If they do not, then a lot of the functionality within
ERDDAP is rendered useless.  There must be one or more instances
of +axisVariable+, and it supports the following subtags:

. +sourceName+ - This [red]*required* tag is the name of the variable used at the data source.  This is
the name used by ERDDAP when requesting data from the data source, and the
name looked for when data is returned from that source.  It is
case sensitive.

. +destinationName+ - This [green]*optional* tag is the name for the variable
that will be shown to and used by ERDDAP users.  If absent, the +sourceName+
is used, although this can be useful if the +sourceName+ is cryptic or odd.
This is case sensitive, and [red]*must* start with a letter (+A-Z+, +a-z+) and
[red]*must* be followed by zero or more characters (+A-Z+, +a-z+, +0-9+, +_+).

. +addAttributes+ - This [green]*optional* tag defines a set of attributes
which are added to the source's attributes for a variable.  This produces
a combined set of attributes for a variable.

If either the variable's +addAttributes+ or +sourceAttributes+ contain
+scale_factor+ and/or +add_offset+ attributes, their values will be
used to unpack the data from the source before distribution to the
client via the equation:
-----
resultValue = sourceValue * scale_factor + add_offset
-----
The unpacked variable will be of the same data type as the
+scale_factor+ and +add_offset+ values.  An example of the
+axisVariable+ tag is:
-----
<axisVariable>
  <sourceName>MT</sourceName> 
  <destinationName>time</destinationName>
  <addAttributes>
    <att name="units">days since 1902-01-01T12:00:00Z</att>
  </addAttributes>
</axisVariable>
-----

+dataVariable+
^^^^^^^^^^^^^^

The +dataVariable+ tag is [red]*required* for almost all datasets within a
+dataset+ tag, and is used to describe a data variable.
See the separate xref:dataVariable[+dataVariable+] section for more about this
complex tag. 

+addAttributes+
^^^^^^^^^^^^^^^

The +addAttributes+ tag is [green]*optional* within the +dataset+  and
+dataVariable+ tags, and
enables ERDDAP administrators to control the metadata attributes associated
with a dataset and its variables.
See the separate xref:addAttributes[+addAttributes+] section for more
about adding attributes.

[[accessibleTo]]
+accessibleTo+
^^^^^^^^^^^^^^

The +accessibleTo+ tag within the +dataset+ tag is [green]*optional* and
specifies a space-separated list of xref:user[+roles+] that are
allowed to access this particular dataset.
If this tag is not present, all users will have access to this dataset.
If this tag is present, the dataset will only be visible and accessible to
users who are logged in and who have one of the specified +roles+, and
will be invisible to users who are not logged in.

[[altitudeMetersPerSourceUnit]]
+altitudeMetersPerSourceUnit+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +altitudeMetersPerSourceUnit+ tag within the +dataset+ tag is
[green]*optional* but [red]*recommended*, and specifies a number that is multiplied by
the source altitude or depth values - after any present +scale_factor+ and/or
+add_offset+ transformations have been made -to convert them into
altitude values in meters above sea level.  Example conversions include:

. 1 - if the source is already measured in meters above sea level;
. -1 - if the source is measured in meters below sea level;
. 0.001 - if the source is measured in km above sea level.

The default value is 1, and an example is:
-----
<altitudeMetersPerSourceUnit>-1</altitudeMetersPerSourceUnit> 
-----

[[fgdcFile]]
+fgdcFile+
^^^^^^^^^^

The +fgdc+ tag within the +dataset+ tag is [green]*optional*, and tells
ERTDDAP to use a pre-made FGDC file instead of have ERDDAP generate the
file.  An example is:
-----
<fgdcFile>fullFileName</fgdcFile> 
-----

If this tag is present but no file is specified, or if the file isn't
found, the dataset will have no FGDC metadata.  That is, this will
additionally stop the ERDDAP server from automatically creating a
FGDC metadata file.  This could be useful if you don't want FGDC data
available for a dataset.  You can also do this globally by adding
the xref:fgdcActive[+fgdcActive+] tag, e.g.
-----
<fgdcActive>false</fgdcActive>
-----
into the +setup.xml+ file.

[[iso19115File]]
+iso19115File+
^^^^^^^^^^^^^^

The +iso19115File+ tag within the +dataset+ tag is [green]*optional*, and
tells ERDDAP to use a pre-made ISO 19115 file instead of having ERDDAP
generate the file.  An example is:
-----
<iso19115File>fullFileName</iso19115File>
-----

If this tag is present but not file is specified, or if the file
isn't found, the dataset will have to ISO 19115 metdata.  This could
be useful if you don't wnat FGDC data available for a dataset.  You
can also do this globally by adding the
xref:iso19115Active[+iso19115Active+] tag, e.g.
-----
<iso19115Active>false</iso19115Active>
-----
into the +setup.xml+ file.

[[fileDir]]
+fileDir+
^^^^^^^^^

The +fileDir+ tag within the +dataset+ tag contains the
absolute path to the directory containing the data files.
If the files are located in +/data/ocean/2010+, then
the tag will be:
-----
<fileDir>/data/ocean/2010</fileDir>
-----

[[onChange]]
+onChange+
^^^^^^^^^^

The +onChange+ tag within the +dataset+ tag is [green]*optional*, and
specifies an action to be performed when the dataset is created (when
ERDDAP is started), or whenever
the dataset changes in any way.
For EDDGrid datasets, any change to the metadata or to an axis
variable - e.g. a new time point for near-real-time data - is
considered a change, although a reloading of the dataset is not (by
itself).
For EDDTable datasets, any reloading of the dataset is considered
a change.

Only two types of actions are allowed at present, although there
can be an arbitrary number of these tags.

+http+ Action
+++++++++++++

If the action begins with +http://+, ERDDAP will send an HTTP GET request to
the specified URL.  The response will be ignored. This might be used to
tell some other web service to do something.
If the URL has a query part, i.e. a +?+ followed by various parameters,
it must be http://en.wikipedia.org/wiki/Percent-encoding[percent encoded].
This simply means that one must make the following conversions within
the URL string:

. +%+ into +%25+
. +&+ into +%26+
. +"+ into +&22+
. +=+ into +%3D+
. +++ into +%2B+
. + + into +%20+

You must also convert Unicode characters above +#126+ into their
+%HH+ form (where +HH+ is the two-digit hex value).  Unicode
characters above +#255+ must be UTF-8 encoded, and then each
byte must be converted into +%HH+ form.
Since +datasets.xml+ is an XML file, you must also convert +&+ into
+&amp;+, +>+ into +&gtu;+, and +<+ into +&lt;+.

An example of a converted URL is:
-----
http://www.company.com/webService?department=R%26D&param2=value2
-----
which would be used in an +onChange+ tag as:
-----
<onChange>http://www.company.com/webService?department=R%26D&amp;param2=value2</onChange>
-----

+mailto+ Action
+++++++++++++++

If the action starts with +mailto:+, ERDDAP will send an email to the given
email address indicating that the database has been updated or changed.

[[reloadEveryNMinutes]]
+reloadEveryNMinutes+
^^^^^^^^^^^^^^^^^^^^^

The +reloadEveryNMinutes+ tag within the +dataset+ tag is [green]*optional*
but [red]*recommended*,
and specifies how often the dataset should be reloaded.
In general, real-time datasets that are updated frequently should be reloaded
at least every hour.  More temporally static datasets should only be updated
daily or even weekly.  The default value is +10080+ minutes.
An example is:
-----
<reloadEveryNMinutes>1440</reloadEveryNMinutes>
-----

The xref:loadDatasetsMinMinutes[+loadDatasetsMinMinutes+] tag in +setup.xml+ prescribes a minimum number
of minutes for reloading datasets, with a default value of +15+ minutes if
it is not set.  Both xref:reloadEveryNMinutes[+reloadEveryNMinutes+] and
xref:loadDatasetsMinMinutes[+loadDatasetsMinMinutes+]  must
be reset to small values if you want to reload at a frequency less than
15 minutes, although it is [red]*recommended* that you use a smaller
value for +reloadEveryNMinutes+ to avoid a situation where reloads are
skipped.  

All of these reloading parameters can be overridden via the use of
a xref:flag_file[flag file].

[[sourceCanConstrainStringEQNE]]
+sourceCanConstrainStringEQNE+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +sourceCanConstrainStringEQNE+ tag within the +dataset+ tag is
[green]*optional* within an EDDTable dataset, and specifies if
the source can constrain string variables with the +=+ and
+!=+ operators.  The valid values are +true+ (default) and
+false+.

For xref:EDDTableFromDapSequence[+EDDTableFromDapSequence+] datasets,
this applies only to the outer sequence string variables.  It is
assumed that the source cannot handle any constraints on inner
sequence variables.  This should be set to +true+ for
http://www.opendap.org/drds_server[OPeNDAP DRDS servers], and
to +false+ for 
http://www.epic.noaa.gov/epic/software/dapper/[Dapper servers].

An example is:
-----
<sourceCanConstrainStringEQNE>true</sourceCanConstrainStringEQNE> 
-----

[[sourceCanConstrainStringGTLT]]
+sourceCanConstrainStringGTLT+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +sourceCanConstrainStringGTLT+ tag within the +dataset+ tag is
[green]*optional* within an EDDTable dataset, and specifies if
the source can constrain string variables with the +<+, +<=+,
+>+, +>=+ operators. The valid values are +true+ (default) and
+false+.

For xref:EDDTableFromDapSequence[+EDDTableFromDapSequence+] datasets,
this applies only to the outer sequence string variables.  It is
assumed that the source cannot handle any constraints on inner
sequence variables.  This should be set to +true+ for
http://www.opendap.org/drds_server[OPeNDAP DRDS servers], and
to +false+ for
http://www.epic.noaa.gov/epic/software/dapper/[Dapper servers].

An example is:
-----
<sourceCanConstrainStringGTLT>true</sourceCanConstrainStringGTLT> 
-----

[[sourceCanConstrainStringRegex]]
+sourceCanConstrainStringRegex+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +sourceCanConstrainStringRegex+ tag within the +dataset+ tag 
is [green]+optional+ within
an EDDTable dataset, and specifies if the source can constrain
string variables by regular expressions and, if so, what the operator
to be used is.  The valid values are +=~+ (the DAP standard),
+~=+ (mistakenly supported by many DAP servers), or no value
to indicate that the source doesn't support regular expressions.
The default is no value.
This should be set to +true+ for both
http://www.opendap.org/drds_server[OPeNDAP DRDS] and
http://www.epic.noaa.gov/epic/software/dapper/[Dapper] servers.
An example is:
-----
<sourceCanConstrainStringRegex>=~</sourceCanConstrainStringRegex>
-----

[[sourceNeedsExpandedFP_EQ]]
+sourceNeedsExpandedFP_EQ+
^^^^^^^^^^^^^^^^^^^^^^^^^^

The +sourceNeedsExpandedFP_EQ+ tag within the +dataset+ tag is
[green]*optional* within an EDDTable dataset, and specifies if
the source needs help with +<numericVariable>=<floatingPointValue>+
and +!=+, +>=+, +<=+.

This may be needed since for some data sources, numeric
queries involving +=+, +!=+, +<=+ or +>=+ may not work as desired
with floating point numbers.  For example, a search for
+longitude=220.2+ may fail if the value is stored as
+220.20000000000001+.
This problem arises because
http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm[floating point numbers are not represented
exactly within computers].
If this is set to +true+ (the default), ERDDAP modifies the queries
it sends to the data source to avoid the problem.  It is a [red]*very good
idea* to leave this as the default +true+ unless you have a really
good reason to do otherwise.

[[addAttributes]]
Dataset and Variable Attributes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The +addAttributes+ tag is [green]*optional* for each +dataset+ and
+dataVariable+ tag, and allows administrators to control the metadata
attributes associated with a dataset and its variables.
Internally, there are two sources of attributes used by ERDDAP
to create the *combinedAttributes* view that users see.
They are:

. +sourceAttributes+ - which are attributes ERDDAP automatically derives
from the source dataset, with an example being the global and variable
attributes typically found in the metadata part of a NetCDF file; and

. +addAttributes+ - which are attributes defined by the administrator
within the +datasets.xml+ file.

The +addAttributes+ tag encloses zero or more +att+ subtags, e.g.
-----
<addAttributes>
  <att name="ioos_category">Location</att>
  <att name="long_name">Latitude</att>
<addAttributes>
-----
which consist of a name (+long_name+) and a value (+Latitude+).
If there is more than one attribute with the same name, the last
one has priority.  The value can be a single value or a space-separated
list of values.

In the matter of syntax, the order of the +att+ subtags is unimportant, unless there is
more than one subtag with the same name.  The canonical format is:
-----
<att name="name" [type="type"] >value</att>
-----
If an +att+ subtag has no value or a value of +null+, it will be
removed from the combined attributes.  The following subtags will
remove, respectively, +rows+ and +coordinates+ from the combined
attributes:
-----
<att name="rows" />
<att name="coordinates">null</att>
-----

An [green]*optional* +type+ value for the +att+ subtags indicates the
data type for the values, with the default type +string+.
Valid types for single values are +byte+, +unsignedShort+,
+short+, +int+, +long+, +float+, +double+ and +string+.
Valid types for space-separated lists of values (or single
values) are +byteList+, +unsignedShortList+, +shortList+,
+intList+, +longList+, +floatList+ and +doubleList+.
There is no +stringList+ data type.  The string values should
be stored in a newline-separated string.

Dataset or Global Attributes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +addAttributes+ tag is [green]*optional* under the +dataset+ tag, and
used to change attributes that apply to the entire dataset.
This tag can be used to add global attributes or redefine or remove
the values of the +sourceAttributes+ derived from the source dataset.

The motivation for and value of adding ostensibly optional additional
metadata to your dataset via +addAttributes+ is that the more and better
metadata you supply, the better the
http://www.fgdc.gov/[FGDC] and
http://en.wikipedia.org/wiki/Geospatial_metadata[ISO 19115-2/19139]
metadata derived from it will be.
ERDDAP automatically generates ISO 19115 and FGDC XML metadata files
for each dataset from the available +sourceAttributes+ and
+addAttributes+ metadata.  These files are what will be used by
machines and humans to find your data and discover whether or not
it might be useful for some purpose.  So the extra time and effort
you put into improving the metadata of your datasets is basically
the difference between it eventually being useful to somebody and
it lying useless in a ditch somewhere along the infobahn.  *The better
your metadata, the longer your data will live.*

Most of the dataset metadata attributes used to generate the
ISO 19115 and FGDC metadata are from the
http://www.unidata.ucar.edu/software/netcdf-java/formats/DataDiscoveryAttConvention.html[ACDD
metadata standard].

In additional to the FGDC and ISO 10115 goodness that comes from
a well-stocked attribute cabinet, many of these attributes are also
used by ERDDAP to enhance its interface and provide the user with
more and better options.
For instance, a link to the +infoUrl+ is included on web pages
with lists of datasets as well as in other places so users
can find out more about the dataset.

When a user selects a subset of a dataset, the attributes related
to the variable's longitude, latitude, altitude, and time ranges
are automatically generated or update.  Thus, if a subset is chosen
and downloaded in, for example, NetCDF format, the global and variable
attribute metadata within the NetCDF file will reflect the values
within the chosen subset.

A sample global +addAttributes+ file is:
-----
<addAttributes> 
  <att name="Conventions">COARDS, CF-1.6, Unidata Dataset Discovery v1.0</att>
  <att name="infoUrl">http://coastwatch.pfeg.noaa.gov/infog/PH_ssta_las.html</att>
  <att name="institution">NOAA CoastWatch, West Coast Node</att>
  <att name="title">SST, Pathfinder Ver 5.0, Day and Night, Global, Science Quality (1 Day Composite)</att>
  <att name="cwhdf_version" />
</addAttributes>
-----

We now go into detail about some of the global attributes considered
especially useful by ERDDAP.

[[acknowledgment]]
+acknowledgment+
++++++++++++++++

The +acknowledgment+ global attribute is a [red]*recommended* way to
acknowledge
the group or groups that provided support - most notably financial - for
the project that created this data.

[[cdm_altitude_proxy]]
+cdm_altitude_proxy+
++++++++++++++++++++

The +cdm_altitude_proxy+ global attribute is used only for EDDTable datasets
that do not have an altitude variable but do have a variable that can serve
as a proxy fo altitude, e.g. +depth+, +pressure+, +sigma+, +bottleNumber+,
etc.  This attribute can be used to identify such a variable as in:
-----
<att name="cdm_altitude_proxy">depth</att> 
-----
If the xref:cdm_data_type[+cdm_data_type+ is either +Profile+ or
+TrajectoryProfile+ and there is no altitude variable, a +cdm_altitude_proxy+
[red]*must* be defined.  When +cdm_altitude_proxy+ is defined, ERDDAP
adds the following metadata to the variable:
-----
_CoordinateAxisType=Height
axis=Z
-----

[[cdm_data_type]]
+cdm_data_type+
+++++++++++++++

The +cdm_data_type+ global attribute indicates the Unidata
http://www.unidata.ucar.edu/software/netcdf-java/CDM/[Common Data Model (CDM)]
data type for the dataset.  While this standard is still evolving as of this
writing (5/13), it is stable enough to use to define the data types.
ERDDAP complies with the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#discrete-sampling-geometries[Discrete
Sampling Geometries] chapter of the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html[CF
1.6] metadata conventions.  

Either the dataset's global +sourceAttributes+ or its global
+addAttributes+ [red]*must* include the +cdm_data_type+
attribute.  A few dataset types such as +EDDTableFromObis+
will set this automatically, but it must usually be done
by the administrator.  Also, all datasets with +cdm_data_type+ specifications
other than +Other+ [red]*must* have +longitude+, +latitude+ and
+time+ variables.

The +cdm_data_type+ options for EDDGrid datasets are:

. +Grid+
. +MovingGrid+
. +Point+
. +Profile+
. +RadialSweep+
. +TimeSeries+
. +TimeSeriesProfile+
. +Swath+
. +Trajectory+
. +TrajectoryProfile+
. +Other+

At present (5/13) ERDDAP does not require that any metadata be specified,
and it also doesn't check the that data actually matches the given
+cdm_data_type+, although that situation will almost certainly evolve.

ERDDAP is much stricter with EDDTable datasets.  If a dataset does not
comply with the requirements for the specific +cdm_data_type+ (as outlined
below), the dataset will not load and an error message will be produced.
The error messages are quite helpful, though, and usually tell you exactly
what you did wrong and how to fix it.

The +cdm_data_type+ options for EDDTable datasets are:

. +Point+ - This is for a dataset with unrelated points.

. +Profile+ - This is for a dataset with data from multiple depths at one of
more longitude/latitude locations.
The dataset [red]*must* include the global attribute
+cdm_profile_variables+, where the value is a comma-separated
list of the variables which have the information about each profile.
For a given profile, the values of these variables will be constant.
Also, one of the variables [red]*must* have the attribute
+cf_role=profile_id+ to identify the variable that
uniquely identifies the profiles.  If no other variable is
suitable, use the +time+ variable.

. +TimeSeries+ - This is for dat from a set of stations with
fixed longitude/latitude (and perhaps altitude).
The dataset [red]*must* include the
global attribute +cdm_timeseries_variables+, where the value is
a comma-separated list of the variables which have the information
about each station.  The values of the variables will be constant
for a given station.  Also, one of the variables [red]*must* the
variable attribute +cf_role=timeseries_id+ to identify the
variable that uniquely identifies the stations. The longitude and
latitude can vary slightly over time.  If they don't vary, include
them in the list of +cdm_timeseries_variables+.  If they do vary,
do not include them.

. +TimeSeriesProfile+ - This is for profiles from a
set of stations.  The dataset [red]*must* include the global
attribute +cdm_timeseries_variables+, where the value is a comma-separated
list of the variables which have the information about each station.
The values of these variables will be constant for a given station.
The dataset [red]*must* also include the global attribute
+cdm_profile_variables+, where the value is a comma-separated list
of the variables that have information about each profile.
One of the variables [red]*must* have the variable attribute
+cf_role=timeseries_id+ to identify the variable that uniquely identifies
the stations, one one of them [red]*must* have the variable attribute
+cf_role=profile_id+ to identify the variable that uniquely identifies
the profiles.  A given +profile_id+ only has to be unique for a given
+timeseries_id+.  Use the +time+ variable is nothing else suitable
is available.

. +Trajectory+ - This is for data from a set of longitude/latitude (and
perhaps altitude) paths called trajectories.  The dataset [red]*must*
include the global attribute +cdm_trajectory_variables+, where the
value is a comma-separated list of the variables which have the information
about each trajectory.  For a given trajectory, the values of these
variables will be constant.  Also, one of the variables [red]*must*
have the attribute +cf_role=trajectory_id+ to identify the variable
that uniquely identifies the trajectories.

. +TrajectoryProfile+ - This is for profiles taken along trajectories.
The data set [red]*must* include the global attribute
+cdm_trajectory_variables+ where the value is a comma-separated
list of the variables which have the information about each profile.
For a given trajectory, the values of these variables will be constant.
This data set [red]*must* also include the global attribute
+cdm_profile_variables+, where the value is a comma-separated list of the
variables which have the information about each profile.  For a given profile,
the values of these variables will be constant.
One of the variables [red]*must* have the variable attribute
+cf_role=trajectory_id+ to identify the variable that uniquely
identifies the trajectories, and one of the variables [red]*must*
have the variable attribute +cf_role=profile_id+ to identify
thev variable that uniquely identifies the profiles.  A given
+profile_id+ only has to be unique for a given +trajectory_id+.
If no other variable is suitable, use the time variable.

. +Other+ - This has no requirements, and should only be used
if the dataset doesn't fit into any of the other options.

All datasets with profiles [red]*must* have either an +altitude+
variable or a xref:cdm_altitude_proxy[+cdm_altitude_proxy+] variable.
If you can't make a dataset comply with *all* of the requirements
for the ideal +cdm_data_type+, then use +Point+ - which has
few requirements - or +Other+ - which has no requirements.

Setting up this attribute can also be useful for setting up
the xref:subsetVariables[+subsetVariables+] attribute, since a good starting point for
the latter is the combined values of the +cdm_..._variables+.
For example for +TimeSeriesProfile+, start xref:subsetVariables[+subsetVariables+]
with the combined contents of +cdm_timeseries_variables+ and
+cdm_profile_variables+.


[[subsetVariables]]
+subsetVariables+
+++++++++++++++++

The +subsetVariables+ global attribute is [red]*recommended* for EDDTable
datasets only.  It lets you specify a comma-separated list of
destination names of variables that have a limited number of values.
That is, variables for which each of the values has many duplicates.
It this attribute is present, the dataset will have a +.subset+ web
page - and a link to it on every dataset list - that lets users quickly
and easily select various subsets of the data.  This is a very useful
global attribute, and you are [red]*greatly encouraged* to create an appropriate
one for each of your datasets if at all possible.

When a dataset is loaded, ERDDAP loads and caches all of the distinct
+subsetVariable+ data.  This enables all user requests for this data
to be served very fast.  The order of the destination names specified
determines the sort order on the +.subset+ web page, so the variables
should be specified from most to least important.  For a dataset
with time series data for several stations, this might look like:
-----
<att name="subsetVariables">station_id, longitude, latitude</att>
-----
so the values will be sorted by +station_id+, the more pertinent
variable for such data.

A few points to consider when choosing the +subsetVariables+ list
are:

. If the number of distinct combinations of these variables is greater than
about 1,000,000, you should probably restrict the list of +subsetVariables+
to reduce that number.  The number of distinct combinations can be found
by multiplying together the number of distinct values for each variable.
For example (using the example immediately above), if you have three different values for the +station_id+ and
ten apiece for +longitude+ and +latitude+, then you have 300 distinct
combinations.  The number of combinations can increase quickly as more
variables are added.
If you don't do this, the +.subset+ web pages could
take quite a long time to be generated.

. If the number of distinct values of any single subset variable is greater
than about 20,000, you should probably omit that from the list of
+subsetVariables+.  If you don't do this, it could take a long time to
transmit some very large +.subset+, +graph+ and +.html+ pages to a user
requesting them.

. You should test the +subsetVariables+ setting for each dataset to see
if there might be a problem with the source data server.  When large
datasets are being obtained from, say, an MySQL server on another machine,
various uncontrollable circumstances can cause the data to load locally very slowly or
even not at all.  If this is the case, you can either reduce the number
of variables specifed or, in the worst case, remove the +subsetVariables+
global attribute entirely.

[red]*Warning*: If someone using the +.subset+ web page selects a value that
includes a carriage return or newline character, +.subset+ will fail.  Certain
limitations of HTML curtail the ability of ERDDAP to work around this problem.
The only remedy that will prevent this is to remove the carriage return and
newline characters from the data.  ERDDAP can be very helpful with this in
that it detects data values that will cause this problem, and will mail a list
of the offending values to anybody whose email address is included in
the xref:emailEverythingTo[+emailEverythingTo+] section of +setup.xml+.

There is a mechanism for ERDDAP to use pre-generated subset tables rather
than to dynamically create them via a data request to a remote server every
time a user makes a request.  This could be useful in the case of an
erratic or overloaded remote server.  The mechanism for doing this is
to supply a table with the subset information in a +.json+ or +.csv+
file with the name
-----
[tomcat_directory]/content/erddap/subset/[datasetID].[json|csv]
-----
If this file is present, ERDDAP will read it once when the dataset
is loaded and use it as the source of the subset data.  A few important
details to keep in mind about this possibility are:

. If there is an error while reading this file, the dataset will fail to load.

. The file must contain exactly the same column names (including case) as
+subsetVariables+, although the columns can be in any order.

. The file can contain extra columns, which will simply be removed.

. Time stamp columsn should have ISO 8601:2004(E) formatted date+timeZ
strings, e.g. +1985-01-31T15:31:00Z+.

. Missing values should be missing and not be replaced with fake numbers
like +-999+.

. A +.json+ file can be trickier to create than a +.csv+ file, but deals
with Unicode characters well.  ERDDAP can create a +.json+ file for you
to alleviate this possible difficulty.

. A +.csv+ file [red]*must* have column names on the first row and data
on subsequent rows, and are suitable only for data consisting of
ISO 8859-1 characters.

[[title]]
+title+
+++++++

The +title+ global attribute is [red]*required*, and contains
a short description of the dataset that's [green]*recommended* to be 80 characters or less.
This [red]*must* be present in either +sourceAttributes+ or in
the dataset's global +addAttributes+.  If the title is longer
than 80 characters, only the first and last 40 characters will be
visible in the list of datasets.  The entire title, though, can
be seen by moving the mouse cursor over the +?+ icon adjacent
to the title on the web page.

When ERDDAP presents lists of datasets, is lists them in alphabetical
order by +title+.  Thus, if you want to specify the order of your
datasets, or have some datasets grouped together, you have to create
appropriate titles to accomplish this within ERDDAP.  Be aware, though,
that while titles can be designed to group datasets together, they
should also still be able to descriptively stand on their own.
Category searches within ERDDAP, for instance, can present results
in an order much different than in the dataset lists so each title
must contain sufficient information to identify it by itself.

[[summary]]
+summary+
+++++++++

The +summary+ global attribute is [red]*required*, and contains a long
description of the dataset that's usually 500 characters or less.
This allows clients to read a description of the dataset with significantly
more detail than can be contained within the +title+, and thus better
understand what the dataset contains.
This [red]*must* be present in either +sourceAttributes+ or in
the dataset's global +addAttributes+.
ERDDAP displays the contents of the summary on the dataset's +Data Access
Form+, +Make a Graph+ web page, and other web pages.  It also uses this
summary when creating FGDC and ISO 19115 documents, so the more useful
you make it the more useful your dataset will be to others.

You are [green]*advised* to write the summary as if you were describing
it to a random person on the street rather than to someone intimately familiar
with such things.  Pertinent questions to answer via the +summary+ are:

. Who created the dataset? 
. What information was collected?
. When was the data collected?
. Where was the data collected?
. Why was the data collected?
. How was the data collected?

[[contributor_name]]
+contributor_name+
++++++++++++++++++

The +contributor_name+ global attribute is a [green]*recommended* way to
identify a person, organization or project that contributed to this
dataset.  For example, this could be the original creator of the data
before it was reprocessed by the creator of this particular dataset.
If this doesn't apply to a dataset, omit it.  This attribute is more
focused on the funding source than is the similar
xref:creator_name[+creator_name+].

[[contributor_role]]
+contributor_role+
++++++++++++++++++

The +contributor_role+ global attribute is a [green]*recommended* way
to identify the role of a xref:contributor_name[+contributor_name+].
An example would be:
-----
<att name="contributor_role">Source of Level 2b data.</att>
-----
If this doesn't apply to a dataset, omit it.

[[coverage_content_type]]
+coverage_content_type+
+++++++++++++++++++++++

The +coverage_content_type+ global attribute is a [green]*recommended*
way to identify to type of gridded data in EDDGrid datasets (and [red]*not* in
EDDTable datasets).  The only
allowed values are:

. +auxiliaryInformation+
. +image+
. +modelResult+
. +physicalMeasurement+ (the default, if not specified, when ISO 19115 information is generated)
. +qualityInformation+
. +referenceInformation+
. +thematicClassification+

[[creator_name]]
+creator_name+
++++++++++++++

The +creator_name+ global attribute is a [green]*recommended* way to identify
the person, organization or project (if not a specific person or organization)
most responsible for the creation - or most recent reprocessing - of this
dataset.  If, for example, the data was extensively reprocessed - e.g.
satellite data from level 2 to level 3 or 4), then the reprocessor should be
listed in the +creator_name+ and the original creator listed in the
+contributor_name+.  This is a more flexible attribute than xref:project[+project+] since
it may identify a person or organization as well as a project.

[[creator_email]]
+creator_email+
+++++++++++++++

The +creator_email+ global attribute is the [green]*recommended* way to
identify the email address of whoever or whatever is listed under
+creator_name+ so they or it might be contacted via email.

[[creator_url]]
+creator_url+
+++++++++++++

This

[[date_created]]
+date_created+
++++++++++++++

This

[[date_modified]]
+date_modified+
+++++++++++++++

This

[[date_issued]]
+date_issued+
+++++++++++++

This

[[featureType]]
+featureType+
+++++++++++++

This

Variable Attributes
^^^^^^^^^^^^^^^^^^^

Some variable attributes have special importance in ERDDAP.
They are described in this section.  These are not the only
variable attributes that can be specified, but they are the
ones that should be specified to ensure the maximum utility
of your datasets within ERDDAP.

[[actual_range]]
+actual_range+
++++++++++++++

This a [red]*recommended* attribute for the +addAttributes+ tag.
If present, it is an array of two values of the same datatype as the
variable that specify the actual - i.e. not theoretical or
allowed - minimum and maximum values of the data for that variable.
If either or both of the +scale_factor+ and +add_offset+ attributes
are present and therefore indicating that the data is packed,
the +actual_range+ values should be identically packed.

ERDDAP automatically determines the +actual_range+ values from
some data sources, but is unable to do this with some sources such as databases and
remote OPeNDAP sources.
It is thus [red]*recommended* that the actual range of the source
variables in the latter cases be set by adding an +actual_range+
attribute to the +addAttributes+ tag.  This is especially true for
such variables as +latitude+, +longitude+, +altitude+, +depth+
and +time+.

This information is essential for the usefulness of the +Data Access Form+
and +Make A Graph+ web pages for the dataset, and also for generating
the FGDC and ISO 19115 metadata.  When a user selects
a subset of data and requests a filetype such as NetCDF that includes
metadata, ERDDAP modifies the +actual_range+ values to correspond to
those in the subset.

An example of the use of this is:
-----
<addAttributes>
   <att name="actual_range" type="doubleList">-180 180</att>
</addAttributes>
-----
Where the range of the variable is from +-180+ to +180+, and
the optional +type+ value +doubleList+ indicates
that this is a space-separated list of double-precision
values.

[[data_min_and_data_max]]
+data_min+ and +data_max+
+++++++++++++++++++++++++

These are [red]*recommended* attributes 
for the +addAttributes+ tag as defined in the
http://woce.nodc.noaa.gov/wdiu/utils/netcdf/woce_conventions/report_nov_01_v3_mtg.htm[World
Ocean Circulation Experiment NetCDF metadata recommendations].
They are essentially the same as the +actual_range+ attribute except for
being two separate attributes with a single value rather than a single
attribute with a pair of values.
If a +scale_factor+ and/or a +data_offset+ are present to indicate
a packed dataset, these values must be identically packed.

[[drawLandMask]]
+drawLandMask+
++++++++++++++

The +drawLandMask+ attribute is an [green]*optional* one for the
+addAttributes+ tag.  It is used by ERDDAP - and not defined in
any of the metadata standards - to specify the default value for the
+Draw Land Mask+ option on the +Make A Graph+ web page, and also
for the +land+ parameter in a URL requesting a graph or map of the
data.

For variables in EDDGrid datasets, this specifies whether the land
mask on a map is drawn +over+ or +under+ the grid data.  It is 
[red]*recommended* to use +over+ for oceanographic data - so grid
data over land is obscured by the landmask - and +under+ for all
other types of data.

For variables in EDDTable datasets, +over+ makes the land mask on
a map visible, with land appearing as a uniform gray area.  The
+over+ value is commonly used for purely oceanographic datasets,
with +under+ making the land mask invisible (with topography
information displayed for ocean and land areas).  All other
data typically use +under+.

If no value or a value other than +over+ or +under+ is specified,
the xref:drawLandMask[+drawLandMask+] value from the dataset's
global attributes section is used.

[[history]]
+history+
+++++++++

The +history+ global attribute is a [green]*recommended* multi-line
global attribute with a line for every processing step that the data
has undergone.
The [green]*recommended* format is that each line has an
ISO 8601:2004(E) formatted date+timeZ, e.g. +1985-01-31T15:31:00Z+, followed
by a brief, concise description of the processing step.
If this doesn't exist, ERDDAP will create it; if it does
exist, ERDDAP will append new information to the existing
information.  This attribute is important in that it allows
data users to backtrack the present state of the data back
to its original state.

[[ioos_category]]
+ioos_category+
+++++++++++++++

The +ioos_category+ attribute is [red]*required* for the
+addAttributes+ tag if xref:variablesMustHaveIoosCategory[+variablesMustHaveIoosCategory+] is
set to +true+ in +setup.xml+.  Otherwise, it is
[green]*optional*.

As of this writing (May 24, 2013), the valid values for this
are +Bathymetry+, +Biology+, +Bottom Character+, +Colored Dissolved Organic Matter+,
+Contaminants+, +Currents+, +Dissolved Nutrients+, +Dissolved O2+, +Ecology+, +Fish
Abundance+, +Fish Species+, +Heat Flux+, +Hydrology+, +Ice Distribution+, +Identifier+,
+Location+, +Meteorology+, +Ocean Color+, +Optical Properties+, +Other+, +Pathogens+,
+pCO2+, +Phytoplankton Species+, +Pressure+, +Productivity+, +Quality+, +Salinity+, +Sea
Level+, +Statistics+, +Stream Flow+, +Surface Waves+, +Taxonomy+, +Temperature+, +Time+,
+Total Suspended Matter+, +Unknown+, +Wind+, +Zooplankton Species+, and +Zooplankton
Abundance+.

If you're stuck with having to specify one of these and don't know which one
to use, +Other+ is there for you.  It is useful, though, to choose one that
fits if it exists, since if you add +ioos_category+ to the list of
+categoryAttributes+ in the ERDDAP +setup.xml+ file, users will be
able to easily find datasets with similar data via the +Search for Datasets
by Category+ function on the ERDDAP home page.

[[long_name]]
+long_name+
+++++++++++

The +long_name+ attribute is [red]*recommended* for the +addAttributes+ tag.
ERDDAP will use it to label axes on graphs, and the graphs will be more
understandable to a wider range of users if a descriptive and reasonably
concise +long_name+ is specified.  The +long_name+ attribute is recommended by
many metadata standards including 
http://ferret.wrc.noaa.gov/noaa_coop/coop_cdf_profile.html[COARDS],
http://www.unidata.ucar.edu/software/netcdf/docs/netcdf.html#Attribute-Conventions[NetCDF],
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html[CF]
and
http://www.unidata.ucar.edu/software/netcdf-java/formats/DataDiscoveryAttConvention.html[ACDD].

Some [red]*recommended* best practices are to capitalize the words in a
+long_name+ as if it were a title, i.e. capitalize the first and
all non-article words, do not include units in the +long_name+, and
keep it to 20 characters long or under while attempting to make
it more fully descriptive than the +destinationName+.

If a +long_name+ isn't defined in either the +sourceAttributes+
or +addAttributes+ sections, ERDDAP will create one by cleaning
up the +standard_name+ (if present) or the +destinationName+.


[[missing_value]]
+missing_value+ and +_FillValue+
++++++++++++++++++++++++++++++++

The default value is +NaN+ for both the +missing_value+ and
+_FillValue+ attributes to +addAttributes+.
These attributes describe a number, e.g. +-9999+, used to
represent missing values in a dataset.  While these are
essentially the same, ERDDAP supports both since some data
sources assign slightly different meanings to them.

If either is present, is is [red]*required* that they be
the same datatype as the variable.  If the data is
packed with +scale_factor+ and/or +add_offset+, it is
[red]*required* that either or both should be identically
packed.  If either is actually used by a variable in
a dataset, then it is [red]*required* that they be present
as an attribute.

For some output data formats, ERDDAP will leave these special
numbers within the data intact.  For other formats, it will
replace them with +NaN+ or +"+.

[[scale_factor_and_add_offset]]
+scale_factor+ and +add_offset+
+++++++++++++++++++++++++++++++

The +scale_factor+ and +add_offset+ attributes are [green]*optional*
under the +addAttributes+ tag.  They describe data that is packed into a
simpler data type via simple multiplicative and/or additive
transformations.

If used, the data type is different from the source data type and
describes the data type of the destination values.
For example, a data source might store float data values with one decimal
digital packed as short integers (+int16+), using +scale_factor = 0.1+
and +add_offset = 0+.  That is,
-----
<att name="scale_factor" type="float">0.1</att>
<att name="add_offset" type="float">0</att>
-----
If these are present, ERDDAP will extract the values from the
attributes, remove the attributes, and automatically unpack the
data for the user. The process can be described as:
-----
destinationValue = sourceValue * scale_factor + add_offset
-----
That is, the process will be transparent to the
ERDDAP end user.

[red]*Warning*:  If the +scale_factor+ and/or +add_offset+
attributes are present - either in +sourceAttributes+ or
+addAttributes+ - but the value is left blank, bad things
will happen.  If you don't need or use them but must include
them for reasons of completeness or whatever, be sure to
specify default values of +1.0+ and +0.0+ for, respectively,
+scale_factor+ and +add_offset+, whether it be in the source
dataset from which the +sourceAttributes+ are extracted or
in the +addAttributes+ that you specify.

[[standard_name]]
+standard_name+
+++++++++++++++

The +standard_name+ attribute is [red]*recommended* under the
+addAttributes+ tag.  A list of standard names can be found
in the http://cf-pcmdi.llnl.gov/documents/cf-standard-names/[CF Standard
Names] document.

This is useful because if you add a +standard_name+ to
+addAttributes+ and also add +standard_name+ to the list
of +categoryAttributes+ in the ERDDAP +setup.xml+ file, users
can easily find datasets with similar data via the
+Search for Datasets by Category+ functionality on the
ERDDAP home page.

It is [red]*highly recommended* that you stick to using only the
terms on the http://cf-pcmdi.llnl.gov/documents/cf-standard-names/[CF Standard 
Names] list, and save any desire to go off reservation for requests
to the standards committee.  A controlled vocabulary is useful
to the extent that only the terms within it are used.

[[time_precision]]
+time_precision+
++++++++++++++++

The +time_precision+ attribute is [green]*optional* under the
+addAttributes+ tag.  It is used by ERDDAP - and specified by no
metadata standards - for time and timestamp variables.
It specifies the precision to be used when displaying time values
from the variable on web pages in ERDDAP.  The value values, in
order of increasing precision, are:

. +1970-01+
. +1970-01-01+
. +1970-01-01T00Z+
. +1970-01-01T00:00Z+
. +1970-01-01T00:00:00Z+ (default)
. +1970-01-01T00:00:00.0Z+
. +1970-01-01T00:00:00.00Z+
. +1970-01-01T00:00:00.000Z+

If +time_precision+ isn't specified or the specified value
isn't matched, the default value will be used.

[red]*Warning*:  A limited +time_precision+ should only be used
to the extent that all of the data values for the variable have
only the minimum value for all of the hidden fields.
For example, you can use a +time_precision+ value of
+1970-01-01+ if all of the data values have hour, minute and
second values of zero.  Example appropriate fields for this would
be:
-----
2005-03-04T00:00:00Z
2005-03-05T00:00:00Z
-----
Inappropriate time fields for this would be:
-----
2005-03-05T12:00:00Z
-----
If a +time_precision+ value of +1970-01-01+ is specified
with this time field present, and a user requests all time
data with +time=2005-03-05+, the request will fail.

[[units]]
+units+
+++++++

The +units+ attribute is [red]*required*
as either a +sourceAttribute+ or +addAttribute+ for
+time+ variables, and is [red]*strongly recommended*
for other variables when appropriate, i.e. almost
always.  The use of
http://www.unidata.ucar.edu/software/udunits/udunits-2/udunits2.html[UDUNITS]-compatible
units - as required by the COARDS and CF standards - is [red]*recommended*.
Another possible and common standard is
http://unitsofmeasure.org/trac/[Unified Code for Units of Measure (UCUM)] -
often referred to alternatively as Units of Measure (UOM) -
which is required by OGC services such as SOS, WCS and WMS.
Whichever standard is used by choice or necessity, it is
[red]*recommended* that the same standard be used for all datasets
within your ERDDAP server.  ERDDAP can be informed of the standard
used via the +units_standard+ tag in the +setup.xml+ file.

For time variables, either the automatically derived +sourceAttributes+ or user-specified +addAttributes+ tag
[red]*MUST* contain units.  There are two situations for this:

*Time Variables with Numeric Data*: For time axis variables or time data variables with numeric data, a
http://www.unidata.ucar.edu/software/udunits/udunits-2/udunits2.html[UDUNITS]-compatible
string - with the format +units since Basetime+ - describing how to interpret
source time values, e.g. +seconds since 1970-01-01T00:00:00Z+, where the base 
time is an ISO 8601:2004(E) formatted date-time string of the form
+yyyy-MM-dd'T'HH:mm:ssZ+.

*Time Variables with String Data*:  For time data variables with string data, an
+org.joda.time.format.DateTimeFormat+ string - which is mostly compatiable
with the +java.text.SimpleDateFormat+ format - describing how to interpret
string times, e.g. the +ISO8601TZ_FORMAT yyyy-MM-dd'T'HH:mm:ssZ+.
A +Z+ at the end of the format string tells Java/Joda/ERDDAP to look
for the character +Z+ - which indicates the Zulu time zone with a zero
offset - or to look for a time zone offset in the form
+hh:mm, +hh, -hh:mm or -hh.  Examples of string dates in this format are:
-----
2012-11-20T10:12:59-07:00
2012-11-20T17:12:59Z
2012-11-20T17:12:59
-----
all of which are equivalent in ERDDAP because its default time
zone is Zulu.  Other examples are:
-----
2012-11-20T10:12 (missing seconds are assumed to be 0)
2012-11-20T17 (missing minutes are assumed to be 0)
2012-11-20 (missing hours are assumed to be 0)
2012-11 (missing date is assumed to be 1)
-----

The main time data variable for EDDTable datasets and the main
time axis variable for EDDGrid datasets are recognized by the
+destinationName+ time specification and the +units+ metadata
therein, which must be suitable or bad things will happen.

For tabular datasets, there can be time variables other
than the main timeStamp variable.
They will behave like the main timeStamp variable - converting the
source's time format into +seconds since 1970-01-01T00:00:00Z+
and/or ISO 8601:2004(E) format - but will have a different
+destinationName+.  TimeStamp variables are recognized by
their +units+ metadata, which must contain +since+ for numeric
dateTimes or +yy+ or +YY+ for formatted string dateTimes.
It is [red]*required* to use +time+ for the +destinationName+
for the main dateTime variable.

[[dataVariable]]
Data Variable Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The +dataVariable+ tag is [red]*required* for almost all datasets within the
+dataset+ tag, and is used to describe a data variable.
There must be one of more instances of this tag within each +dataset+ tag,
and it supports several subtags.

[[sourceName]]
+sourceName+
^^^^^^^^^^^^

The +sourceName+ tag is [red]*required* within the +dataVariable+ tag, and
is the name ERDDAP uses when requesting data from the data source.  This is
the name that ERDDAP will look for when data is returned from the data
source, and is case sensitive.

You can create a variable with a fixed value that isn't in the source
dataset via:
-----
<dataVariable>
  <sourceName>=fixedValue</sourceName>
</dataVariable>
-----
where the equals sign before +fixedValue+ tells ERDDAP that a fixed
value will follow.  For example, if you want to create a +variable+
called +altitude+ with a float value of 0.0, use:
-----
<dataVariable>
  <sourceName>=0</sourceName>
  <destinationName>altitude</destinationName>
  <dataType>float</dataType>
</dataVariable>
-----
where we've used a couple of tags whose use and necessity will be explained
in following sections.

+destinationName+
^^^^^^^^^^^^^^^^^

The +destinationName+ tag is [green]*optional* within the +dataVariable+ tag,
and is the name for the data variable that will be shown to and used by
ERDDAP users.  This can be the same or different than the +sourceName+,
and if it is absent the +sourceName+ will be used for it.
The availablity of separate source and destination names is useful in that
it allows one to change a cryptic, odd, or inscrutably terse source name
into something easier for ERDDAP users to understand.

A +destinationName+ is case sensitive and must start with an upper- or
lower-case letter, and be followed by zero or more of the characters
+A-Z+, +a-z+, +0-9+ and +_+.  Dashes are [red]*disallowed* - although they
were permitted up to version 1.10 of ERDDAP - since they
limit the portability of variable names to programming languages such as
Matlab.

+dataType+
^^^^^^^^^^

The +dataType+ tag is [red]*required* by some dataset types and ignored
by others.  The dataset types that [red]*require* this for their
+dataVariable+ tags are +EDDGridFromXxxFiles+, +EDDTableFromXxxFiles+,
+EDDTableFromMWFS+, +EDDTableFromNOS+ and +EDDTableFromSOS+.
The other dataset types ignore this tag since they get their datatype
information from the dataset source.
The valid values are:

. +double+ - 64-bit
. +float+ - 32-bit
. +long+ - 64-bit signed
. +int+ - 32-bit signed
. +short+ - 16-bit signed
. +byte+ - 8-bit signed
. +char+ - essentially 16-bit unsigned
. +boolean+
. +string+ - any length

The +boolean+ type is a special case since:

. ERDDAP doesn't support this type sine booleans can't store missing values;
and
. DAP doesn't support booleans so there is no standard way to query
boolean variables.

Thus, if the +dataType+ is specified as +boolean+, the +boolean+ values
will be stored and represented as bytes, i.e. 0=false, 1=true.
Clients can therefore specify constraints via the numeric values, with
an example being +isAlive=1+.
ERDDAP administrators need to use the boolean +dataType+ in 
+datasets.xml+ to tell ERDDAP how to interact with the data source.

+addAttributes+
^^^^^^^^^^^^^^^

The +addAttributes+ tag is [green]*optional*, and enables one to define
a set of attributes to be added to those defined in the +sourceAttribute+ tag
to create a combined set of attributes.  These are fully described in
the separate xref:addAttributes[Dataset and Variable Attributes] section.

[[how_erddap_handles_time]]
How ERDDAP Handles Time
~~~~~~~~~~~~~~~~~~~~~~~

The needs of ERDDAP concerning time variables are detailed at:

http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html[+http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html+]

The goal of ERDDAP is to have a single system that allows time from any dataset to be
directly compared to time data from any other dataset.  The consequences of
this goal are some fairly strict requirements:

. ERDDAP only deals with time points, i.e. a combined date/time for each
point in time.

. ERDDAP always uses the Zulu (UTC,GMT) time zone and never uses daylight
savings times.  If a data source has other time zone and/or DST information, ERDDAP will convert
it to Zulu time.

. ERDDAP deals with time internally as "+seconds since 1970-00-00T00:00:00Z+",
with the times stored as double precision floating point numbers.

. ERDDAP uses the http://en.wikipedia.org/wiki/ISO_8601[+ISO 8601+] extended
format string truncated to the second.  An example is +2011-04-26T14:07:12Z+.

. The Gregorian Calendar is used for times after 1582, and the Julian Calendar
for times before that (although this doesn't follow the
http://en.wikipedia.org/wiki/ISO_8601[+ISO 8601+] standard).  Basically,
ERDDAP uses the
http://docs.oracle.com/javase/6/docs/api/java/util/GregorianCalendar.html[Java
Gregorian calendar class].  For example, 17611992 hours since
+0001-01-01T00:00:00Z+ is equivalent to +2010-03-01T00:00:00Z+.

. No other calendars (e.g. noleap, 365_day, 360_day, Julian calendars as
defined by the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#calendar[CF
metadata conventions]) are supported, although such information can be stored
in a variable named something other than +time+.  This variable [red]*should not* include the
word +since+ in the metadata for +units+ to prevent ERDDAP from
automatically converting the information into the +seconds since
1970-01-01T00:00:00Z+ format.

. As specified in the ISO 8601 standard, the AD era is used for year numbers.
While a normal Julian sequence of years near the transition is 2 BC,
1 BC, 1 AD, 2 AD, ERDDAP specifies these as -0001, 0000, 0001, 0002,
respectively.  The general formula is ERDDAP_Year = 1 - BC_Year.

. Whenever possible, ERDDAP uses the
http://en.wikipedia.org/wiki/Coordinated_Universal_Time[Coordinated Universal
Time (UTC)] system, where UTC is the modern version of the traditional GMT
time system.

. Leap seconds are quasi-handled in a way about which you almost certainly
don't want to know the
http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html#erddap[details]
unless it's absolutely necessary.

. If time data was recorded to a resolution lower than a second, or represents
a time span rather than a time point, a time point called a *nominal time* must be chosen to represent
that time or time span.  This allows time points, low resolution time points,
and time spans to be interoperable.  It is most common and recommended to use
the start time or centered time for time spans.  This should be
identified via +long_name+ metadata attribute, e.g. +Start Time+ or +Centered Time+.
For low resolution times, use +Date+, +Month+ or +Year+ for +long_name+ and/or
place the details in the +comment+ metadata.

A helpful online, interactive tool that converts numeric times to and from
string times is found at the top of:

http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html[+http://coastwatch.pfeg.noaa.gov/erddap/convert/time.html+]

It looks like this:

image::erddap/erddap_time.png[width=800]

Dataset Type Details
--------------------

[[EDDGridFromFiles]]
+EDDGridFromFiles+
~~~~~~~~~~~~~~~~~~

The +EDDGridFromFiles+ class is the superclass for all
+EDDGridFrom...Files+ classes, of which there presently only
exists xref:EDDGridFromNcFiles[+EDDGridFromNcFiles+].
This class is not directly used to create a dataset.
The subclasses are used and inherit the requirements and
features of this superclass.

Currently only the xref:EDDGridFromNcFiles[+EDDGridFromNcFiles+] class
exists, although it can handle GRIB and HDF as well as NetCDF gridded
files, and probably whatever other gridded files are supported
by the http://www.unidata.ucar.edu/software/netcdf-java/[NetCDF Java
library].  It is [green]*recommended* that gridded files not supported
be converted in NetCDF files, since that format is widely supported and
used, and for which many tools and clients are available in addition
to ERDDAP.

File Aggregation
^^^^^^^^^^^^^^^^

The +EDDGridFromFiles+ class *aggregates* data from *local* files, with
the resulting virtual dataset appearing as if the data from all of
the aggregated files were in one file.  The local files being aggregated
[red]*must* have the same +dataVariables+ as defined in +datasets.xml+,
and all of the +dataVariables+ [red]*must* also use the same
+axisVariables+/dimensions as defined in the configuration file.

The files will be aggregated based on the first or left-most
dimension, sorted in ascending order.  Each file may have data for one or
more values of the first dimension, but there [red]*cannot* be any overlap
between files.
If a file has more than one value for the first dimension, the
values [red]*must* be sorted in ascending order, with no ties.
All files [red]*must* have exactly the same +units+ metadata for
all +axisVariable+ and +dataVariable+ tags.  A typical situation has
a set of files with the dimensions:
-----
[time][altitude][latitude][longitude]
-----
with each file having a single +time+ entry for, say, the data
for a given hour or day.  There can be multiple time values in 
each file, as long as there is no overlapping between files and
the times are in ascending order within each file.

Aggregation of many actual files into a single virtual file presents
some useful advantages including:

* the size of an aggregated or virtual dataset can be much larger
than the size limits of any filesystem or program, since the
aggregated file is just a set of links to the appropriate actual
files; and

* the ease of adding near-real-time data to an aggregate or virtual
dataset wherein a new actual file is simply added to a directory
and another link created to the aggregate dataset.

The actual files [green]*may* be in a single directory, or in
a directory and its subdirectories (recursively).  If there are a
large number of files - e.g. 1000 or more - the operating system
and +EDDGridFromFiles+ will operate more efficiently if the files
are stored in a series of subdirectories.

[[cache]]
[[caching]]
File Information Caching
^^^^^^^^^^^^^^^^^^^^^^^^

Information about individual files is *cached* when an
+EDDGridFromFiles+ dataset is first loaded.  Information about
each actual file is gathered and dynamic memory tables are created from it.
The tables are also stored on disc as +.json+ files in
-----
[bigParentDirectory]/dataset
-----
in files named:

* +[datasetID].dirs.json+ - which holds a list of unique directory names
* +[datasetID].files.json+ - which holds the table with each valid file's
information;
* +[datasetID].bad.json+ - which holds the table with each bad file's
information.

The tables stored on disc save time and resources since when
ERDDAP is restarted it reads the file information from the tables rather than
attempting to read all the files again.
If any changes are made to the (sub)directories containing the
actual datasets, the stored information can be updated via the
use of the xref:flag[flag] system to force ERDDAP to update
the cached information.

Reloading and Updating
^^^^^^^^^^^^^^^^^^^^^^

The cached information is updated whenever the dataset is reloaded.
Mechanisms that can induce reloading are:

* periodic reloading as per the +reloadEveryNMinutes+ tag in
+datasets.xml+;
* situational reloading as soon as possible whenever ERDDAP detects that
you have added, removed, http://en.wikipedia.org/wiki/Touch_%28Unix%29[touched], or otherwise changed a data file; and
* forced reloading as soon as possible via the xref:flag[flag] system.

When a dataset is reloaded, the actions of ERDDAP include:

* comparing the currently available files to the cached file information
tables;
* reading new files and adding to the valid files table;
* dropping files that no longer exist from the valid files table;
* reading and updating the information of files whose timestamps
have changed;
* replacing the old tables in memory and on disk;
* emailing a table of bad files and the reasons the files were declared
bad (e.g. corrupted file, missing variables, etc.) to those included
in the xref:emailEverythingTo[+emailEverythingTo+] tags.

Addition of New Files
^^^^^^^^^^^^^^^^^^^^^

If new data files are added to the ERDDAP server via +scp+ or +ftp+ while
it is running, there is a nonvanishing and fairly good chance that ERDDAP
will be reloading the dataset during the downloading process.
If this happens, the file will appear to be valid (since it has a valid
name), but it isn't.  If ERDDAP attempts to read data from the invalid
file, the result error will cause the file to be added to the table of
invalid files and not become part of the aggregate dataset.
This is not good, and it is [green]*recommended* to use a temporary
name when transferring the file that will cause the +fileNameRegex+
test to ignore it.  For example, if it is looking for files of the form +.nc+
an effective temporary name would be +file.nc_TMP+.  The file can
be appropriately renamed - that is, the +_TMP+ removed - after the
downloading process has completed.  This will cause it to become
immediately relevant to ERDDAP and appropriately processed.

Skeleton XML Code
^^^^^^^^^^^^^^^^^

A skeleton XML code for all +EDDGridFromFiles+ subclasses is:
[source,xml]
-----
<dataset type="EDDGridFrom...Files" datasetID="..." active="..." >
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit> 
  <fileDir>...</fileDir> <-- The directory (absolute) with the data files. -->
  <recursive>true|false</recursive> <-- Indicates if subdirectories
    of fileDir have data files, too. -->
  <fileNameRegex>...</fileNameRegex> <-- A regular expression 
    (tutorial) describing valid data files names, 
    e.g., ".*\.nc" for all .nc files. -->
  <metadataFrom>...</metadataFrom> <-- The file to get 
    metadata from ("first" or "last" (the default) based on file's 
    lastModifiedTime). -->
  <addAttributes>...</addAttributes>
  <axisVariable>...</axisVariable> <!-- 1 or more -->
  <dataVariable>...</dataVariable> <!-- 1 or more -->
</dataset>
-----

[[EDDGridFromNcFiles]]
+EDDGridFromNcFiles+
~~~~~~~~~~~~~~~~~~~~

The +EDDGridFromNcFiles+ dataset type aggregates data from local, gridded
GRIB +.grb+ and +.grb2+ files, HDF +.hdf4+ and +.hdf5+ files and NetCDF
+.nc+ files.  Other gridded files supported by the
http://www.unidata.ucar.edu/software/netcdf-java/[NetCDF Java library] may
also
work, although this has yet to be tested.
For GRIB files, ERDDAP will create a +.gbx+ index file the first time
it reads each GRIB file, so the GRIB files must be in a directory
where the Tomcat user has both read and write permissions.

See the superclass xref:EDDGridFromFiles[+EDDGridFromFiles+] for the requirements
and features common to all datasets in the +*Files+ classes.

[[EDDGridFromDap]]
+EDDGridFromDap+ 
~~~~~~~~~~~~~~~~

The +EDDGridFromDap+ dataset type handles grid variables from DAP servers.
It can get data from any multi-dimensional variable from a DAP server.
More information about the OPeNDAP protocol and DAP servers can be found at
the http://docs.opendap.org/index.php/QuickStart[OPeNDAP Quick Start Guide]
and at
http://wiki.esipfed.org/index.php/Making_Science_Data_Easier_to_Use_with_OPeNDAP[Making
Science Data Easier to Use with OPeNDAP].

Obtaining Configuration Information from a DAP Server
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The information needed to create or complete the XML code for an
+EDDGridFromDap+ dataset can be found by inspecting the source
dataset's DDS and DAS files in your browser.  You can do this by
adding +.das+ and +.dds+ to the xref:sourceUrl[+sourceUrl+].
Appending +.das+ to the URL will show you the data's
*Dataset Descriptor Structure (DDS)*, which provides a
description of the shape (i.e. variables and dimensions) of the data. For example, the URL

http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.dds[+http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.dds+]

obtains the following list of variables and their dimensions for the +5day.nc+
dataset:
-----
Dataset {
    Float64 altitude[altitude = 1];
    Float64 lat[lat = 1501];
    Float64 lon[lon = 3601];
    Float64 time[time = 3658];
    Grid {
     ARRAY:
        Float32 BAssta[time = 3658][altitude = 1][lat = 1501][lon = 3601];
     MAPS:
        Float64 time[time = 3658];
        Float64 altitude[altitude = 1];
        Float64 lat[lat = 1501];
        Float64 lon[lon = 3601];
    } BAssta;
} satellite/BA/ssta/5day;
-----

Appending +.das+ to the URL will show you the data's
*Data Attribute Structure (DAS)*, which provides you with
the attributes used to describe each of the variables within the dataset.
For example, the URL

http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.das[+http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.das+]

provides the following attribute information for the +altitude+ variable in
the +5day.nc+ dataset:
-----
...
    altitude {
        Float64 actual_range 0.0, 0.0;
        Int32 fraction_digits 0;
        String long_name "Altitude";
        String positive "up";
        String standard_name "altitude";
        String units "m";
        String axis "Z";
        String _CoordinateAxisType "Height";
        String _CoordinateZisPositive "up";
    }
...
-----

The +OPeNDAP Dataset Access Form+ can be obtained by appending +.html+ to
our URL, e.g.

http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.html[+http://thredds1.pfeg.noaa.gov/thredds/dodsC/satellite/BA/ssta/5day.html+]

which provides a web interface for selecting variables from the dataset,
constraining them in time and space, and obtaining them in ASCII or binary
format.

Skeleton XML Code
^^^^^^^^^^^^^^^^^

The skeleton XML code for an +EDDGridFromDap+ dataset is:

[source,xml]
-----
<dataset type="EDDGridFromDap" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <updateEveryNMillis>...</updateEveryNMillis>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <addAttributes>...</addAttributes>
  <axisVariable>...</axisVariable> <!-- 1 or more -->
  <dataVariable>...</dataVariable> <!-- 1 or more -->
</dataset>
-----

[[EDDGridFromErddap]]
+EDDGridFromErddap+ 
~~~~~~~~~~~~~~~~~~~

See the xref:EDDTableFromErddap[+EDDTableFromErddap+] section for
an explanation of the +EDDGridFromErddap+ dataset type, since the
two work identically.

The skeleton XML for an +EDDGridFromErddap+ dataset is:
[source,xml]
-----
<dataset type="EDDGridFromErddap" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <updateEveryNMillis>...</updateEveryNMillis>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
</dataset>
-----

and is short because it simply mimics the remote
dataset which is already suitable for use in ERDDAP.

[[EDDGridFromEtopo]]
+EDDGridFromEtopo+
~~~~~~~~~~~~~~~~~~

The +EDDGridFromEtopo+ dataset type just serves the
http://www.ngdc.noaa.gov/mgg/global/global.html[ETOPO1 Global 1-Minute
Gridded Elevation Data Set] that is distributed with ERDDAP.
Two xref:datasetID[+datasetID+] values  are supported for this
to allow the data to be accessed via longitude values ranging from
either -180 to 180 or 0 to 360.
No subtags are needed since the data is already described within
ERDDAP.  Examples showing the full xref:dataset[+dataset+] tag
for +EDDGridFromEtopo+ for both longitude value options are:
[source,xml]
-----
<dataset type="EDDGridFromEtopo" datasetID="etopo180" />
-----
for serving the data from longitude range -180 to 180, and
[source,xml]
-----
<dataset type="EDDGridFromEtopo" datasetID="etopo360" />
-----
for serving the data from 0 to 360.


[[EDDGridSideBySide]]
+EDDGridSideBySide+
~~~~~~~~~~~~~~~~~~~

The xref:EDDGridSideBySide[+EDDGridSideBySide+] dataset type aggregates two or
more EDDGrid datasets (the children) side by side.
The resulting dataset contains all of the variables from all of the child
datasets.
This can be useful for combining a source dataset containing the u-component
of a vector with another source dataset containing the v-component, so
the full vector information can be served.

Requirements and features of this dataset type are:

. The parent and child datasets [red]*must* have different +datasetID+ values,
as dataset will fail to load in any names in a family are exactly the same.
If this is violated, it will throw an error message that the values of
the aggregated aixs are not in sorted order.

. The children [red]*must* all have the same source values within
the +axisVariable+ (1+) tags, e.g. +latitude+ and +longitude+.

. The children may have different source values for the
+axisVariable+ (0) tag, e.g. +time+, although these are
usually the same.  The parent dataset will appear to have all of the
+axisVariable+ (0) tags from all of the children.

. The only allowed subtags are +dataset+ tags specifying the
child datasets.

. The global metadata and settings for the parent comes from the global
metadata and settings of the first child.

. If there is an exception while creating the first child, the parent will
not be created.

. If there is an exception while creating other children, an email is sent
to xref:emailEverythingTo[+emailEverythingTo+] as specified in +setup.xml+, and ithen the other
children
are processed.

A skeleton XML for an xref:EDDGridSideBySide[+EDDGridSideBySide+] dataset is:
[source,xml]
-----
<dataset type="EDDGridSideBySide" datasetID="..." active="..." >
  <dataset>...</dataset> <!-- 2 or more -->
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
</dataset>
-----

[[EDDGridAggregateExistingDimensions]]
+EDDGridAggregateExistingDimension+
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDGridAggregateExistingDimension+ dataset type aggregates two or more
EDDGrid datasets based on different values of the first dimension.
For example, one child dataset might be for 2004 with 366 daily values, and
the other from 2005 with 365 daily values.
The requirements and features are:

* The values for all of the other dimensions - e.g. longitude, latitude, etc.
-
[red]*must* be identical for all of the children.

* The parent and child datasets [red]*must* have different values for
the +datasetID+ tag.  If any of the names are identical, the dataset
will fail to load and throw an error message that th evalues of the
aggregated axis are not in sorted order.

* The first child dataset [red]*must* be an
xref:EDDGridFromDap[+EDDGridFromDap+]
dataset and [red]*must* have the lowest values of the aggregated
dimension, e.g. the oldest time values.  All of the other children
[red]*must* be almost identical datasets, differing only in the values
for the first dimension, and are specified by their +sourceUrl+.

* The aggregate dataset gets its metadata from the first specified child.

* An [green]*optional* +ensureAxisValuesAreEqual+ tag can have the
values +true+ or +false+.  The default is +true+ and specifies that the
non-first-axis values [red]*must* be exactly equal in all children.
If this is +false+, then some minor variations are allowed, e.g. this
would allow a value of 0.1 in one child and a value of 0.1000000002
for an analogous value in another child.  The +false+ value should only
be used if you need to, and if you are ceretain that the variation that
is present is acceptable.

The xref:GenerateDatasetsXml[+GenerateDatasetsXml+] program can produce a
rought draft of the +datasets.xml+ entry for an
+EDDGridAggregateExistingDimension+ dataset based on a set of files
served by a Hyrax or THREDDS server.  An example of the input for
the program for this situation is:
-----
EDDType?
  EDDGridAggregateExistingDimension

Server type (hyrax or thredds)?
  hyrax

Parent URL (e.g., for hyrax, ending in "contents.html"; for thredds, ending in
"catalog.xml")?
  http://dods.jpl.nasa.gov/opendap/ocean_wind/ccmp/L3.5a/data/flk/1988/contents.html

File name regex (e.g., ".*\.nc")?
  month.*flk\.nc\.gz

ReloadEveryNMinutes (e.g., 10080)?
  10080
-----
You can use the resulting xref:sourceUrl[+sourceUrl+] tags or delete them and
uncomment the +sourceUrl+ tag so that new files are noticed each time
the dataset is reloaded.

The skeleton XML for an +EDDGridAggregateExistingDimension+ dataset is:
[source,xml]
-----
<dataset type="EDDGridAggregateExistingDimension" datasetID="..." 
    active="..." >
  <dataset>...</dataset> <!-- This is a regular EDDGridFromDap 
    dataset description child with the lowest values for the aggregated dimensions. -->
  <sourceUrl>...</sourceUrl> <!-- 0 or many; the sourceUrls for 
    other children.  These children must be listed in order of ascending values 
    for the aggregated dimension. -->
  <sourceUrls serverType="..." regex="..." recursive="true">http://someServer/someDirectory/someSubdirectory/catalog.xml</sourceUrls> 
    <!-- 0 or 1. This specifies how to find the other children, instead 
    of using separate sourceUrl tags for each child.  The advantage of this
    is: new children will be detected each time the dataset is reloaded. 
    The serverType must be "thredds", "hyrax", or "dodsindex".  
    An example of a regular expression (regex) (tutorial) is .*\.nc 
    recursive can be "true" or "false".  
    An example of a thredds catalogUrl is
    http://thredds1.pfeg.noaa.gov/thredds/catalog/Satellite/aggregsatMH/chla/catalog.xml
    An example of a hyrax catalogUrl is
    http://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.5a/monthly/flk/1988/contents.html
    An example of a dodsindex URL is . An example is
    http://www.marine.csiro.au/dods/nph-dods/dods-data/bl/BRAN2.1/bodas/ 
    (Note the "DODS Index of /..." at the top of the page.)
    When these children are sorted by file name, they must be in order of
    ascending values for the aggregated dimension. -->
  <ensureAxisValuesAreEqual>true(the default) or 
    false</ensureAxisValuesAreEqual> 
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
</dataset>
-----

[[EDDGridCopy]]
+EDDGridCopy+
~~~~~~~~~~~~~

The +EDDGridCopy+ data type or class makes and maintains a local copy of
data from another EDDGrid and serves the data from the local copy.
This data type is easy to use and was created because serving data
from remote sources can be slow due to the remote server being
inefficient or overwhelmed by too many requests, or because bandwidth
is limited on one or both ends.  The remote dataset may also be
intermittently available for a variety of reasons.
Basically, relying on just one source for data just
http://stackoverflow.com/questions/936461/what-does-it-mean-to-say-that-a-framework-scales-well[doesn't
scale well].

+EDDGridCopy+ works by automatically creating and maintaining a local
copy of a dataset, and serving data from that local copy.
This allows ERDDAP to serve the data very quickly, and relieves the
burden from the remote server.  The local copy also effectively serves
as a backup for the original data, which is a good thing.
This class makes it easy to create and maintain a local copy of
data from a variety of types of remote data sources, and also
to add extra metadata while copying the data.

The local copy is made by requesting *chunks* of data from the
remote dataset, with one chunk corresponding to a single value
of the leftmost axis variable.  +EDDGridCopy+ doesn't use the
remote dataset's index numbers for that axis since those may
change.  Some [red]*warnings* apply:

* If the size of individual chunks of data are big
enough to cause problems - usually when they're bigger than
about 1 GB - +EDDGridCopy+ cannot be used.  This is due to the
size limitations of some parts of ERDDAP, and a solution is being
sought to limit this problem.

* If a given value of the leftmost axis variable disappears from
the remote dataset, +EDDGridCopy+ does not delete the local
copied file.  This has to be done manually.

* +EDDGridCopy+ assumes that the data values for each chunk
never change.  If they do, you have to manually delete the chunk
files located in +[bigParentDirectory]/copy/datasetID/+ that
changed, and xref:flag[flag] the dataset to be reloaded so the
deleted chunks can be replaced.  If you have an email subscription
to the dataset, you will get an email when the dataset first
reloads and starts to copy the data, and another when the dataset
loads again (automatically) and detects the new local data files.

Once obtained, each chunk of data is stored locally in a separate
NetCDF file in a subdirectory of
-----
[bigParentDirectory]/copy/datasetID/
-----
as is specified within +setup.xml+.  File names created from axis
values are modified to make them file-name-safe - e.g. hyphens
are replaced by +x2D+ - but this doesn't affect the actual data.

Each time +EDDGridCopy+ is reloaded, it checks the remote
datasets to see what chunks are available.  If a file for a given
chunk of data doesn't already exist locally, a request to obtain
the chunk is added to a queue.  An internal ERDDAP service called
+taskThread+ processes the queued requests for data chunks
one-by-one.  Statistics for the activity of +taskThread+ can
be found on the xref:Status_Page[Status Page] as well as in
the xref:daily_report[Daily Report].  The first time an
+EDDGridCopy+ is loaded, many requests for chunks of data
will be added to the queue, but no local data files will have
yet been created.  Therefore, the constructor will initially
fail but +taskThread+ will continue to work and eventuall create
the local files.  Some local files should be created in about
15 minutes or so, and the next attempt to reload the dataset
should succeed, albeit initially with a limited amount of data.

[red]*Warning*: If the remote dataset is very large and/or the
server is slow, it could take a very long time to create a complete
local copy.  For example, transmitting 1 TB of data over a T1
line with a bandwidth of 0.15 GB/s will take at least 60 days under
optimal conditions.  This will also use a considerable amount
of bandwidth, memory and CPU time on both the local and remote
computers.  One way around this is to mail an actual hard drive
to the administrator of the remote dataset so they can make a
copy of the dataset on it and mail it back to you.  This will
almost certainly take less than 60 days.  This is a solution used
by even the http://aws.amazon.com/importexport/[Amazon EC2
Cloud Service], which has about as much bandwidth as anybody.
Hard drives have simply become much bigger and cheaper faster
than bandwidth.

The [green]*recommended* usage of +EDDGridCopy+ is:

* Create the +dataset+ entry - the native type, not +EDDGridCopy+ - for
the remote data source, and get it working correctly with all the
desired metadata.

* If you find that it is working too slowly, add XML code to create an
+EDDGridCopy+ dataset for it, making sure to use a different
+datasetID+.

* You [red]*must* copy the +accessibleTo+, +reloadEveryNMinutes+ and +onChange+
tags from the remote EDDGrid's XML to the XML of
+EDDGridCopy+.

* Wait for ERDDAP to create the local copy, keeping in mind
the warnings described above.

If you need to change any of the metadata such as any of the
+addAttributes+, or change the order of the variables associated
with the source dataset, you must:

* change the +addAttributes+ for the source dataset in +datasets.xml+ as
needed;

* delete one of the copied file chunks;

* set a xref:flag[flag] to reload the dataset immediately, which will
send you a couple of emails showing the progress of the procedure if
you have a subscription to the dataset; and

* the deleted file will be regenerated containing the new metadata,
with the metadata obtained from the regenerated file by ERDDAP if the
source dataset becomes unavailable (since it is the youngest file).

A skeleton XML file for an +EDDGridCopy+ dataset is:
[source,xml]
-----
<dataset type="EDDGridCopy" datasetID="..." active="..." >
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <fileTableInMemory>...</fileTableInMemory> <!-- 0 or 1 (true or false (the default)) -->
  <dataset>...</dataset> <!-- 1 -->
</dataset>
-----

[[EDDTableFromAsciiFiles]]
+EDDTableFromAsciiFiles+
~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromAsciiFiles+ dataset type aggregates data from
comma-, tab- or space-deleted tabular ASCII data files.

A common convention is that the file will have column names in the
first row, with data starting on the second row.  The is the expected
default, although it can be changed via the +columnNamesRow+ and
+firstDataRow+ tags.  If you have a large number of such ASCII files,
it is recommended that you combine them into NetCDF files - with one dimension
+row+ shared by all variables - and use
the xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] dataset type
to serve them.

The xref:EDDTableFromFiles[+EDDTableFromFiles+] superclass section
describes the requirements common to all +EDDTableFrom*Files+ dataset
types.

[[EDDTableFromAwsXmlFiles]]
+EDDTableFromAwsXmlFiles+
~~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromAwsXmlFiles+ dataset type aggregates data from a set of
Automatic Weather Station (AWS) XML files.
This type of file is a simple but inefficient way to store such data, since
each file usually seems to contain the observations from just a single
time point.  This could lead to a very large number of files over a
significant amount of time.
If you have such a large number of files, you should consider
consolidating groups of observations - e.g. a full day, week or
month's worth - in NetCDF files and using
xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] to serve the
data.

The xref:EDDTableFromFiles[+EDDTableFromFiles+] superclass section
describes the requirements common to all +EDDTableFrom*Files+ dataset
types.  The
http://developer.weatherbug.com/docs/read/WeatherBug_Rest_XML_API[WeatherBug
REST XML API] contains the details about how such files are constructed,
and thus how they might be read and understood.

[[EDDTableFromDapSequence]]
+EDDTableFromDapSequence+
~~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromDapSequence+ dataset type handles variables within 1- and
2-level sequences from http://opendap.org/[DAP] servers such 
as http://www.epic.noaa.gov/epic/software/dapper/[DAPPER].
The DAPPER server implements a set of 
http://www.epic.noaa.gov/epic/software/dapper/dapperdocs/conventions/[conventions] for accessing in-situ
geophysical data via the DAP protocol.  In-situ data consists of a set
of measurements of physical quantities sampled at irregular space and
time coordinates.  Examples include time series of weather data collected by
weather stations, CTD measurements of ocean salinity and temperature as a
function of depth, and radiosonde measurements of atmospheric temperature and
pressure as a function of height.
These conventions were developed to supplement those that already exist
for gridded data which do not work very well with in-situ data.

Each dataset is represented as a DAP two-level nested sequence.  The sequence
consists of an outer and an inner sequence, where the inner sequence is
contained by the outer sequence.  The outer sequence contains a latitude,
longitude, unique ID number, and either a height/depth variable or
a time variable, e.g. +lat+, +lon+ and +time+ in the example below.
The inner sequence contains all the measurement
variables, e.g. +temp+ and +salinity+ in the example below,
and either a time or a height/depth variable depending on which
is defined in the outer sequence, e.g. +depth+ in the below
example since +time+ is in the outer sequence.

As with the xref:EDDTableFromDap[+EDDTableFromDap+] dataset type, information
required to create or complete the xref:dataset[+dataset+] tag for a dataset
can be found by inspecting the source dataset's DDS and DAS files.
An example DDS file for a DAPPER-served sequence is:
-----
Dataset {
    Sequence {
        Float32 lat;
        Float64 time;
        Float32 lon;
        Int32 _id;
        Sequence {
            Float32 temp;
            Float32 salinity;
            Float32 depth;
        } profile;
        Structure {
            String title;
            String data_origin;
            String source;
            String Conventions;
            String country_code;
            String cruise;
            String cruise_number;
            String cast_number;
        } attributes;
        Structure {
            Structure {
                Float32 valid_range[2];
            } depth;
        } variable_attributes;
    } location;
    Structure {
        Float32 lon_range[2];
        Float32 lat_range[2];
        Float32 depth_range[2];
        Float64 time_range[2];
    } constrained_ranges;
} northPacific;
-----
The variable is in a DAP sequence if the information returned shows it
to be within a +Sequence+ category, which in the above response is:
-----
        Sequence {
            Float32 temp;
            Float32 salinity;
            Float32 depth;
        }
-----
The DAS information for the same file is:
-----
Attributes {
    NC_GLOBAL {
        Int32 max_profiles_per_request 15000;
        Int32 total_profiles_in_dataset 1707616;
        String version "1.1.0";
        String owner "";
        String contact "";
        String Conventions "epic-insitu-1.0";
        Float64 lon_range 100.0, 289.180000305176;
        Float64 lat_range 1.20000004244503E-5, 90.0;
        Float64 depth_range 0.0, 10860.900390625;
        Float64 time_range -1.1432412E12, 1.10778048E12;
    }
    location._id {
        String long_name "sequence id";
        Int32 missing_value 2147483647;
        String units "";
    }
    location.time {
        String units "msec since 1970-01-01 00:00:00 GMT";
        String long_name "time";
        Float64 missing_value NaN;
        String axis "T";
    }
    location.lat {
        String units "degree_north";
        String long_name "latitude";
        Float32 missing_value NaN;
        String axis "Y";
    }
    location.lon {
        String units "degree_east";
        String long_name "longitude";
        Float32 missing_value NaN;
        String axis "X";
    }   
    location.profile.salinity {
        String units "psu";
        String long_name "salinity";
        Float32 missing_value NaN;
    }    
    location.profile.depth {
        String units "m";
        String long_name "depth";
        Float32 missing_value NaN;
        String axis "Z";
    }
    location.profile.temp {
        String units "degc";
        String long_name "temperature";
        Float32 missing_value NaN;
    }
}
-----

The skeleton xref:dataset[+dataset+] XML code for a
+EDDTableFromDapSequence+ dataset is:
[source,xml]
-----
<dataset type="EDDTableFromDapSequence" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <outerSequenceName>...</outerSequenceName>
    <!-- The name of the outer sequence for DAP sequence data. 
    This tag is REQUIRED. -->
  <innerSequenceName>...</innerSequenceName>
    <!-- The name of the inner sequence for DAP sequence data. 
    This tag is OPTIONAL; use it if the DAP data is a two level 
    sequence. -->
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <sourceCanConstrainStringEQNE>true|false</sourceCanConstrainStringEQNE>
  <sourceCanConstrainStringGTLT>true|false</sourceCanConstrainStringGTLT>
  <sourceCanConstrainStringRegex>...</sourceCanConstrainStringRegex>
  <skipDapperSpacerRows>...</skipDapperSpacerRows>
    <!-- skipDapperSpacerRows specifies whether the dataset 
    will skip the last row of each innerSequence other than the 
    last innerSequence (because Dapper servers put NaNs in the 
    row to act as a spacer).  This tag is OPTIONAL. The default 
    is false.  It is recommended that you set this to true for 
    all Dapper sources and false for all other data sources. -->
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more -->
</dataset>
-----




[[EDDTableFromDatabase]]
+EDDTableFromDatabase+
~~~~~~~~~~~~~~~~~~~~~~

The skeleton XML for an +EDDTableFromDatabase+ dataset is:

[source,xml]
-----
<dataset type="EDDTableFromDatabase" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
    <!-- Put the database name at the end, for example,
      "jdbc:postgresql://123.45.67.89:5432/databaseName". REQUIRED. -->
  <driverName>...</driverName>
    <!-- The high-level name of the database driver, e.g.,
      "org.postgresql.Driver".  You need to put the actual database
      driver .jar file (for example, postgresql.jdbc.jar) in
      [tomcat]/webapps/erddap/WEB-INF/lib.  REQUIRED. -->
  <connectionProperty name="name">value</connectionProperty>
    <!-- The names (e.g., "user", "password", and "ssl") and values
      of the properties needed for ERDDAP to establish the connection
      to the database.  0 or more. -->
  <catalogName>...</catalogName>
    <!-- The name of the catalog which has the schema which has the
      table, default = "".  OPTIONAL. -->
  <schemaName>...</schemaName> <!-- The name of the
    schema which has the table, default = "".  OPTIONAL. -->
  <tableName>...</tableName>  <!-- The name of the
    table, default = "".  REQUIRED. -->
  <orderBy>...</orderBy>  <!-- A comma-separated list of
    sourceNames to be used in an ORDER BY clause at the end of the
    every query sent to the database (unless the user's request
    includes an &orderBy() filter, in which case the user's
    orderBy is used).  The order of the sourceNames is important.
    The leftmost sourceName is most important; subsequent
    sourceNames are only used to break ties.  Only relevant
    sourceNames are included in the ORDER BY clause for a given user
    request.  If this is not specified, the order of the returned
    values in not specified. Default = "".  OPTIONAL. -->
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit>
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more.
     For date and timestamp database columns, set dataType=double and
     units=seconds since 1970-01-01T00:00:00Z -->
</dataset>
-----

[[EDDTableFromErddap]]
+EDDTableFromErddap+
~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromErddap+ (and +EDDGridFromErddap+) dataset types handle
tabular (and gridded) data from a remote data server.  These dataset types
behave differently from all other types of datasets in ERDDAP.
They are similar to other dataset types in that they get information about
a dataset from the source and keep it in memory, and then use the information
stored in memory when ERDDAP searches for datasets or displays the +Data Access Form+
or the +Make a Graph+ pages.

+EDDTableFromErddap+ datasets differ in that when ERDDAP receives a request for
data or images from a dataset, it
http://en.wikipedia.org/wiki/URL_redirection[redirects] the request to the
remote ERDDAP server.
The consequences of this process are:

* very high efficiency in CPU usage, memory and bandwidth since a request
is not sent to the remote ERDDAP server and it does not have to get the
data, reformat, and transmit it to the local ERDDAP server where it does
not have to be received, reformatted and transferred to the user;
* the remote ERDDAP server sends the response directly to the user,
which requires essentially no CPU time, memory or bandwidth on the local
machine; and
* the redirect is transparent to the user regardless of the client
software (e.g. a browser, command-line tool, etc.

When an +EDDTableFromErddap+ dataset type is (re)loaded on your ERDDAP,
an attempt is made to add a subscription to the remote dataset via the
remote ERDDAP's email/URL subscription system.  This enables the remote
ERDDAP to contact the appropriate xref:setDatasetFlag[+setDatasetFlag+] URL
on your ERDDAP whenever the remote dataset changes so the local dataset
can be reloaded ASAP, and thus always mimic the remote dataset.
The initial time this happens, you should get an email requesting that
you validate the subscription.  However, if the local ERDDAP cannot
send email or if the remote ERDDAP has an inactive email/URL subscription
system, you need to email the remote ERDDAP administrator to request
that xref:onChange[onChange] tags be added to all of the relevant
datasets to call your dataset's
xref:setDatasetFlag[+setDatasetFlag+] URL.
The +Daily Report+ contains a list of
xref:setDatasetFlag[+setDatasetFlag+] URLs, from which you
will select for ones for +EDDTableFromErddap+ to send to
the remote ERDDAP administrator.

+EDDTableFromErddap+ and +EDDGridFromErddap+ are also the basis
for xref:clusters[clusters and federations] of ERDDAPs,
which efficiently distribute CPU usage (mostly for making maps),
memory usage, dataset storage, and bandwidth usage of a large
data center with many thousands of datasets.

There are a couple of limitations:

* +EDDTableFromErddap+ and +EDDGridFromErddap+ cannot be used with
remote datasets that require one to log in because those datasets
use xref:accessibleTo[+accessibleTo+]; and
* +EDDTableFromErddap+ and +EDDGridFromErddap+ do not support
the xref:accessibleTo[+accessibleTo+] tag for security reasons.

The skeleton XML for a +EDDTableFromErddap+ dataset is shown below.
It is fairly simple since the intent is to simply mimic the remote
dataset that is already suitable for use through the local ERDDAP.

[source,xml]
-----
<dataset type="EDDTableFromErddap" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
</dataset>
-----

[[EDDTableFromFiles]]
+EDDTableFromFiles+
~~~~~~~~~~~~~~~~~~~

This is the superclass for all classes of the form +EDDTableFrom...Files+.
This is not used directory, but it used via subclasses for handling specific
file types.  The available subclasses are:

* xref:EDDTableFromAsciiFiles[+EDDTableFromAsciiFiles+]
* xref:EDDTableFromAwsXmlFiles[+EDDTableFromAwsXmlFiles+]
* xref:EDDTableFromHyraxFiles[+EDDTableFromHyraxFiles+]
* xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+]
* xref:EDDTableFromNcCFFiles[+EDDTableFromNcCFFiles+]
* xref:EDDTableFromThreddsFiles[+EDDTableFromThreddsFiles+]

If your data is not in ASCCI, AWS XML, Hyrax, NetCDF or
THREDDS format files, then your best bet is to convert it
into one of these formats.  It is [green]*recommended* to
convert them into NetCDF files as it is an increasingly
widely supported format for which a wide variety of tools
and clients - including ERDDAP - are available.  The spin-up
is not trivial, but it will be worth it if you wish to prolong
the life and increase the distribution of your data.

Aggregation
^^^^^^^^^^^

The +EDDTableFromFiles+ superclass and thus all its subclasses
*aggregate* data from local files, with each file holding a relatively
small data table.
The resulting aggregated dataset appears as if all the tables from
all the files had been combined, e.g. all the rows of data from
the first file, plus all the rows of data from the second file, etc.

All the files don't have to have all of the specified variables,
although the variables in all the files [red]*must* have the
same values for the +add_offset+, +missing_value+, +_FillValue+,
+scale_factor+ and +units+ attributes, if present.
This is checked by ERDDAP, although it has insufficient information
to know which is the correct value if there are differing values,
and thus cannot discern between valid and invalid files.

The actual files [green]*may* be in a single directory, or in
a directory and its subdirectories (recursively).  If there are a
large number of files - e.g. 1000 or more - the operating system
and +EDDGridFromFiles+ will operate more efficiently if the files
are stored in a series of subdirectories.

File Information Caching
^^^^^^^^^^^^^^^^^^^^^^^^

Information about individual files is *cached* when an
+EDDGridFromFiles+ dataset is first loaded.  Information about
each actual file is gathered and dynamic memory tables are created from it.
The tables are also stored on disc as +.json+ files in
-----
[bigParentDirectory]/dataset
-----
in files named:

* +[datasetID].dirs.json+ - which holds a list of unique directory names
* +[datasetID].files.json+ - which holds the table with each valid file's
information;
* +[datasetID].bad.json+ - which holds the table with each bad file's
information.

The tables stored on disc save time and resources since when
ERDDAP is restarted it reads the file information from the tables rather than
attempting to read all the files again.
If any changes are made to the (sub)directories containing the
actual datasets, the stored information can be updated via the
use of the xref:flag[flag] system to force ERDDAP to update
the cached information.

Handling Requests With Constraints
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Requests for ERDDAP tabular data can be made by putting constraints
on any variable.  When a client's request for data is processed,
+ERDDAPTableFromFiles+ can quickly look in the table with the valid
file information to see while files might have relevant data.  For
example, if each source file has one fixed-location buoy,
+EDDTableFromFiles+ can very efficiently determine which files will
have data within a given longitude and latitude range.

The valid file information table also includes the minimum and maximum
value of every variable for every valid file, so queries related to
these values can be handled efficiently.  For example, if some buoys
don't have an air pressure sensor, and a client requests data for
+airPressure!=NaN+, +EDDTableFromFiles+ can quickly
determine which buoys have air pressure data.

Reloading and Updating
^^^^^^^^^^^^^^^^^^^^^^

The cached information is updated whenever the dataset is reloaded.
Mechanisms that can induce reloading are:

* periodic reloading as per the +reloadEveryNMinutes+ tag in
+datasets.xml+;
* situational reloading as soon as possible whenever ERDDAP detects that
you have added, removed,
http://en.wikipedia.org/wiki/Touch_%28Unix%29[touched], or otherwise changed a
data file; and
* forced reloading as soon as possible via the xref:flag[flag] system.

When a dataset is reloaded, the actions of ERDDAP include:

* comparing the currently available files to the cached file information
tables;
* reading new files and adding to the valid files table;
* dropping files that no longer exist from the valid files table;
* reading and updating the information of files whose timestamps
have changed;
* replacing the old tables in memory and on disk;
* emailing a table of bad files and the reasons the files were declared
bad (e.g. corrupted file, missing variables, etc.) to those included
in the xref:emailEverythingTo[+emailEverythingTo+] tags.

Near-Real-Time Data
^^^^^^^^^^^^^^^^^^^

Requests for very recent data are treated as a special case by
+EDDTableFromFiles+ to deal with the situation wherein if the
files comprising the dataset are updated frequently, it is likely
that the dataset won't be updated every time a file is changed.
Therefore, +EDDTableFromFiles+ will not be aware of the changed
files, and the user won't be able to obtain the most recent
data.  The xref:flag[flag] system could be used for more frequent
dataset updates, but if the data is updated very often, say, every
few minutes, then ERDDAP will be grinding the system to a halt
updating datasets almost continuously.  To avoid this situation,
ERDDAP does two things:

* When the dataset is laoded, if the maximum value for the time variable
is within the last 24 hours, ERDDAP sets the maximum time to be
+NaN+, i.e. now.

* When ERDDAP gets a request for data within the last 20 hours - e.g.
8 hours until now - ERDDAP will search all files which have any data
in the last 20 hours.

These enable ERDDAP to not have to be perfectly up-to-date for all of
the files in order to find the latest data.  The +reloadEveryNMinutes+
flag value can still be set to a small value such as 60 minutes, but
needn't be set to something as small as 3 minutes.

Organization of Near-Real-Time Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is [green]*recommended* that real-time-data be stored in chunks,
an example of which would be storing all data for one station for one
year (or month) in a single file.  Thus, when new data arrives, ERDDAP
only has to read and rewrite the file with this year's (or month's)
data.  All the files for previous years of months will remain unchanged,
and when ERDDAP reloads the dataset, most files are unchanged and only
a few need to be read.  It is [red]*not recommended* to store data for
numerous stations (or buoys) for many years in single files, since
an update for any single station within that file will require that
the entire large old file be read and a new large file be written.
This is much less efficient than chunking the data.

Addition of New Files
^^^^^^^^^^^^^^^^^^^^^

If new data files are added to the ERDDAP server via +scp+ or +ftp+ while
it is running, there is a nonvanishing and fairly good chance that ERDDAP
will be reloading the dataset during the downloading process.
If this happens, the file will appear to be valid (since it has a valid
name), but it isn't.  If ERDDAP attempts to read data from the invalid
file, the result error will cause the file to be added to the table of
invalid files and not become part of the aggregate dataset.
This is not good, and it is [green]*recommended* to use a temporary
name when transferring the file that will cause the +fileNameRegex+
test to ignore it.  For example, if it is looking for files of the form +.nc+
an effective temporary name would be +file.nc_TMP+.  The file can
be appropriately renamed - that is, the +_TMP+ removed - after the
downloading process has completed.  This will cause it to become
immediately relevant to ERDDAP and appropriately processed.

File Name Extracts
^^^^^^^^^^^^^^^^^^

+EDDTableFromFiles+ has a system for extracting a string from each
filename and using that to create a pseudo data variable.  There is
currently no mechanism for interpeting these strings as dates/times,
though.  There are several XML tags for setting up this system.

* +preExtractRegex+ - A
http://www.vogella.com/articles/JavaRegularExpressions/article.html[regular
expression] used to identify text to be removed from the start of the
filename.  The removal only occurs if the regular expression is
matched, and the expression usually begins with +^+ to match the beginning
of the filename.

* +postExtractRegex+ - A regular expression used to identify the text
to be removed from the end of the filename.  The removal only occurs
if the regular expression is matched, and the expression usually
beings with +$+ to match the end of the filename.

* +extractRegex+ - If present, this regular expression is used after
+preExtractRegex+ and/or +postExtractRegex+ to identify a string
to be extracted from the filename, e.g. the +stationID+.  If the
regular expression isn't matched, the entire remaining filename - after
being trimmed front and/or back - is used.  The entire remaining
filename can be matched with +.*+.

* +columnNameForExtract+ - The data column name for the strings to
be extracted.  A +dataVariable+ with the +sourceName+ must
be in the +dataVariable+ list (with any datatype, but usually
with string type).

An example would be a dataset containing filenames like +XYZAble.nc+,
+XYZBaker.nc+, +XYZCharlie.nc+, etc.  If you want to create a new
variable - e.g. +stationID+ - when each file is read which will
contain the station ID values +Able+, +Baker+, +Charlie+, etc., then
you could use the following tags and values:

* +<preExtractRegex>^XYZ</preExtractRegex>+ - This finds and removes
the string +XYZ+ from the start of the filename, e.g. +XYZAble.nc+
becomes +Able.nc+;

* +<postExtractRegex>\x2Enc$</postExtractRegex>+ - This finds and
removes the string +.nc+, which must be encoded as +\x2Enc+ since
the period is a regular expression special character and +2E+ is
the hexadecimal character number for a period.  This transforms
+Able.nc+ into +Able+.

* +<extractRegex>.*</extractRegex>+ - This matches all the remaining
characters in +Able+, so this partial filename becomes the extract
for the first file.

* +<columnNameForExtract>stationID</columnNameForExtract>+ - This tells
ERDDAP to create a new column called +stationID+ when reading
each file.  Every row of data for a given file will have the
text extracted from its filename - e.g. +Able+ - as the value
for the +stationID+ column.

In most cases, there are numerous values for these extraction tags that
will yield the same results.  There's usually more than one way to do just
about everything with regular expressions, and there's always at
least one way to do nearly anything.

Global metadata in each file can also be converted into data.
If the +sourceName+ of a variable begins with +global:+ - e.g.
+global:PI+ - ERDDAP will look for a global attribute with
the name +PI+ when reading the data from the file and create
a column filled with the name of the attribute.

Skeleton XML Code
^^^^^^^^^^^^^^^^^

A skeleton XML for all +EDDTableFromFiles+ subclasses is:
[source,xml]
-----
<dataset type="EDDTableFrom...Files" datasetID="..." active="..." >
  <nDimensions>...</nDimensions>  <!-- This was used prior to ERDDAP version1.30 but is now ignored. -->
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit>
  <specialMode>mode</specialMode>  <-- This rarely-used, optional tag can be
used 
    with EDDTableFromThreddsFiles to specify that special, hard-coded rules 
    should be used to determine which files should be downloaded from the
server.
    Currently, the only valid mode is SAMOS which is used with datasets from
    http://coaps.fsu.edu/thredds/catalog/samos to download only the files with 
    the last version number. -->
  <sourceUrl>...</sourceUrl>  <-- For subclasses like EDDTableFromHyraxFiles
and 
    EDDTableFromThreddsFiles, this is where you specify the base URL for the
files 
    on the remote server.  For subclasses that get data from local files,
ERDDAP 
    doesn't use this information to get the data, but does display the
information 
    to users. So I usually use "(local files)". -->
  <fileDir>...</fileDir> <-- The directory (absolute) with the data files. -->
  <recursive>true|false</recursive> <-- Indicates if subdirectories
    of fileDir have data files, too. -->
  <fileNameRegex>...</fileNameRegex> <-- A regular expression 
    (tutorial) describing valid data files names, e.g., ".*\.nc" for 
    all .nc files. -->
  <metadataFrom>...</metadataFrom> <-- The file to get metadata
    from ("first" or "last" (the default) based on file's 
    lastModifiedTime). -->
  <columnNamesRow>...</columnNamesRow> <-- (For 
    EDDTableFromAsciiFiles only) This specifies the number of the row
    with the column names in the files. (The first row is "1". 
    Default = 1.)  If you specify 0, ERDDAP will not look for column names
    and will assign names: Column#1, Column#2, ... -->
  <firstDataRow>...</firstDataRow> <-- (For 
    EDDTableFromAsciiFiles only) This specifies the number of the first
    row with data in the files. (The first row is "1". default = 2.) -->
  <-- For the next four tags, see File Name Extracts. -->
  <preExtractRegex>...</preExtractRegex>
  <postExtractRegex>...</postExtractRegex>
  <extractRegex>...</extractRegex>
  <columnNameForExtract>...</columnNameForExtract> 
  <sortedColumnSourceName>...</sortedColumnSourceName> 
    <-- The sourceName of the numeric column that the data files are 
    usually already sorted by within each file, e.g., "time".
    Use null or "" if no variable is suitable.
    It is ok if not all files are sorted by this column.
    If present, this can greatly speed up some data requests. 
    For EDDTableFromHyraxFiles, EDDTableFromNcFiles and 
    EDDTableFromThreddsFiles, this must be the leftmost axis 
    variable. -->
  <sortFilesBySourceNames>...</sortFilesBySourceNames>
    <-- This is a space-separated list of source variable names 
    which specifies how the internal list of files should be sorted
    (in ascending order), for example "id time". 
    It is the minimum value of the specified columns in each file
    that is used for sorting.
    When a data request is filled, data is obtained from the files
    in this order. Thus it determines the overall order of the data
    in the response.  If you specify more than one column name, the
    second name is used if there is a tie for the first column; the
    third is used if there is a tie for the first and second columns; ...
    This is OPTIONAL (the default is fileDir+fileName order). -->
  <isLocal>false<isLocal> <!-- (may be true or false, 
    the default). This is only used by EDDTableFromNcCFFiles. It 
    indicates if the files are local (actual files) or remote 
    (accessed via the web). The two types are treated slightly 
    differently.
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more -->
    <-- For EDDTableFromHyraxFiles, EDDTableFromNcFiles, and 
    EDDTableFromThreddsFiles, the axis variables (e.g., time) needn't
    be first or in any specific order. -->
</dataset>
-----


[[EDDTableFromHyraxFiles]]
+EDDTableFromHyraxFiles+
~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromHyraxFiles+ dataset type aggregates data files with several
variables, each with one or more shared dimensions, e.g. +time+,
+altitude+, +latitude+ and +longitude+, that are served by a
http://www.opendap.org/download/hyrax[Hyrax OPeNDAP server].

General characteristics of files from Hyrax servers include:

* they usually have multiple values for the leftmost dimension,
which is almost always +time+;

* they often (but not always) have a single value for the other
dimensions, e.g. +altitude+, +latitude+ and +longitude+.

* they may have character variables with an additional
dimension, e.g. +nCharacters+.

Hyrax Servers
^^^^^^^^^^^^^

A Hyrax server can be identified by the presence of either
-----
/dods-bin/nph-dods/
-----
or
-----
/opendap/
-----
in the URL.  The +EDDTableFromHyraxFiles+ class
http://en.wikipedia.org/wiki/Data_scraping#Screen_scraping[screen scrapes]
the Hyrax web pages with the lists of files in each directory.  As such,
it is very specific to whatever the current format of Hyrax web pages
might be, although the ERDDAP maintainers will strive to adjust it quickly
if Hyrax changes the format of its web pages.

The xref:fileDir[+fileDIR+] setting is ignored since this class downloads and makes
a local copy of each remote data file.  ERDDAP forces the
xref:fileDir[+fileDir+] to be
-----
[bigParentDirectory]/copy/datasetID/
-----
The URL of the base directory of the dataset in the Hyrax server should
be used for the +sourceURL+ value.  For example:
-----
<sourceUrl>http://edac-dap.northerngulfinstitute.org/dods-bin/nph-dods/WCOS/nmsp/wcos/</sourceUrl>
-----
The +sourceURL+ web page usually has +OPeNDAP Server Index of [directoryName]+
at the top.

This dataset should [red]*never* be wrapped in
xref:EDDTableCopy[+EDDTableCopy+] since this class always downloads and
creates a local copy of each remote data file.  There's no reason to create
two copies of each remote data file.

The section for the superclass xref:EDDTableFromFiles[+EDDTableFromFiles+]
should be consulted for further information about how this class works and
how to use it.  Examples of the configuration and use of 1-, 2-, 3- and 4-D datasets
can be found in the documentation for the
xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] class.

[[EDDTableFromNcFiles]]
+EDDTableFromNcFiles+
~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromNcFiles+ dataset type and class aggregates data from
+.nc+ files with several variables, each with one shared dimension (e.g.
+time+) or more than one shared dimension (e.g. +time+, +altitude+,
+latitude+, +longitude+).  The files must all have the same dimension
names.  A given file may have multiple values for each of the dimensions,
and the values may be different in different files.  The files may have
character variables with an additional dimension (e.g. +nCharacters+).
The section for the superclass for this -
xref:EDDTableFromFiles[+EDDTableFromFiles+] - contains most of the
required configuration information.

Datasets of 1, 2, 3 and 4 dimensions are all slightly different, with each
having slightly different configuration requirements.  We now discuss each
in turn.

1-D Files
^^^^^^^^^

An example of 1-D files would be a set of +.nc+ files wherein each
file contains one month of data from one drifting buoy.  In these files:

* there will be one dimension - +time+ - where +time+ is the number
of times within the file;

* each file will have one
or more 1-D variables that use that dimension, e.g. +time(time)+,
+longitude(time)+,
+latitude(time)+, +air_temperature(time)+, etc.; and

* each file may also have 2-D character variables, e.g. with
dimensions +(time,nCharacters)+.

2-D Files
^^^^^^^^^

An example of 2-D files would be a set of +.nc+ files wherein each
file contains one month of data from two or more drifting buoys.
In these files:

* there will be two dimensions - +time+ and +id+ - where
the dimension +id+ is the number of drifting buoys;

* each file will have
2 1-D variables with the same names as the dimensions, i.e. the
variable/dimension combinations will be +time(time)+ and +id(id)+.
These 1-D variables should be included in the +dataVariable+ list
in the dataset's XML;

* each file will have one of more 2-D variables, e.g. +longitude(time,id)+,
+latitude(time,id)+, +air_temperature(time,id)+, etc.; and

* each file may have 3-D character variables with the dimensions
+(time,id,nCharacters)+.

3-D Files
^^^^^^^^^

An example of 3-D files would be a set of +.nc+ files wherein
each file has one month's worth of data from one stationary buoy.
In these files:

* each file will have three dimensions - +time+, +lat+ and +lon+;

* each file will have three 1-D variables with the same names as
the dimensions, e.g. +time(time)+, +longitude(longitude)+ and
+latitude(latitude)+, and these variables should be included in
the +dataVariable+ list in the dataset's XML;

* each file will have one or more 3-D variables, e.g.
+air_temperature(time,lat,lon)+ and +water_temperature(time,lat,lon)+;

* each file may have 4-D characters variables, e.g. with
dimensions +(time,lat,lon,nCharacters)+; and

* the file's name might contain the name of the buoy.

4-D Files
^^^^^^^^^

An example of 4-D files would be a set of +.nc+ files wherein each
file has one month's worth of data from one station, and at each
time point the station takes readings at a series of depths.
In these files:

* each file will have four dimensions - +time+, +depth+, +lat+,
+lon+;

* each file will have four 1-D variables with the same names as the
dimensions, e.g.  +time(time)+, +longitude(longitude)+,
+latitude(latitude)+ and +depth(depth)+, and these variables
should be included in the +dataVariable+ list in the dataset's XML:

* each file will have one or more 4-D variables, e.g.
+air_temperature(time,depth,lat,lon)+ and +water_temperature(time,depth,lat,lon)+;

* each file may have 5-D character variables, e.g. with
dimensions +(time,depth,lat,lon,nCharacters)+; and

* the file's name might contain the name of the buoy.

[[EDDTableFromNcCFFiles]]
+EDDTableFromNcCFFiles+
~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromNcCFFiles+ dataset type aggregates data from +.nc+ files
which use one of the file formats specified by the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#discrete-sampling-geometries[CF
Discrete Sampling Geometries] conventions.
Most of the information about configuring and using this class is found
in the section for its superclass xref:EDDTableFromFiles[+EDDTableFromFiles+],
although nearly all of the details about how to make one's datasets conform
to the CF Discrete Sampling Geometries conventions is scattered about
elsewhere.
This
section should provide you with enough information to at least recognize if
a file properly conforms to the the conventions and, if not, a good
idea about how to make it conform.
This is important because if the file doesn't properly conform, then
it almost certainly won't work within this particular ERDDAP
dataset type (although as an alternative you can simply try to serve it
via the xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] class).

Discrete sampling geometry (DSG) datasets are characterized by a dimensionality
that is lower than that of the space-time region that is samples, i.e. they
are typically "paths" through space-time.  Each type of DSG is defined
by the relationships among its spatiotemporal coordinates, and each
separate type of DSG is referred to as a *featureType*.
These conventions offer the advantages of efficiency and clarity for
story a collection of features in a single file.

The http://www.nodc.noaa.gov/data/formats/netcdf/[NOAA NetCDF Templates]
created by the NODC are a good place to start looking for examples
of each of these features types as well as for templates of NetCDF
files containing them.  Also useful are the
http://www.unidata.ucar.edu/software/netcdf-java/reference/FeatureDatasets/CFpointImplement.html[Short
Guide to Writing Files Using CF-1.6 Discrete Sampling Features Conventions]
and the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#appendix-examples-discrete-geometries[Annotated
Examples of Discrete Geometries] appendix to the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html[CF-1.6
conventions document].

The CF DSG file formats will each be described in a separate section.

+point+
^^^^^^^

The +point+ feature type or DSG is a single data point having no
implied coordinate relationship to other points.
An example would be one or more recorded observations that
have no temporal or spatial relationship (where each
observation equals one point in time and space), e.g.
a Lagrangian drifter track.
The form of a data variable containing values defined on a
collection of the features is +data(i)+, and the mandatory
space-time coordinates for a collection of these features
are +x(i)+, +y(i)+ and +t(i)+.

An example of the NetCDF header for a +point+ dataset that conforms
the CF-1.6 standards follows.  It could be a template for the measurements
taken by a weather balloon. In it we find the single dimension +obs+
whose value is the total number of observations, with
the 1-D variables +time+, +lon+, +lat+ and +alt+ holding the unrelated
time, longitude, latitude and altitude values that specify the
balloon's position in space and time each time the 4-D variables +humidity+ and
+temp+ are measured.  The attributes for each variable include the
http://cf-pcmdi.llnl.gov/documents/cf-standard-names/[+standard
name+] required by CF-1.6 as well as the recommended 
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#units[+units+]
and
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#coordinate-system[+coordinates+]
variable attributes.
Note also the use of the
http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#featureType[+featureType+]
global attribute.
-----
dimensions:
      obs = 1234 ;

   variables:
      double time(obs) ; 
          time:standard_name = time;
          time:long_name = "time of measurement" ;
          time:units = "days since 1970-01-01 00:00:00" ;
      float lon(obs) ; 
          lon:standard_name = "longitude";
          lon:long_name = "longitude of the observation";
          lon:units = "degrees_east";
      float lat(obs) ; 
          lat:standard_name = "latitude";
          lat:long_name = "latitude of the observation" ;
          lat:units = "degrees_north" ;
      float alt(obs) ;
          alt:long_name = "vertical distance above the surface" ;
          alt:standard_name = "height" ;
          alt:units = "m";
          alt:positive = "up";
          alt:axis = "Z";

      float humidity(obs) ;
          humidity:standard_name = "specific_humidity" ;
          humidity:coordinates = "time lat lon alt" ;
      float temp(obs) ;
          temp:standard_name = "air_temperature" ;
          temp:units = "Celsius" ;
          temp:coordinates = "time lat lon alt" ;

   attributes:
      :featureType = "point";
-----

+timeSeries+
^^^^^^^^^^^^

The +timeSeries+ feature type or DSG is a series of data points at the
same spatial location with monotonically increasing times.
An example would be sea surface temperature (SST) from one or
more fixed platforms with the exact same increasing time
intervals, or multiple platforms collection observations
at different time intervals.

An example of a header from a compliant NetCDF file for a +timeSeries+
feature type is:

-----
dimensions:
     station = 10 ;  // measurement locations
     time = UNLIMITED ;
   variables:
     float humidity(station,time) ;
       humidity:standard_name = "specific humidity" ;
       humidity:coordinates = "lat lon alt" ;
     double time(time) ; 
       time:standard_name = "time";
       time:long_name = "time of measurement" ;
       time:units = "days since 1970-01-01 00:00:00" ;
     float lon(station) ; 
       lon:standard_name = "longitude";
       lon:long_name = "station longitude";
       lon:units = "degrees_east";
     float lat(station) ; 
       lat:standard_name = "latitude";
       lat:long_name = "station latitude" ;
       lat:units = "degrees_north" ; 
     float alt(station) ;
       alt:long_name = "vertical distance above the surface" ;
       alt:standard_name = "height" ;
       alt:units = "m";
       alt:positive = "up";
       alt:axis = "Z";
     char station_name(station, name_strlen) ;
       station_name:long_name = "station name" ;
       station_name:cf_role = "timeseries_id";
   attributes:
       :featureType = "timeSeries";
-----

wherein the 2D variable +humidity+ contains a time series of data
at a set of 10 stations.  The *station variables* (or *station variables*) are
those that contain information describing the stations.  These variables
remain constant for each station in this example are +lon+, +lat+, +alt+
and +station_name+, with the latter containing the +cf_role+ variable
+timeseries_id+ for the variable whose values uniquely identify the stations.

+trajectory+
^^^^^^^^^^^^

The +trajectory+ feature type or DSG is a series of data points along
a path through space with monotonically increasing times.
An example would be one or more events where a platform underway
collected data from a 
http://www.aoml.noaa.gov/phod/tsg/index.php[thermosalinograph].

+profile+
^^^^^^^^^

The +profile+ feature type or DSG is an ordered set of data points along a
vertical line at a fixed horizontal position and fixed time.
Examples would be:

* one or more 
http://www.whoi.edu/instruments/viewInstrument.do?id=1003[CTD] or 
http://www.aoml.noaa.gov/goos/uot/xbt-what-is.php[XBT] casts
that have the exact same depth (z) values (but do not have to
have the same number of depth levels), or 

* multiple CDT or XBT
casts that do not have the exact same depth values.

+timeSeriesProfile+
^^^^^^^^^^^^^^^^^^^

The +timeSeriesProfile+ feature type or DSG is a series of +profile+
features at the same horizontal position with monotonically
increasing times.
Examples would be:

* single or multiple 
http://en.wikipedia.org/wiki/Mooring_%28oceanography%29[mooring lines]
with stationary instruments at the same depths across all the mooring
lines, and all the instruments measuring at the same points in time;

* multiple mooring lines with stationary instruments at different
depths across the mooring lines, but all the instruments measuring
at the same points in time;

* multiple mooring lines with stationary instruments at the
same depth across all the mooring lines, but the instruments
measuring at different points in time; and

* multiple mooring lines with stationary instruments at different
depths across the mooring lines, and the instruments measuring at
different points in time.


+trajectoryProfile+
^^^^^^^^^^^^^^^^^^^

The +trajectoryProfile+ feature type or DSG is a series of +profile+
features located at points ordered along a +trajectory+.
Examples would be the data collected by
http://en.wikipedia.org/wiki/Underwater_glider[undulating gliders] and
http://en.wikipedia.org/wiki/Argo_%28oceanography%29[Argo floats].

[[EDDTableFromThreddsFiles]]
+EDDTableFromThreddsFiles+
~~~~~~~~~~~~~~~~~~~~~~~~~~

The +EDDTableFromThreddsFiles+ dataset type aggregates data files with several
variables - each with one or more shared dimensions, e.g. time, altitude,
latitude, longitude - served by a
http://www.unidata.ucar.edu/projects/THREDDS/[THREDDS OPeNDAP server].
The characteristics of THREDDS files include:

* each file has multiple values for the leftmost dimension, e.g. time;
* the files often - although they don't have to - have a single value for
the other dimensions, e.g. altitude, latitude, longitude;
* the files may have character variables with an additional
dimension, e.g. +nCharacters+;
* they can be identified by the +/thredds/+ in their URLS, e.g.
+http://data.nodc.noaa.gov/thredds/catalog/nmsp/wcos/catalog.html+

This class reads the +catalog.xml+ files served by THREDDS containing:

* +catalogRefs+ tags that contain references to additional +catalog.xml+
sub-files; and
* +dataset+ tags that describe datasets.

The xref:fileDir[+fileDir+] setting is ignored since this class downloads
and creates a local copy of each remote data file.  ERDDAP forces the
xref:fileDir[+fileDir+] to be +[bigParentDirectory]/copy/datasetID/+.
The xref:sourceUrl:[+sourceUrl+] value should be the URL of the
+catalog.xml+ file for the dataset in the THREDDS server.  If the
URL used to access the dataset with THREDDS is:
-----
http://data.nodc.noaa.gov/thredds/catalog/nmsp/wcos/catalog.html
-----
then the tag will be:
-----
<sourceUrl>http://data.nodc.noaa.gov/thredds/catalog/nmsp/wcos/catalog.xml</sourceUrl>
-----
This class should [red]*not* be wrapped in
xref:EDDTableCopy[+EDDTableCopy+] since it always downloads and makes a local
copy of each remote data file.

This dataset type supports an [green]*optional*, rarely-used, special tag
+specialMode+, which can be used to specify that special, hard-coded rules
should be used to determine which files should be downloaded from the
server.  At present, the only valid mode is +SAMOS+ which is used with
datasets from:

http://coaps.fsu.edu/thredds/catalog_samos.html[+http://coaps.fsu.edu/thredds/catalog_samos.html+]

to download only the files with the last version number.

See the superclass xref:EDDGridFromFiles[+EDDGridFromFiles+] for requirements
and features common to all datasets in the +*Files+ classes.
See also the 1-, 2-, 3- and 4-D examples in the
xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+] section.


[[EDDTableFromNOS]]
+EDDTableFromNOS+
~~~~~~~~~~~~~~~~~

The +EDDTableFromNOS+ dataset type handles data from a NOAA
http://opendap.co-ops.nos.noaa.gov/axis/[NOS] source, which
uses http://en.wikipedia.org/wiki/SOAP[SOAP] and XML for
requests and responses.  This is very specific to the NOAA
NOS XML format.


[[EDDTableFromOBIS]]
+EDDTableFromOBIS+
~~~~~~~~~~~~~~~~~~

The +EDDTableFromOBIS+ dataset type handles data from an
http://www.iobis.org/[Ocean Biogeographic Information System (OBIS)]
server.
OBIS servers expect an XML request and return an XML response.

All OBIS servers use the same
http://www.iobis.org/data/schema-and-metadata[data schema]
to serve the same variables in the same way, so not much
is required to set up an OBIS dataset in ERDDAP.
You [red]*must* include a +creator_email+ attribute in the
global xref:globalAttributes[+globalAttributes+] tag since
that information is used within the license.
A suitable email address can be found by reading the
XML response from the xref:sourceUrl[+sourceUrl+].
You may or may not be able to get the global
attribute xref:subsetVariables[+subsetVariables+] to
work with a given OBIS server.  If you try, then try
just one variable to start, e.g. +ScientificName+ or
+Genus+.

The skeleton XML for an +EDDTableFromOBIS+ file is:

[source,xml]
-----
<dataset type="EDDTableFromOBIS" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <sourceCode>...</sourceCode>
    <!-- If you read the XML response from the sourceUrl, the 
    source code (e.g., GHMP) is the value from one of the 
    <resource><code> tags. -->
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <-- All ...SourceMinimum and Maximum tags are OPTIONAL -->
  <longitudeSourceMinimum>...</longitudeSourceMinimum> 
  <longitudeSourceMaximum>...</longitudeSourceMaximum> 
  <latitudeSourceMinimum>...</latitudeSourceMinimum> 
  <latitudeSourceMaximum>...</latitudeSourceMaximum> 
  <altitudeSourceMinimum>...</altitudeSourceMinimum> 
  <altitudeSourceMaximum>...</altitudeSourceMaximum> 
  <-- For timeSource... tags, use yyyy-MM-dd'T'HH:mm:ssZ format. -->
  <timeSourceMinimum>...</timeSourceMinimum> 
  <timeSourceMaximum>...</timeSourceMaximum> 
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <addAttributes>...</addAttributes> 
</dataset>
-----

[[EDDTableFromSOS]]
+EDDTableFromSOS+
~~~~~~~~~~~~~~~~~

The +EDDTableFromSOS+ dataset type handles data from a
http://www.opengeospatial.org/projects/groups/sensorwebdwg[Sensor Observation
Service (SWE/SOS)] server, and aggregates data from a group of stations
that are all served by a single SOS server.
The stations all serve the same variables, although the source for
each station doesn't have to serve all variables.
SOS servers expect an XML request and return an XML response.

It is not trivial to generate the dataset XML for SOS datasets.
To obtain the needed information, you must visit:
-----
sourceUrl+"?service=SOS&request=GetCapabilities"
-----
with a web browser, look through the XML for what you need,
make a +GetObservation+ request by hand, and then look through
the XML response to that request.
The following section is an overview of SOS that should supply enough
information to appreciate how it works, as well as how good an idea
it is the let ERDDAP perform all the tedious XML parsing and data munging
for you.

The SOS Standard and Making Queries from an SOS Server
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The OGC http://www.opengeospatial.org/projects/groups/sensorwebdwg[Sensor Web Enablement (SWE)] framework defines a suite of
http://en.wikipedia.org/wiki/Web_service[web service] interfaces
and communication protocols that are especially well suited
for environmental monitoring. 
The http://www.opengeospatial.org/standards/sos[Sensor Observation Service (SOS)] is part of
the SWE framework, and is a web service for
querying real-time sensor data and sensor data time series.
The standards document that explains the core, transactional and extended
operations required or recommended for an SOS server can be found at:

http://www.opengeospatial.org/standards/sos[+http://www.opengeospatial.org/standards/sos+]

We are primarily concerned with the three
three core operations required by the standard:

* +GetCapabilities+ - a query that returns an XML description with information
about the interface - e.g. offered operations and endpoints - as well as the
available sensor data;
* +GetObservation+ - a query for obtaining data from specific sensors from
information obtained via +GetCapabilities+; and
* +DescribeSensor+ - a query for obtaining sensor metadata in
http://en.wikipedia.org/wiki/SensorML[SensorML] format.

The +GetCapabilities+ Query
+++++++++++++++++++++++++++

A +getCapabilities+ request obtains a list of stations and the
+observedProperty+ values for which they have data from an SOS server.
We now obtain some illustrative examples from the
http://sdf.ndbc.noaa.gov/sos/[SOS Server] at the
http://www.ndbc.noaa.gov/[National Data Buoy Center].
A +getCapabilities+ request for the entire available holdings of
the NDBC SOS server is:

http://sdf.ndbc.noaa.gov/sos/server.php?request=GetCapabilities&service=SOS[+http://sdf.ndbc.noaa.gov/sos/server.php?request=GetCapabilities&service=SOS+]

The result is a very large XML file,
with the +Contents+ tag/section of the +GetCapabilities+ response describing
the data offered by the SOS server. The concept of
an +ObservationOffering+ is used to group the offered observations.
An +ObservationOfferingList+ tag/section holds all the observating offerings, beginning with an
+ObservationOffering+ tag for the holdings of the entire server.
This initial section provides a list of all the +procedure+,
+observedProperty+ and +responseFormat+ tags available in the
remainder of the response, as well as overall space and time
bounds for the entire holdings of the server.

The initial +ObservationOffering+ tag is followed by other
+ObservationOffering+ tags, one for the observations
produced by each +procedure+, i.e. sensor systems or stations, which can be as
simple as a thermometer, or as complicated as a system of sensors
attached to a weather station or a network of spatially distributed
sensors.

Each +ObservationOffering+ tag contains +description+, +name+,
+srsName+, +boundedBy+, +time+, +procedure+, +observedProperty+,
+featureOfInterest+, +responseFormat+, +resultModel+,
+responseMode+ and possibly other tags.
The tags prefixed with +sos:+ are derived from another part of the
SWE framework called
http://www.opengeospatial.org/standards/om[Observations & Measurements (O&M)],
a standard that defines a conceptual schema for encoding observations and
for features involving in sampling when making the observations.

The O&M standard defines a core set of properties for an observation:

* feature of interest;
* observed property;
* result;
* procedure;
* phenomenon time - the time for which the observation result provides
an estimate of the value of the property;
* result time - the time when the result becomes available;
* valid time - the time period during which the result should be used.

The overall observation model connects these concepts as follows: an observation is an action whose result
is an estimate of the value of some property of the feature-of-interest,
obtained using a specified procedure.

The general structure is of the +GetCapabilities+ request shown above is:
[source,xml]
-----
<sos:Contents>
  <sos:ObservationOfferingList>
<!----------------------------------------------------------------------------------->
<!-- Observation offering for entire server. -->
<!----------------------------------------------------------------------------------->
    <sos:ObservationOffering gml:id="network-all">
      <gml:description>All stations on the NDBC SOS server</gml:description>
      <gml:name>urn:ioos:network:noaa.nws.ndbc:all</gml:name>
      <gml:srsName>http://www.opengis.net/def/crs/EPSG/0/4326</gml:srsName>
<!-- Overall bounding box of all server offerings. -->
      <gml:boundedBy>
        <gml:Envelope srsName="http://www.opengis.net/def/crs/EPSG/0/4326">
          <gml:lowerCorner>-77.466 -179.995</gml:lowerCorner>
          <gml:upperCorner>80.81 180</gml:upperCorner>
        </gml:Envelope>
      </gml:boundedBy>
<!-- Overall time bounds of all server offerings. -->
      <sos:time>
        <gml:TimePeriod>
          <gml:beginPosition>2006-07-27T21:10:00Z</gml:beginPosition>
          <gml:endPosition indeterminatePosition="now"/>
        </gml:TimePeriod>
      </sos:time>
<!-- List of all procedures or sensor systems, with a separate ObservationOffering section
added below for each procedure in this list. -->
      <sos:procedure xlink:href="urn:ioos:station:wmo:0y2w3"/>
      <sos:procedure xlink:href="urn:ioos:station:wmo:21346"/>
...
<!-- List of all available observed properties. -->
      <sos:observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/air_temperature"/>
      <sos:observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/air_pressure_at_sea_level"/>
...
<!-- List of all features of interest. -->
      <sos:featureOfInterest xlink:href="urn:cgi:Feature:CGI:EarthOcean"/>
      <sos:featureOfInterest xlink:href="urn:ioos:event:tsunami::false:20080827103645_51406"/>
...
      <sos:responseFormat>text/xml;subtype="om/1.0.0"</sos:responseFormat>
      <sos:responseFormat>text/csv</sos:responseFormat>
      <sos:responseFormat>text/tab-separated-values</sos:responseFormat>
      <sos:responseFormat>application/vnd.google-earth.kml+xml</sos:responseFormat>
      <sos:responseFormat>text/xml;schema="ioos/0.6.1"</sos:responseFormat>
      <sos:responseFormat>application/ioos+xml;version=0.6.1</sos:responseFormat>
      <sos:resultModel>om:ObservationCollection</sos:resultModel>
      <sos:responseMode>inline</sos:responseMode>
    </sos:ObservationOffering>
<!----------------------------------------------------------------------------------->
<!-- Observation offering for single sensor system Station 0y2w3. -->
<!----------------------------------------------------------------------------------->
    <sos:ObservationOffering gml:id="station-0y2w3">
      <gml:description>Sturgeon Bay CG Station, WS</gml:description>
      <gml:name>urn:ioos:station:wmo:0y2w3</gml:name>
      <gml:srsName>http://www.opengis.net/def/crs/EPSG/0/4326</gml:srsName>
<!-- Spatial bounds of individual sensor. -->
      <gml:boundedBy>
...
      </gml:boundedBy>
<!-- Time bounds of individual sensor -->
      <sos:time>
...
      </sos:time>
      <sos:procedure xlink:href="urn:ioos:station:wmo:0y2w3"/>
<!-- Observed property values offering by individual sensor. -->
      <sos:observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/air_temperature"/>
      <sos:observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/air_pressure_at_sea_level"/>
      <sos:observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/winds"/>
      <sos:featureOfInterest xlink:href="urn:cgi:Feature:CGI:EarthOcean"/>
<!-- Response formats offered by individual sensor. -->
      <sos:responseFormat>text/xml;subtype="om/1.0.0"</sos:responseFormat>
      <sos:responseFormat>text/csv</sos:responseFormat>
      <sos:responseFormat>text/tab-separated-values</sos:responseFormat>
      <sos:responseFormat>application/vnd.google-earth.kml+xml</sos:responseFormat>
      <sos:responseFormat>text/xml;schema="ioos/0.6.1"</sos:responseFormat>
      <sos:responseFormat>application/ioos+xml;version=0.6.1</sos:responseFormat>
      <sos:resultModel>om:ObservationCollection</sos:resultModel>
      <sos:responseMode>inline</sos:responseMode>
    </sos:ObservationOffering>
<!----------------------------------------------------------------------------------->
<!-- Observation offering for single sensor system 21346. -->
<!----------------------------------------------------------------------------------->
    <sos:ObservationOffering gml:id="station-21346">
...
    </sos:ObservationOffering>
...
<!----------------------------------------------------------------------------------->
<!-- Observation offerings for each sensor station in list of all stations.-->
<!----------------------------------------------------------------------------------->
...
  </sos:ObservationOfferingList>
</sos:Contents>
-----

The +GetObservation+ Query
++++++++++++++++++++++++++

The +GetObservation+ query is used to obtain data from a specific +procedure+
or sensor, and the information needed to perform such a query is obtained from
the +GetCapabilities+ query described in the previous section.
In this section we will show explicitly which information is needed, where
to find it, and how to construct a proper +GetObservation+ query from it.

A +GetObservation+ query is a URL of the general form:

-----
http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0
  &offering=<value>&observedproperty=<value>&responseformat=<value>[&eventtime=<value>]
-----

with the first line boilerplate that's the same for every request, and the second
line containing discretionary choices that will vary between requests.

This URL contains the required and optional parameters for a +GetObservation+ request for the NDBC SOS
server. The available values for the required parameters are:

* *+request+* - only +GetObservation+
* *+service+* - only +SOS+
* *+version+* - only +1.0.0+
* *+offering+* - this can be either +urn:ioos:station:wmo::<stationID>+ for a single station - with
+stationID+ being one of the station ID numbers found in the +sos:procedure+
list in the +GetCapabilities+ response, or
+urn:ioos:network:noaa.nws.ndbc:all+ to collect pertinent information from
all available sensors;
* *+observedProperty+* - one of the values listed below; and
* *+responseFormat+* - one of the values listed below.

The available *+observedProperty+* values are found
within the +ows:OperationsMetadata+
tag/section of the +GetCapabilities+ response in an +ows:Operation+ tag with
attribute
+GetObservation+.  This section contains an +ows:Parameter+ tag with the
attribute
+observedProperty+ that lists the allowed values for requesting an
observed property.  This section is:
[source,xml]
-----
<ows:Parameter name="observedProperty">
  <ows:AllowedValues>
    <ows:Value>air_temperature</ows:Value>
    <ows:Value>air_pressure_at_sea_level</ows:Value>
    <ows:Value>sea_water_electrical_conductivity</ows:Value>
    <ows:Value>currents</ows:Value>
    <ows:Value>sea_water_salinity</ows:Value>
    <ows:Value>sea_floor_depth_below_sea_surface</ows:Value>
    <ows:Value>sea_water_temperature</ows:Value>
    <ows:Value>waves</ows:Value>
    <ows:Value>winds</ows:Value>
  </ows:AllowedValues>
</ows:Parameter>
-----
The available *+responseFormat+* values are similarly
found in an +ows:Parameter+ section with attribute +responseFormat+:
[source,xml]
-----
<ows:Parameter name="responseFormat">
  <ows:AllowedValues>
    <ows:Value>text/xml;subtype="om/1.0.0"</ows:Value>
    <ows:Value>text/csv</ows:Value>
    <ows:Value>text/tab-separated-values</ows:Value>
    <ows:Value>application/vnd.google-earth.kml+xml</ows:Value>
    <ows:Value>text/xml;schema="ioos/0.6.1"</ows:Value>
    <ows:Value>application/ioos+xml;version=0.6.1</ows:Value>
  </ows:AllowedValues>
</ows:Parameter>
-----

Optional parameters for the +GetObservations+ query are:

* *+eventtime+* - which can be +DateTime+, +DateTime1/DateTime2+ or +latest+,
with +DateTime+ formatted as +2008-06-04-T00:00:00Z+ or
+2008-06-04-T00:00Z+, and with +latest+ returning only the most
recent observation; and

* *+featureofinterest+* - a bounding box coded as
+BBOX:<min_lon>,<min_lat>,<max_lon>,<max_lat>+ that limits
responses to stations located within the specified spatial
bounds.

Now we will try some sample +getObservation+ requests on the NDBC SOS
server.
If we send a request with the following parameters:

* *+offering+* - +urn:ioos:station:wmo:46088+
* *+observedProperty+* - +currents+
* *+responseformat+* - +text/xml+
* *+eventtime+* - +latest+

which will make the non-boilerplate part of our URL:
-----
&offering=urn:ioos:station:wmo:46088&observedproperty=Currents&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest
-----

we are asking for the most recent value of currents at station 46088 in XML
format.  The full URL for this request is:

http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:station:wmo:46088&observedproperty=Currents&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest[http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:station:wmo:46088&observedproperty=Currents&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest]

which returns (although the exact value returned will change over time since
this is the latest value from an operational server) a large amount of XML,
with the following XML fragment the one containing the data we requested:
[source,xml]
-----
<swe2:values>
  urn:ioos:sensor:wmo:46088::pscm0,12,27.7,,,,,,,,,,,,,,,,,,,
</swe2:values>
-----
A similar request for +air_temperature+, where we simply replace
+currents+ in the previous URL with +air_temperature+ as the value for
*+observedproperty+*, is:

http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:station:wmo:46088&observedproperty=air_temperature&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest[http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:station:wmo:46088&observedproperty=air_temperature&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest]

which returns another XML fragment containing our requested data:
[source,xml]
-----
<swe2:values>
  urn:ioos:sensor:wmo:46088::airtemp1,13.40
</swe2:values>
-----

A request for the latest +air_temperature+ values for all stations in the
network - where the *+offering+* value is +urn:ioos:network:noaa.nws.ndbc:all+ - is:

http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:network:noaa.nws.ndbc:all&observedproperty=air_temperature&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest[+http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:network:noaa.nws.ndbc:all&observedproperty=air_temperature&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest+]

which returns a very large XML file containing the requested data at every
station, each in a separate XML fragment within the file:
[source,xml]
-----
...
<swe2:values>urn:ioos:sensor:wmo:0y2w3::airtemp1,13.00</swe2:values>
...
<swe2:values>urn:ioos:sensor:wmo:32st0::airtemp1,17.80</swe2:values>
...
-----

If we add the optional parameter *+featureofinterest+* with the value
+BBOX:-90,25,-85,30+ to the previous request, we obtain the following URL
requesting the latest +air_temperature+ values for all stations within the
network within a bounding box between -90 and -85 degrees longitude and
25 and 30 degrees latitude.

http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:network:noaa.nws.ndbc:all&featureofinterest=BBOX:-90,25,-85,30&observedproperty=air_temperature&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest[+http://sdf.ndbc.noaa.gov/sos/server.php?request=GetObservation&service=SOS&version=1.0.0&offering=urn:ioos:network:noaa.nws.ndbc:all&featureofinterest=BBOX:-90,25,-85,30&observedproperty=air_temperaturel&responseformat=text/xml;subtype=%22om/1.0.0%22&eventtime=latest+]

To be blunt about it, obtaining data from an SOS server via these types of URL
requests is a lot of tedious manual work that gets you a tremendous amount
of XML code within which only very small fragments contain what you actually
need. ERDDAP does this for you and offers the extracted data via the same
interface as everything else.

Skeleton Code for an +EDDTableFromSOS+ Dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The skeleton +dataset+ XML code for a +EDDTableFromSOS+ dataset is:
[source,xml]
-----
<dataset type="EDDTableFromSOS" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <defaultDataQuery>...</defaultDataQuery> <!-- 0 or 1 -->
  <defaultGraphQuery>...</defaultGraphQuery> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <sosServerType>...</sosServerType> <!-- 0 or 1, but strongly recommended.
    This lets you specify the type of SOS server (so ERDDAP doesn't have to figure it out).
    Valid values are: IOOS_NDBC, IOOS_NOS, OOSTethys, and WHOI. -->
  <stationIdSourceName>...</stationIdSourceName> <!-- 0 or 1. 
    Default="station_id". -->
  <longitudeSourceName>...</longitudeSourceName>
  <latitudeSourceName>...</latitudeSourceName>
  <altitudeSourceName>...</altitudeSourceName>
  <altitudeSourceMinimum>...</altitudeSourceMinimum> <!-- 0 or 1 -->
  <altitudeSourceMaximum>...</altitudeSourceMaximum> <!-- 0 or 1 -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit> 
  <timeSourceName>...</timeSourceName>
  <timeSourceFormat>...</timeSourceFormat>
    <!-- timeSourceFormat MUST be either
    * For numeric data: a 
      UDUnits-compatible
      string (with the format 
      "units since baseTime") describing how to interpret
      source time values (e.g., "seconds since 1970-01-01T00:00:00Z"),
      where the base time is an ISO 8601:2004(E) formatted date time string 
      (yyyy-MM-dd'T'HH:mm:ssZ).
    * For String String data: an org.joda.time.format.DateTimeFormat 
      string (which is mostly compatible with java.text.SimpleDateFormat)
      describing how to interpret string times  (e.g., the 
      ISO8601TZ_FORMAT "yyyy-MM-dd'T'HH:mm:ssZ").  See Joda DateTimeFormat -->
  <observationOfferingIdRegex>...</observationOfferingIdRegex>
    <!-- Only observationOfferings with IDs (usually the station names) 
    which match this regular expression (tutorial) will be included 
    in the dataset (".+" will catch all station names). -->
  <requestObservedPropertiesSeparately>true|false(default)
    </requestObservedPropertiesSeparately>
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more. 
    * Each dataVariable MUST include the dataType tag.
    * Each dataVariable MUST include the observedProperty attribute. 
    * For IOOS SOS servers, *every* variable returned in the text/csv
      response MUST be included in this ERDDAP dataset definition. -->
</dataset>
-----


[[Automatic_Dataset_Configuration]]
Automatic Dataset XML Creation with +GenerateDatasetsXml+
---------------------------------------------------------

+GenerateDatasetsXml+
~~~~~~~~~~~~~~~~~~~~~

+GenerateDatasetsXML+ is a command-line program that generates a
rough - albeit sometimes immediately useable - draft of the dataset XML for most of the kinds of datasets one usually
deals with in the geosciences.  
When you invoke +GenerateDatasetsXML+ on the command-line, it
will first ask you for an +EDDType+, which is one of a number
of dataset types.  The available dataset types - along with a
brief description of what kind of data each applies to - are:

* xref:EDDGridAggregateExistingDimension[+EDDGridAggregateExistingDimension+] - aggregates two or more EDDGrid
datasets based on differing values of the first dimension, e.g. aggregating
otherwise identical daily grid files over a year;
* +EDDGridFromThreddsCatalog+ - aggregates a collection of
xref:EDDGridFromDap[+EDDGridFromDap+] datasets it finds by crawling recursively through a
THREDDS catalog;
* xref:EDDGridFromDap[+EDDGridFromDap+] - processes a single file of gridded data on
a DAP server;
* +EDDGridFromErddap+ - processes a single file of gridded data on a remote
ERDDAP server;
* xref:EDDGridFromNcFiles[+EDDGridFromNcFiles+] - aggregates two or
more local GRIB, HDF or NetCDF files;
* +EDDTableFromAsciiFiles+ - aggregates data from comma- and tab-delimited
tabular ASCII data files;
* +EDDTableFromDapSequence+ - processes data within 1- and 2-level sequences
from DAP servers;
* +EDDTableFromDatabase+ - processes data from one database table or view;
* +EDDTableFromErddap+ - processes tabular data from a remote ERDDAP
server;
* +EDDTableFromIoosSOS+ - processes data from an IOOS Sensor Observation
Service (SOS) server;
* +EDDTableFromFromNcfiles+ - aggregates data from NetCDF files with
several variables, each with a shared dimension (e.g. time) or more
than one shared dimension (e.g. time, lon, lat);
* +EDDTableFromOBIS+ - processes data from an Ocean Biogeographic
Information System (OBIS) server;
* +EDDTableFromSOS+ - processes data from an SOS server; and
* +EDDTableFromThreddsFiles+ - aggregates data files served on a THREDDS
server which have several variables, each with one or more shared
dimensions (e.g. time, lon, lat).



When you run the +GenerateDatasetsXml+ script you obtain the
following command-line:

-----
*** GenerateDatasetsXml ***
Results are shown on the screen, put on the clipboard and put in
/raid/erddap/logs/log.txt
Press ^C to exit at any time.
Type "" to change from a non-nothing default back to nothing.
DISCLAIMER:
  The chunk of datasets.xml made by GenerateDatasetsXml isn't perfect.
  YOU MUST READ AND EDIT THE XML BEFORE USING IT IN A PUBLIC ERDDAP.
  GenerateDatasetsXml relies on a lot of rules-of-thumb which aren't always
  correct.  *YOU* ARE RESPONSIBLE FOR ENSURING THE CORRECTNESS OF THE XML
  THAT YOU ADD TO ERDDAP'S datasets.xml FILE.
For detailed information, see
http://coastwatch.pfeg.noaa.gov/erddap/download/setupDatasetsXml.html

The EDDType options are:
  EDDGridAggregateExistingDimension
  EDDGridFromDap
  EDDGridFromErddap
  EDDGridFromNcFiles
  EDDGridFromThreddsCatalog
  EDDTableFromAsciiFiles
  EDDTableFromDapSequence
  EDDTableFromDatabase
  EDDTableFromErddap
  EDDTableFromIoosSOS
  EDDTableFromNcFiles
  EDDTableFromOBIS
  EDDTableFromSOS
  EDDTableFromThreddsFiles
Which EDDType (default="EDDGridFromDap")
?
-----

Each of the +EDDType+ options usually results in a different set of
queries.  Each will be demonstrated with an example below.

[[EDDTableFromSOS]]
+EDDTableFromSOS+ Example
~~~~~~~~~~~~~~~~~~~~~~~~~

In this section we will automatically create a +dataset+ XML tag
for the NDBC SOS server found at:

http://sdf.ndbc.noaa.gov/sos/[http://sdf.ndbc.noaa.gov/sos/]

The required URL prefix for the SOS server is shown at the top
of that page to be:
-----
http://sdf.ndbc.noaa.gov/sos/server.php
-----

Script
^^^^^^

The +GenerateDatasetsXml+ script for this is:
-----
The EDDType options are:
  EDDGridAggregateExistingDimension
  EDDGridFromDap
  EDDGridFromErddap
  EDDGridFromNcFiles
  EDDGridFromThreddsCatalog
  EDDTableFromAsciiFiles
  EDDTableFromAwsXmlFiles
  EDDTableFromDapSequence
  EDDTableFromDatabase
  EDDTableFromErddap
  EDDTableFromIoosSOS
  EDDTableFromNcFiles
  EDDTableFromNcCFFiles
  EDDTableFromOBIS
  EDDTableFromSOS
  EDDTableFromThreddsFiles
Which EDDType (default="EDDTableFromNcFiles")
? EDDTableFromSOS
URL (default="/home/baum/ERDDAP3/")
? http://sdf.ndbc.noaa.gov/sos/server.php     
SOS version (or "" for default) (default="")
? 1.0.0
working...
-----

Dataset Tag Generated
^^^^^^^^^^^^^^^^^^^^^

The +dataset+ tag initially generated by the script is:
-----
<dataset type="EDDTableFromSOS" datasetID="noaa_ndbc_ba12_4f37_91da" active="true">
    <sourceUrl>http://sdf.ndbc.noaa.gov/sos/server.php</sourceUrl>
    <reloadEveryNMinutes>1440</reloadEveryNMinutes>
    <observationOfferingIdRegex>.+</observationOfferingIdRegex>
    <requestObservedPropertiesSeparately>false</requestObservedPropertiesSeparately>
    <longitudeSourceName>longitude</longitudeSourceName>
    <latitudeSourceName>latitude</latitudeSourceName>
    <altitudeSourceName>???depth???ioos:VerticalPosition</altitudeSourceName>
    <altitudeMetersPerSourceUnit>-1</altitudeMetersPerSourceUnit>
    <timeSourceName>time</timeSourceName>
    <timeSourceFormat>yyyy-MM-dd'T'HH:mm:ss'Z'</timeSourceFormat>
    <addAttributes>
        <att name="cdm_data_type">Station</att>
        <att name="Conventions">???COARDS, CF-1.6, Unidata Dataset Discovery v1.0</att>
        <att name="infoUrl">http://sdf.ndbc.noaa.gov/</att>
        <att name="institution">National Data Buoy Center</att>
        <att name="license">???[standard]</att>
        <att name="Metadata_Conventions">???COARDS, CF-1.6, Unidata Dataset Discovery v1.0</att>
        <att name="standard_name_vocabulary">???CF-12</att>
        <att name="summary">National Data Buoy Center SOS</att>
        <att name="title">National Data Buoy Center SOS</att>
    </addAttributes>
    <!-- Please specify the actual cdm_data_type (TimeSeries?) and related info below, for example...
        <att name="cdm_timeseries_variables">station, longitude, latitude</att>
        <att name="subsetVariables">station, longitude, latitude</att>
    -->
    <dataVariable>
        <sourceName>air_temperature</sourceName>
        <destinationName>air_temperature</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Unknown</att>
            <att name="long_name">???Air Temperature</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/air_temperature</att>
            <att name="standard_name">air_temperature</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>air_pressure_at_sea_level</sourceName>
        <destinationName>air_pressure_at_sea_level</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Pressure</att>
            <att name="long_name">???Air Pressure</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/air_pressure_at_sea_level</att>
            <att name="standard_name">air_pressure_at_sea_level</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>sea_water_electrical_conductivity</sourceName>
        <destinationName>sea_water_electrical_conductivity</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Salinity</att>
            <att name="long_name">???Sea Water Electrical Conductivity</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/sea_water_electrical_conductivity</att>
            <att name="standard_name">sea_water_electrical_conductivity</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>currents</sourceName>
        <destinationName>currents</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Currents</att>
            <att name="long_name">???Currents</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/currents</att>
            <att name="standard_name">currents</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>sea_water_salinity</sourceName>
        <destinationName>sea_water_salinity</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Salinity</att>
            <att name="long_name">???Sea Water Salinity</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/sea_water_salinity</att>
            <att name="standard_name">sea_water_salinity</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>sea_floor_depth_below_sea_surface</sourceName>
        <destinationName>sea_floor_depth_below_sea_surface</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Location</att>
            <att name="long_name">???Sea Floor Depth Below Sea Surface</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/sea_floor_depth_below_sea_surface</att>
            <att name="standard_name">sea_floor_depth_below_sea_surface</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>sea_water_temperature</sourceName>
        <destinationName>sea_water_temperature</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Unknown</att>
            <att name="long_name">???Sea Water Temperature</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/sea_water_temperature</att>
            <att name="standard_name">sea_water_temperature</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>waves</sourceName>
        <destinationName>waves</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Surface Waves</att>
            <att name="long_name">???Waves</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/waves</att>
            <att name="standard_name">waves</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>winds</sourceName>
        <destinationName>winds</destinationName>
        <dataType>double</dataType>
        <addAttributes>
            <att name="ioos_category">???Wind</att>
            <att name="long_name">???Winds</att>
            <att name="observedProperty">http://mmisw.org/ont/cf/parameter/winds</att>
            <att name="standard_name">winds</att>
            <att name="units">???</att>
        </addAttributes>
    </dataVariable>
</dataset>
-----

Initial Error Messages
^^^^^^^^^^^^^^^^^^^^^^

The error message found in +log.txt+ when the NDBC SOS +dataset+
tag was added to +datasets.xml+ and the Tomcat server restarted.

-----
*** constructing EDDTableFromSOS noaa_ndbc_ba12_4f37_91da
  quickRestartFile exists=true
  downloading getCapabilities from http://sdf.ndbc.noaa.gov/sos/server.php?service=SOS&request=GetCapabilities
  download time=10162ms
  sosPrefix=sos: sosVersion=1.0.0
Warning while parsing gml: lowerCorner!=upperCorner tLat=-77.466 ttLat=80.81 tLon=-179.995 ttLon=180.0
...
Warning while parsing gml: lowerCorner!=upperCorner tLat=-77.466 ttLat=-76.089 tLon=170.193 ttLon=179.914
ndbcIoosServer=true nosIoosServer=false
Table.saveAsJson done. time=28
Station Table=
{
  "table": {
    "columnNames": ["Lon", "Lat", "BeginTime", "EndTime", "ID", "Procedure"],
    "columnTypes": ["double", "double", "String", "double", "String", "String"],
    "columnUnits": [null, null, "UTC", null, null, null],
    "rows": [
      [-87.313, 44.794, "2012-05-31T23:40:00Z", null, "urn:ioos:station:wmo:0y2w3", "urn:ioos:station:wmo:0y2w3"],
...
      [-79.064, 43.261, "2009-05-14T16:10:00Z", null, "urn:ioos:station:wmo:ygnn6", "urn:ioos:station:wmo:ygnn6"]
    ]
  }
}

datasets.xml error on line #1862
While trying to load datasetID=noaa_ndbc_ba12_4f37_91da (after 10347 ms)
java.lang.RuntimeException: datasets.xml error on or before line #1862:
Currently, EDDTableFromSOS only supports cdm_data_type=Other, Point,
TimeSeries, TimeSeriesProfile, Trajectory, TrajectoryProfile, not Station.
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:344)
 at gov.noaa.pfel.erddap.LoadDatasets.run(LoadDatasets.java:300)
Caused by: java.lang.RuntimeException: Currently, EDDTableFromSOS only
supports cdm_data_type=Other, Point, TimeSeries, TimeSeriesProfile,
Trajectory, TrajectoryProfile, not Station.
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromSOS.<init>(EDDTableFromSOS.java:829)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromSOS.fromXml(EDDTableFromSOS.java:257)
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:340)
-----

We will try this again after replacing +Station+
with +TimeSeries+ in the NDBC SOS +data+ tag.  The new error is:
-----
datasets.xml/EDV.ensureValid error for variable
destinationName=air_temperature:
ioos_category="???Unknown" isn't a valid category.
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:344)
 at gov.noaa.pfel.erddap.LoadDatasets.run(LoadDatasets.java:300)
Caused by: java.lang.RuntimeException:
ERROR in Test.ensureTrue:
datasets.xml/EDD.ensureValid error for datasetID=noaa_ndbc_ba12_4f37_91da:
 for dataVariable #5=air_temperature:
-----

We replace all the +ioos_category+ values with the appropriate values from
the xref:ioos_category[+ioos_category+] section, and also put in the
units values as best we can, and try again.  The next error is:
-----
datasets.xml error on line #1862
While trying to load datasetID=noaa_ndbc_ba12_4f37_91da (after 11868 ms)
java.lang.RuntimeException: datasets.xml error on or before line #1862: For
cdm_data_type=TimeSeries, the global attribute cdm_timeseries_variables must
be set.
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:344)
 at gov.noaa.pfel.erddap.LoadDatasets.run(LoadDatasets.java:300)
Caused by: com.cohort.util.SimpleException: For cdm_data_type=TimeSeries, the
global attribute cdm_timeseries_variables must be set.
 at
gov.noaa.pfel.erddap.dataset.EDDTable.accessibleViaNcCF(EDDTable.java:10344)
 at gov.noaa.pfel.erddap.dataset.EDD.ensureValid(EDD.java:557)
 at gov.noaa.pfel.erddap.dataset.EDDTable.ensureValid(EDDTable.java:424)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromSOS.<init>(EDDTableFromSOS.java:921)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromSOS.fromXml(EDDTableFromSOS.java:257)
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:340)
-----

[[EDDTableFromNcCFFiles Example]]
+EDDTableFrom NcCFFiles Example+
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A NetCDF file containing a time series of river discharge values for the
significant rivers that empty into the Gulf of Mexico looks includes the
following header information:

-----
netcdf gom_discharge_1900_present {
dimensions:
	time = UNLIMITED ; // (38808 currently)
	station = 55 ;
	id_strlen = 8 ;
	river_name_strlen = 24 ;
variables:
	float time(time) ;
		time:standard_name = "time" ;
		time:long_name = "time of measurement" ;
		time:units = "days since 1900-01-01T00:00:00Z" ;
	float discharge(station, time) ;
		discharge:_FillValue = -999.f ;
		discharge:units = "cubic meters per second" ;
		discharge:long_name = "discharge" ;
		discharge:standard_name = "discharge" ;
		discharge:valid_min = "0.0" ;
		discharge:coordinates = "gauge_latitude gauge_longitude alt" ;
	char station_id(station, id_strlen) ;
		station_id:long_name = "gauging station ID number" ;
		station_id:cf_role = "timeseries_id" ;
	char river_name(station, river_name_strlen) ;
		river_name:long_name = "name of river" ;
	float end_date(station) ;
		end_date:units = "days since 1900-01-01T00:00:00Z" ;
		end_date:long_name = "date of most recent gauge station data" ;
	float start_date(station) ;
		start_date:units = "days since 1900-01-01T00:00:00Z" ;
		start_date:long_name = "date of first gauge station data" ;
	float gauge_latitude(station) ;
		gauge_latitude:units = "degrees_north" ;
		gauge_latitude:long_name = "latitude of gauge station" ;
	float gauge_longitude(station) ;
		gauge_longitude:units = "degrees_east" ;
		gauge_longitude:long_name = "longitude of gauge station" ;
	float mouth_latitude(station) ;
		mouth_latitude:units = "degrees_north" ;
		mouth_latitude:long_name = "latitute of river mouth" ;
	float mouth_longitude(station) ;
		mouth_longitude:units = "degrees_east" ;
		mouth_longitude:long_name = "longitude of river mouth" ;
	float alt(station) ;
		alt:long_name = "vertical distance above the surface" ;
		alt:standard_name = "height" ;
		alt:units = "m" ;
		alt:positive = "up" ;
		alt:axis = "Z" ;

// global attributes:
		:title = "Time-series discharge data for rivers emptying in the Gulf of Mexico." ;
		:featureType = "timeSeries" ;
		:conventions = "CF-1.6" ;
-----

An initial XML code fragment for this file is created using
+GenerateDatasetsXml+ via:

-----
Which EDDType (default="")
? EDDTableFromNcCFFiles
Parent directory (default="")
? /data/rivers
File name regex (e.g., ".*\.nc") (default="")
? .*\.nc
Full file name of one file
(default="")
? /data/rivers/gom_discharge_1900_present.nc
ReloadEveryNMinutes (e.g., 10080) (default="")
?
PreExtractRegex (default="")
?
PostExtractRegex (default="")
?
ExtractRegex (default="")
?
Column name for extract (default="")
?
Sort files by sourceName (default="")
?
infoUrl (default="")
?
institution (default="")
?
summary (default="")
?
title (default="")
?
working.......
-----

The initial XML code is:

[source,xml]
-----
<dataset type="EDDTableFromNcCFFiles" datasetID="rivers_c356_96e2_45a3" active="true">
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <fileDir>/data/rivers/</fileDir>
    <recursive>true</recursive>
    <fileNameRegex>.*\.nc</fileNameRegex>
    <metadataFrom>last</metadataFrom>
    <preExtractRegex></preExtractRegex>
    <postExtractRegex></postExtractRegex>
    <extractRegex></extractRegex>
    <columnNameForExtract></columnNameForExtract>
    <sortFilesBySourceNames></sortFilesBySourceNames>
    <fileTableInMemory>false</fileTableInMemory>
    <!-- sourceAttributes>
        <att name="cdm_data_type">TimeSeries</att>
        <att name="cdm_timeseries_variables">station_id, river_name, end_date, start_date, gauge_latitude, gauge_longitude, mouth_latitude, mouth_longitude, alt</att>
        <att name="contact">mkhoward@tamu.edu</att>
        <att name="conventions">CF-1.6</att>
        <att name="featureType">timeSeries</att>
        <att name="processing_information_url">http://stommel.tamu.edu/~baum/river.html</att>
        <att name="rivers">U.S. Gulf of Mexico Rivers</att>
        <att name="source">USGS and U.S. Army Corps of Engineers</att>
        <att name="source_url1">http://waterdata.usgs.gov/nwis</att>
        <att name="source_url2">http://www2.mvn.usace.army.mil/eng/edhd/dailystagedisplay.asp</att>
        <att name="subsetVariables">station_id, river_name, end_date, start_date, gauge_latitude, gauge_longitude, mouth_latitude, mouth_longitude, alt</att>
        <att name="title">Time-series discharge data for rivers emptying in the Gulf of Mexico.</att>
    </sourceAttributes -->
    <addAttributes>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="conventions">null</att>
        <att name="creator_email">mkhoward@tamu.edu</att>
        <att name="creator_name">TAMU</att>
        <att name="history">USGS and U.S. Army Corps of Engineers</att>
        <att name="infoUrl">???</att>
        <att name="institution">TAMU</att>
        <att name="keywords">above, altitude, atmosphere,
Atmosphere &gt; Altitude &gt; Station Height,
data, date, discharge, distance, emptying, first, gauge, gauging, gulf, gulf of mexico, height, latitute, measurement, mexico., most, mouth, name, number, recent, river, rivers, series, station, statistics, surface, tamu, time, time-series, vertical</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="sourceUrl">(local files)</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">Time-series discharge data for rivers emptying in the Gulf of Mexico.</att>
    </addAttributes>
    <dataVariable>
        <sourceName>time</sourceName>
        <destinationName>time</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="HDF5_chunksize" type="int">1</att>
            <att name="long_name">time of measurement</att>
            <att name="standard_name">time</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>discharge</sourceName>
        <destinationName>discharge</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-999.0</att>
            <att name="coordinates">gauge_latitude gauge_longitude alt</att>
            <att name="HDF5_chunksize" type="intList">55 1</att>
            <att name="long_name">discharge</att>
            <att name="missing_value" type="float">-999.0</att>
            <att name="standard_name">discharge</att>
            <att name="units">cubic meters per second</att>
            <att name="valid_max">43041.5</att>
            <att name="valid_min">0.0</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">50000.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Time</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>station_id</sourceName>
        <destinationName>station_id</destinationName>
        <dataType>String</dataType>
        <!-- sourceAttributes>
            <att name="cf_role">timeseries_id</att>
            <att name="long_name">gauging station ID number</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">100.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Statistics</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>river_name</sourceName>
        <destinationName>river_name</destinationName>
        <dataType>String</dataType>
        <!-- sourceAttributes>
            <att name="long_name">name of river</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>end_date</sourceName>
        <destinationName>end_date</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">date of most recent gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="standard_name">time</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>start_date</sourceName>
        <destinationName>start_date</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">date of first gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="standard_name">time</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>gauge_latitude</sourceName>
        <destinationName>latitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">latitude of gauge station</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">90.0</att>
            <att name="colorBarMinimum" type="double">-90.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">latitude</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>gauge_longitude</sourceName>
        <destinationName>longitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">longitude of gauge station</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">180.0</att>
            <att name="colorBarMinimum" type="double">-180.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">longitude</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>mouth_latitude</sourceName>
        <destinationName>mouth_latitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">latitute of river mouth</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">90.0</att>
            <att name="colorBarMinimum" type="double">-90.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">latitude</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>mouth_longitude</sourceName>
        <destinationName>mouth_longitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">longitude of river mouth</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">180.0</att>
            <att name="colorBarMinimum" type="double">-180.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">longitude</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>alt</sourceName>
        <destinationName>altitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="axis">Z</att>
            <att name="long_name">vertical distance above the surface</att>
            <att name="positive">up</att>
            <att name="standard_name">height</att>
            <att name="units">m</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
            <att name="standard_name">altitude</att>
        </addAttributes>
    </dataVariable>
</dataset>
-----

As created, this fragment throws errors when ERDDAP attempts to read and
process it.  After some debugging, a working fragment is:

[source,xml]
-----
<dataset type="EDDTableFromNcCFFiles" datasetID="rivers_c356_96e2_45a3" active="true">
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <fileDir>/data/rivers/</fileDir>
    <recursive>true</recursive>
    <fileNameRegex>.*\.nc</fileNameRegex>
    <metadataFrom>last</metadataFrom>
    <preExtractRegex></preExtractRegex>
    <postExtractRegex></postExtractRegex>
    <extractRegex></extractRegex>
    <columnNameForExtract></columnNameForExtract>
    <sortFilesBySourceNames></sortFilesBySourceNames>
    <fileTableInMemory>false</fileTableInMemory>
    <!-- sourceAttributes>
        <att name="cdm_data_type">TimeSeries</att>
        <att name="cdm_timeseries_variables">station_id, river_name, end_date, start_date, latitude, longitude, mouth_latitude, mouth_longitude, altitude</att>
        <att name="contact">mkhoward@tamu.edu</att>
        <att name="conventions">CF-1.6</att>
        <att name="featureType">timeSeries</att>
        <att name="processing_information_url">http://stommel.tamu.edu/~baum/river.html</att>
        <att name="rivers">U.S. Gulf of Mexico Rivers</att>
        <att name="source">USGS and U.S. Army Corps of Engineers</att>
        <att name="source_url1">http://waterdata.usgs.gov/nwis</att>
        <att name="source_url2">http://www2.mvn.usace.army.mil/eng/edhd/dailystagedisplay.asp</att>
        <att name="subsetVariables">station_id, river_name, end_date, start_date, latitude, longitude, mouth_latitude, mouth_longitude, altitude</att>
        <att name="title">Time-series discharge data for rivers emptying in the Gulf of Mexico.</att>
    </sourceAttributes -->
    <addAttributes>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="conventions">null</att>
        <att name="creator_email">mkhoward@tamu.edu</att>
        <att name="creator_name">TAMU</att>
        <att name="history">USGS and U.S. Army Corps of Engineers</att>
        <att name="infoUrl">???</att>
        <att name="institution">TAMU</att>
        <att name="keywords">above, altitude, atmosphere,
Atmosphere &gt; Altitude &gt; Station Height,
data, date, discharge, distance, emptying, first, gauge, gauging, gulf, gulf of mexico, height, latitute, measurement, mexico., most, mouth, name, number, recent, river, rivers, series, station, statistics, surface, tamu, time, time-series, vertical</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="sourceUrl">(local files)</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">Time-series discharge data for rivers emptying in the Gulf of Mexico.</att>
        <att name="cdm_data_type">TimeSeries</att>
        <att name="cdm_timeseries_variables">station_id, river_name, end_date, start_date, latitude, longitude, mouth_latitude, mouth_longitude, altitude</att>
        <att name="contact">mkhoward@tamu.edu</att>
        <att name="conventions">CF-1.6</att>
        <att name="featureType">timeSeries</att>
        <att name="processing_information_url">http://stommel.tamu.edu/~baum/river.html</att>
        <att name="rivers">U.S. Gulf of Mexico Rivers</att>
        <att name="source">USGS and U.S. Army Corps of Engineers</att>
        <att name="source_url1">http://waterdata.usgs.gov/nwis</att>
        <att name="source_url2">http://www2.mvn.usace.army.mil/eng/edhd/dailystagedisplay.asp</att>
        <att name="subsetVariables">station_id, river_name, end_date, start_date, latitude, longitude, mouth_latitude, mouth_longitude, altitude</att>
        <att name="title">Time-series discharge data for rivers emptying in the Gulf of Mexico.</att>
    </addAttributes>
    <dataVariable>
        <sourceName>time</sourceName>
        <destinationName>time</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="HDF5_chunksize" type="int">1</att>
            <att name="long_name">time of measurement</att>
            <att name="standard_name">time</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="HDF5_chunksize" type="int">1</att>
            <att name="long_name">time of measurement</att>
            <att name="standard_name">time</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>discharge</sourceName>
        <destinationName>discharge</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-999.0</att>
            <att name="coordinates">gauge_latitude gauge_longitude alt</att>
            <att name="HDF5_chunksize" type="intList">55 1</att>
            <att name="long_name">discharge</att>
            <att name="missing_value" type="float">-999.0</att>
            <att name="standard_name">discharge</att>
            <att name="units">cubic meters per second</att>
            <att name="valid_max">43041.5</att>
            <att name="valid_min">0.0</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">50000.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Time</att>
            <att name="_FillValue" type="float">-999.0</att>
            <att name="coordinates">gauge_latitude gauge_longitude alt</att>
            <att name="HDF5_chunksize" type="intList">55 1</att>
            <att name="long_name">discharge</att>
            <att name="missing_value" type="float">-999.0</att>
            <att name="standard_name">discharge</att>
            <att name="units">cubic meters per second</att>
            <att name="valid_max">43041.5</att>
            <att name="valid_min">0.0</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>station_id</sourceName>
        <destinationName>station_id</destinationName>
        <dataType>String</dataType>
        <!-- sourceAttributes>
            <att name="cf_role">timeseries_id</att>
            <att name="long_name">gauging station ID number</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">100.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Statistics</att>
            <att name="cf_role">timeseries_id</att>
            <att name="long_name">gauging station ID number</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>river_name</sourceName>
        <destinationName>river_name</destinationName>
        <dataType>String</dataType>
        <!-- sourceAttributes>
            <att name="long_name">name of river</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Unknown</att>
            <att name="long_name">name of river</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>end_date</sourceName>
        <destinationName>end_date</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">date of most recent gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="standard_name">time</att>
            <att name="long_name">date of most recent gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>start_date</sourceName>
        <destinationName>start_date</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">date of first gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="standard_name">time</att>
            <att name="long_name">date of first gauge station data</att>
            <att name="units">days since 1900-01-01T00:00:00Z</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>gauge_latitude</sourceName>
        <destinationName>latitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">latitude of gauge station</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">90.0</att>
            <att name="colorBarMinimum" type="double">-90.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">latitude</att>
            <att name="long_name">latitude of gauge station</att>
            <att name="units">degrees_north</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>gauge_longitude</sourceName>
        <destinationName>longitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">longitude of gauge station</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">180.0</att>
            <att name="colorBarMinimum" type="double">-180.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">longitude</att>
            <att name="long_name">longitude of gauge station</att>
            <att name="units">degrees_east</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>mouth_latitude</sourceName>
        <destinationName>mouth_latitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">latitute of river mouth</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">90.0</att>
            <att name="colorBarMinimum" type="double">-90.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">latitude</att>
            <att name="long_name">latitute of river mouth</att>
            <att name="units">degrees_north</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>mouth_longitude</sourceName>
        <destinationName>mouth_longitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="long_name">longitude of river mouth</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">180.0</att>
            <att name="colorBarMinimum" type="double">-180.0</att>
            <att name="ioos_category">Location</att>
            <att name="standard_name">longitude</att>
            <att name="long_name">longitude of river mouth</att>
            <att name="units">degrees_east</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>alt</sourceName>
        <destinationName>altitude</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="axis">Z</att>
            <att name="long_name">vertical distance above the surface</att>
            <att name="positive">up</att>
            <att name="standard_name">height</att>
            <att name="units">m</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
            <att name="standard_name">altitude</att>
            <att name="axis">Z</att>
            <att name="long_name">vertical distance above the surface</att>
            <att name="positive">up</att>
            <att name="standard_name">height</att>
            <att name="units">m</att>
        </addAttributes>
    </dataVariable>
</dataset>

-----


[[EDDGridFromThreddsCatalog]]
+EDDGridFromThreddsCatalog+ Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most of the +EODType+ options generate an XML chunk for one dataset
from one specific data source.  The +EDDGridFromThreddsCatalog+ option
generates all of the XML chunks needed for all of the individual
+EDDGridFromDap+ datasets it finds by crawling recursively through
a THREDDS catalog.  

A THREDDS catalog URL is supplied as an answer
to one of the queries for this option.  A THREDDS URL containing
+/catalog/+ is *required*.  Examples of appropriate URLs would be:

+http://oceanwatch.pfeg.noaa.gov/thredds/catalog/catalog.xml+

+http://oceanwatch.pfeg.noaa.gov/thredds/catalog/Satellite/aggregsatMH/chla/catalog.xml+

Note that the URL you use to actually view THREDDS catalogs on a web
browser ends in +.html+, and that the required URL for this option is
basically that URL with +.html+ replaced with +.xml+.

An example wherein we'll use the EDDType +EDDGridFromThreddsCatalog+ to crawl
through the files at the THREDDS URL

+http://megara.tamu.edu:8080/thredds/catalog/adcp2/catalog.xml+

is:

-----
Which EDDType (default="EDDGridFromDap")
? EDDGridFromThreddsCatalog
URL (usually ending in /catalog.xml) (default="")
? http://megara.tamu.edu:8080/thredds/catalog/adcp2/catalog.xml
Dataset name regex (e.g., ".*") (default="")
? .*
ReloadEveryNMinutes (e.g., 10080) (default="")
? 10080

Starting http://megara.tamu.edu:8080/thredds/catalog/adcp2/catalog.xml
  GOM ADCP2
    42887b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42887b2010.nc  (62 ms)
    42385b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42385b2010.nc  (43 ms)
    42375b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42375b2010.nc  (26 ms)
    42374b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42374b2010.nc  (32 ms)
    42373b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42373b2010.nc  (24 ms)
    42372b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42372b2010.nc  (37 ms)
    42370b2010_top.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42370b2010_top.nc  (21 ms)
    42370b2010_bot.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42370b2010_bot.nc  (28 ms)
    42365b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42365b2010.nc  (24 ms)
    42364b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42364b2010.nc  (35 ms)
    42363b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42363b2010.nc  (20 ms)
    42362b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42362b2010.nc  (27 ms)
    42361b2010.nc
      http://megara.tamu.edu/thredds/dodsC/adcp2/42361b2010.nc  (23 ms)
Finished http://megara.tamu.edu:8080/thredds/catalog/adcp2/catalog.xml  (490
ms)


*** GenerateDatasetsXml ***
Results are shown on the screen, put on the clipboard and put in
/raid/erddap/logs/log.txt
Press ^C to exit at any time.
Type "" to change from a non-nothing default back to nothing.`
? ^C
-----

The XML created for just the first file (+42887b2010.nc+) in the list
above is:

[source,xml]
-----
<dataset type="EDDGridFromDap" datasetID="tamu_76b4_434b_dc44" active="true">
    <sourceUrl>http://megara.tamu.edu/thredds/dodsC/adcp2/42887b2010.nc</sourceUrl>
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <altitudeMetersPerSourceUnit>1</altitudeMetersPerSourceUnit>
    <!-- sourceAttributes>
        <att name="comment">Flag values: 0 = quality not evaluated; 1 = failed
                  quality test; 2 = questionable or suspect data; 3 = good data/passed quality
                  test; 9 = missing data</att>
        <att name="note1">Total lines in ASCII file = 1461342</att>
        <att name="note2">Total lines with depth column problems = 0</att>
        <att name="note3">Total lines with speed etc. problems = 6263</att>
        <att name="original_filename">42887b2010.txt</att>
    </sourceAttributes -->
    <addAttributes>
        <att name="cdm_data_type">Grid</att>
        <att name="Conventions">COARDS, CF-1.6, Unidata Dataset Discovery v1.0</att>
        <att name="infoUrl">http://megara.tamu.edu/thredds/dodsC/adcp2/42887b2010.nc.html</att>
        <att name="institution">TAMU</att>
        <att name="keywords">42887b2010,
                  Oceans &gt; Ocean Circulation &gt; Ocean Currents,
                  adcp2, bin, circulation, clockwise, currents, degrees, eastward,
                  eastward_sea_water_velocity, flag, from, gom, north, northward,
                  northward_sea_water_velocity, ocean, oceans, overall, quality, sea, seawater,
                  speed, status, tamu, true, velocity, water</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">COARDS, CF-1.6, Unidata Dataset Discovery v1.0</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">Flag values: 0 = quality not evaluated; 1 = failed
                  quality test; 2 = questionable or suspect data; 3 = good data/passed quality
                  test; 9 = missing data</att>
        <att name="title">GOM ADCP2, 42887b2010</att>
    </addAttributes>
    <axisVariable>
        <sourceName>time</sourceName>
        <destinationName>time</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">time</att>
            <att name="units">minutes since 1970-01-01 00:00:00</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
            <att name="standard_name">time</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>depth</sourceName>
        <destinationName>depth</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">depth below surface of measurements</att>
            <att name="positive">down</att>
            <att name="units">m</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
            <att name="standard_name">depth</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lat</sourceName>
        <destinationName>lat</destinationName>
        <!-- sourceAttributes>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
            <att name="long_name">Lat</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lon</sourceName>
        <destinationName>lon</destinationName>
        <!-- sourceAttributes>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
            <att name="long_name">Lon</att>
        </addAttributes>
    </axisVariable>
    <dataVariable>
        <sourceName>Speed</sourceName>
        <destinationName>Speed</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">sea_water_speed</att>
            <att name="missing_value">99.0</att>
            <att name="units">cm/s</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Dir</sourceName>
        <destinationName>Dir</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">degrees_clockwise_from_true_north</att>
            <att name="missing_value">99.0</att>
            <att name="units">degT</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>u</sourceName>
        <destinationName>u</destinationName>
        <!-- sourceAttributes>
            <att name="missing_value">99.0</att>
            <att name="note">Derived from Speed and true north Dir: u = Speed * sin(Dir).</att>
            <att name="standard_name">eastward_sea_water_velocity</att>
            <att name="units">m/s</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
            <att name="long_name">Eastward Sea Water Velocity</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>v</sourceName>
        <destinationName>v</destinationName>
        <!-- sourceAttributes>
            <att name="missing_value">99.0</att>
            <att name="note">Derived from Speed and true north Dir: v = Speed * cos(Dir).</att>
            <att name="standard_name">northward_sea_water_velocity</att>
            <att name="units">m/s</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
            <att name="long_name">Northward Sea Water Velocity</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Bin_Status_Flag</sourceName>
        <destinationName>Bin_Status_Flag</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">overall_bin_status_flag</att>
            <att name="units">0,1,2,3,9</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">128.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Quality</att>
        </addAttributes>
    </dataVariable>
</dataset>
-----

[[EDDGridFromNcFilesEx]]
+EDDGridFromNcFiles+ Example


The +EDDGridFromNcFiles+ dataset type aggregates data from local, gridded
GRIB +.grb+ and +.grb2+ files, HDF +.hdf4+ and +.hdf5+ files and NetCDF
+.nc+ files.  Other gridded files supported by the
http://www.unidata.ucar.edu/software/netcdf-java/[NetCDF Java library] may also
work, although this has yet to be tested.
For GRIB files, ERDDAP will create a +.gbx+ index file the first time
it reads each GRIB file, so the GRIB files must be in a directory
where the Tomcat user has both read and write permissions.

See the superclass xref:EDDGridFromFiles[+EDDGridFromFiles+] for requirements
and features common to all datasets in the +*Files+ classes.

[source,xml]
-----
<!--
 DISCLAIMER:
   The chunk of datasets.xml made by GenerageDatasetsXml isn't perfect.
   YOU MUST READ AND EDIT THE XML BEFORE USING IT IN A PUBLIC ERDDAP.
   GenerateDatasetsXml relies on a lot of rules-of-thumb which aren't always
   correct.  *YOU* ARE RESPONSIBLE FOR ENSURING THE CORRECTNESS OF THE XML
   THAT YOU ADD TO ERDDAP'S datasets.xml FILE.

 DIRECTIONS:
 * Read about this type of dataset in
   http://coastwatch.pfeg.noaa.gov/erddap/download/setupDatasetsXml.html .
 * Read
 * http://coastwatch.pfeg.noaa.gov/erddap/download/setupDatasetsXml.html#addAttributes
   so that you understand about sourceAttributes and addAttributes.
 * Note: Global sourceAttributes and variable sourceAttributes are listed
   below as comments, for informational purposes only.
   ERDDAP combines sourceAttributes and addAttributes (which have
   precedence) to make the combinedAttributes that are shown to the user.
   (And other attributes are automatically added to longitude, latitude,
   altitude, and time variables).
 * If you don't like a sourceAttribute, override it by adding an
   addAttribute with the same name but a different value
   (or no value, if you want to remove it).
 * All of the addAttributes are computer-generated suggestions. Edit them!
   If you don't like an addAttribute, change it.
 * If you want to add other addAttributes, add them.
 * If you want to change a destinationName, change it.
   But don't change sourceNames.
 * You can change the order of the dataVariables or remove any of them.
!!! The source for HIS_7b9e_ac91_4024 has nGridVariables=3,
but this dataset will only serve 2 because the others use different
dimensions.
-->

<dataset type="EDDGridFromNcFiles" datasetID="HIS_7b9e_ac91_4024" active="true">
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <fileDir>/raid/TGLO/GNOME/HIS/</fileDir>
    <recursive>true</recursive>
    <fileNameRegex>.*\.nc</fileNameRegex>
    <metadataFrom>last</metadataFrom>
    <altitudeMetersPerSourceUnit>1</altitudeMetersPerSourceUnit>
    <!-- sourceAttributes>
        <att name="comment">none</att>
        <att name="Conventions">CF-1.0</att>
        <att name="grid_type">REGULAR</att>
        <att name="history">none</att>
        <att name="institution">Texas A&amp;M University, Dept.of Oceanography</att>
        <att name="references">none</att>
        <att name="source">ROMS 2.1</att>
        <att name="title">ROMS output 48-hour regular grid surface currents</att>
    </sourceAttributes -->
    <addAttributes>
        <att name="cdm_data_type">Grid</att>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="infoUrl">???</att>
        <att name="keywords">48-hour,
                  Oceans &gt; Ocean Circulation &gt; Ocean Currents,
                  a&amp;m, circulation, currents, dept.of, eastward, grid, hour, model,
                  modeling, northward, ocean, oceanography, oceans, output, regional, regional
                  ocean model, regular, roms, sea, seawater, surface,
                  surface_eastward_sea_water_velocity, surface_northward_sea_water_velocity,
                  system, texas, university, velocity, water</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">none</att>
    </addAttributes>
    <axisVariable>
        <sourceName>time</sourceName>
        <destinationName>time2</destinationName>
        <!-- sourceAttributes>
            <att name="axis">T</att>
            <att name="long_name">Valid Time</att>
            <att name="standard_name">time</att>
            <att name="units">seconds since 2003-10-28 0:00:00 0:00</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lat</sourceName>
        <destinationName>latitude</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">Latitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">latitude</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lon</sourceName>
        <destinationName>longitude</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">Longitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">longitude</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </axisVariable>
    <dataVariable>
        <sourceName>water_u</sourceName>
        <destinationName>water_u</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">X</att>
            <att name="long_name">Eastward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att name="standard_name">surface_eastward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>water_v</sourceName>
        <destinationName>water_v</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">Y</att>
            <att name="long_name">Northward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att name="standard_name">surface_northward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
</dataset>
-----

[[EDDGridAggregateExistingDimensionsEx]]
+EDDGridAggregateExistingDimension+ Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The +EDDGridAggregateExistingDimension+ dataset type aggregates two or more
EDDGrid datasets based on different values of the first dimension.
For example, one child dataset might be for 2004 with 366 daily values, and
the other from 2005 with 365 daily values.
The requirements and features are:

. The values for all of the other dimensions - e.g. longitude, latitude, etc. -
[red]*must* be identical for all of the children.

. The parent and child datasets [red]*must* have different values for
the +datasetID+ tag.  If any of the names are identical, the dataset
will fail to load and throw an error message that th evalues of the
aggregated axis are not in sorted order.

. The first child dataset [red]*must* be an xref:EDDGridFromDap[+EDDGridFromDap+] 
dataset and [red]*must* have the lowest values of the aggregated
dimension, e.g. the oldest time values.  All of the other children
[red]*must* be almost identical datasets, differing only in the values
for the first dimension, and are specified by their +sourceUrl+.

. The aggregate dataset gets its metadata from the first specified child.

. An [green]*optional* +ensureAxisValuesAreEqual+ tag can have the
values +true+ or +false+.  The default is +true+ and specifies that the
non-first-axis values [red]*must* be exactly equal in all children.
If this is +false+, then some minor variations are allowed, e.g. this
would allow a value of 0.1 in one child and a value of 0.1000000002
for an analogous value in another child.  The +false+ value should only
be used if you need to, and if you are ceretain that the variation that
is present is acceptable.

The xref:GenerateDatasetsXml[+GenerateDatasetsXml+] program can produce a
rought draft of the +datasets.xml+ entry for an
+EDDGridAggregateExistingDimension+ dataset based on a set of files
served by a Hyrax or THREDDS server.  An example of the input for
the program for this situation is:
-----
EDDType? 
  EDDGridAggregateExistingDimension

Server type (hyrax or thredds)?
  hyrax

Parent URL (e.g., for hyrax, ending in "contents.html"; for thredds, ending in "catalog.xml")?
  http://dods.jpl.nasa.gov/opendap/ocean_wind/ccmp/L3.5a/data/flk/1988/contents.html

File name regex (e.g., ".*\.nc")? 
  month.*flk\.nc\.gz

ReloadEveryNMinutes (e.g., 10080)? 
  10080 
-----
You can use the resulting xref:sourceUrl[+sourceUrl+] tags or delete them and
uncomment the +sourceUrl+ tag so that new files are noticed each time
the dataset is reloaded.

The skeleton XML for an +EDDGridAggregateExistingDimensions+ dataset is:

[source,xml]
-----
<dataset type="EDDGridAggregateExistingDimension" datasetID="..." 
    active="..." >
  <dataset>...</dataset> <!-- This is a regular EDDGridFromDap 
    dataset description child with the lowest values for the aggregated
dimensions. -->
  <sourceUrl>...</sourceUrl> <!-- 0 or many; the sourceUrls for 
    other children.  These children must be listed in order of ascending
values 
    for the aggregated dimension. -->
  <sourceUrls serverType="..." regex="..." recursive="true" 
    >http://someServer/thredds/someSubdirectory/catalog.xml</sourceUrls> 
    <!-- 0 or 1. This specifies how to find the other children, instead 
    of using separate sourceUrl tags for each child.  The advantage of this
    is: new children will be detected each time the dataset is reloaded. 
    The serverType must be "thredds" or "hyrax".  
    An example of a regular expression (regex) (tutorial) is .*\.nc 
    recursive can be "true" or "false".  
    An example of a thredds catalogUrl is
    http://thredds1.pfeg.noaa.gov/thredds/catalog/Satellite/aggregsatMH/chla/catalog.xml
    An example of a hyrax catalogUrl is
    http://podaac-opendap.jpl.nasa.gov/opendap/allData/ccmp/L3.5a/monthly/flk/1988/contents.html
    When these children are sorted by file name, they must be in order of
    ascending values for the aggregated dimension. -->
  <ensureAxisValuesAreEqual>true(the default) or 
    false<ensureAxisValuesAreEqual> 
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
</dataset>
-----

The command-line procedure for creating an ERDDAP XML file for the dataset series at:

http://megara.tamu.edu:8080/thredds/catalog/GROM-hind-reg-sfc-24/catalog.html[+http://megara.tamu.edu:8080/thredds/catalog/GROM-hind-reg-sfc-24/catalog.html+]

is:

-----
Which EDDType (default="EDDGridFromNcFiles")
? EDDGridAggregateExistingDimension
Server type (hyrax or thredds) (default="/raid/TGLO/GNOME/HIS")
? thredds
Parent URL (e.g., for hyrax, ending in "contents.html";  for thredds, ending in "catalog.xml")
? http://megara.tamu.edu:8080/thredds/catalog/GROM-hind-reg-sfc-24/catalog.xml
File name regex (e.g., ".*\.nc")
? *\.nc
ReloadEveryNMinutes (e.g., 10080) (default="10080")
? 10080
-----

Notes:

* If there are any extraneous *06.nc or *72.nc files in the directory, this
will generate an error message about non-monotonically increasing times and
stop.

The result for this - which can be found in +/raid/erddap/logs/log.txt+ - is:

[source,xml]
-----
<dataset type="EDDGridAggregateExistingDimension"
datasetID="tamu_megara_5f79_c1c6_2208" active="true">

<dataset type="EDDGridFromDap" datasetID="tamu_dbd1_c066_a87f" active="true">
    <sourceUrl>http://megara.tamu.edu/thredds/dodsC/GROM-hind-reg-sfc-24/GROM-hind-reg-08-07-07-00-24.nc</sourceUrl>
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <altitudeMetersPerSourceUnit>1</altitudeMetersPerSourceUnit>
    <!-- sourceAttributes>
        <att name="comment">none</att>
        <att name="Conventions">CF-1.0</att>
        <att name="grid_type">REGULAR</att>
        <att name="history">Mon Apr 27 14:02:45 2009: /usr/local/bin/ncrcat -O
                  /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-08-07-07-00-06.nc
                  /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-08-07-07-06-06.nc
                  /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-08-07-07-12-06.nc
                  /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-08-07-07-18-06.nc
                  /home/baum/TGLO/OUT/HIS/GROM-hind-reg-08-07-07-00-24.nc
                  Mon Apr 27 13:20:38 2009: /usr/local/bin/ncrcat -O -d time,0,5
                  /home/baum/TGLO/OUT/HIS/GROM-fore-reg-08-07-07-00-48.nc
                  /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-08-07-07-00-06.nc
                  none</att>
        <att name="institution">Texas A&amp;M University, Dept.of Oceanography</att>
        <att name="references">none</att>
        <att name="source">ROMS 2.1</att>
        <att name="title">ROMS output 48-hour regular grid surface currents</att>
    </sourceAttributes -->
    <addAttributes>
        <att name="cdm_data_type">Grid</att>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att
        <att name="infoUrl">http://megara.tamu.edu/thredds/dodsC/GROM-hind-reg-sfc-24/GROM-hind-reg-08-07-07-00-24.nc.html</att>
        <att name="keywords">48-hour,
                  Oceans &gt; Ocean Circulation &gt; Ocean Currents,
                  a&amp;m, circulation, currents, dept.of, eastward, grid, hour, model,
                  modeling, northward, ocean, oceanography, oceans, output, regional, regional
                  ocean model, regular, roms, sea, seawater, surface,
                  surface_eastward_sea_water_velocity, surface_northward_sea_water_velocity,
                  system, texas, university, velocity, water</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset Discovery v1.0</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">none</att>
        <att name="title">ROMS output 48-hour regular grid surface currents [time2][latitude][longitude]</att>
    </addAttributes>
    <axisVariable>
        <sourceName>time</sourceName>
        <destinationName>time2</destinationName>
        <!-- sourceAttributes>
            <att name="axis">T</att>
            <att name="long_name">Valid Time</att>
            <att name="standard_name">time</att>
            <att name="units">seconds since 2003-10-28 0:00:00 0:00</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lat</sourceName>
        <destinationName>latitude</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">Latitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">latitude</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </axisVariable>
    <axisVariable>
        <sourceName>lon</sourceName>
        <destinationName>longitude</destinationName>
        <!-- sourceAttributes>
            <att name="long_name">Longitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">longitude</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </axisVariable>
    <dataVariable>
        <sourceName>water_u</sourceName>
        <destinationName>water_u</destinationName>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">X</att>
            <att name="long_name">Eastward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att name="standard_name">surface_eastward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>water_v</sourceName>
        <destinationName>water_v</destinationName>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">Y</att>
            <att name="long_name">Northward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att name="standard_name">surface_northward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
</dataset>

<sourceUrls serverType="thredds" regex=".*\.nc"
recursive="true">http://megara.tamu.edu:8080/thredds/catalog/GROM-hind-reg-sfc-24/catalog.xml</sourceUrls>

</dataset>
-----

The *Make a Graph* page for this located at:

http://barataria.tamu.edu/erddap/griddap/tamu_megara_5f79_c1c6_2208.graph[+http://barataria.tamu.edu/erddap/griddap/tamu_megara_5f79_c1c6_2208.graph+]

looks like:

image::erddap/erddap_GROM-hind-reg-sfc-24.png[width=1000]

[[EDDTableFromFilesEx]]
+EDDTableFromFiles+ Superclass
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is the superclass for all classes of the form +EDDTableFrom...Files+.
This is not used directory, but it used via subclasses for handling specific
file types.  The available subclasses are:

* xref:EDDTableFromAsciiFiles[+EDDTableFromAsciiFiles+]
* xref:EDDTableFromAwsXmlFiles[+EDDTableFromAwsXmlFiles+]
* xref:EDDTableFromHyraxFiles[+EDDTableFromHyraxFiles+]
* xref:EDDTableFromNcFiles[+EDDTableFromNcFiles+]
* xref:EDDTableFromNcCFFiles[+EDDTableFromNcCFFiles+]
* xref:EDDTableFromThreddsFiles[+EDDTableFromThreddsFiles+]

If your data is not in ASCCI, AWS XML, Hyrax, NetCDF or
THREDDS format files, then your best bet is to convert it
into one of these formats.  It is [green]*recommended* to
convert them into NetCDF files as it is an increasingly
widely supported format for which a wide variety of tools
and clients - including ERDDAP - are available.  The spin-up
is not trivial, but it will be worth it if you wish to prolong
the life and increase the distribution of your data.

The skeleton XML for all +EDDTableFromFiles+ subclasses is:

[source,xml]
-----
<dataset type="EDDTableFrom...Files" datasetID="..." active="..." >
  <nDimensions>...</nDimensions>  <!-- This was used prior to ERDDAP 
    version 1.30, but is now ignored. -->
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit> 
  <fileDir>...</fileDir> <-- The directory (absolute) with the data files. -->
  <recursive>true|false</recursive> <-- Indicates if subdirectories
    of fileDir have data files, too. -->
  <fileNameRegex>...</fileNameRegex> <-- A regular expression 
    (tutorial) describing valid data files names, e.g., ".*\.nc" for 
    all .nc files. -->
  <metadataFrom>...</metadataFrom> <-- The file to get metadata
    from ("first" or "last" (the default) based on file's 
    lastModifiedTime). -->
  <columnNamesRow>...</columnNamesRow> <-- (For 
    EDDTableFromAsciiFiles only) This specifies the number of the row
    with the column names in the files. (The first row is "1". 
    Default = 1.) -->
  <firstDataRow>...</firstDataRow> <-- (For 
    EDDTableFromAsciiFiles only) This specifies the number of the first
    row with data in the files. (The first row is "1". default = 2.) -->
  <-- For the next four tags, see File Name Extracts. -->
  <preExtractRegex>...</preExtractRegex>
  <postExtractRegex>...</postExtractRegex>
  <extractRegex>...</extractRegex>
  <columnNameForExtract>...</columnNameForExtract> 
  <sortedColumnSourceName>...</sortedColumnSourceName> 
    <-- The sourceName of the numeric column that the data files are 
    usually already sorted by within each file, e.g., "time".
    Use null or "" if no variable is suitable.
    It is ok if not all files are sorted by this column.
    If present, this can greatly speed up some data requests. 
    For EDDTableFromHyraxFiles, EDDTableFromNcFiles and 
    EDDTableFromThreddsFiles, this must be the leftmost axis 
    variable. -->
  <sortFilesBySourceNames>...</sortFilesBySourceNames>
    <-- This is a space-separated list of source variable names 
    which specifies how the internal list of files should be sorted
    (in ascending order), for example "id time". 
    It is the minimum value of the specified columns in each file
    that is used for sorting.
    When a data request is filled, data is obtained from the files
    in this order. Thus it determines the overall order of the data
    in the response.  If you specify more than one column name, the
    second name is used if there is a tie for the first column; the
    third is used if there is a tie for the first and second columns; ...
    This is OPTIONAL (the default is fileDir+fileName order). -->
  <isLocal>false<isLocal> <!-- (may be true or false, 
    the default). This is only used by EDDTableFromNcCFFiles. It 
    indicates if the files are local (actual files) or remote 
    (accessed via the web). The two types are treated slightly 
    differently.
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more -->
    <-- For EDDTableFromHyraxFiles, EDDTableFromNcFiles, and 
    EDDTableFromThreddsFiles, the axis variables needn't be first or 
    in any specific order. -->
</dataset>
-----

+EDDTableFromNcFiles+ Example for NODC NetCDF Trajectory GISR Test File +GISR_fl_despike.nc+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The file was obtained from:

http://pod.tamu.edu/~cyhsu/GISR/GISR02/NetCDF/[+http://pod.tamu.edu/~cyhsu/GISR/GISR02/NetCDF/+]

The modified header from +GISR_fl_despike.nc+ follows.  The major changes are:

. Changing the +time+ attribute +units+ value from +JD+ to +days since
1968-05-23 00:00:00+.  As explained at
https://www.myroms.org/forum/viewtopic.php?t=69[+https://www.myroms.org/forum/viewtopic.php?t=69+],
the Julian day number for this is 2440000.  So we can extract this from all of
the original +time+ variable values and make the indicated change in the
+units+ attribute for +time+, which will make the numbers significantly
smaller and reduce the round-off error for data values that are literally
seconds apart.  The subtraction can be done globally within the file via the
NCO command: +ncap2 -s time=time-2440000 gis.nc gis2.nc+.

. Changed the variable name +lat+ to +latitude+.

. Changed the variable name +lon+ to +longitude+.

. Changed the global attribute +featureType+ value from +timeSeries+ to
+trajectory+.

. Added the global attribute +cdm_data_type+ with value
+Trajectory+.

. Added the global attribute +cdm_trajectory_variables+ with the
value +time,latitude,longitude+.

. Deleted the +time+ variable attribute +calendar+.

. Added the variable +profile+ with the attribute +cf_role+ and
value +trajectory_id+, e.g.
-----
        int profile ;
                profile:cf_role = "trajectory_id" ;
-----

The new header is:

-----
netcdf gis {
dimensions:
	dim1 = 42044 ;
variables:
	double time(dim1) ;
		time:ancillary_variables = " " ;
		time:axis = "T" ;
		time:comment = " " ;
		time:long_name = " " ;
		time:standard_name = "time" ;
		time:units = "days since 1968-05-23 00:00:00" ;
	double latitude(dim1) ;
		latitude:long_name = "latitude" ;
		latitude:standard_name = "latitude" ;
		latitude:units = "degrees_north" ;
		latitude:axis = "Y" ;
		latitude:valid_min = 28.26581 ;
		latitude:valid_max = 28.41471 ;
		latitude:ancillary_variables = " " ;
		latitude:comment = " " ;
	double longitude(dim1) ;
		longitude:long_name = "longitude" ;
		longitude:standard_name = "longitude" ;
		longitude:units = "degrees_east" ;
		longitude:axis = "X" ;
		longitude:valid_min = -88.97901 ;
		longitude:valid_max = -88.78499 ;
		longitude:ancillary_variables = " " ;
		longitude:comment = " " ;
	double pressure(dim1) ;
		pressure:long_name = "pressure" ;
		pressure:standard_name = "depth" ;
		pressure:units = "db" ;
		pressure:positive = "down" ;
		pressure:valid_min = 974.98 ;
		pressure:valid_max = 1203.29 ;
		pressure:ancillary_variables = " " ;
		pressure:comment = " " ;
	double sal(dim1) ;
		sal:long_name = "salinity" ;
		sal:standard_name = "sea_water_salinity" ;
		sal:nodc_name = " " ;
		sal:units = "PSU" ;
		sal:scale_factor = " " ;
		sal:add_offset = " " ;
		sal:valid_min = 34.9223 ;
		sal:valid_max = 34.95146 ;
		sal:coordinates = "time latitude longitude" ;
		sal:grid_mapping = "crs" ;
		sal:source = " " ;
		sal:cell_method = " " ;
		sal:ancillary_variables = " " ;
		sal:platform = " " ;
		sal:instrument = " " ;
		sal:comment = " " ;
	double temp(dim1) ;
		temp:long_name = "temperature" ;
		temp:standard_name = "sea_water_temperature" ;
		temp:nodc_name = " " ;
		temp:units = "degree_Celsius" ;
		temp:scale_factor = " " ;
		temp:add_offset = " " ;
		temp:valid_min = 4.57954 ;
		temp:valid_max = 5.21677 ;
		temp:coordinates = "time latitude longitude" ;
		temp:grid_mapping = "crs" ;
		temp:source = " " ;
		temp:cell_method = " " ;
		temp:ancillary_variables = " " ;
		temp:platform = " " ;
		temp:instrument = " " ;
		temp:comment = " " ;
	double sigma(dim1) ;
		sigma:long_name = "tracer_density" ;
		sigma:standard_name = "" ;
		sigma:nodc_name = " " ;
		sigma:units = "kg m^-3" ;
		sigma:scale_factor = " " ;
		sigma:add_offset = " " ;
		sigma:valid_min = 32.17011 ;
		sigma:valid_max = 32.28077 ;
		sigma:coordinates = "time latitude longitude" ;
		sigma:grid_mapping = "crs" ;
		sigma:source = " " ;
		sigma:cell_method = " " ;
		sigma:ancillary_variables = " " ;
		sigma:platform = " " ;
		sigma:instrument = " " ;
		sigma:comment = " " ;
	byte boolean_flag_ini_tracer(dim1) ;
		boolean_flag_ini_tracer:long_name = "tracer_index" ;
		boolean_flag_ini_tracer:standard_name = "" ;
		boolean_flag_ini_tracer:flag_masks = "" ;
		boolean_flag_ini_tracer:flag_meanings = "Initial tracer releasing with depth" ;
		boolean_flag_ini_tracer:reference = " " ;
		boolean_flag_ini_tracer:comment = "CF3SF5 tracer was released on a target isopycnal of a potential density of 1032.254 kg m^-3.  This isopycnal was located between 1090 and 1180 dbar." ;
	int profile ;
		profile:cf_role = "trajectory_id" ;

// global attributes:
		:Conventions = "CF-1.6" ;
		:Metadata_Conventions = "Unidata Dataset Discovery v1.0" ;
		:featureType = "trajectory" ;
		:cdm_data_type = "Trajectory" ;
		:nodc_template_version = "NODC_NetCDF_TimeSeries_Orthogonal_Template_v1.0" ;
		:standard_name_vocabulary = "CF-1.6" ;
		:title = "GISR Trajectory" ;
		:summary = "Tracer Deployment and Initial Sampling" ;
		:source = " " ;
		:platform = "R/V Brooks McCall" ;
		:instrument = "RAFOS float, Sampling Sled, Sampling array, Surface drifter" ;
		:uuid = " " ;
		:sea_name = "Northern Gulf of Mexico" ;
		:id = "GISR02" ;
		:naming_authority = " " ;
		:time_coverage_start = "2012-07-30" ;
		:time_coverage_end = "2012-08-09" ;
		:time_coverage_resolution = " " ;
		:geospatial_lat_min = 28.26581 ;
		:getspatial_lat_max = 28.41471 ;
		:geospatial_lat_units = "degree_north" ;
		:geospatial_lon_min = -88.97901 ;
		:geospatial_lon_max = -88.78499 ;
		:geospatial_lon_units = "degree_east" ;
		:geospatial_vertical_min = 974.98 ;
		:geospatial_vertical_max = 1203.29 ;
		:geospatial_vertical_units = "db" ;
		:geospatial_vertical_resolution = " " ;
		:geospatial_vertical_positive = "down" ;
		:institution = "Woods Hole Oceanographic Institution" ;
		:creator_name = "James R. Ledwell" ;
		:creator_url = " " ;
		:creator_email = "jledwell@whoi.edu" ;
		:project = "GISR" ;
		:processing_level = "good" ;
		:references = " " ;
		:keywords_vocabulary = " " ;
		:keywords = " " ;
		:acknowledgment = "The skill and cooperation of the master and crew of R/V Brooks McCall were essential to the success of the cruise. The staff of TDI Brooks, and especially Les Bender, liason for the cruise were of great help with preparations. The work described here is part of the Gulf Integrated Spill Response Consortium (GISR), whose lead institution is Texas A&M University (TAMU). Piers Chapman, Principal Investigator for GISR has been an outstanding supporter of the tracer experiment. He, co-PI Scott Solocofsky, their administrative assistant, Laura Caldwell, and others at TAMU as well as Linda Cannata and staff at WHOI have worked hard to keep contractual and other arrangements for our work on track. Steve DiMarco and the science party of the prior GISR cruise, G01, provided hydrographic data which helped us prepare for this cruise. The moorings set during G01 served as a guide to our tracer release. Steve also encouraged graduate student Laura Spencer to join our cruise to act as a bridge between the two cruises. Numerical simulations of the motion of particles released at the time and place of the tracer release were run by Ruoying He and his group at North Carolina State University and by Ping Chang and his group at TAMU to help track the tracer. RAFOS floats were prepared by Jim Valdes at WHOI and by Brian Guest, and tracking assistance was provided by Heather Furey at WHOI. Funding for GISR is provided by British Petroleum through the Gulf of Mexico Research Institute (GOMRI), and is administered by the Consortium for Ocean Leadership." ;
		:comment = "The data was collected when towing the CTDs at certain level depth." ;
		:contributor_name = " " ;
		:contributor_role = " " ;
		:date_created = " " ;
		:date_modified = " " ;
		:publisher_name = "GISR" ;
		:publisher_email = "Matthew Howard <mkhoward@tamu.edu>" ;
		:publisher_url = "http://gisr.tamu.edu/" ;
		:history = "Tue May  7 16:25:31 2013: ncatted -a units,time,o,c,days since 1968-05-23 00:00:00 gis.nc\n",
			"Tue May  7 16:24:12 2013: ncatted -a units,time,o,c,days since 1970-01-01 00:00:00 gis2.nc\n",
			"Tue May  7 16:23:18 2013: ncap2 -s time=time-2440000 gis.nc gis2.nc\n",
			"Tue May  7 16:19:49 2013: ncap2 -v -s time=time-2440000 gis.nc\n",
			"Tue May  7 16:16:57 2013: ncap2 -s time=time-2440000 gis.nc\n",
			"Tue May  7 16:02:01 2013: ncatted -a calendar,time,d,c,seconds since 1970-01-01 00:00:00 gis.nc\n",
			"Tue May  7 15:51:41 2013: ncatted -a units,time,o,c,seconds since 1970-01-01 00:00:00 gis.nc\n",
			"Tue May  7 15:28:59 2013: ncatted -a coordinates,sigma,o,c,time latitude longitude gis.nc\n",
			"Tue May  7 15:28:51 2013: ncatted -a coordinates,temp,o,c,time latitude longitude gis.nc\n",
			"Tue May  7 15:28:43 2013: ncatted -a coordinates,sal,o,c,time latitude longitude gis.nc\n",
			"Tue May  7 15:27:15 2013: ncrename -v lon,longitude gis.nc\n",
			"Tue May  7 15:27:08 2013: ncrename -v lat,latitude gis.nc\n",
			"Tue May  7 15:19:40 2013: ncatted -a cdm_trajectory_variables,global,o,c,time,latitude,longitude gis.nc\n",
			"Tue May  7 15:16:16 2013: ncatted -a cdm_trajectory_variables,global,o,c,time,lat,lon gis.nc\n",
			"Tue May  7 15:08:58 2013: ncatted -a cdm_trajectory_variables,global,o,c,sal gis.nc\n",
			"Tue May  7 15:08:53 2013: ncatted -a cdm_trajectory_variables,global,o,c,sal\' gis.nc\n",
			"Tue May  7 15:03:15 2013: ncatted -a cdm_trajectory_variables,global,o,c,time,lat,lon gis.nc\n",
			"Tue May  7 14:44:46 2013: ncatted -a project,global,o,c,GISR gis.nc\n",
			"Tue May  7 14:41:48 2013: ncatted -a title,global,o,c,GISR Trajectory gis.nc\n",
			"Tue May  7 14:41:24 2013: ncatted -a title,global,o,c,GISR_Trajectory gis.nc\n",
			"Tue May  7 14:22:24 2013: ncatted -a cf_role,profile,o,c,trajectory_id gis.nc\n",
			"Tue May  7 14:21:51 2013: ncatted -a cf_role,profile,d,c,trajectory_id gis.nc\n",
			"Tue May  7 14:17:09 2013: ncatted -a cf_role,profile,d,, gis.nc\n",
			"Tue May  7 14:16:51 2013: ncatted -a cf_role,global,d,, gis.nc\n",
			"Tue May  7 14:02:36 2013: ncks -v profile cf_role.nc ../../ERDDAP2/gis.nc\n",
			"Tue May  7 13:47:57 2013: ncatted -a cdm_data_type,global,o,c,Trajectory gis.nc\n",
			"Tue May  7 13:43:14 2013: ncatted -a featureType,global,o,c,trajectory gis.nc\n",
			"Tue May  7 13:42:55 2013: ncatted -a history,global,o,c,trajectory gis.nc\n",
			"trajectory" ;
		:license = " " ;
		:metadata_link = " " ;
		:NCO = "4.2.1" ;
		:cdm_trajectory_variables = "time,latitude,longitude" ;
		:nco_openmp_thread_number = 1 ;
}
-----

The commands used after invoking +GenerateDatasetsXml+:

-----
Which EDDType (default="EDDTableFromNcCFFiles")
? EDDTableFromNcFiles
Starting directory (default="/home/baum/ERDDAP2/")
? /home/baum/ERDDAP/
File name regex (e.g., ".*\.nc") (default="")
?
A sample full file name (default="/home/baum/ERDDAP/g01l01s01.nc")
? /home/baum/ERDDAP2/gis.nc
DimensionsCSV (or "" for default) (default="")
?
ReloadEveryNMinutes (e.g., 10080) (default="")
?
PreExtractRegex (default="")
?
PostExtractRegex (default="")
?
ExtractRegex (default="")
?
Column name for extract (default="")
?
Sorted column source name (default="")
?
Sort files by sourceName (default="")
?
infoUrl (default="")
?
institution (default="")
?
summary (default="")
?
title (default="")
?
working...

*** GenerateDatasetsXml ***
Results are shown on the screen, put on the clipboard and put in
/data/erddap/logs/log.txt
...
-----


+EDDTableFromNcFiles+ Example for NODC NetCDF Profile GISR Test File +g01l01s01.nc+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The file was obtained from:

http://pod.tamu.edu/~cyhsu/GISR/GISR01/GISR01_NetCDF/[+http://pod.tamu.edu/~cyhsu/GISR/GISR01/GISR01_NetCDF/+]

with other similar files also found at:

http://pod.tamu.edu/~cyhsu/GISR/GISR02/NetCDF/[+http://pod.tamu.edu/~cyhsu/GISR/GISR02/NetCDF/+]

The header from +g01l01s01.nc+ is:

-----
netcdf g01l01s01 {
dimensions:
	z = 1661 ;
variables:
	float z(z) ;
		z:long_name = "depth" ;
		z:standard_name = "depth" ;
		z:units = "meter" ;
		z:axis = "Z" ;
		z:positive = "down" ;
		z:valid_min = 2.f ;
		z:valid_max = 832.f ;
		z:ancillary_variables = " " ;
		z:comment = " " ;
	float Prz(z) ;
		Prz:long_name = "pressure" ;
		Prz:standard_name = "sea_water_pressure" ;
		Prz:nodc_name = " " ;
		Prz:units = "db" ;
		Prz:positive = "down" ;
		Prz:scale_factor = " " ;
		Prz:add_offset = " " ;
		Prz:_FillValue = -9.99e-29f ;
		Prz:valid_min = 2.023f ;
		Prz:valid_max = 839.234f ;
		Prz:coordinates = "z" ;
		Prz:source = " " ;
		Prz:references = " " ;
		Prz:cell_methods = " " ;
		Prz:ancillary_variables = " " ;
		Prz:comment = " " ;
	float T1(z) ;
		T1:long_name = "temperature" ;
		T1:standard_name = "sea_water_temperature" ;
		T1:nodc_name = " " ;
		T1:units = "degree_Celsius" ;
		T1:scale_factor = " " ;
		T1:add_offset = " " ;
		T1:_FillValue = -9.99e-29f ;
		T1:valid_min = 5.5291f ;
		T1:valid_max = 29.3525f ;
		T1:coordinates = "z" ;
		T1:grid_mapping = "crs" ;
		T1:source = " " ;
		T1:references = " " ;
		T1:cell_methods = " " ;
		T1:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		T1:comment = "measurement 1" ;
	float T2(z) ;
		T2:long_name = "temperature" ;
		T2:standard_name = "sea_water_temperature" ;
		T2:nodc_name = " " ;
		T2:units = "degree_Celsius" ;
		T2:scale_factor = " " ;
		T2:add_offset = " " ;
		T2:_FillValue = -9.99e-29f ;
		T2:valid_min = 5.5287f ;
		T2:valid_max = 29.3536f ;
		T2:coordinates = "z" ;
		T2:grid_mapping = "crs" ;
		T2:source = " " ;
		T2:references = " " ;
		T2:cell_methods = " " ;
		T2:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		T2:comment = "measurement 2" ;
	float Sal(z) ;
		Sal:long_name = "salinity" ;
		Sal:standard_name = "sea_water_salinity" ;
		Sal:nodc_name = " " ;
		Sal:units = "PSU" ;
		Sal:scale_factor = " " ;
		Sal:add_offset = " " ;
		Sal:_FillValue = -9.99e-29f ;
		Sal:valid_min = 34.9056f ;
		Sal:valid_max = 36.6286f ;
		Sal:coordinates = "lat lon z" ;
		Sal:grid_mapping = "crs" ;
		Sal:source = " " ;
		Sal:references = " " ;
		Sal:cell_methods = " " ;
		Sal:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Sal:comment = " " ;
	float Cdt1(z) ;
		Cdt1:long_name = "conductivity" ;
		Cdt1:standard_name = "sea_water_electrical_conductivity" ;
		Cdt1:nodc_name = " " ;
		Cdt1:units = "S/m" ;
		Cdt1:scale_factor = " " ;
		Cdt1:add_offset = " " ;
		Cdt1:_FillValue = -9.99e-29f ;
		Cdt1:valid_min = 3.424181f ;
		Cdt1:valid_max = 5.927437f ;
		Cdt1:coordinates = "lat lon z" ;
		Cdt1:grid_mapping = "crs" ;
		Cdt1:source = " " ;
		Cdt1:references = " " ;
		Cdt1:cell_methods = " " ;
		Cdt1:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Cdt1:comment = "measurement 1" ;
	float Cdt2(z) ;
		Cdt2:long_name = "conductivity, 2" ;
		Cdt2:standard_name = "sea_water_electrical_conductivity" ;
		Cdt2:nodc_name = " " ;
		Cdt2:units = "S/m" ;
		Cdt2:scale_factor = " " ;
		Cdt2:add_offset = " " ;
		Cdt2:_FillValue = -9.99e-29f ;
		Cdt2:valid_min = 3.424058f ;
		Cdt2:valid_max = 5.927219f ;
		Cdt2:coordinates = "lat lon z" ;
		Cdt2:grid_mapping = "crs" ;
		Cdt2:source = " " ;
		Cdt2:references = " " ;
		Cdt2:cell_methods = " " ;
		Cdt2:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Cdt2:comment = "measurement 2" ;
	float Dst(z) ;
		Dst:long_name = "density [sigma-t]" ;
		Dst:standard_name = "sea_water_sigma_t" ;
		Dst:nodc_name = " " ;
		Dst:units = "Kg m^-3" ;
		Dst:scale_factor = " " ;
		Dst:add_offset = " " ;
		Dst:_FillValue = -9.99e-29f ;
		Dst:valid_min = 22.7306f ;
		Dst:valid_max = 27.5493f ;
		Dst:coordinates = "lat lon z" ;
		Dst:grid_mapping = "crs" ;
		Dst:source = " " ;
		Dst:references = " " ;
		Dst:cell_methods = " " ;
		Dst:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Dst:comment = " " ;
	float Oxy1(z) ;
		Oxy1:long_name = "oxygen, SBE 43, 1" ;
		Oxy1:standard_name = "volume_fraction_of_oxygen_in_sea_water"
;
		Oxy1:nodc_name = " " ;
		Oxy1:units = "ml/l" ;
		Oxy1:scale_factor = " " ;
		Oxy1:add_offset = " " ;
		Oxy1:_FillValue = -9.99e-29f ;
		Oxy1:valid_min = 2.69395f ;
		Oxy1:valid_max = 5.38068f ;
		Oxy1:coordinates = "lat lon z" ;
		Oxy1:grid_mapping = "crs" ;
		Oxy1:source = " " ;
		Oxy1:references = " " ;
		Oxy1:cell_methods = " " ;
		Oxy1:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Oxy1:comment = " " ;
	float Oxy2(z) ;
		Oxy2:long_name = "oxygen, SBE 43, 2" ;
		Oxy2:standard_name = "volume_fraction_of_oxygen_in_sea_water"
;
		Oxy2:nodc_name = " " ;
		Oxy2:units = "ml/l" ;
		Oxy2:scale_factor = " " ;
		Oxy2:add_offset = " " ;
		Oxy2:_FillValue = -9.99e-29f ;
		Oxy2:valid_min = 2.26251f ;
		Oxy2:valid_max = 4.62798f ;
		Oxy2:coordinates = "lat lon z" ;
		Oxy2:grid_mapping = "crs" ;
		Oxy2:source = " " ;
		Oxy2:references = " " ;
		Oxy2:cell_methods = " " ;
		Oxy2:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Oxy2:comment = " " ;
	float Xmi(z) ;
		Xmi:long_name = "beam_transmission" ;
		Xmi:standard_name = "transmission" ;
		Xmi:nodc_name = " " ;
		Xmi:units = "percentage" ;
		Xmi:scale_factor = " " ;
		Xmi:add_offset = " " ;
		Xmi:_FillValue = -9.99e-29f ;
		Xmi:valid_min = 95.6864f ;
		Xmi:valid_max = 98.7963f ;
		Xmi:coordinates = "lat lon z" ;
		Xmi:grid_mapping = "crs" ;
		Xmi:source = " " ;
		Xmi:references = " " ;
		Xmi:cell_methods = " " ;
		Xmi:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Xmi:comment = " " ;
	float Flu(z) ;
		Flu:long_name = "fluorescence" ;
		Flu:standard_name = " " ;
		Flu:nodc_name = "Fluorescence" ;
		Flu:units = "mg m-3" ;
		Flu:scale_factor = " " ;
		Flu:add_offset = " " ;
		Flu:_FillValue = -9.99e-29f ;
		Flu:valid_min = 1.8401f ;
		Flu:valid_max = 4.1824f ;
		Flu:coordinates = "lat lon z" ;
		Flu:grid_mapping = "crs" ;
		Flu:source = " " ;
		Flu:references = " " ;
		Flu:cell_methods = " " ;
		Flu:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Flu:comment = " " ;
	float Par(z) ;
		Par:long_name = "PAR/Irradiance" ;
		Par:standard_name =
"omnidirectional_photosynthetic_spherical_irradiance_in_sea_water" ;
		Par:nodc_name = " " ;
		Par:units = "W m^-2" ;
		Par:scale_factor = " " ;
		Par:add_offset = " " ;
		Par:_FillValue = -9.99e-29f ;
		Par:valid_min = 0.13587f ;
		Par:valid_max = 2.8388f ;
		Par:coordinates = "lat lon z" ;
		Par:grid_mapping = "crs" ;
		Par:source = " " ;
		Par:references = " " ;
		Par:cell_methods = " " ;
		Par:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Par:comment = " " ;
	float Rho(z) ;
		Rho:long_name = "density" ;
		Rho:standard_name = "sea_water_density" ;
		Rho:nodc_name = " " ;
		Rho:units = "kg m^-3" ;
		Rho:scale_factor = " " ;
		Rho:add_offset = " " ;
		Rho:_FillValue = -9.99e-29f ;
		Rho:valid_min = 1022.739f ;
		Rho:valid_max = 1031.391f ;
		Rho:coordinates = "lat lon z" ;
		Rho:grid_mapping = "crs" ;
		Rho:source = " " ;
		Rho:references = " " ;
		Rho:cell_methods = " " ;
		Rho:ancillary_variables = "Dst  Sigt" ;
		Rho:comment = "sigma-t, Value = density - 1000" ;
	float Sigt(z) ;
		Sigt:long_name = "density [sigma-theta]" ;
		Sigt:standard_name = "sea_water_potential_density" ;
		Sigt:nodc_name = " " ;
		Sigt:units = "Kg m^-3" ;
		Sigt:scale_factor = " " ;
		Sigt:add_offset = " " ;
		Sigt:_FillValue = -9.99e-29f ;
		Sigt:valid_min = 22.7307f ;
		Sigt:valid_max = 27.5578f ;
		Sigt:coordinates = "lat lon z" ;
		Sigt:grid_mapping = "crs" ;
		Sigt:source = " " ;
		Sigt:references = " " ;
		Sigt:cell_methods = " " ;
		Sigt:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Sigt:comment = " " ;
	float Des(z) ;
		Des:long_name = "descent_rate" ;
		Des:standard_name = " " ;
		Des:nodc_name = " " ;
		Des:units = "m/s" ;
		Des:scale_factor = " " ;
		Des:add_offset = " " ;
		Des:_FillValue = -9.99e-29f ;
		Des:valid_min = 0.099f ;
		Des:valid_max = 0.906f ;
		Des:coordinates = "lat lon z" ;
		Des:grid_mapping = "crs" ;
		Des:source = " " ;
		Des:references = " " ;
		Des:cell_methods = " " ;
		Des:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Des:comment = " " ;
	float Gpa(z) ;
		Gpa:long_name = "geopotential_anomaly" ;
		Gpa:standard_name = "geopotential" ;
		Gpa:nodc_name = " " ;
		Gpa:units = "J Kg^-1" ;
		Gpa:scale_factor = " " ;
		Gpa:add_offset = " " ;
		Gpa:_FillValue = -9.99e-29f ;
		Gpa:valid_min = -0.109f ;
		Gpa:valid_max = 11.143f ;
		Gpa:coordinates = "lat lon z" ;
		Gpa:grid_mapping = "crs" ;
		Gpa:source = " " ;
		Gpa:references = " " ;
		Gpa:cell_methods = " " ;
		Gpa:ancillary_variables = "boolean_flag_variable
enumerated_flag_variable" ;
		Gpa:comment = " " ;
	float PoT(z) ;
		PoT:long_name = "potential_temperature" ;
		PoT:standard_name = "sea_water_potential_temperature" ;
		PoT:nodc_name = " " ;
		PoT:units = "degree_Celsius" ;
		PoT:scale_factor = " " ;
		PoT:add_offset = " " ;
		PoT:_FillValue = -9.99e-29f ;
		PoT:valid_min = 5.4563f ;
		PoT:valid_max = 29.3493f ;
		PoT:coordinates = "lat lon z" ;
		PoT:grid_mapping = "crs" ;
		PoT:source = " " ;
		PoT:references = " " ;
		PoT:cell_methods = " " ;
		PoT:ancillary_variables = "T1" ;
		PoT:comment = " " ;

// global attributes:
		:Conventions = "CF-1.6" ;
		:Metadata_Conventions = "Unidata Dataset Discovery v1.0" ;
		:featureType = "profile" ;
		:cdm_data_type = "Profile" ;
		:nodc_template_version =
"NODC_NetCDF_Profile_Orthogonal_Template_v1.0" ;
		:standard_name_vocabulary = "CF-1.6" ;
		:title = "Gulf Integrated Spill Research" ;
		:summary = "1. To deploy six deepwater current meter moorings
in the Mississippi Fan region of the Northern Gul
f of Mexico. 2. To make shipboard observations of current velocity (ADCP:75
and 300 kHz), flow-through thermosalinography, fluo
rometer (chlorophyll), Meteorology. 3. To Collect water samples using Niskin
bottles for chemical analyses." ;
		:source = " " ;
		:platform = " " ;
		:instrument = "ADCP, CTD, Niskin Bottles" ;
		:uuid = " " ;
		:sea_name = "Northern Gulf of Mexico" ;
		:id = "GISR01" ;
		:naming_authority = "ocean.tamu.edu" ;
		:time_coverage_start = "2012-07-11 02:22:32" ;
		:time_coverage_end = "2012-07-11 02:22:32" ;
		:time_coverage_resolution = " " ;
		:geospatial_lat_min = 28.25017f ;
		:geospatial_lat_max = 28.25017f ;
		:geospatial_lat_units = "degrees_north" ;
		:geospatial_lat_resolution = "point" ;
		:geospatial_lon_min = -89.25034f ;
		:geospatial_lon_max = -89.25034f ;
		:geospatial_lon_units = "degrees_east" ;
		:geospatial_lon_resolution = "point" ;
		:geospatial_vertical_min = 2.f ;
		:geospatial_vertical_max = 832.f ;
		:geospatial_vertical_units = "meter" ;
		:geospatial_vertical_resolution = " " ;
		:geospatial_vertical_positive = "down" ;
		:institution = "Texas A&M University" ;
		:creator_name = "Dr. Steven DiMarco" ;
		:creator_url = "http://ocean.tamu.edu/profile/SDiMarco" ;
		:creator_email = "sdimarco@tamu.edu" ;
		:project = "Gulf Integrated Spill Research" ;
		:processing_level = " " ;
		:references = " " ;
		:keywords_vocabulary = " " ;
		:keywords = " " ;
		:acknowledgment = " " ;
		:comment = " " ;
		:contributor_name = " " ;
		:contributor_role = " " ;
		:date_created = " " ;
		:date_modified = " " ;
		:publisher_name = " " ;
		:publisher_email = " " ;
		:publisher_url = " " ;
		:history = " " ;
		:license = " " ;
		:metadata_link = " " ;
data:
 ...
-----

The commands used after invoking +GenerateDatasetsXml+:

-----
Which EDDType (default="EDDTableFromNcCFFiles")
? EDDTableFromNcFiles
Starting directory (default="/home/baum/ERDDAP/")
? /home/baum/ERDDAP/
File name regex (e.g., ".*\.nc") (default="")
? 
A sample full file name (default="/home/baum/ERDDAP/g01l01s01.nc")
? /home/baum/ERDDAP/g01l01s01.nc
DimensionsCSV (or "" for default) (default="")
? 
ReloadEveryNMinutes (e.g., 10080) (default="")
? 
PreExtractRegex (default="")
? 
PostExtractRegex (default="")
? 
ExtractRegex (default="")
? 
Column name for extract (default="")
? 
Sorted column source name (default="")
? 
Sort files by sourceName (default="")
? 
infoUrl (default="")
? 
institution (default="")
? 
summary (default="")
? 
title (default="")
? 
working...

*** GenerateDatasetsXml ***
Results are shown on the screen, put on the clipboard and put in
/data/erddap/logs/log.txt
...
-----

The XML code generated for this example:

-----
<dataset type="EDDTableFromNcFiles" datasetID="ERDDAP_72d0_82a5_3c32"
active="true">
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <fileDir>/home/baum/ERDDAP/</fileDir>
    <recursive>true</recursive>
    <fileNameRegex>.*\.nc</fileNameRegex>
    <metadataFrom>last</metadataFrom>
    <preExtractRegex></preExtractRegex>
    <postExtractRegex></postExtractRegex>
    <extractRegex></extractRegex>
    <columnNameForExtract></columnNameForExtract>
    <sortedColumnSourceName></sortedColumnSourceName>
    <sortFilesBySourceNames></sortFilesBySourceNames>
    <altitudeMetersPerSourceUnit>1</altitudeMetersPerSourceUnit>
    <!-- sourceAttributes>
        <att name="cdm_data_type">Profile</att>
        <att name="Conventions">CF-1.6</att>
        <att name="creator_email">sdimarco@tamu.edu</att>
        <att name="creator_name">Dr. Steven DiMarco</att>
        <att name="creator_url">http://ocean.tamu.edu/profile/SDiMarco</att>
        <att name="featureType">profile</att>
        <att name="geospatial_lat_max" type="float">28.250166</att>
        <att name="geospatial_lat_min" type="float">28.250166</att>
        <att name="geospatial_lat_resolution">point</att>
        <att name="geospatial_lat_units">degrees_north</att>
        <att name="geospatial_lon_max" type="float">-89.250336</att>
        <att name="geospatial_lon_min" type="float">-89.250336</att>
        <att name="geospatial_lon_resolution">point</att>
        <att name="geospatial_lon_units">degrees_east</att>
        <att name="geospatial_vertical_max" type="float">832.0</att>
        <att name="geospatial_vertical_min" type="float">2.0</att>
        <att name="geospatial_vertical_positive">down</att>
        <att name="geospatial_vertical_units">meter</att>
        <att name="id">GISR01</att>
        <att name="institution">Texas A&amp;M University</att>
        <att name="instrument">ADCP, CTD, Niskin Bottles</att>
        <att name="Metadata_Conventions">Unidata Dataset Discovery v1.0</att>
        <att name="naming_authority">ocean.tamu.edu</att>
        <att
name="nodc_template_version">NODC_NetCDF_Profile_Orthogonal_Template_v1.0</att>
        <att name="project">Gulf Integrated Spill Research</att>
        <att name="sea_name">Northern Gulf of Mexico</att>
        <att name="standard_name_vocabulary">CF-1.6</att>
        <att name="summary">1. To deploy six deepwater current meter moorings
in the Mississippi Fan region of the Northern Gulf of Mexico. 2. To make
shipboard observations of current velocity (ADCP:75 and 300 kHz), flow-through
thermosalinography, fluorometer (chlorophyll), Meteorology. 3. To Collect
water samples using Niskin bottles for chemical analyses.</att>
        <att name="time_coverage_end">2012-07-11 02:22:32</att>
        <att name="time_coverage_start">2012-07-11 02:22:32</att>
        <att name="title">Gulf Integrated Spill Research</att>
    </sourceAttributes -->
    <!-- Please specify the actual cdm_data_type (TimeSeries?) and related
info below, for example...
        <att name="cdm_timeseries_variables">station, longitude,
latitude</att>
        <att name="subsetVariables">station, longitude, latitude</att>
    -->
    <addAttributes>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery
v1.0</att>
        <att name="infoUrl">http://ocean.tamu.edu/profile/SDiMarco</att>
        <att name="keywords">
Atmosphere &gt; Altitude &gt; Geopotential Height,
Oceans &gt; Ocean Chemistry &gt; Oxygen,
Oceans &gt; Ocean Optics &gt; Photosynthetically Active Radiation,
Oceans &gt; Ocean Optics &gt; Radiance,
Oceans &gt; Ocean Pressure &gt; Water Pressure,
Oceans &gt; Ocean Temperature &gt; Potential Temperature,
Oceans &gt; Ocean Temperature &gt; Water Temperature,
Oceans &gt; Salinity/Density &gt; Conductivity,
Oceans &gt; Salinity/Density &gt; Density,
Oceans &gt; Salinity/Density &gt; Potential Density,
Oceans &gt; Salinity/Density &gt; Salinity,
a&amp;m, active, altitude, anomaly, atmosphere, beam, chemistry, conductivity,
currents, density, depth, descent, dissolved, dissolved o2, electrical,
fluorescence, fraction, geopotential, gulf, height, integrated, irradiance,
o2, ocean, oceanography, oceans, omnidirectional,
omnidirectional_photosynthetic_spherical_irradiance_in_sea_water, optical,
optical properties, optics, oxygen, par, par/irradiance, photosynthetic,
photosynthetically, physical, physical oceanography, potential, pressure,
properties, radiance, radiation, rate, research, salinity, sbe, sea,
sea_water_density, sea_water_electrical_conductivity,
sea_water_potential_density, sea_water_potential_temperature,
sea_water_pressure, sea_water_salinity, sea_water_sigma_t,
sea_water_temperature, seawater, sigma, sigma-t, sigma-theta, spherical,
spill, temperature, texas, theta, transmission, university, volume,
volume_fraction_of_oxygen_in_sea_water, water</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset
Discovery v1.0</att>
        <att name="sourceUrl">(local files)</att>
    </addAttributes>
    <dataVariable>
        <sourceName>z</sourceName>
        <destinationName>z</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="axis">Z</att>
            <att name="long_name">depth</att>
            <att name="positive">down</att>
            <att name="standard_name">depth</att>
            <att name="units">meter</att>
            <att name="valid_max" type="float">832.0</att>
            <att name="valid_min" type="float">2.0</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">8000.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="colorBarPalette">OceanDepth</att>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Prz</sourceName>
        <destinationName>Prz</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="coordinates">z</att>
            <att name="long_name">pressure</att>
            <att name="positive">down</att>
            <att name="standard_name">sea_water_pressure</att>
            <att name="units">db</att>
            <att name="valid_max" type="float">839.234</att>
            <att name="valid_min" type="float">2.023</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">5000.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Pressure</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>T1</sourceName>
        <destinationName>T1</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="comment">measurement 1</att>
            <att name="coordinates">z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">temperature</att>
            <att name="standard_name">sea_water_temperature</att>
            <att name="units">degree_Celsius</att>
            <att name="valid_max" type="float">29.3525</att>
            <att name="valid_min" type="float">5.5291</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">32.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Temperature</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>T2</sourceName>
        <destinationName>T2</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="comment">measurement 2</att>
            <att name="coordinates">z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">temperature</att>
            <att name="standard_name">sea_water_temperature</att>
            <att name="units">degree_Celsius</att>
            <att name="valid_max" type="float">29.3536</att>
            <att name="valid_min" type="float">5.5287</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">32.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Temperature</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Sal</sourceName>
        <destinationName>Sal</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">salinity</att>
            <att name="standard_name">sea_water_salinity</att>
            <att name="units">PSU</att>
            <att name="valid_max" type="float">36.6286</att>
            <att name="valid_min" type="float">34.9056</att>
        </sourceAttributes -->
       <addAttributes>
            <att name="colorBarMaximum" type="double">37.0</att>
            <att name="colorBarMinimum" type="double">32.0</att>
            <att name="ioos_category">Salinity</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Cdt1</sourceName>
        <destinationName>Cdt1</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="comment">measurement 1</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">conductivity</att>
            <att name="standard_name">sea_water_electrical_conductivity</att>
            <att name="units">S/m</att>
            <att name="valid_max" type="float">5.927437</att>
            <att name="valid_min" type="float">3.424181</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">40.0</att>
            <att name="colorBarMinimum" type="double">30.0</att>
            <att name="ioos_category">Salinity</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Cdt2</sourceName>
        <destinationName>Cdt2</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="comment">measurement 2</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">conductivity, 2</att>
            <att name="standard_name">sea_water_electrical_conductivity</att>
            <att name="units">S/m</att>
            <att name="valid_max" type="float">5.927219</att>
            <att name="valid_min" type="float">3.424058</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">40.0</att>
            <att name="colorBarMinimum" type="double">30.0</att>
            <att name="ioos_category">Salinity</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Dst</sourceName>
        <destinationName>Dst</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">density [sigma-t]</att>
            <att name="standard_name">sea_water_sigma_t</att>
            <att name="units">Kg m^-3</att>
            <att name="valid_max" type="float">27.5493</att>
            <att name="valid_min" type="float">22.7306</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">28.0</att>
            <att name="colorBarMinimum" type="double">22.0</att>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Oxy1</sourceName>
        <destinationName>Oxy1</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">oxygen, SBE 43, 1</att>
            <att
name="standard_name">volume_fraction_of_oxygen_in_sea_water</att>
            <att name="units">ml/l</att>
            <att name="valid_max" type="float">5.38068</att>
            <att name="valid_min" type="float">2.69395</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">10.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Dissolved O2</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Oxy2</sourceName>
        <destinationName>Oxy2</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">oxygen, SBE 43, 2</att>
            <att
name="standard_name">volume_fraction_of_oxygen_in_sea_water</att>
            <att name="units">ml/l</att>
            <att name="valid_max" type="float">4.62798</att>
            <att name="valid_min" type="float">2.26251</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">10.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Dissolved O2</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Xmi</sourceName>
        <destinationName>Xmi</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">beam_transmission</att>
            <att name="standard_name">transmission</att>
            <att name="units">percentage</att>
            <att name="valid_max" type="float">98.7963</att>
            <att name="valid_min" type="float">95.6864</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">100.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Flu</sourceName>
        <destinationName>Flu</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">fluorescence</att>
            <att name="nodc_name">Fluorescence</att>
            <att name="units">mg m-3</att>
            <att name="valid_max" type="float">4.1824</att>
            <att name="valid_min" type="float">1.8401</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">4.5</att>
            <att name="colorBarMinimum" type="double">1.5</att>
            <att name="ioos_category">Optical Properties</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Par</sourceName>
        <destinationName>Par</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">PAR/Irradiance</att>
            <att
name="standard_name">omnidirectional_photosynthetic_spherical_irradiance_in_sea_water</att>
            <att name="units">W m^-2</att>
            <att name="valid_max" type="float">2.8388</att>
            <att name="valid_min" type="float">0.13587</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">360.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Rho</sourceName>
        <destinationName>Rho</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">Dst  Sigt</att>
            <att name="comment">sigma-t, Value = density - 1000</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">density</att>
            <att name="standard_name">sea_water_density</att>
            <att name="units">kg m^-3</att>
            <att name="valid_max" type="float">1031.3914</att>
            <att name="valid_min" type="float">1022.7392</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">28.0</att>
            <att name="colorBarMinimum" type="double">20.0</att>
            <att name="ioos_category">Physical Oceanography</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Sigt</sourceName>
        <destinationName>Sigt</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">density [sigma-theta]</att>
            <att name="standard_name">sea_water_potential_density</att>
            <att name="units">Kg m^-3</att>
            <att name="valid_max" type="float">27.5578</att>
            <att name="valid_min" type="float">22.7307</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">28.0</att>
            <att name="colorBarMinimum" type="double">20.0</att>
            <att name="ioos_category">Physical Oceanography</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Des</sourceName>
        <destinationName>Des</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">descent_rate</att>
            <att name="units">m/s</att>
            <att name="valid_max" type="float">0.906</att>
            <att name="valid_min" type="float">0.099</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">1.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>Gpa</sourceName>
        <destinationName>Gpa</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">boolean_flag_variable
enumerated_flag_variable</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">geopotential_anomaly</att>
            <att name="standard_name">geopotential</att>
            <att name="units">J Kg^-1</att>
            <att name="valid_max" type="float">11.143</att>
            <att name="valid_min" type="float">-0.109</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">10.0</att>
            <att name="colorBarMinimum" type="double">-10.0</att>
            <att name="ioos_category">Unknown</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>PoT</sourceName>
        <destinationName>PoT</destinationName>
        <dataType>float</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="float">-9.99E-29</att>
            <att name="ancillary_variables">T1</att>
            <att name="coordinates">lat lon z</att>
            <att name="grid_mapping">crs</att>
            <att name="long_name">potential_temperature</att>
            <att name="standard_name">sea_water_potential_temperature</att>
            <att name="units">degree_Celsius</att>
            <att name="valid_max" type="float">29.3493</att>
            <att name="valid_min" type="float">5.4563</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">32.0</att>
            <att name="colorBarMinimum" type="double">0.0</att>
            <att name="ioos_category">Temperature</att>
        </addAttributes>
    </dataVariable>
</dataset>
-----

Unfortunately, this doesn't work because of a problem with the CF-1.6 standard
at:

http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html[+http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html+]

Section 9.5 states that: 

[quote]
________
"Where feasible a variable with the attribute cf_role
should be included.  The only acceptable values of cf_role for Discrete
Geometry CF data sets are timeseries_id, profile_id, and trajectory_id.   The
variable carrying the cf_role attribute may have any data type.  When a
variable is assigned this attribute, it must provide a unique identifier for
each feature instance.   CF files that contain timeSeries, profile or
trajectory featureTypes, should include only a single occurrence of a cf_role
attribute;  CF files that contain timeSeriesProfile or trajectoryProfile may
contain two occurrences, corresponding to the two levels of structure in these
feature types."
________


An example of a NetCDF header with +featureType = profile+ that also contains
the required +cf_role = "profile_id"+ is at:

http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#idp8363696[+http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html#idp8363696+]

A short excerpt showing the use is:

-----
   dimensions:
      z = 42 ;

   variables:
      int profile ;
          profile:cf_role = "profile_id";

      double time; 
          time:standard_name = "time";
...
-----

The python-netcdf4 interface doesn't like this file because the +scale_factor+
and +add_offset+ attributes are set to null instead of real values.  We will
change that using the NCO operator +ncatted+, e.g.

-----
ncatted -a scale_factor,Prz,o,f,1.0 g01l01s01.nc
ncatted -a add_offset,Prz,o,f,0.0 g01l01s01.nc
...
-----

Neither does it like attempting to to transfer a +_FillValue+
attribute from one file to another, so we must nuke the +_FillValue+
attributes via:

-----
ncatted -a _FillValue,Prz,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,T1,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,T2,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Sal,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Cdt1,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Cdt2,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Dst,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Oxy1,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Oxy2,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Xmi,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Flu,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Par,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Rho,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Sigt,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Des,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,Gpa,d,f,1.0 g01l01s01.nc
ncatted -a _FillValue,PoT,d,f,1.0 g01l01s01.nc
-----


A Python script to create a dummy NetCDF file holding just the +profile+
variable is:

-----
#!/usr/bin/python2.7

from netCDF4 import Dataset
role = Dataset('cf_role.nc','w',format='NETCDF3_CLASSIC')
roles = role.createVariable('profile','i4')
roles.cf_role = 'profile_id'
role.close()
-----

The fragment file +cf_role.nc+ can then be appended to an existing file +gis.nc+ via:

-----
ncks -v profile cf_role.nc gis.nc
-----


With no attribute o+cf_role=profile_id+ for the variable +profile+ in +gis.nc+ we get the error:

-----
datasets.xml error on line #839
While trying to load datasetID=ERDDAP2_a032_4122_b16d (after 153 ms)
java.lang.RuntimeException: datasets.xml error on or before line #839: For
cdm_data_type=Trajectory, no variable should have cf_role=profile_id (see
profile).
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:344)
 at gov.noaa.pfel.erddap.LoadDatasets.run(LoadDatasets.java:300)
Caused by: com.cohort.util.SimpleException: For cdm_data_type=Trajectory, no
variable should have cf_role=profile_id (see profile).
 at
gov.noaa.pfel.erddap.dataset.EDDTable.accessibleViaNcCF(EDDTable.java:10315)
 at gov.noaa.pfel.erddap.dataset.EDD.ensureValid(EDD.java:557)
 at gov.noaa.pfel.erddap.dataset.EDDTable.ensureValid(EDDTable.java:424)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromFiles.<init>(EDDTableFromFiles.java:1503)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromNcFiles.<init>(EDDTableFromNcFiles.java:111)
 at
gov.noaa.pfel.erddap.dataset.EDDTableFromFiles.fromXml(EDDTableFromFiles.java:217)
 at gov.noaa.pfel.erddap.dataset.EDD.fromXml(EDD.java:333)
-----

A program to do the conversions is:

-----
#!/usr/bin/python2.7

from netCDF4 import Dataset
from datetime import datetime, timedelta
from netCDF4 import num2date,date2num

timestart = "days since 1970-01-01 00:00:00"

infile = Dataset('g01l01s01.nc','r')

#  Extract the date in the form YYYY-MM-DD HH:mm:SS.
ymd = str(infile.time_coverage_start)
in_lon = infile.geospatial_lon_min
in_lat = infile.geospatial_lat_min

#  Convert date to hours since Jan. 1, 1970.
timeout = date2num(datetime.strptime(ymd,"%Y-%m-%d %H:%M:%S"),units=timestart)
outfile = Dataset('test.nc','w',format='NETCDF3_CLASSIC')

outfile.cdm_trajectory_variables = "longitude, latitude, time"
outfile.cdm_altitude_proxy = "z"
roles = outfile.createVariable('profile','i4')
roles.cf_role = 'profile_id'
lats = outfile.createVariable('latitude','f4')
lons = outfile.createVariable('longitude','f4')
time = outfile.createVariable('time','f8')
time[:] = timeout
time.units = timestart
time.standard_name = "time"
time.long_name = "time"
lats[:] = in_lat
lats.standard_name = "latitude"
lats.long_name = "latitude"
lats.units = "degrees_north"
lons[:] = in_lon
lons.standard_name = "longitude"
lons.long_name = "longitude"
lons.units = "degrees_east"

z = infile.dimensions['z']
zdim = len(z)

for att in infile.ncattrs():
      setattr(outfile,att,getattr(infile,att))

outfile.createDimension('z',zdim)
Prz = infile.variables['Prz']
oPrz = outfile.createVariable('Prz','f4',('z',),fill_value=-9.99e-29)
oPrz[:] = Prz[:]
for att in Prz.ncattrs():
        setattr(oPrz,att,getattr(Prz,att))

T1 = infile.variables['T1']
oT1 = outfile.createVariable('T1','f4',('z',),fill_value=-9.99e-29)
oT1[:] = T1[:]
for att in T1.ncattrs():
        setattr(oT1,att,getattr(T1,att))

T2 = infile.variables['T2']
oT2 = outfile.createVariable('T2','f4',('z',),fill_value=-9.99e-29)
oT2[:] = T2[:]
for att in T2.ncattrs():
        setattr(oT2,att,getattr(T2,att))

Sal = infile.variables['Sal']
oSal = outfile.createVariable('Sal','f4',('z',),fill_value=-9.99e-29)
oSal[:] = Sal[:]
for att in Sal.ncattrs():
        setattr(oSal,att,getattr(Sal,att))

Cdt1 = infile.variables['Cdt1']
oCdt1 = outfile.createVariable('Cdt1','f4',('z',),fill_value=-9.99e-29)
oCdt1[:] = Cdt1[:]
for att in Cdt1.ncattrs():
        setattr(oCdt1,att,getattr(Cdt1,att))

Cdt2 = infile.variables['Cdt2']
oCdt2 = outfile.createVariable('Cdt2','f4',('z',),fill_value=-9.99e-29)
oCdt2[:] = Cdt2[:]
for att in Cdt2.ncattrs():
        setattr(oCdt2,att,getattr(Cdt2,att))

Dst = infile.variables['Dst']
oDst = outfile.createVariable('Dst','f4',('z',),fill_value=-9.99e-29)
oDst[:] = Dst[:]
for att in Dst.ncattrs():
        setattr(oDst,att,getattr(Dst,att))

Oxy1 = infile.variables['Oxy1']
oOxy1 = outfile.createVariable('Oxy1','f4',('z',),fill_value=-9.99e-29)
oOxy1[:] = Oxy1[:]
for att in Oxy1.ncattrs():
        setattr(oOxy1,att,getattr(Oxy1,att))

Oxy2 = infile.variables['Oxy2']
oOxy2 = outfile.createVariable('Oxy2','f4',('z',),fill_value=-9.99e-29)
oOxy2[:] = Oxy2[:]
for att in Oxy2.ncattrs():
        setattr(oOxy2,att,getattr(Oxy2,att))

Xmi = infile.variables['Xmi']
oXmi = outfile.createVariable('Xmi','f4',('z',),fill_value=-9.99e-29)
oXmi[:] = Xmi[:]
for att in Xmi.ncattrs():
        setattr(oXmi,att,getattr(Xmi,att))

Flu = infile.variables['Flu']
oFlu = outfile.createVariable('Flu','f4',('z',),fill_value=-9.99e-29)
oFlu[:] = Flu[:]
for att in Flu.ncattrs():
        setattr(oFlu,att,getattr(Flu,att))

Par = infile.variables['Par']
oPar = outfile.createVariable('Par','f4',('z',),fill_value=-9.99e-29)
oPar[:] = Par[:]
for att in Par.ncattrs():
        setattr(oPar,att,getattr(Par,att))

Rho = infile.variables['Rho']
oRho = outfile.createVariable('Rho','f4',('z',),fill_value=-9.99e-29)
oRho[:] = Rho[:]
for att in Rho.ncattrs():
        setattr(oRho,att,getattr(Rho,att))

Sigt = infile.variables['Sigt']
oSigt = outfile.createVariable('Sigt','f4',('z',),fill_value=-9.99e-29)
oSigt[:] = Sigt[:]
for att in Sigt.ncattrs():
        setattr(oSigt,att,getattr(Sigt,att))

Des = infile.variables['Des']
oDes = outfile.createVariable('Des','f4',('z',),fill_value=-9.99e-29)
oDes[:] = Des[:]
for att in Des.ncattrs():
        setattr(oDes,att,getattr(Des,att))

Gpa = infile.variables['Gpa']
oGpa = outfile.createVariable('Gpa','f4',('z',),fill_value=-9.99e-29)
oGpa[:] = Gpa[:]
for att in Gpa.ncattrs():
        setattr(oGpa,att,getattr(Gpa,att))

PoT = infile.variables['PoT']
oPoT = outfile.createVariable('PoT','f4',('z',),fill_value=-9.99e-29)
oPoT[:] = PoT[:]
for att in PoT.ncattrs():
        setattr(oPoT,att,getattr(PoT,att))

infile.close()
outfile.close()

-----

This can now be appended to one of the file without +cf_role+ - e.g.
+g01l01s01.nc+ - using
the NCO program +ncks+, e.g.

-----
ncks -v profile cf_role.nc g01l01s01.nc
-----

This didn't work, either.  After running the NetCDF file through
+GenerateDatasetsXml+ again and getting the extra XML code:

-----
    <dataVariable>
        <sourceName>profile</sourceName>
        <destinationName>profile</destinationName>
        <dataType>int</dataType>
        <addAttributes>
            <att name="cf_role">profile_id</att>
            <att name="ioos_category">Unknown</att>
            <att name="long_name">Profile</att>
        </addAttributes>
    </dataVariable>
-----

and restarting the Tomcat server, we get the new error:

-----
datasets.xml error on line #1507
While trying to load datasetID=ERDDAP_72d0_82a5_3c32 (after 172 ms)
java.lang.RuntimeException: datasets.xml error on or before line #1507: For
cdm_data_type=Profile, the global attribute cdm_profile_variables must be set.
-----

Thus, we add the global attribute +cdm_profile_variables+ to the file
+g01l01s01.nc+ via:

-----
ncatted -a cdm_profile_variables,global,c,c,"Prz" g01l01s01.nc
-----

and try +GenerateDatasetsXml+ again.  This time we get the error:

-----
While trying to load datasetID=ERDDAP_72d0_82a5_3c32 (after 82 ms)
java.lang.RuntimeException: datasets.xml error on or before line #1507: For
cdm_data_type=Profile, when there is no altitude variable, you MUST define the
global attribute cdm_altitude_proxy.
-----

[[EDDTableFromNcFilesEx]]
+EDDTableFromNcFiles+ Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Run the program with +EDDTableFromNcFiles+ with directory
+barataria:/raid/TGLO/TMP+ with the files
+GROM-hind-reg-12-08-05-00-24.nc+ and +GROM-hind-reg-12-08-05-00-24.nc+.

The header from +GROM-hind-reg-12-08-05-00-24.nc+ is:

------
netcdf GROM-hind-reg-12-08-05-00-24 {
dimensions:
	lon = 171 ;
	lat = 131 ;
	time = UNLIMITED ; // (24 currently)
variables:
	double lon(lon) ;
		lon:long_name = "Longitude" ;
		lon:standard_name = "longitude" ;
		lon:units = "degrees_east" ;
		lon:point_spacing = "even" ;
	double lat(lat) ;
		lat:long_name = "Latitude" ;
		lat:standard_name = "latitude" ;
		lat:units = "degrees_north" ;
		lat:point_spacing = "even" ;
	double mask(lat, lon) ;
		mask:long_name = "Land Mask" ;
		mask:units = "nondimensional" ;
		mask:standard_name = "land_binary_mask" ;
	double water_u(time, lat, lon) ;
		water_u:long_name = "Eastward Water Velocity" ;
		water_u:standard_name = "surface_eastward_sea_water_velocity"
;
		water_u:units = "m s-1" ;
		water_u:axis = "X" ;
		water_u:_FillValue = -9.9999e+32 ;
		water_u:scale_factor = 1. ;
		water_u:add_offset = 0. ;
	double water_v(time, lat, lon) ;
		water_v:long_name = "Northward Water Velocity" ;
		water_v:standard_name = "surface_northward_sea_water_velocity"
;
		water_v:units = "m s-1" ;
		water_v:axis = "Y" ;
		water_v:_FillValue = -9.9999e+32 ;
		water_v:scale_factor = 1. ;
		water_v:add_offset = 0. ;
	double time(time) ;
		time:long_name = "Valid Time" ;
		time:standard_name = "time" ;
		time:units = "seconds since 2003-10-28 0:00:00 0:00" ;
		time:axis = "T" ;

// global attributes:
		:title = "ROMS output 48-hour regular grid surface currents" ;
		:institution = "Texas A&M University, Dept.of Oceanography" ;
		:source = "ROMS 2.1" ;
		:history = "Sun Aug  5 20:09:16 2012: /usr/bin/ncrcat -O
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-0
8-05-00-06.nc /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-06-06.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-0
5-12-06.nc /home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-18-06.nc
/home/baum/TGLO/OUT/HIS/GROM-hind-reg-12-08-05-0
0-24.nc\n",
			"Sun Aug  5 04:16:27 2012: /usr/bin/ncrcat -O -d
time,0,5 /home/baum/TGLO/OUT/HIS/GROM-fore
-reg-12-08-05-00-72.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-00-06.nc\n",
			"none" ;
		:references = "none" ;
		:comment = "none" ;
		:Conventions = "CF-1.0" ;
		:grid_type = "REGULAR" ;
		:nco_openmp_thread_number = 1 ;
------

The commands:

------
Which EDDType (default="EDDTableFromNcFiles")
? 
Starting directory (default="/raid/TGLO/HIS/")
? /raid/TGLO/TMP/
File name regex (e.g., ".*\.nc") (default="")
? 
A sample full file name (default="/raid/TGLO/HIS/TGLO-his-12-08-07-00-06.nc")
? /raid/TGLO/TMP/GROM-hind-reg-12-08-05-00-24.nc
DimensionsCSV (or "" for default) (default="")
? 
ReloadEveryNMinutes (e.g., 10080) (default="")
? 
PreExtractRegex (default="")
? 
PostExtractRegex (default="")
? 
ExtractRegex (default="")
? 
Column name for extract (default="")
? 
Sorted column source name (default="")
? 
Sort files by sourceName (default="")
? 
infoUrl (default="")
? 
institution (default="")
? 
summary (default="")
? 
title (default="")
? 
------

The XML result:

------
<dataset type="EDDTableFromNcFiles" datasetID="TMP_d836_947a_43e1"
active="true">
    <reloadEveryNMinutes>10080</reloadEveryNMinutes>
    <fileDir>/raid/TGLO/TMP/</fileDir>
    <recursive>true</recursive>
    <fileNameRegex>.*\.nc</fileNameRegex>
    <metadataFrom>last</metadataFrom>
    <preExtractRegex></preExtractRegex>
    <postExtractRegex></postExtractRegex>
    <extractRegex></extractRegex>
    <columnNameForExtract></columnNameForExtract>
    <sortedColumnSourceName>time</sortedColumnSourceName>
    <sortFilesBySourceNames>time</sortFilesBySourceNames>
    <altitudeMetersPerSourceUnit>1</altitudeMetersPerSourceUnit>
    <!-- sourceAttributes>
        <att name="comment">none</att>
        <att name="Conventions">CF-1.0</att>
        <att name="grid_type">REGULAR</att>
        <att name="history">Sun Aug  5 20:09:16 2012: /usr/bin/ncrcat -O
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-00-06.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-06-06.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-12-06.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-18-06.nc
/home/baum/TGLO/OUT/HIS/GROM-hind-reg-12-08-05-00-24.nc
Sun Aug  5 04:16:27 2012: /usr/bin/ncrcat -O -d time,0,5
/home/baum/TGLO/OUT/HIS/GROM-fore-reg-12-08-05-00-72.nc
/home/baum/TGLO/OUT/HIS/GROM-nowc-reg-12-08-05-00-06.nc
none</att>
        <att name="institution">Texas A&amp;M University, Dept.of
Oceanography</att>
        <att name="nco_openmp_thread_number" type="int">1</att>
        <att name="references">none</att>
        <att name="source">ROMS 2.1</att>
        <att name="title">ROMS output 48-hour regular grid surface
currents</att>
    </sourceAttributes -->
    <!-- Please specify the actual cdm_data_type (TimeSeries?) and related
info below, for example...
        <att name="cdm_timeseries_variables">station, longitude,
latitude</att>
        <att name="subsetVariables">station, longitude, latitude</att>
    -->
    <addAttributes>
        <att name="cdm_data_type">Point</att>
        <att name="Conventions">CF-1.6, COARDS, Unidata Dataset Discovery
v1.0</att>
        <att name="infoUrl">???</att>
        <att name="keywords">48-hour,
Oceans &gt; Ocean Circulation &gt; Ocean Currents,
a&amp;m, circulation, currents, dept.of, eastward, grid, hour, model,
modeling, northward, ocean, oceanography, oceans, output, regional, regional
ocean model, regular, roms, sea, seawater, surface,
surface_eastward_sea_water_velocity, surface_northward_sea_water_velocity,
system, texas, time, university, valid, velocity, water</att>
        <att name="keywords_vocabulary">GCMD Science Keywords</att>
        <att name="license">[standard]</att>
        <att name="Metadata_Conventions">CF-1.6, COARDS, Unidata Dataset
Discovery v1.0</att>
        <att name="sourceUrl">(local files)</att>
        <att name="standard_name_vocabulary">CF-12</att>
        <att name="summary">none</att>
    </addAttributes>
    <dataVariable>
        <sourceName>time</sourceName>
        <destinationName>time2</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="axis">T</att>
            <att name="long_name">Valid Time</att>
            <att name="standard_name">time</att>
            <att name="units">seconds since 2003-10-28 0:00:00 0:00</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="ioos_category">Time</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>lat</sourceName>
        <destinationName>latitude</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="long_name">Latitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">latitude</att>
            <att name="units">degrees_north</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">90.0</att>
            <att name="colorBarMinimum" type="double">-90.0</att>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>lon</sourceName>
        <destinationName>longitude</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="long_name">Longitude</att>
            <att name="point_spacing">even</att>
            <att name="standard_name">longitude</att>
            <att name="units">degrees_east</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">180.0</att>
            <att name="colorBarMinimum" type="double">-180.0</att>
            <att name="ioos_category">Location</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>water_u</sourceName>
        <destinationName>water_u</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">X</att>
            <att name="long_name">Eastward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att
name="standard_name">surface_eastward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
    <dataVariable>
        <sourceName>water_v</sourceName>
        <destinationName>water_v</destinationName>
        <dataType>double</dataType>
        <!-- sourceAttributes>
            <att name="_FillValue" type="double">-9.999900000000001E32</att>
            <att name="add_offset" type="double">0.0</att>
            <att name="axis">Y</att>
            <att name="long_name">Northward Water Velocity</att>
            <att name="scale_factor" type="double">1.0</att>
            <att
name="standard_name">surface_northward_sea_water_velocity</att>
            <att name="units">m s-1</att>
        </sourceAttributes -->
        <addAttributes>
            <att name="colorBarMaximum" type="double">0.5</att>
            <att name="colorBarMinimum" type="double">-0.5</att>
            <att name="ioos_category">Currents</att>
        </addAttributes>
    </dataVariable>
</dataset>
------


[[EDDTableFromDatabaseEx]]
+EDDTableFromDatabase+ Example
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The skeleton XML for an +EDDTableFromDatabase+ dataset is:

[source,xml]
-----
<dataset type="EDDTableFromDatabase" datasetID="..." active="..." >
  <sourceUrl>...</sourceUrl>
    <!-- Put the database name at the end, for example, 
      "jdbc:postgresql://123.45.67.89:5432/databaseName". REQUIRED. -->
  <driverName>...</driverName>
    <!-- The high-level name of the database driver, e.g., 
      "org.postgresql.Driver".  You need to put the actual database 
      driver .jar file (for example, postgresql.jdbc.jar) in 
      [tomcat]/webapps/erddap/WEB-INF/lib.  REQUIRED. -->
  <connectionProperty name="name">value</connectionProperty>
    <!-- The names (e.g., "user", "password", and "ssl") and values 
      of the properties needed for ERDDAP to establish the connection
      to the database.  0 or more. -->
  <catalogName>...</catalogName>
    <!-- The name of the catalog which has the schema which has the 
      table, default = "".  OPTIONAL. -->
  <schemaName>...</schemaName> <!-- The name of the 
    schema which has the table, default = "".  OPTIONAL. -->
  <tableName>...</tableName>  <!-- The name of the 
    table, default = "".  REQUIRED. -->
  <orderBy>...</orderBy>  <!-- A comma-separated list of
    sourceNames to be used in an ORDER BY clause at the end of the 
    every query sent to the database (unless the user's request
    includes an &orderBy() filter, in which case the user's 
    orderBy is used).  The order of the sourceNames is important. 
    The leftmost sourceName is most important; subsequent 
    sourceNames are only used to break ties.  Only relevant 
    sourceNames are included in the ORDER BY clause for a given user 
    request.  If this is not specified, the order of the returned 
    values in not specified. Default = "".  OPTIONAL. -->
  <sourceNeedsExpandedFP_EQ>true(default)|false</sourceNeedsExpandedFP_EQ>
  <accessibleTo>...</accessibleTo> <!-- 0 or 1 -->
  <reloadEveryNMinutes>...</reloadEveryNMinutes>
  <fgdcFile>...</fgdcFile> <!-- 0 or 1 -->
  <iso19115File>...</iso19115File> <!-- 0 or 1 -->
  <onChange>...</onChange> <!-- 0 or more -->
  <altitudeMetersPerSourceUnit>...</altitudeMetersPerSourceUnit>
  <addAttributes>...</addAttributes>
  <dataVariable>...</dataVariable> <!-- 1 or more.
     For date and timestamp database columns, set dataType=double and 
     units=seconds since 1970-01-01T00:00:00Z -->
</dataset>
-----

[[DasDds]]
[[Interactive_Dataset_Refinement]]
Interactive Dataset Refinement with +DasDds+
--------------------------------------------

+DasDds+ is a command-line program that you can use after you have
created an initial +dataset+ XML tag to add to +datasets.xml+ for
a new dataset.
It is used to repeatedly test and refine your XML.  The
general procedure for using +DasDds+ is:

. run +DasDds+ and it asks your for the xref:datasetID[+datasetID+]
for the dataset on which you are working;

. hit return after you've supplied that information and +DasDds+ will
start up and attempt to create the data with that +datasetID+ while also:

** printing a large number of diagnostic messages to
the screen, for which close scrutiny is a good thing;

** deleting all +/dataset/+ files for the dataset (for safety) before
attempting to create it, so if you're dealing with an aggregated
dataset it would be a good idea to limit the number of files the
data constructor finds by adjusting xref:fileNameRegex[+fileNameRegex+]
or temporarily relocating or renaming some files;

. if it fails to create the dataset, it will show you an error message
which, along with the diagnostic messages, should give you a good idea
of what changes need to be made in the +dataset+ XML tag;

. make the changes to the +dataset+ XML tag, and restart this procedure 
at step 1 until you and
ERDDAP are satisfied;

. when +DasDds+ successfully creates the dataset, it will show you the
+.das+ and +.dds+ for the dataset and put the information on the
system clipboard;

. even at this point with an error-free dataset, you might still want
to make changes to the +dataset+ XML tag to, say, improve the
metadata content, after which you restart this procedure to see if
at step 1
the changes you made are properly reflected in the displayed
metadata.

[[clusters]]
ERDDAP Grids, Clusters, Confederations and Cloud Computing
----------------------------------------------------------

See the page

http://coastwatch.pfeg.noaa.gov/erddap/download/grids.html[+http://coastwatch.pfeg.noaa.gov/erddap/download/grids.html+]

for now.



[appendix]
AsciiDoc
--------

This document was created and is maintained using the AsciiDoc text document
format since AsciiDoc files - nothing more than ASCII text with structured
ASCII markup - can be translated into many formats including HTML, PDF, EPUB,
etc.  The AsciiDoc software can be found at:

http://www.methods.co.nz/asciidoc/index.html[+http://www.methods.co.nz/asciidoc/index.html+]

The installation is easy enough, but the manual isn't explicit enough about
how to use certain features.  A few notes here to help the confused.

*  To get syntax highlighting for languages, you need to install the GNU
+source-highlight+ package found at:
****
http://www.gnu.org/software/src-highlite/[+http://www.gnu.org/software/src-highlite/+]
****
*  To have the sections numbered but not the preface and appendices, you
must place +:numbered!+ before the preface, and then +:numbered:+ before
the sections, and finally +:numbered!+ again before the appendices.




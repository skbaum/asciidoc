Software Notes
==============
Steven K. Baum
v1.7, 2015-03-02
:doctype: book
:toc:
:icons:

:numbered!:

[preface]

Pedagogy
--------

Github
~~~~~~

asascience-open - https://github.com/asascience-open[+https://github.com/asascience-open+]

cerfacs-globc - https://github.com/cerfacs-globc[+https://github.com/cerfacs-globc+]

* icclim - https://github.com/cerfacs-globc/icclim[+https://github.com/cerfacs-globc/icclim+]

cf-convention - https://github.com/cf-convention[+https://github.com/cf-convention+]

cf-metadata - https://github.com/cf-metadata[+https://github.com/cf-metadata+]

ClimateCodeFoundation - https://github.com/ClimateCodeFoundation[+https://github.com/ClimateCodeFoundation+]

geopython - https://github.com/geopython[+https://github.com/geopython+]

google - https://github.com/google[+https://github.com/google+]

ioos - https://github.com/ioos[+https://github.com/ioos+]

jswhit - https://github.com/jswhit[+https://github.com/jswhit+]

matplotlib - https://github.com/matplotlib[+https://github.com/matplotlib+]

ocefpaf - https://github.com/ocefpaf[+https://github.com/ocefpaf+]

ooici - https://github.com/ooici[+https://github.com/ooici+]

pyoceans - https://github.com/pyoceans[+https://github.com/pyoceans+]

pyugrid - https://github.com/pyugrid[+https://github.com/pyugrid+]

SciTools - https://github.com/SciTools[+https://github.com/SciTools+]

sci-wms - https://github.com/sci-wms[+https://github.com/sci-wms+]

sgrid - https://github.com/sgrid[+https://github.com/sgrid+]

signell - https://github.com/rsignell-usgs[+https://github.com/rsignell-usgs+]

TEOS-10 - https://github.com/TEOS-10[+https://github.com/TEOS-10+]

ugrid-conventions - https://github.com/ugrid-conventions[+https://github.com/ugrid-conventions+]

Unidata - https://github.com/Unidata[+https://github.com/Unidata+]

USGS-CIDA - https://github.com/USGS-CIDA[+https://github.com/USGS-CIDA+]

Binstar
~~~~~~~

anaconda - https://binstar.org/anaconda[+https://binstar.org/anaconda+]

blaze - https://binstar.org/blaze[+https://binstar.org/blaze+]

IOOS - https://binstar.org/ioos[+https://binstar.org/ioos+]

NESII - https://binstar.org/nesii[+https://binstar.org/nesii+]

ocefpaf - https://binstar.org/ocefpaf[+https://binstar.org/ocefpaf+]

OSGEO - https://binstar.org/osgeo[+https://binstar.org/osgeo+]

SciTools - https://binstar.org/scitools[+https://binstar.org/scitools+]

shoyer - https://binstar.org/shoyer[+https://binstar.org/shoyer+]

Signell - https://conda.binstar.org/rsignell[+https://conda.binstar.org/rsignell+]

travis - https://binstar.org/travis[+https://binstar.org/travis+]

Unidata - https://binstar.org/unidata[+https://binstar.org/unidata+]

Software Repositories
~~~~~~~~~~~~~~~~~~~~~

*List of Awesome Lists* -
https://github.com/sindresorhus/awesome[+https://github.com/sindresorhus/awesome+]

* *Awesome Awesomeness* -
https://github.com/bayandin/awesome-awesomeness[+https://github.com/bayandin/awesome-awesomeness+]

*****
An awesome list is a Github entity that lists interesting links for a given software
topic.  This are metalists of such lists.
*****

*52 North Github* - https://github.com/52North[+https://github.com/52North+]

*****
Open source geospatial software.
*****

*ACM Queue* - http://queue.acm.org/[+http://queue.acm.org/+]

*****
Interesting occasional essays about software topics from clever ACM folks.
*****

*Clever Algorithms: Nature-Inspired Programming Recipes* -
http://cleveralgorithms.com/nature-inspired/index.html[+http://cleveralgorithms.com/nature-inspired/index.html+]

*****
Example Ruby programs from a book about nature-inspired programming algorithms.
*****

*Cluster Monkey* -
http://www.clustermonkey.net/[+http://www.clustermonkey.net/+]

*****
A repository of information about cluster computing.
*****

*commandlinefu* -
http://www.commandlinefu.com/commands/browse[+http://www.commandlinefu.com/commands/browse+]

*****
A very large collection of clever command-line examples.
*****

*CPC Library* -
http://cpc.cs.qub.ac.uk/[+http://cpc.cs.qub.ac.uk/+]

*****
Library of programs presented in Computer Physics Communication journal.
*****

*CPUShack Museum* - http://www.cpushack.com/[+http://www.cpushack.com/+]

*****
A CPU history museum.
*****

*Dataisnature* - http://www.dataisnature.com/[+http://www.dataisnature.com/+]

*****
Interrelationships between natural processes, computational systems and procedural-based art practices.
*****

*DevFreeBooks* - http://devfreebooks.org/[+http://devfreebooks.org/+]

*****
A huge collection of free computer programming books for developers.
*****

*Experimental Mathematics* -
http://www.experimentalmath.info/[+http://www.experimentalmath.info/+]

*****
A repository of information on experimental and computer-assisted mathematics.
*****

*Fedora-Related Software Repositories*

* *Extra Packages for Enterprise Linux* -
https://fedoraproject.org/wiki/EPEL[+https://fedoraproject.org/wiki/EPEL+]

* *Fedora Project* -
http://fedoraproject.org/wiki/Repositories[+http://fedoraproject.org/wiki/Repositories+]

* *Fedora People Repositories* -
https://repos.fedorapeople.org/[+https://repos.fedorapeople.org/+]

* *Fedora Third Party Repos* -
http://rpmfusion.org/FedoraThirdPartyRepos[+http://rpmfusion.org/FedoraThirdPartyRepos+]

*Fortran Wiki* -
http://fortranwiki.org/fortran/show/HomePage[+http://fortranwiki.org/fortran/show/HomePage+]

*****
An open venue for discussing all aspects of the Fortran programming language and scientific computing.
*****

*Free Programming Books* -
https://github.com/vhf/free-programming-books/blob/master/free-programming-books.md[+https://github.com/vhf/free-programming-books/blob/master/free-programming-books.md+]

*****
A list of free programming books in many popular and obscure languages.
*****

*Hackery, Math and Design* - http://acko.net/[+http://acko.net/+]

*****
Interesting art and musings about computer-related topics.
*****

*Hacking for Artists* -
http://hackingforartists.com/[+http://hackingforartists.com/+]

*****
A list of software packages useful to artists.
*****

*HEPForge* -
https://hepforge.org/projects[+https://hepforge.org/projects+]

*****
HEPForge is a development environment for high energy physics software projects.
*****

*IBM*

* *IBM Developerworks Open* - https://developer.ibm.com/open/[+https://developer.ibm.com/open/+]

* *IBM Developerworks Open Source* -
http://www.ibm.com/developerworks/opensource/[+http://www.ibm.com/developerworks/opensource/+]

* *IBM Open Source at GitHub* - http://ibm.github.io/[+http://ibm.github.io/+]

* *Linux Open Source Projects* -
http://www.ibm.com/developerworks/views/linux/projects.jsp[+http://www.ibm.com/developerworks/views/linux/projects.jsp+]

*Internet of Things Software* -
http://postscapes.com/internet-of-things-software-guide[+http://postscapes.com/internet-of-things-software-guide+]

*****
A list of software packages related to the IoT next big thing.
*****

*Libre Graphics World* -
http://libregraphicsworld.org/[+http://libregraphicsworld.org/+]

*****
A blog about computer graphics.
*****

*Linux Virtualization Wiki* -
http://virt.kernelnewbies.org/[+http://virt.kernelnewbies.org/+]

*****
A wiki dedicated documenting the different virtualization technologies available in Linux, including an overview of the way each virtualization technology works, how to get started, where to get involved with development, etc.
*****

*List of Languages That Compile to JS* -
https://github.com/jashkenas/coffeescript/wiki/List-of-languages-that-compile-to-JS[+https://github.com/jashkenas/coffeescript/wiki/List-of-languages-that-compile-to-JS+]

*Mathematical Atlas* -
http://www.math.niu.edu/Papers/Rusin/known-math/welcome.html[+http://www.math.niu.edu/Papers/Rusin/known-math/welcome.html+]

*****
A gateway to modern mathematics.
*****

*NASA History Series Publications* -
http://history.nasa.gov/series95.html[+http://history.nasa.gov/series95.html+]

*****
Titles published in the NASA History Series of publications.
*****

*National Science Digital Library* -
https://nsdl.oercommons.org/[+https://nsdl.oercommons.org/+]

*****
Provides high quality online educational resources for teaching and learning, with current emphasis on the sciences, technology, engineering, and mathematics (STEM) disciplines—both formal and informal, institutional and individual, in local, state, national, and international educational settings.
*****

*Nature of Code (The)* -
http://natureofcode.com/book/[+http://natureofcode.com/book/+]

*****
A free online book: "We want to take a look at something that naturally occurs in our physical world, then determine how we can write code to simulate that occurrence."
*****

*New General Catalog of Old Books and Authors* -
http://www.kingkong.demon.co.uk/ngcoba/ngcoba.htm[+http://www.kingkong.demon.co.uk/ngcoba/ngcoba.htm+]

*****
The aim of this site is to catalog all deceased authors, all authors of books published before 1964, and at least some more recent authors, including their full name(s), date of death, date of birth, pseudonyms, sex & nationality (for those who died from 1920 onwards), and their books published before 1964.
*****

*Online Books Page* -
http://digital.library.upenn.edu/books/[+http://digital.library.upenn.edu/books/+]

*Practical Common Lisp* -
http://www.gigamonkeys.com/book/[+http://www.gigamonkeys.com/book/+]

*Pythonic Perambulations* -
https://jakevdp.github.io/[+https://jakevdp.github.io/+]

*repo.or.cz* - http://repo.or.cz/[+http://repo.or.cz/+]

*Stacklet* - https://stacklet.com/[+https://stacklet.com/+]

*txt2re Regular Expression Generator* - http://txt2re.com/[+http://txt2re.com/+]

*Unitools* - https://www.unicod.es/[+https://www.unicod.es/+]

*Virt Tools Blog Planet* -
http://planet.virt-tools.org/[+http://planet.virt-tools.org/+]

*The Whiteboard* -
https://rhinohide.wordpress.com/[+https://rhinohide.wordpress.com/+]

Interactivity
~~~~~~~~~~~~~

*Circuit Simulator* - http://lushprojects.com/circuitjs/[+http://lushprojects.com/circuitjs/+]

*EDA Playground* - http://www.edaplayground.com/[+http://www.edaplayground.com/+]

*Overtype* - http://uniqcode.com/typewriter/[+http://uniqcode.com/typewriter/+]

Repositories
~~~~~~~~~~~~

*negativo17* -
http://negativo17.org/nvidia-driver/[+http://negativo17.org/nvidia-driver/+]

*Source Codes in Fortran 90* - http://people.sc.fsu.edu/\~jburkardt/f_src/f_src.html[+http://people.sc.fsu.edu/~jburkardt/f_src/f_src.html+]

*****
Over 800 math and science related programs written in Fortran 90.
*****

== EVEN NEWER SECTION

#AAAA

3DEX
~~~~

3DEX is a Fortran/CXX package providing programs and functions
to perform fast Fourier-Bessel decomposition of 3D fields.
It can be applied to cosmological data or 3D data in spherical coordinates in
other scientific fields.
We present an equivalent formulation of the spherical Fourier-Bessel
decomposition that separates radial and tangential calculations. We propose
the use of the existing pixelisation scheme HEALPix for a rapid calculation of
the tangential modes. 3DEX (3D EXpansions) is a public code for fast spherical
Fourier-Bessel decomposition of 3D all-sky surveys that takes advantage of
HEALPix for the calculation of tangential modes.
3DEX can also be used in other disciplines, where 3D data are to be analysed
in spherical coordinates.

https://github.com/ixkael/3DEX[+https://github.com/ixkael/3DEX+]

http://arxiv.org/abs/1111.3591[+http://arxiv.org/abs/1111.3591+]

3DLDF
~~~~~

3DLDF is a GNU package for three-dimensional drawing with MetaPost output. It is written in Cxx using CWEB. It is intended, among other things, to provide a convenient way of including 3D graphics in TeX documents.

http://savannah.gnu.org/projects/3dldf[+http://savannah.gnu.org/projects/3dldf+]

https://www.gnu.org/software/3dldf/[+https://www.gnu.org/software/3dldf/+]

3DTiles
~~~~~~~

An open specification for streaming massive heterogeneous 3D geospatial datasets.
3D Tiles defines a spatial data structure and a set of tile formats designed for streaming and rendering 3D geospatial content such as Photogrammetry, 3D Buildings, BIM/CAD, Instanced Features, and Point Clouds.

Bringing techniques from graphics research, the movie industry, and the game industry to 3D geospatial, 3D Tiles define a spatial data structure and a set of tile formats designed for 3D and optimized for streaming and rendering. Tiles for 3D models use glTF.

The primary purpose of 3D Tiles is to improve streaming and rendering performance of massive heterogeneous datasets. The foundation of 3D Tiles is a spatial data structure that enables Hierarchical Level of Detail (HLOD) so only visible tiles are streamed - and only those tiles which are most important for a given 3D view. Tile payloads can be binary and context-aware compressed, e.g., using Open3DGC or oct-encoding.

https://github.com/AnalyticalGraphicsInc/3d-tiles[+https://github.com/AnalyticalGraphicsInc/3d-tiles+]

https://cesium.com/blog/2015/08/10/introducing-3d-tiles/[+https://cesium.com/blog/2015/08/10/introducing-3d-tiles/+]

accelerate
~~~~~~~~~~

Accelerate is a free, general-purpose, open-source library that simplifies the process of developing software that targets massively parallel architectures including multicore CPUs and GPUs. 

Accelerate is a language for array-based computations, designed to exploit massive parallelism. Programs in Accelerate are expressed in the form of collective operations on dense multi-dimensional arrays, which are online compiled and executed on a range of architectures including multicore CPUs and GPUs. 

Data.Array.Accelerate defines an embedded language of array computations for high-performance computing in Haskell. Computations on multi-dimensional, regular arrays are expressed in the form of parameterised collective operations (such as maps, reductions, and permutations). These computations are online-compiled and executed on a range of architectures.

https://github.com/AccelerateHS/accelerate[+https://github.com/AccelerateHS/accelerate+]

http://www.acceleratehs.org/[+http://www.acceleratehs.org/+]

accelerate-llvm
^^^^^^^^^^^^^^^

This package compiles Accelerate code to LLVM IR, and executes that code on multicore CPUs as well as NVIDIA GPUs. This avoids the need to go through nvcc or clang.

https://github.com/AccelerateHS/accelerate-llvm[+https://github.com/AccelerateHS/accelerate-llvm+]

Accumulo
~~~~~~~~

Apache Accumulo is a highly scalable sorted, distributed key-value store based on Google's Bigtable.[1] It is a system built on top of Apache Hadoop, Apache ZooKeeper, and Apache Thrift. Written in Java, Accumulo has cell-level access labels and server-side programming mechanisms.

Apache Accumulo extends the Bigtable data model, adding a new element to the key called Column Visibility. This element stores a logical combination of security labels that must be satisfied at query time in order for the key and value to be returned as part of a user request. This allows data of varying security requirements to be stored in the same table, and allows users to see only those keys and values for which they are authorized.

In addition to Cell-Level Security, Apache Accumulo provides a server-side programming mechanism called Iterators that allows users to perform additional processing at the Tablet Server. The range of operations that can be applied is equivalent to those that can be implemented within a MapReduce Combiner function, which produces an aggregate value for several key-value pairs. 

https://accumulo.apache.org/[+https://accumulo.apache.org/+]

https://github.com/calrissian/accumulo-recipes[+https://github.com/calrissian/accumulo-recipes+]

https://github.com/apache/accumulo-examples[+https://github.com/apache/accumulo-examples+]

Graphulo
^^^^^^^^

Graphulo is a Java library for the Apache Accumulo database delivering server-side sparse matrix math primitives that enable higher-level graph algorithms and analytics.

Graph primitives loosely follow the GraphBLAS spec. Example algorithms include Breadth First Search, finding a k-Truss subgraph, computing Jaccard coefficients, transforming by TF-IDF, and performing non-negative matrix factorization. We encourage developers to use Graphulo's primitives to build more algorithms and applications.

Graphulo's design resembles that of a stored procedure in classic relational databases. The client calls (directly or through delagate functions) the OneTable or TwoTable core functions, which create new tables to store results in Accumulo rather than gather results at the client. Both functions scan Accumulo with iterators that themselves open Scanners and BatchWriters, allowing reading from multiple tables and writing to multiple tables in one client call, as opposed to ordinary scans that read from a single table and send results back to the client.

https://github.com/Accla/graphulo/[+https://github.com/Accla/graphulo/+]

http://graphulo.mit.edu/[+http://graphulo.mit.edu/+]

http://graphblas.org/[+http://graphblas.org/+]

ADaM
~~~~

The Algorithm Development and Mining System (ADaM) developed by the Information Technology and Systems Center at the University of Alabama in Huntsville is used to apply data mining technologies to remotely-sensed and other scientific data. The mining and image processing toolkits consist of interoperable components that can be linked together in a variety of ways for application to diverse problem domains. ADaM has over 100 components that can be configured to create customized mining processes. Preprocessing and analysis utilities aid users in applying data mining to their specific problems. New components can easily be added to adapt the system to different science problems. 

The 4.0 release of ADaM is a significant architectural paradigm shift from previous versions. The latest version (4.0.2) (see release note) provides a solution that easily supports the integration of 3rd party algorithms and the reuse of ADaM components by other systems. ADaM 4.0.2 provides this support through the use of autonomous components in a distributed architecture. Each component is provided with a C, C++, or other application programming interface (API), an executable in support of generic scripting tools (e.g. Perl, Python, shell scripts) and eventually web service interfaces to support web and grid applications. ADaM 4.0.2 components are general purpose mining and image processing modules that can be easily reused for multiple solutions and disciplines. These components are well positioned to address the needs for distributed mining and image processing services in web and grid applications. 

*NOTE*: Only binaries are available for Linux, and they require the 32 bit version of libstdc++.so.5.

http://projects.itsc.uah.edu/datamining/adam/index.html[+http://projects.itsc.uah.edu/datamining/adam/index.html+]

http://projects.itsc.uah.edu/datamining/adam/documentation.html[+http://projects.itsc.uah.edu/datamining/adam/documentation.html+]

ADIOS2
~~~~~~

ADIOS2 is the latest implementation of the ADaptable Input Output System, ADIOS. This brand new architecture was designed to continue supporting the performance legacy of ADIOS, and extend its current capabilities to address current and future input/output (IO) challenges in the scientific data lifecycle through effective research and development (R&D) activities.

Exascale computing, Big Data, Internet of Things (IoT), Burst Buffers, High Bandwidth Memory (HBM), Remote Direct Memory Access (RDMA)…all these novel terms and technologies have one thing in common: Data Management at large scales has become more relevant when making informed decisions. The goal of ADIOS2 is to provide an adaptable, scalable, and unified framework to aid scientific applications in their data transfer needs going beyond file input/output (IO) storage.

We are attempting to create a framework to provide:

* Custom application management of massive data sets from generation, analysis, movement, to short-term and long-term storage.
* Self-describing data in binary-packed (BP) format for quick information extraction
* Ability to separate and extract relevant information from large data sets for better decision making
* Ability to make near real-time decisions based on in-transit or in-situ analytics
* Expand to other transport mechanisms with minimal overhead to the user: Wide-Area-Network (WAN), Remote Direct Memory Access (ibverbs, NVLink, etc.), Shared-Memory
* Exploit new memory hierarchy and scalability in novel hardware architectures: HBM, Burst Buffers, many-cores, etc.

The engines provided by ADIOS2 include:

* BP3 - writes and reads in ADIOS2 native binary pack (bp) format
* HDF5 - reads and writes HDF5 files
* Dataman - designed for data transfers over the wide area network
* SST - the Sustainable Staging Transport (SST) is an engine that allows direct connection of data producers and consumers via the ADIOS2 write/read APIs

https://adios2.readthedocs.io/en/latest/[+https://adios2.readthedocs.io/en/latest/+]

https://github.com/ornladios/ADIOS2[+https://github.com/ornladios/ADIOS2+]

ADLB
~~~~

ADLB is a software library designed to help rapidly build scalable parallel programs.
The name (pronounced adlib) is the acronym for Asynchronous Dynamic Load Balancing. However, ADLB does not achieve scalability solely by load balancing. It also includes some features that exploit work-stealing as well. Indeed, we sometimes use the phrase instantaneous load balancing via work-stealing to describe ADLB.

https://www.cs.mtsu.edu/\~rbutler/adlb/[+https://www.cs.mtsu.edu/~rbutler/adlb/+]

https://journals-sagepub-com/doi/full/10.1177/1094342017703448[+https://journals-sagepub-com/doi/full/10.1177/1094342017703448+]

AED
~~~

The AED/AED2 modelling code-base is a community-driven library of modules and algorithms for simulation of "aquatic ecodynamics" - water quality, aquatic biogeochemsitry, biotic habitat and aquatic ecosystem dynamics. Each module aims to be based on state-of-the-art science sourced from a wide variety of scientific literature, making the library one of the most advanced available to aquatic ecosystem modellers.

AED2 is suitable for a wide range water bodies, including lakes, reservoirs, wetlands, estuaries and coastal waters. It has been applied to many research and water engineering projects across the world. Users select water quality and ecosystem variables they wish to simulate and are able to customize link and dependencies between modules.

The AED modules cover most aquatic biogeochemical processes including nutrient cycling, oxygen dynamics, sediment/soil biogeochemistry, vegetation, etc. Each module can work alone or combined with other modules, depending on the complexity of targeted system.

The current AED2 software is divided into 2 separate code-libraries : the first is the core AED2 library (libaed2) and the second is the advanced modules which are bundled together as AED2+ (libaed2-plus).

Currently there are 8 core modules in AED2, and 12 advanced modules in AED2+. For routine water quality assessments of lakes or estuaries, AED2 will suit most applications. For advanced users and researchers seeking to extend their simulation abilities, then AED2+ includes numerous advanced options.

http://aed.see.uwa.edu.au/research/models/AED/index.html[+http://aed.see.uwa.edu.au/research/models/AED/index.html+]

http://aed.see.uwa.edu.au/research/models/AED/modules.html[+http://aed.see.uwa.edu.au/research/models/AED/modules.html+]

AeroBulk
~~~~~~~~

AeroBulk is a Fortran package/library which gathers state-of-the-art algorithms, turbulent closures, parameterizations, and thermodynamics (empirical) functions used to compute turbulent fluxes at the air-sea interface. These turbulent fluxes are wind stress, evaporation (latent heat flux) and sensible heat flux. These fluxes are estimated by means of so-called aerodynamic bulk formulae from the sea surface temperature, and atmospheric surface parameters: wind speed, air temperature and specific humidity.

In AeroBulk, 4 algorithms are available to compute the drag, sensible heat and moisture transfer coefficients (CD, CH and CE) used in the bulk formulaes:  COARE v3.0, COARE v3.5, ECMWF and NCAR.
In the COARE and ECMWF algorithms, a cool-skin/warm layer scheme is included and can be activated if the input sea-surface temperature is the bulk SST (usually measured a few tenths of meters below the surface). Activation of these cool-skin/warm layer schemes requires the surface downwelling shortwave and longwave radiative flux components to be provided. The NCAR algorithm is to be used only with the bulk SST.

Beside bulk algorithms AeroBulk also provides a variety of functions to accurately estimate relevant atmospheric variable such as density of air, different expressions of the humidity of air, viscosity of air, specific humidity at saturation, Monin-Obukhov length, wind gustiness, etc.

https://github.com/brodeau/aerobulk[+https://github.com/brodeau/aerobulk+]

https://brodeau.github.io/aerobulk/[+https://brodeau.github.io/aerobulk/+]

afivo
~~~~~

Afivo is a framework for simulations on adaptively refined quadtree and octree grids. Because Afivo has no built-in support for specific physics applications a user has to write his/her own numerical methods. Some key features/characteristics of the framework are:

* Adaptively refined quadtree and octree grids
* OpenMP parallelization
* Geometric multigrid routines
* Flexible handling of refinement and physical boundaries
* Written in modern Fortran
* Fully open source
* Application-independent
* Silo and VTK unstructured output

Afivo can be used to simulate physical systems exhibiting multiscale features, e.g. features that appear at different spatial and temporal scales. Numerical simulations of such systems benefit from Afivo's adaptive mesh refinement (AMR), especially if a high-resolution mesh is only required in a small fraction of the total volume.

http://cwimd.nl/other_files/afivo_doc/html/index.html[+http://cwimd.nl/other_files/afivo_doc/html/index.html+]

https://gitlab.com/MD-CWI-NL/afivo[+https://gitlab.com/MD-CWI-NL/afivo+]

https://www.sciencedirect.com/science/article/pii/S0010465518302261[+https://www.sciencedirect.com/science/article/pii/S0010465518302261+]

agate
~~~~~

A Python data analysis library that is optimized for humans instead of machines. It is an alternative to numpy and pandas that solves real-world problems with readable code.
The features include:

* A readable and user-friendly API.
* A complete set of SQL-like operations.
* Unicode support everywhere.
* Decimal precision everywhere.
* Exhaustive user documentation.
* Pluggable extensions that add SQL integration, Excel support, and more.
* Designed with iPython, Jupyter and atom/hydrogen in mind.
* Pure Python. No C dependencies to compile.
* Exhaustive test coverage.

https://agate.readthedocs.io[+https://agate.readthedocs.io+]

AGRIF
~~~~~

AGRIF is an adaptive mesh refinement package written in Fortran 90 for the integration of full adaptive mesh refinement features within an existing multidimensional finite difference model written in the Fortran language.

It is possible to use some staggered grids with different space and time refinement factors wich is defined for each space direction and each fine grid. The software deals simultaneously with fixed and moving grids.

http://agrif.imag.fr/[+http://agrif.imag.fr/+]

http://forge.ipsl.jussieu.fr/nemo/wiki/Users/SetupNewConfiguration/AGRIF-nesting-tool[+http://forge.ipsl.jussieu.fr/nemo/wiki/Users/SetupNewConfiguration/AGRIF-nesting-tool+]

aircrack-ng
~~~~~~~~~~~

Aircrack-ng is a network software suite consisting of a detector, packet sniffer, WEP and WPA/WPA2-PSK cracker and analysis tool for 802.11 wireless LANs. It works with any wireless network interface controller whose driver supports raw monitoring mode and can sniff 802.11a, 802.11b and 802.11g traffic. The program runs under Linux, FreeBSD, OS X, OpenBSD, and Windows; the Linux version is packaged for OpenWrt and has also been ported to the Android, Zaurus PDA and Maemo platforms; and a proof of concept port has been made to the iPhone.

https://en.wikipedia.org/wiki/Aircrack-ng[+https://en.wikipedia.org/wiki/Aircrack-ng+]

http://www.aircrack-ng.org/[+http://www.aircrack-ng.org/+]

Akaros
~~~~~~

Akaros is an open source, GPL-licensed operating system for manycore architectures. Its goal is to provide better support for parallel and high-performance applications in the datacenter. Unlike traditional OSs, which limit access to certain resources (such as cores), Akaros provides native support for application-directed resource management and 100% isolation from other jobs running on the system.

Although not yet integrated as such, it is designed to operate as a low-level node OS with a higher-level Cluster OS, such as Mesos, governing how resources are shared amongst applications running on each node. Its system call API and "Many Core Process" abstraction better match the requirements of a Cluster OS, eliminating many of the obstacles faced by other systems when trying to isolate simultaneously running processes. Moreover, Akaros’s resource provisioning interfaces allow for node-local decisions to be made that enforce the resource allocations set up by a Cluster OS. This can be used to simplify global allocation decisions, reduce network communication, and ultimately promote more efficient sharing of resources. There is limited support for such functionality on existing operating systems.

Akaros is still very young, but preliminary results show that processes running on Akaros have an order of magnitude less noise than on Linux, as well as fewer periodic signals, resulting in better CPU isolation. Additionally, its non-traditional threading model has been shown to outperform the Linux NPTL across a number of representative application workloads. This includes a 3.4x faster thread context switch time, competitive performance for the NAS parallel benchmark suite, and a 6% increase in throughput over nginx for a simple thread-based webserver we wrote. We are actively working on expanding Akaros's capabilities even further.

http://akaros.cs.berkeley.edu/akaros-web/overview.php[+http://akaros.cs.berkeley.edu/akaros-web/overview.php+]

https://github.com/brho/akaros[+https://github.com/brho/akaros+]

Aladin
~~~~~~

Aladin Desktop is the main application of the Aladin Sky Atlas suite. This application allows the user to visualize and manipulate digitized astronomical images or full surveys, superimpose entries from astronomical catalogues or databases, and interactively access related data and information from the Simbad database, the VizieR service and other archives for all known astronomical objects in the field.

Created in 1999 by the CDS, Aladin Desktop has become a widely-used VO tool capable of addressing challenges such as locating data of interest, accessing and exploring distributed datasets, visualizing multi-wavelength data. Compliance with existing or emerging VO standards, interconnection with other visualisation or analysis tools, ability to easily compare heterogeneous data are key topics allowing Aladin to be a powerful data exploration and integration tool as well as a science enabler. 

Main features:

* 000+ data collections(DSS, SDSS, PanSTARRS, Skymapper, Gaia, Simbad, NED, VizieR, ...); 
* Zoom; Pan; Rotate; Overlays; Multi-views; 
* Many projections (Sin, Tan, Aitoff, Mollweide, etc); 
* Any coordinate systems (FK4, FK5, ICRS, GAL, SGAL, ECL); 
* No image size limit; 
* Million source overlays, 
* Most of astronomical formats (images: HiPS, FITS, PDS, HEALPix map, JPEG, PNG; cubes (HiPS, FITS); 
* tables: HiPS, FITS, VOTable, S-extractor, IPAC TBL, ASCII, etc; 
* regions: XML, STC, DS9, IDL); 
* Powerful tool boxes (images: color map, contours, crop, re-encode, color composition, pixel computation, resampling, astrometrical calibration, mosaic, photometric measurements, etc; 
* Catalogs: filter, split, merge, x-match, scatter plot, etc); 
* VO standards: JSAMP, TAP, CS, SSA, SIA, etc; 
* Extendable: plugins, VOApp interface; Fully scriptable; Multi-language

https://aladin.u-strasbg.fr/AladinDesktop/[+https://aladin.u-strasbg.fr/AladinDesktop/+]

https://aladin.u-strasbg.fr/aladin.gml[+https://aladin.u-strasbg.fr/aladin.gml+]

Albany
~~~~~~

Albany is an implicit, unstructured grid, finite element code for the solution and analysis of multiphysics problems. The Albany repository on the GitHub site contains hundreds of regression tests and examples that demonstrate the code's capabilities on a wide variety of problems including fluid mechanics, solid mechanics (elasticity and plasticity), ice-sheet flow, quantum device modeling, and many other applications.

Albany supports the solution of very large problems (those over 2.1 billion degrees of freedom) using MPI, and also demonstrates the use of the Kokkos hardware abstraction package to support generic manycore computing across a variety of platforms - MPI + [threads, OpenMP, Cuda, Intel MIC].

In addition to supporting embedded sensitivity analysis and uncertainty quantification, Albany can be tightly-coupled to Dakota using the Trilinos TriKota package.

https://github.com/SNLComputation/Albany[+https://github.com/SNLComputation/Albany+]

http://snlcomputation.github.io/Albany/[+http://snlcomputation.github.io/Albany/+]

https://fastmath-scidac.llnl.gov/software-catalog.html[+https://fastmath-scidac.llnl.gov/software-catalog.html+]

ALE
~~~

The Arcade Learning Environment (ALE) is a simple object-oriented framework that allows researchers and hobbyists to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design. This video depicts over 50 games currently supported in the ALE.  The features include:

* Object-oriented framework with support to add agents and games.
* Emulation core uncoupled from rendering and sound generation modules for fast emulation with minimal library dependencies.
* Automatic extraction of game score and end-of-game signal for more than 50 Atari 2600 games.
* Multi-platform code (compiled and tested under OS X and several Linux distributions, with Cygwin support).
* Communication between agents and emulation core can be accomplished through pipes, allowing for cross-language development (sample Java code included).
* Python development is supported through ctypes.
* Agents programmed in Cxx have access to all features in the ALE.
* Visualization tools.

https://github.com/mgbellemare/Arcade-Learning-Environment[+https://github.com/mgbellemare/Arcade-Learning-Environment+]

Algoim
~~~~~~

Algoim is a collection of high-order accurate numerical methods and Cxx algorithms for working with implicitly defined geometry and level set methods. Motivated by multi-phase multi-physics applications, particularly those with evolving dynamic interfaces, these algorithms target core, fundamental techniques in level set methods. They have been designed with a view to standard finite difference implementations as well as more advanced finite element and discontinuous Galerkin implementations, multi-threading and massively parallel MPI computation. The collection includes:

* High-order accurate quadrature algorithms for implicitly defined domains in hyperrectangles, for example in computing integrals over curved surfaces or domains.
* High-order accurate closest point calculations to implicitly defined surfaces (Coming soon…)
* k-d trees optimised for codimension-one point clouds (Coming soon…)
* Accurate level set reinitialisation and extension velocity schemes (Coming soon…)
* Voronoi implicit interface methods for multi-phase interconnected interface dynamics 

An implicitly defined domain is either a volumetric region or codimension-one surface whose shape is characterised implicitly by an isosurface of a continuous scalar function. A variety of applications involving implicitly defined geometry require the evaluation of integrals over such domains, including level set methods for propagating interfaces in computational physics, embedded boundary methods for solving partial differential equations on curved domains, and in treating jump conditions and singular source terms in weak formulations. A specific example is that of implicit mesh discontinuous Galerkin methods, which have been developed to facilitate high-order accurate modelling of interfacial fluid dynamics.

https://algoim.github.io/[+https://algoim.github.io/+]

https://fastmath-scidac.llnl.gov/software-catalog.html[+https://fastmath-scidac.llnl.gov/software-catalog.html+]

ALGOL
~~~~~

ALGOL (/ˈælɡɒl, -ɡɔːl/; short for "Algorithmic Language")[1] is a family of imperative computer programming languages, originally developed in the mid-1950s, which greatly influenced many other languages and was the standard method for algorithm description used by the ACM in textbooks and academic sources for more than thirty years.[2]

In the sense that the syntax of most modern languages is "Algol-like",[3] it was arguably the most influential of the four high-level programming languages among which it was roughly contemporary: FORTRAN, Lisp, and COBOL.[4] It was designed to avoid some of the perceived problems with FORTRAN and eventually gave rise to many other programming languages, including PL/I, Simula, BCPL, B, Pascal, and C.

ALGOL introduced code blocks and the begin…end pairs for delimiting them. It was also the first language implementing nested function definitions with lexical scope. Moreover, it was the first programming language which gave detailed attention to formal language definition and through the Algol 60 Report introduced Backus–Naur form, a principal formal grammar notation for language design.

There were three major specifications, named after the year they were first published:

    ALGOL 58 – originally proposed to be called IAL, for International Algebraic Language.
    ALGOL 60 – first implemented as X1 ALGOL 60 in mid-1960. Revised 1963.[5][6]
    ALGOL 68 – introduced new elements including flexible arrays, slices, parallelism, operator identification. Revised 1973.[7]

ALGOL 68 is substantially different from ALGOL 60 and was not well received, so that in general "Algol" means ALGOL 60 and dialects thereof. 

https://en.wikipedia.org/wiki/ALGOL[+https://en.wikipedia.org/wiki/ALGOL+]

http://www.algol68.org/[+http://www.algol68.org/+]

http://www.softwarepreservation.org/projects/ALGOL/[+http://www.softwarepreservation.org/projects/ALGOL/+]

http://algol68.sourceforge.net/[+http://algol68.sourceforge.net/+]

algol-60-compiler
^^^^^^^^^^^^^^^^^

In the period 2002, 2003 I wrote in spare time, as a hobby project,
a simple Algol 60 to C translator. The rationale was to show the
semantics of (some of the) Algol 60 constructs in terms of C expressions. 

The objective was therefore to translate Algol 60 constructs into
C equivalents  rather than to some "intermediate stack machine".

Since it was a fun project, the compiler was named "jff-algol", i.e.
Just-For-Fun-Algol.

Recently, when cleaning up some old archives on disks, I found 
the sources of the compiler and - with a very few modifications -
the software compiled properly and just ran.

The directory contains an automake/configure combination to create
two executables and a small library.

https://github.com/JvanKatwijk/algol-60-compiler[+https://github.com/JvanKatwijk/algol-60-compiler+]

Algol 68 Genie
^^^^^^^^^^^^^^

The development of Algol played an important role in establishing computer science as an academic discipline. The Algol 68 Genie project preserves and promotes Algol 68 out of educational as well as scientific-historical interest, by making available a recent checkout compiler-interpreter written from scratch by Marcel van der Veer, together with extensive documentation for both the language and this new implementation. Algol 68 Genie is free software distributed under the GNU General Public License; it is a fast compiler-interpreter which ranks among the most complete implementations of the language.

https://jmvdveer.home.xs4all.nl/en.algol-68-genie.html[+https://jmvdveer.home.xs4all.nl/en.algol-68-genie.html+]

https://jmvdveer.home.xs4all.nl/en.download.learning-algol-68-genie-283.html[+https://jmvdveer.home.xs4all.nl/en.download.learning-algol-68-genie-283.html+]

Alida
~~~~~

Alida defines a concept for designing libraries and toolkits in data analysis. It supports and simplifies integrated algorithm development by inherently joining algorithm implementation, automatic analysis process documentation and fully generic generation of user interfaces. In Alida each data analysis or manipulation action is realized in terms of an operator that acts on given data to produce desired output data. As all operators implement a common interface definition, their input and output parameters are accessible in a standardized manner, and they can also be invoked in a predefined way. Alida's concept is well-suited to ease algorithm development and their application to real-world problems by non-expert users. Due to the operator interface definition and the unified handling of operators it is for example possible to automatically generate user interfaces for operators, i.e. graphical frontends or commandline interfaces. 

The Java implementation of the Alida concept is in a quite mature state. It provides a framework for implementing and running operators. It also includes automatic process documentation and automatically generated command line and graphical user interfaces. Calls to operators not only produce data analysis results, but are at the same time registered within the framework together with all input and output objects as well as parameters settings of the various operators. These data acquired during an analysis process and the order of operator calls form a directed graph datastructure containing all relevant information for later reconstruction or verification of the analysis procedure. The Java implementation of Alida allows to make the directed graph datastructure explicit in terms of XML representations which can be visually explored with appropriate graphical frontends like Chipory, or might be stored in data bases for archival purposes. 

http://www2.informatik.uni-halle.de/agprbio/alida/index.php/Main_Page[+http://www2.informatik.uni-halle.de/agprbio/alida/index.php/Main_Page+]

https://github.com/alida-hub/alida[+https://github.com/alida-hub/alida+]

https://zenodo.org/record/47586#.XHQ_EK3SRUR[+https://zenodo.org/record/47586#.XHQ_EK3SRUR+]

Alpine Linux
~~~~~~~~~~~~

Alpine Linux is a Linux distribution based on musl and BusyBox, primarily designed for security, simplicity, and resource efficiency.[2][3][4][5][6] It uses a hardened kernel and compiles all user space binaries as position-independent executables with stack-smashing protection.[7]

Because of its small size, it's heavily used in containers providing quick boot up times.[8]

A fork of the distribution, postmarketOS, is designed to run on mobile devices. 

https://en.wikipedia.org/wiki/Alpine_Linux[+https://en.wikipedia.org/wiki/Alpine_Linux+]

Altair
~~~~~~

Altair is a declarative statistical visualization library for Python, based on Vega and Vega-Lite, and the source is available on GitHub.

With Altair, you can spend more time understanding your data and its meaning. Altair’s API is simple, friendly and consistent and built on top of the powerful Vega-Lite visualization grammar. This elegant simplicity produces beautiful and effective visualizations with a minimal amount of code.

The key idea is that you are declaring links between data columns and visual encoding channels, such as the x-axis, y-axis, color, etc. The rest of the plot details are handled automatically. Building on this declarative plotting idea, a surprising range of simple to sophisticated plots and visualizations can be created using a relatively concise grammar.

https://altair-viz.github.io/[+https://altair-viz.github.io/+]

https://github.com/altair-viz/altair[+https://github.com/altair-viz/altair+]

http://joss.theoj.org/papers/cd11f880b3f81bf5b5a225007212dc8b[+http://joss.theoj.org/papers/cd11f880b3f81bf5b5a225007212dc8b+]

AMG2013
~~~~~~~

AMG2013 is a parallel algebraic multigrid solver for linear systems arising from problems on unstructured grids.  It has been derived directly from the BoomerAMG solver in the hypre library, a large linear solver library that is being developed in the Center for Applied Scientific Computing (CASC) at LLNL. The driver provided in the benchmark can build various test problems. The default problem is a Laplace type problem on an unstructured domain with various jumps and an anisotropy in one part.

AMG2013 is written in ISO-C.  It is an SPMD code which uses MPI as well as OpenMP.  Parallelism is achieved by data decomposition.  The driver provided with AMG2013 achieves this decomposition by simply subdividing the grid into logical P x Q x R (in 3D) chunks of equal size. The benchmark was designed to test parallel weak scaling efficiency.

AMG2013 is a highly synchronous code.  The communications and computations patterns exhibit the surface-to-volume relationship common to many parallel scientific codes.  Hence, parallel efficiency is largely determined by the size of the data “chunks” mentioned above, and the speed of communications and computations on the machine.  AMG2013 is also memory-access bound, doing only about 1-2 computations per memory access, so memory-access speeds will also have a large impact on performance.

https://computation.llnl.gov/projects/co-design/amg2013[+https://computation.llnl.gov/projects/co-design/amg2013+]

AMReX
~~~~~

AMReX is a publicly available software framework designed for building massively parallel block- structured adaptive mesh refinement (AMR) applications.  The features include:

* Cxx and Fortran interfaces
* Support for cell-centered, face-centered, edge-centered, and nodal data
* Support for hyperbolic, parabolic, and elliptic solves on hierarchical adaptive grid structure
* Optional subcycling in time for time-dependent PDEs
* Support for particles
* Embedded boundary description of irregular geometry
* Parallelization via flat MPI, OpenMP, hybrid MPI/OpenMP, or MPI/MPI
* Parallel I/O
* Plotfile format supported by Amrvis, yt, VisIt, and ParaView.
* Built-in profiling tools

Block-structured AMR provides a way to exploit varying resolution requirements in space and time by focusing computational resources in spatiotemporal regions of interest. 

AMReX provides data structures and iterators for performing data-parallel particle simulations. Our approach is particularly suited to particles that interact with data defined on a (possibly adaptive) block-structured hierarchy of meshes. Example applications include Particle-in-Cell (PIC) simulations, Lagrangian tracers, or particles that exert drag forces onto a fluid, such as in multi-phase flow calculations.

For computations with complex geometries, AMReX provides data structures and algorithms to employ an embedded boundary (EB) approach to PDE discretizations. In this approach, the underlying computational mesh is uniform and block-structured, but the boundary of the irregular-shaped computational domain conceptually cuts through this mesh.

https://amrex-codes.github.io/[+https://amrex-codes.github.io/+]

https://fastmath-scidac.llnl.gov/software-catalog.html[+https://fastmath-scidac.llnl.gov/software-catalog.html+]

AMReX-Astro
^^^^^^^^^^^

An astrophysical hydrodynamics code suite built on the AMReX library.
AMReX Astrophysics codes can model subsonic convection and compressible flows in stars, explosive burning in stellar environments, and large scale structure on cosmological scales. They share a common design and an open development model.

The codes include:

* *CASTRO* - Compressible (radiation) hydrodynamics with full self gravity, generation equations of state, reaction networks, and a variety of integration methods.

* *MAESTRoX* - Low-Mach number hydrodynamics for stellar hydrostatic flows. Maestro supports a general equation of state, arbitrary reaction networks, thermal diffusion, and more.

* *NYX* - A cosmological hydrodynamics / N-body code.

https://amrex-astro.github.io/[+https://amrex-astro.github.io/+]

https://github.com/starkiller-astro/Microphysics[+https://github.com/starkiller-astro/Microphysics+]

AMReX-Combustion
^^^^^^^^^^^^^^^^

A suite of adaptive mesh hydrodynamics simulation codes for reacting flows.
The codes include:

* *PeleM* - an adaptive-mesh low Mach number hydrodynamics code for reacting flows
* *PeleC* - an adaptive-mesh compressible hydrodynamics code for reacting flows
* *PelePhysics* - a  collection of physics databases and implementation code for use with the Pele suite of of code
* *PeleAnalysis* - a collection of processing tools for reacting flow simulations with AMReX-based CFD tools

https://amrex-combustion.github.io/[+https://amrex-combustion.github.io/+]

IAMR
^^^^

IAMR is a parallel, adaptive mesh refinement (AMR) code that solves the variable-density incompressible Navier-Stokes equations.

https://amrex-codes.github.io/IAMR/[+https://amrex-codes.github.io/IAMR/+]

MFiX-Exa
^^^^^^^^

MFiX-Exa is a new massively parallel code for computing multiphase flow in which solid particles interact with the gas surrounding them.

https://amrex-codes.github.io/MFIX-Exa/[+https://amrex-codes.github.io/MFIX-Exa/+]

WarpX
^^^^^

WarpX is an advanced electromagnetic Particle-In-Cell code.

https://ecp-warpx.github.io/[+https://ecp-warpx.github.io/+]

https://github.com/ECP-WarpX[+https://github.com/ECP-WarpX+]

Android
~~~~~~~

Blah.

Anbox
^^^^^

Anbox puts the Android operating system into a container, abstracts hardware access and integrates core system services into a GNU/Linux system. Every Android application will be integrated with your operating system like any other native application. 

https://anbox.io/[+https://anbox.io/+]

https://postmarketos.org/faq.html[+https://postmarketos.org/faq.html+]

UserLand
^^^^^^^^

The easiest way to run a Linux distribution or application on Android.
With UserLand you can run full linux distros or specific applications on top of Android.
You install and uninstall like a regular app, with no root required.

https://github.com/CypherpunkArmory/UserLAnd[+https://github.com/CypherpunkArmory/UserLAnd+]

ANN
~~~

ANN is a library written in Cxx, which supports data structures and algorithms for both exact and approximate nearest neighbor searching in arbitrarily high dimensions.

In the nearest neighbor problem a set of data points in d-dimensional space is given. These points are preprocessed into a data structure, so that given any query point q, the nearest or generally k nearest points of P to q can be reported efficiently. The distance between two points can be defined in many ways. ANN assumes that distances are measured using any class of distance functions called Minkowski metrics. These include the well known Euclidean distance, Manhattan distance, and max distance.

Based on our own experience, ANN performs quite efficiently for point sets ranging in size from thousands to hundreds of thousands, and in dimensions as high as 20. (For applications in significantly higher dimensions, the results are rather spotty, but you might try it anyway.)

The library implements a number of different data structures, based on kd-trees and box-decomposition trees, and employs a couple of different search strategies.

The library also comes with test programs for measuring the quality of performance of ANN on any particular data sets, as well as programs for visualizing the structure of the geometric data structures. 

http://www.cs.umd.edu/\~mount/ANN/[+http://www.cs.umd.edu/~mount/ANN/+]

http://mrzv.org/software/pyANN/[+http://mrzv.org/software/pyANN/+]

ANUGA
~~~~~

ANUGA is a Free & Open Source Software (FOSS) package capable of modelling the impact of hydrological disasters such as dam breaks, riverine flooding, storm-surge or tsunamis.

ANUGA is based on the Shallow Water Wave Equation discretised to unstructured triangular meshes using a finite-volumes numerical scheme. A major capability of ANUGA is that it can model the process of wetting and drying as water enters and leaves an area. This means that it is suitable for simulating water flow onto a beach or dry land and around structures such as buildings. ANUGA is also capable of modelling difficult flows involving shock waves and rapidly changing flow speed regimes (transitions from sub critical to super critical flows). 

https://anuga.anu.edu.au/[+https://anuga.anu.edu.au/+]

https://github.com/GeoscienceAustralia/anuga_core[+https://github.com/GeoscienceAustralia/anuga_core+]

https://github.com/stoiver/anuga-clinic-2018[+https://github.com/stoiver/anuga-clinic-2018+]

aospy
~~~~~

An open source Python package for automating your computations that use gridded climate and weather data (namely data stored as netCDF files) and the management of the results of those computations.

aospy enables firing off multiple calculations in parallel using the permutation of an arbitrary number of climate models, simulations, variables to be computed, date ranges, sub-annual-sampling, and many other parameters. In other words, it is possible using aospy to submit and execute all calculations for a particular project (e.g. paper, class project, or thesis chapter) with a single command!

The results get saved in a highly organized directory tree as netCDF files, making it easy to subsequently find and use the data (e.g. for plotting) and preventing “orphan” files with vague filenames and insufficient metadata to remember what they are and/or how they were computed.

The eventual goal is for aospy to become the community standard for gridded climate data analysis and, in so doing, accelerate progress in climate science and make the results of climate research more easily reproducible and shareable.

https://aospy.readthedocs.io/en/stable/[+https://aospy.readthedocs.io/en/stable/+]

https://github.com/spencerahill/aospy[+https://github.com/spencerahill/aospy+]

APL
~~~

http://www.hakank.org/k/[+http://www.hakank.org/k/+]

https://en.wikipedia.org/wiki/K_%28programming_language%29[+https://en.wikipedia.org/wiki/K_%28programming_language%29+]

https://news.ycombinator.com/item?id=15907840[+https://news.ycombinator.com/item?id=15907840+]

A+
^^

A+ is a powerful and efficient programming language. It is freely available under the GNU General Public License. It embodies a rich set of functions and operators, a modern graphical user interface with many widgets and automatic synchronization of widgets and variables, asynchronous execution of functions associated with variables and events, dynamic loading of user compiled subroutines, and many other features. Execution is by a rather efficient interpreter. A+ was created at Morgan Stanley. Primarily used in a computationally-intensive business environment, many critical applications written in A+ have withstood the demands of real world developers over many years. Written in an interpreted language, A+ applications tend to be portable.

http://www.aplusdev.org/index.html[+http://www.aplusdev.org/index.html+]

APLX
^^^^

http://www.dyalog.com/aplx.htm[+http://www.dyalog.com/aplx.htm+]

co-dfns
^^^^^^^

The Co-dfns project aims to provide a high-performance, high-reliability compiler for a parallel extension of the Dyalog dfns programming language. The dfns language is a functionally oriented, lexically scoped dialect of APL. The Co-dfns language extends the dfns language to include explicit task parallelism with implicit structures for synchronization and determinism. The language is designed to enable rigorous formal analysis of programs to aid in compiler optimization and programmer productivity, as well as in the general reliability of the code itself.

Our mission is to deliver scalable APL programming to information and domain experts across many fields, expanding the scope and capabilities of what you can effectively accomplish with APL.

https://github.com/Co-dfns/Co-dfns[+https://github.com/Co-dfns/Co-dfns+]

GNU APL
^^^^^^^

GNU APL is a free interpreter for the programming language APL.
The APL interpreter is an (almost) complete implementation of ISO standard 13751 aka. Programming Language APL, Extended.
The APL interpreter has implemented:

* nested arrays and related functions
* complex numbers, and
* a shared variable interface 

In addition, GNU APL can be scripted.

https://www.gnu.org/software/apl/[+https://www.gnu.org/software/apl/+]

https://www.gnu.org/software/apl/apl.html[+https://www.gnu.org/software/apl/apl.html+]

http://savannah.gnu.org/projects/apl[+http://savannah.gnu.org/projects/apl+]

iv
^^

An APL interpreter and stream processor.
This includes:

* *apl* - an extendable and embeddable APL interpreter written in Go
* *aplextra* - extension packages requiring 3rd party software

https://github.com/ktye/iv[+https://github.com/ktye/iv+]

J Language
^^^^^^^^^^

J (J language) is a high-level, general-purpose, high-performance programming language. J is portable and runs on 32/64-bit Windows/Linux/Mac as well as iOS, Android, and other platforms. J source (required only if Jsoftware binaries don't meet your requirements) is available under both commercial and GPL 3 license. J systems can be installed and distributed for free.

Jd (J database) is a high-performance, columnar RDBMS (relational database management system) implemented in J.

http://www.jsoftware.com/[+http://www.jsoftware.com/+]

Joy
^^^

The language Joy is a purely functional programming language that was created by Manfred von Thun. Whereas all other functional programming languages are based on the application of functions to arguments, Joy is based on the composition of functions. All such functions take a stack as argument and produce a stack as value. Consequently much of Joy looks like ordinary postfix notation. However, in Joy a function can consume any number of parameters from the stack and leave any number of results on the stack. The concatenation of appropriate programs denotes the composition of the functions which the programs denote. One of the datatypes of Joy is that of quoted programs, of which lists are a special case.

Some functions expect quoted programs on top of the stack and execute them in many different ways, effectively by dequoting. So, where other functional languages use abstraction and application, Joy uses quotation and combinators -- functions which perform dequotation. As a result, there are no named formal parameters, no substitution of actual for formal parameters, and no environment of name-value pairs. Combinators in Joy behave much like functionals or higher order functions in other languages, they minimise the need for recursive and non-recursive definitions. Because there is no need for an environment, Joy has an exceptionally simple algebra, and its programs are easily manipulated by hand and by other programs. Many programs first construct another program which is then executed.

https://www.latrobe.edu.au/humanities/research/research-projects/past-projects/joy-programming-language[+https://www.latrobe.edu.au/humanities/research/research-projects/past-projects/joy-programming-language+]

Kerf
^^^^

Kerf is a columnar tick database and time-series language for Linux/OSX/BSD/iOS/Android. It is written in C and natively speaks JSON and SQL. Kerf can be used for trading platforms, feedhandlers, low-latency networking, high-volume analysis of realtime and historical data, logfile processing, and more.

https://github.com/kevinlawler/kerf/[+https://github.com/kevinlawler/kerf/+]

https://getkerf.wordpress.com/[+https://getkerf.wordpress.com/+]

Kona
^^^^

Kona is the open-source implementation of the K programming language. K is a synthesis of APL and LISP. Although many of the capabilities come from APL, the fundamental data construct is quite different. In APL the construct is a multi-dimensional matrix-like array, where the dimension of the array can range from 0 to some maximum (often 9). In K, like LISP, the fundamental data construct is a list. Also, like LISP, the K language is ASCII-based, so you don't need a special keyboard.

For many people, K was the preferred APL dialect. When it was available, it tended to be popular with investment bankers, the performance obsessed, and analysts dealing with lots of data. It is a demanding language.

K was originally designed by Arthur Whitney and Kx Systems. Praise for K should be taken to refer to Kx's K. Kx sells a popular database called KDB+. People can and do create networked trading platforms in hours. If your business needs production support, you can evaluate KDB+ prior to purchasing from Kx, or possibly speak with Kx consulting partner First Derivatives. The 32-bit version of KDB+ is available for free.

Kx's KDB+ uses the Q language, and is built on top of K4. Kx used to sell a database called KDB, which used the KSQL language, and was built on top of K3. Earlier, Kx sold K2 as its primary product. Before K2, UBS had a 5-year exclusive license to K1. To the confusion of all, these terms are used interchangeably. Kx's K3, K2 and K1 are basically no longer available. While you get K4 with KDB+, K4 is proprietary to Kx and no documentation is available. Kona is a reimplementation that targets K3 but includes features inferred from K4 or implemented elsewhere. Kona is unaffiliated with Kx.

https://github.com/kevinlawler/kona

q2c
^^^

Compile any Q or C-DSL function or set of functions into a C shared library to be loaded into a Q process. Provide an easy way to create proxy functions to any native C library. Obfuscate Q code.

https://github.com/quintanar401/q2c[+https://github.com/quintanar401/q2c+]

QNial
^^^^^

The Nial language was developed by Mike Jenkins and Trenchard More in a collaborative research project supported by Queen's University at Kingston and IBM Cambridge Scientific Center from 1979 to 1982.
The language combines Trenchard More's theory of nested arrays with Mike's ideas on how to build an interactive programming system. The goal was to combine the strengths of APL array-based programming with implementation concepts borrowed from LISP, structured programming ideas from Algol, and functional programming concepts from FP. THe interpreter, originally developed for Unix, was small enough to run on the then newly released IBM PC and portable enough to execute on IBM mainframes computers.

Nial Systems limited licensed the interpreter from Queen's Uinversity and marketed it widely. Mike Jenkins continued to refine both the language and its implementation. In 2006 Mike released Version 6.3 as an open source project to encourage continued development of Nial.

In 2014 Mike started working with John Gibbons to develop a 64-bit version and to add capabilities that John needed for his work. The decision was made to target the open source for Unix-based platforms and release it on GitHub.

This version of Q'Nial is intended for people who want to integrate the functionality of Nial into projects that can take advantage of its powerful array computations for numeric, symbolic, or data analysis problems. 

https://github.com/danlm/QNial7[+https://github.com/danlm/QNial7+]

https://github.com/PlanetAPL/nial[+https://github.com/PlanetAPL/nial+]

https://tangentstorm.github.io/nial/intro.ndf.html[+https://tangentstorm.github.io/nial/intro.ndf.html+]

http://www.billhowell.ca/QNial/[+http://www.billhowell.ca/QNial/+]

https://gibbonsja.files.wordpress.com/2017/09/v6languagedefinition.pdf[+https://gibbonsja.files.wordpress.com/2017/09/v6languagedefinition.pdf+]

https://www.reddit.com/r/apljk/[+https://www.reddit.com/r/apljk/+]

Remora
^^^^^^

Dependently-typed language with Iverson-style implicit lifting.

https://github.com/jrslepak/Remora[+https://github.com/jrslepak/Remora+]

Arb
~~~

Arb is a C library for rigorous real and complex arithmetic with arbitrary precision. Arb tracks numerical errors automatically using ball arithmetic, a form of interval arithmetic based on a midpoint-radius representation. On top of this, Arb provides a wide range of mathematical functionality, including polynomials, power series, matrices, integration, root-finding, and many transcendental functions. Arb is designed with efficiency as a primary goal, and is usually competitive with or faster than other arbitrary-precision packages. The code is thread-safe, portable, and extensively tested.

Besides basic arithmetic, Arb allows working with univariate polynomials, truncated power series, and matrices over both real and complex numbers.
Basic linear algebra is supported, including matrix multiplication, determinant, inverse, nonsingular solving, matrix exponential, and computation of eigenvalues and eigenvectors.
Support for polynomials and power series is quite extensive, including methods for composition, reversion, product trees, multipoint evaluation and interpolation, complex root isolation, and transcendental functions of power series.
Other features include root isolation for real functions, rigorous numerical integration of complex functions, and discrete Fourier transforms (DFTs).

Arb can compute a wide range of transcendental and special functions, including the gamma function, polygamma functions, Riemann zeta and Hurwitz zeta function, Dirichlet L-functions, polylogarithm, error function, Gauss hypergeometric function 2F1, confluent hypergeometric functions, Bessel functions, Airy functions, Legendre functions and other orthogonal polynomials, exponential and trigonometric integrals, incomplete gamma and beta functions, Jacobi theta functions, modular functions, Weierstrass elliptic functions, complete and incomplete elliptic integrals, arithmetic-geometric mean, Bernoulli numbers, partition function, Barnes G-function, Lambert W function.

http://arblib.org/[+http://arblib.org/+]

https://arxiv.org/abs/1611.02831[+https://arxiv.org/abs/1611.02831+]

https://github.com/fredrik-johansson/arb/[+https://github.com/fredrik-johansson/arb/+]

arduino-cli
~~~~~~~~~~~

arduino-cli is an all-in-one solution that provides builder, boards/library manager, uploader, discovery and many other tools needed to use any Arduino compatible board and platforms.

This software is currently in alpha state: new features will be added and some may be changed.

It will be soon used as a building block in the Arduino IDE and Arduino Create.

https://github.com/arduino/arduino-cli[+https://github.com/arduino/arduino-cli+]

Argobots
~~~~~~~~

A lightweight runtime system that supports integrated computation and data movement with massive concurrency. It will directly leverage the lowest-level constructs in the hardware and OS: lightweight notification mechanisms, data movement engines, memory mapping, and data placement strategies. It consists of an execution model and a memory model.

Argobots has been expanding its ecosystem both inside and outside the Argo project. Various programming models are integrating Argobots into their runtime so that their applications can take advantage of Argobots without modifying the code.

https://press3.mcs.anl.gov/argobots/[+https://press3.mcs.anl.gov/argobots/+]

https://press3.mcs.anl.gov/bolt/[+https://press3.mcs.anl.gov/bolt/+]

aria2
~~~~~

aria2 is a lightweight multi-protocol & multi-source command-line download utility. It supports HTTP/HTTPS, FTP, SFTP, BitTorrent and Metalink. aria2 can be manipulated via built-in JSON-RPC and XML-RPC interfaces.
The features include:

* Multi-Connection Download. aria2 can download a file from multiple sources/protocols and tries to utilize your maximum download bandwidth. Really speeds up your download experience.

* Lightweight. aria2 doesn’t require much memory and CPU time. When disk cache is off, the physical memory usage is typically 4MiB (normal HTTP/FTP downloads) to 9MiB (BitTorrent downloads). CPU usage in BitTorrent with download speed of 2.8MiB/sec is around 6%.

* Fully Featured BitTorrent Client. All features you want in BitTorrent client are available: DHT, PEX, Encryption, Magnet URI, Web-Seeding, Selective Downloads, Local Peer Discovery and UDP tracker.

* Metalink Enabled. aria2 supports The Metalink Download Description Format (aka Metalink v4), Metalink version 3 and Metalink/HTTP. Metalink offers the file verification, HTTP/FTP/SFTP/BitTorrent integration and the various configurations for language, location, OS, etc.

* Remote Control. aria2 supports RPC interface to control the aria2 process. The supported interfaces are JSON-RPC (over HTTP and WebSocket) and XML-RPC.

Usage examples are:

* Download from WEB: `aria2c http://example.org/mylinux.iso`
* Download from 2 sources: `aria2c http://a/f.iso ftp://b/f.iso`
* Download using 2 connections per host: `aria2c -x2 http://a/f.iso`
* BitTorrent: `aria2c http://example.org/mylinux.torrent`
* BitTorrent Magnet URI: `aria2c 'magnet:?xt=urn:btih:248D0A1CD08284299DE78D5C1ED359BB46717D8C'`
* Metalink: `aria2c http://example.org/mylinux.metalink`
* Download URIs found in text file: `aria2c -i uris.txt`

https://aria2.github.io/[+https://aria2.github.io/+]

https://github.com/aria2/aria2[+https://github.com/aria2/aria2+]

https://xyne.archlinux.ca/projects/python3-aria2jsonrpc/[+https://xyne.archlinux.ca/projects/python3-aria2jsonrpc/+]

https://ugetdm.com/[+https://ugetdm.com/+]

https://en.wikipedia.org/wiki/Metalink[+https://en.wikipedia.org/wiki/Metalink+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

ARPACK
~~~~~~

ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.

The package is designed to compute a few eigenvalues and corresponding eigenvectors of a general n by n matrix A. It is most appropriate for large sparse or structured matrices A where structured means that a matrix-vector product w <- Av requires order n rather than the usual order n2 floating point operations. This software is based upon an algorithmic variant of the Arnoldi process called the Implicitly Restarted Arnoldi Method (IRAM). When the matrix A is symmetric it reduces to a variant of the Lanczos process called the Implicitly Restarted Lanczos Method (IRLM). These variants may be viewed as a synthesis of the Arnoldi/Lanczos process with the Implicitly Shifted QR technique that is suitable for large scale problems. For many standard problems, a matrix factorization is not required. Only the action of the matrix on a vector is needed.

ARPACK software is capable of solving large scale symmetric, nonsymmetric, and generalized eigenproblems from significant application areas. The software is designed to compute a few (k) eigenvalues with user specified features such as those of largest real part or largest magnitude. Storage requirements are on the order of n*k locations. No auxiliary storage is required. A set of Schur basis vectors for the desired k-dimensional eigen-space is computed which is numerically orthogonal to working precision. Numerically accurate eigenvectors are available on request. 

The features include:

* Reverse Communication Interface.
* Single and Double Precision Real Arithmetic Versions for Symmetric, Non-symmetric,
* Standard or Generalized Problems.
* Single and Double Precision Complex Arithmetic Versions for Standard or Generalized Problems.
* Routines for Banded Matrices - Standard or Generalized Problems.
* Routines for The Singular Value Decomposition.
* Example driver routines that may be used as templates to implement numerous Shift-Invert
* strategies for all problem types, data types and precision. 

https://www.caam.rice.edu/software/ARPACK/[+https://www.caam.rice.edu/software/ARPACK/+]

Arrow
~~~~~

Apache Arrow is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. It also provides computational libraries and zero-copy streaming messaging and interprocess communication. Languages currently supported include C, Cxx, Java, JavaScript, Python, and Ruby. 

Apache Arrow™ enables execution engines to take advantage of the latest SIMD (Single input multiple data) operations included in modern processors, for native vectorized optimization of analytical data processing. Columnar layout is optimized for data locality for better performance on modern hardware like CPUs and GPUs.

The Arrow memory format supports zero-copy reads for lightning-fast data access without serialization overhead.

The reference Arrow libraries contain a number of distinct software components:

* Columnar vector and table-like containers (similar to data frames) supporting flat or nested types
* Fast, language agnostic metadata messaging layer (using Google's Flatbuffers library)
* Reference-counted off-heap buffer memory management, for zero-copy memory sharing and handling memory-mapped files
* Low-overhead IO interfaces to files on disk, HDFS (Cxx only)
* Self-describing binary wire formats (streaming and batch/file-like) for remote procedure calls (RPC) and interprocess communication (IPC)
* Integration tests for verifying binary compatibility between the implementations (e.g. sending data from Java to Cxx)
* Conversions to and from other in-memory data structures

https://arrow.apache.org/[+https://arrow.apache.org/+]

https://blog.rstudio.com/2018/04/19/arrow-and-beyond/[+https://blog.rstudio.com/2018/04/19/arrow-and-beyond/+]

http://wesmckinney.com/blog/apache-arrow-pandas-internals/[+http://wesmckinney.com/blog/apache-arrow-pandas-internals/+]

PyArrow
^^^^^^^

The Arrow Python bindings (also named “PyArrow”) have first-class integration with NumPy, pandas, and built-in Python objects. They are based on the Cxx implementation of Arrow.

Here will we detail the usage of the Python API for Arrow and the leaf libraries that add additional functionality such as reading Apache Parquet files into Arrow structures.

https://arrow.apache.org/docs/python/[+https://arrow.apache.org/docs/python/+]

ArtiSynth
~~~~~~~~~

ArtiSynth is a 3D modeling platform that supports the combined simulation of multibody and finite element models, together with contact and constraints. While targeted at biomechanical and biomedical applications, it can also be used for general purpose mechanical simulation. It is freely available under a two-clause BSD-style open source license.

The system is implemented in Java, and provides a rich set of modeling components, including particles, rigid bodies, finite elements with both linear and nonlinear materials, point-to-point muscles, and various bilateral and unilateral constraints including contact. A graphical interface allows interactive component navigation, model editing, and simulation control. 

ArtiSynth has been used to develop a variety of biomechanical models, including upper airway and oral structures such as the jaw, hyoid, tongue, soft palate and pharyngeal wall; a muscle activated FEM model of the face; a combined multibody/FEM model of the foot; point-to-point muscle models of the arm and torso; and detailed FEM models of individual models including fiber fields and tendon sheets. It is the simulation platform for the OPAL and Parametric Human projects, and has also been used to create airway models for use in articulatory speech synthesis. 

https://www.artisynth.org/Main/HomePage[+https://www.artisynth.org/Main/HomePage+]

https://github.com/artisynth[+https://github.com/artisynth+]

ArviZ
~~~~~

ArviZ is a Python package for exploratory analysis of Bayesian models. Includes functions for posterior analysis, sample diagnostics, model checking, and comparison.

The goal is to provide backend-agnostic tools for diagnostics and visualizations of Bayesian inference in Python, by first converting inference data into xarray objects. 

ArviZ will plot NumPy arrays, dictionaries of arrays, xarray datasets, and has built-in support for PyMC3, PyStan, Pyro, and emcee objects. Support for PyMC4, TensorFlow Probability, Edward2, and Edward are on the roadmap.

https://arviz-devs.github.io/arviz/index.html[+https://arviz-devs.github.io/arviz/index.html+]

http://joss.theoj.org/papers/83e0e4048aa30a256a89f3b35b90f065[+http://joss.theoj.org/papers/83e0e4048aa30a256a89f3b35b90f065+]

arxiv-sanity-preserver
~~~~~~~~~~~~~~~~~~~~~~

Web interface for browsing, search and filtering recent arxiv submissions.
This project is a web interface that attempts to tame the overwhelming flood of papers on Arxiv. It allows researchers to keep track of recent papers, search for papers, sort papers by similarity to any paper, see recent popular papers, to add papers to a personal library, and to get personalized recommendations of (new or old) Arxiv papers. This code is currently running live at www.arxiv-sanity.com/, where it's serving 25,000+ Arxiv papers from Machine Learning (cs.[CV|AI|CL|LG|NE]/stat.ML) over the last ~3 years. With this code base you could replicate the website to any of your favorite subsets of Arxiv by simply changing the categories in fetch_papers.py.

https://github.com/karpathy/arxiv-sanity-preserver[+https://github.com/karpathy/arxiv-sanity-preserver+]

http://www.arxiv-sanity.com/[+http://www.arxiv-sanity.com/+]

ASAP Python Toolbox
~~~~~~~~~~~~~~~~~~~

The ASAP Python Toolbox is a collection of stand-alone tools for doing simple tasks, from managing print messages with a set verbosity level, to keeping timing information, to managing simple MPI communication.

The modules contained in this package include:

* vprinter:	For managing print messages with verbosity-level specification
* timekeeper:	For managing multiple "stop watches" for timing metrics
* partition:	For various data partitioning algorithms
* simplecomm:	For simple MPI communication

Only the simplecomm module depends on anything beyond the basic built-in Python packages.

https://github.com/NCAR/ASAPPyTools[+https://github.com/NCAR/ASAPPyTools+]

AsciidocFX
~~~~~~~~~~

Asciidoc FX is a book / document editor to build PDF, Epub, Mobi and HTML books, documents and slides.
The features include:

* Real-Time Preview
* Multi-platform (Windows, Mac, Linux)
* Creating Asciidoc Books
* Creating Markdown Books
* Creating PDF, HTML, Epub, Mobi, Odt, Docbook
* Epub Viewer
* External Browser Support
* Table Generator
* MathJax Extension
* PlantUML Extension
* ditaa Extension
* Filesystem Tree Extension
* JavaFX Charts Extension
* Source Code Highlighter
* Reveal.js Converter
* Deck.js Converter
* Nashorn support
* Pseudo Terminal Emulator

https://asciidocfx.com/[+https://asciidocfx.com/+]

Asciidoctor
~~~~~~~~~~~

Asciidoctor is a fast, open source text processor and publishing toolchain for converting AsciiDoc content to HTML5, DocBook, PDF, and other formats. Asciidoctor is written in Ruby and runs on all major operation systems. To simplify installation, Asciidoctor is packaged and distributed as a gem to RubyGems.org and is packaged for popular Linux distributions and macOS. Asciidoctor can also be run in a JVM using AsciidoctorJ or in any JavaScript environment using Asciidoctor.js.

AsciiDoc belongs to the family of lightweight markup languages, the most renowned of which is Markdown. AsciiDoc stands out from this group because it supports all the structural elements necessary for drafting articles, technical manuals, books, presentations and prose. In fact, it’s capable of meeting even the most advanced publishing requirements and technical semantics.

Asciidoctor reads and parses text written in the AsciiDoc syntax, then feeds the parse tree to a set of built-in converters to produce HTML5, DocBook 5, and man(ual) page output. You have the option of using your own converter or loading Tilt-supported templates to customize the generated output or produce additional formats.

Asciidoctor is a drop-in replacement for the original AsciiDoc Python processor (asciidoc.py). In addition to the classic AsciiDoc syntax, Asciidoctor recognizes additional markup and formatting options, such as font-based icons (e.g., icon:fire[]) and UI elements (e.g., button:[Save]). Asciidoctor also offers a modern, responsive theme based on Foundation to style the HTML5 output.

The built-in Asciidoctor converters are for html(5), xhtml(5), docbook(5), docbook45 and manpage.  Add-on converters 
enable conversion to pdf, epub3, latex and mallard.  The Tilt package can be used to convert to many other formats.

https://asciidoctor.org/[+https://asciidoctor.org/+]

https://github.com/asciidoctor/asciidoctor/[+https://github.com/asciidoctor/asciidoctor/+]

https://github.com/asciidoctor/asciidoctor-pdf[+https://github.com/asciidoctor/asciidoctor-pdf+]

http://xed.ch/h/asciidoc.html[+http://xed.ch/h/asciidoc.html+]

Tilt - https://github.com/rtomayko/tilt[+https://github.com/rtomayko/tilt+]

Kramdown - https://kramdown.gettalong.org/[+https://kramdown.gettalong.org/+]

Markdown conversion script - https://gist.github.com/cheungnj/38becf045654119f96c87db829f1be8e[+https://gist.github.com/cheungnj/38becf045654119f96c87db829f1be8e+]

Antora
^^^^^^
Antora is a modular, multi-repository site generator designed for creating documentation sites from content composed in AsciiDoc and processed with Asciidoctor.

Antora’s toolchain and workflow help documentation and engineering teams create, manage, collaborate on, remix, and publish documentation sites sourced from multiple versioned git repositories without needing expertise in web technologies, build automation, or system administration.

This project includes a command line interface (CLI) and a preassembled site generator pipeline so you can quickly start publishing documentation sites with Antora.

https://antora.org/[+https://antora.org/+]

https://gitlab.com/antora/antora[+https://gitlab.com/antora/antora+]

ASDF
~~~~

ASDF is the de facto standard build facility for Common Lisp.
ASDF 3 is the current successor to Daniel Barlow's ASDF. It was rewritten for improved portability, robustness, usability, extensibility, configurability, internal consistency, and the ability to deliver standalone executables. 

ASDF is what Common Lisp hackers use to build and load software. It is the successor of the Lisp DEFSYSTEM of yore. ASDF stands for Another System Definition Facility. 
ASDF 3 contains two parts:

* `asdf/defsystem` - a tool to describe how Lisp source code is organized in systems, and how to build and load these systems. The build happens based on a plan in term of actions that depend on previous actions; the plan is computed from the structure of the systems. 
* `uiop` - the Utilities for Implementation- and OS- Portability, formerly known as asdf/driver, is a Common Lisp portability library and runtime support system that helps you write Common Lisp software in a portable way. 

ASDF will not download missing software components for you. For that, you want Quicklisp, that builds upon ASDF, and is great for pulling and installing tarballs of packages you may depend upon. We also recommend clbuild, that now builds upon Quicklisp, as a great tool for pulling from version control packages you need to modify or want to contribute to. We recommend you should not use asdf-install anymore, as it is an older similar piece of software that is both unmaintained and obsolete. 

ASDF is also not a tool to build or run Common Lisp software from the Unix command-line. For that, you want cl-launch, buildapp, or roswell. 

https://common-lisp.net/project/asdf/[+https://common-lisp.net/project/asdf/+]

https://github.com/roswell/roswell[+https://github.com/roswell/roswell+]

https://www.xach.com/lisp/buildapp/[+https://www.xach.com/lisp/buildapp/+]

https://cliki.net/cl-launch[+https://cliki.net/cl-launch+]

https://www.quicklisp.org/beta/[+https://www.quicklisp.org/beta/+]

Assimp
~~~~~~

Open Asset Import Library (short name: Assimp) is a portable Open Source library to import various well-known 3D model formats in a uniform manner. The most recent version also knows how to export 3d files and is therefore suitable as a general-purpose 3D model converter.

Assimp aims to provide a full asset conversion pipeline for use in game engines / realtime rendering systems of any kind, but it is not limited to this purpose. In the past, it has been used in a wide range of applications. 

Written in Cxx, it is available under a liberal BSD license. There is a C API as well as bindings to various other languages, including C#/.net, Python and D. Assimp loads all input model formats into one straightforward data structure for further processing. This feature set is augmented by various post processing tools, including frequently-needed operations such as computing normal and tangent vectors.

http://www.assimp.org/[+http://www.assimp.org/+]

https://github.com/assimp/assimp[+https://github.com/assimp/assimp+]

Assimulo
~~~~~~~~

Assimulo is a simulation package for solving ordinary differential equations. It is written in the high-level programming language Python and combines a variety of different solvers written in FORTRAN, C and even Python via a common high-level interface. The primary aim of Assimulo is not to develop new integration algorithms. The aim is to provide a high-level interface for a wide variety of solvers, both new and old, solvers of industrial standards as well as experimental solvers. The aim is to allow comparison of solvers for a given problem without the need to define the problem in a number of different programming languages to accommodate the different solvers.

https://jmodelica.org/assimulo/[+https://jmodelica.org/assimulo/+]

https://trac.jmodelica.org/assimulo/wiki[+https://trac.jmodelica.org/assimulo/wiki+]

Atlas
~~~~~

Atlas is a ECMWF library for parallel data-structures supporting unstructured grids and function spaces, with the aim to investigate alternative more scalable dynamical core options for Earth System models, and to support modern interpolation and product generation software

Atlas is predominantly Cxx code, with main features available to Fortran codes through a F2003 interface. It requires some flavour of Unix (such as Linux). It is known to run on a number of systems, some of which are directly supported by ECMWF.

https://github.com/ecmwf/atlas[+https://github.com/ecmwf/atlas+]

attrs
~~~~~

attrs is the Python package that will bring back the joy of writing classes by relieving you from the drudgery of implementing object protocols.

In order to fulfill its ambitious goal of bringing back the joy to writing classes, it gives you a class decorator and a way to declaratively define the attributes on that class.

After declaring your attributes attrs gives you:

* a concise and explicit overview of the class?s attributes,
* a nice human-readable __repr__,
* a complete set of comparison methods,
* an initializer,

without writing dull boilerplate code again and again and without runtime performance penalties.

https://github.com/python-attrs/attrs[+https://github.com/python-attrs/attrs+]

https://github.com/python-attrs/attrs/wiki/Extensions-to-attrs[+https://github.com/python-attrs/attrs/wiki/Extensions-to-attrs+]

https://www.attrs.org/en/stable/[+https://www.attrs.org/en/stable/+]

https://glyph.twistedmatrix.com/2016/08/attrs.html[+https://glyph.twistedmatrix.com/2016/08/attrs.html+]

autoconj
~~~~~~~~

Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this project, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call AutoConj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.

https://github.com/google-research/autoconj[+https://github.com/google-research/autoconj+]

https://arxiv.org/abs/1811.11926[+https://arxiv.org/abs/1811.11926+]

AutoParallel-Fortran
~~~~~~~~~~~~~~~~~~~~

A domain specific, automatically parallelising source-to-source compiler for Fortran-95 that takes scientific Fortran as input and produces parallel Fortran/OpenCL.

A domain specific, automatically parallelising source-to-source compiler for Fortran-95 that takes scientific Fortran as input and produces Fortran code parallelised using the OpenCL framework. The Fortran parser used for this compiler is Language-Fortran, a Haskell based Fortran parser. The original parser is available at https://github.com/dagit/language-fortran; the current project contains a modified version.

The compiler generates Fortran code in two parts: host code using the OclWrapper Fortran OpenCL API and kernel code in Fortran. OpenCL does not support Fortran so this code needs to be translated to OpenCL C code. This is done using a separate compiler.

https://github.com/wimvanderbauwhede/AutoParallel-Fortran[+https://github.com/wimvanderbauwhede/AutoParallel-Fortran+]

https://github.com/wimvanderbauwhede/AutoParallel-Fortran/blob/master/docs/dissertation_Gavin_Davidson_2016.pdf[+https://github.com/wimvanderbauwhede/AutoParallel-Fortran/blob/master/docs/dissertation_Gavin_Davidson_2016.pdf+]

AutoWIG
~~~~~~~

AutoWIG is a Python library that wraps automatically compiled libraries into high-level languages. Our approach consists in parsing C++ code using the LLVM/Clang technologies and generating the wrappers using the Mako templating engine. Our approach is automatic, extensible, and applies to very complex C++ libraries, composed of thousands of classes or incorporating modern meta-programming constructs.

https://github.com/StatisKit/AutoWIG[+https://github.com/StatisKit/AutoWIG+]

https://arxiv.org/abs/1705.11000[+https://arxiv.org/abs/1705.11000+]

https://peerj.com/articles/cs-149/[+https://peerj.com/articles/cs-149/+]

AV1
~~~

AOMedia Video 1 (AV1) is an open, royalty-free video coding format designed for video transmissions over the Internet. It is being developed by the Alliance for Open Media (AOMedia), a consortium of firms from the semiconductor industry, video on demand providers, and web browser developers, founded in 2015. The AV1 bitstream specification includes a reference video codec.
AV1 is intended for use in HTML5 web video and WebRTC together with the Opus audio format.

https://en.wikipedia.org/wiki/AV1[+https://en.wikipedia.org/wiki/AV1+]

dav1d
^^^^^

dav1d is a new AV1 cross-platform decoder, open-source, and focused on speed and correctness.

https://code.videolan.org/videolan/dav1d[+https://code.videolan.org/videolan/dav1d+]

http://www.jbkempf.com/blog/post/2018/Introducing-dav1d[+http://www.jbkempf.com/blog/post/2018/Introducing-dav1d+]

http://www.jbkempf.com/blog/post/2018/AV1-open-source-tools[+http://www.jbkempf.com/blog/post/2018/AV1-open-source-tools+]

AWIPS
~~~~~

AWIPS (formerly know as AWIPS II or AWIPS2) is a meteorological display and analysis package developed by the National Weather Service and Raytheon for operational forecasting. AWIPS is a Java application consisting of a data-rendering client (CAVE, which runs on Red Hat/CentOS Linux, macOS, and Windows), and a backend data server (EDEX, which runs on x86_64 Red Hat/CentOS 6 and 7).

AWIPS takes a unified approach to data ingest, and most data types follow a path through the system starting with an LDM client requesting data from the Unidata IDD. Various raw data and product files (grib, BUFR, text, gini, McIDAS, NetCDF, more) are decoded and stored as HDF5 and Postgres metadata by EDEX, which then serves these data and products to clients over http.

AWIPS takes a unified approach to data ingest, and most data types follow a standard path through the system, starting with an LDM client requesting data from Unidata's IDD, which are then decoded and stored as HDF5 and PostgreSQL/PostGIS metadata. Unidata supports two visualization frameworks for rendering AWIPS data:

* CAVE - the Common AWIPS Visualization Environment
* python-awips - a Python data access framework for requesting Numpy data arrays and Shapely geometries.

CAVE is the Common AWIPS Visualization Environment, the data rendering and visualization tool for AWIPS. CAVE contains of a number of different data display configurations called perspectives. Perspectives used in operational forecasting environments include D2D (Display Two-Dimensional), GFE (Graphical Forecast Editor), and NCP (National Centers Perspective). 

EDEX is the main server for AWIPS.
Qpid sends alerts to EDEX when data stored by the LDM is ready for processing. These Qpid messages include file header information which allows EDEX to determine the appropriate data decoder to use. The default ingest server (simply named ingest) handles all data ingest other than grib messages, which are processed by a separate ingestGrib server. After decoding, EDEX writes metadata to the database via Postgres and saves the processed data in HDF5 via PyPIES. A third EDEX server, request, feeds requested data to CAVE clients. 

PyPIES, Python Process Isolated Enhanced Storage, was created for AWIPS to isolate the management of HDF5 Processed Data Storage from the EDEX processes. PyPIES manages access, i.e., reads and writes, of data in the HDF5 files. In a sense, PyPIES provides functionality similar to a DBMS (i.e PostgreSQL for metadata); all data being written to an HDF5 file is sent to PyPIES, and requests for data stored in HDF5 are processed by PyPIES.

https://github.com/Unidata/awips2[+https://github.com/Unidata/awips2+]

http://unidata.github.io/awips2/[+http://unidata.github.io/awips2/+]

python-awips
^^^^^^^^^^^^

The python-awips package provides a data access framework for requesting grid and geometry datasets from an AWIPS EDEX server. AWIPS and python-awips packages are released and maintained by UCAR's Unidata Program Center in Boulder, Colorado.

https://github.com/Unidata/python-awips[+https://github.com/Unidata/python-awips+]

AWK
~~~

AWK is a special-purpose programming language designed for text processing and typically used as a data extraction and reporting tool. It is a standard feature of most Unix-like operating systems.

The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data – either run directly on files or used as part of a pipeline – for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. While AWK has a limited intended application domain and was especially designed to support one-liner programs, the language is Turing-complete, and even the early Bell Labs users of AWK often wrote well-structured large AWK programs.[2]

AWK was created at Bell Labs in the 1970s,[3] and its name is derived from the surnames of its authors—Alfred Aho, Peter Weinberger, and Brian Kernighan. The acronym is pronounced the same as the name of the bird auk (which acts as an emblem of the language such as on The AWK Programming Language book cover[4] – the book is often referred to by the abbreviation TAPL). When written in all lowercase letters, as awk, it refers to the Unix or Plan 9 program that runs scripts written in the AWK programming language. 

https://www.gnu.org/software/gawk/manual/[+https://www.gnu.org/software/gawk/manual/+]

https://www.ibm.com/developerworks/library/l-awk1/[+https://www.ibm.com/developerworks/library/l-awk1/+]

https://www.ibm.com/developerworks/library/l-awk2/[+https://www.ibm.com/developerworks/library/l-awk2/+]

https://www.ibm.com/developerworks/library/l-awk3/[+https://www.ibm.com/developerworks/library/l-awk3/+]

http://awklang.org/[+http://awklang.org/+]

http://www.grymoire.com/Unix/Awk.html[+http://www.grymoire.com/Unix/Awk.html+]

https://likegeeks.com/awk-command/[+https://likegeeks.com/awk-command/+]

https://linuxhandbook.com/awk-command-tutorial/[+https://linuxhandbook.com/awk-command-tutorial/+]

https://en.wikipedia.org/wiki/AWK[+https://en.wikipedia.org/wiki/AWK+]

Awka
^^^^

Awka is an open-source implementation of the AWK programming language.
Awka is not an interpreter like Gawk, Mawk or Nawk, but instead it converts the program to ANSI-C, then compiles this using gcc or a native C compiler to create a binary executable. This means you must have an ANSI C compiler present on your system for Awka to work.

The Extended Library Methods (ELM) provide a way of adding new functions to the AWK language, so that they appear in your AWK code as if they were builtin functions such as substr() or index().
Awka-ELM code interfaces with the internal Awka variable structures and functions, and is suitable for anyone with some experience and proficiency in C programming. 

http://awka.sourceforge.net/index.html[+http://awka.sourceforge.net/index.html+]

http://awka.sourceforge.net/elm.html[+http://awka.sourceforge.net/elm.html+]

GNU awk
^^^^^^^

An GNU implementation of awk that is the nly implementation that makes serious progress implementing internationalization and localization and TCP/IP networking. It was written before the original implementation became freely available. It includes its own debugger, and its profiler enables the user to make measured performance enhancements to a script. It also enables the user to extend functionality with shared libraries.

https://www.gnu.org/software/gawk/[+https://www.gnu.org/software/gawk/+]

mawk
^^^^

mawk  is an interpreter for the AWK Programming Language.  The AWK language
is useful for manipulation of data files, text retrieval and
processing,  and  for prototyping and experimenting with algorithms.  mawk
is a new awk meaning it implements the AWK language as defined in  Aho,
Kernighan  and Weinberger, The AWK Programming Language, Addison-Wesley
Publishing, 1988 (hereafter referred to as the AWK  book.)   mawk  con-
forms  to  the POSIX 1003.2 (draft 11.3) definition of the AWK language
which contains a few features not described in the AWK book,  and  mawk
provides a small number of extensions.

https://invisible-island.net/mawk/[+https://invisible-island.net/mawk/+]

#BBBB

Babel
~~~~~

Babel is a high-performance language interoperability tool. The project is mainly developed at the Center for Applied Scientific Computing (CASC) at Lawrence Livermore National Laboratory (LLNL). Babel started as an internal Lab Directed Research and Development (LDRD) project in 2000 and has been under constant development since then. It is now funded mainly under the U.S. Department of Energy (DOE) Office of Science's SciDAC program.

Babel currently fully supports C, Cxx, FORTRAN 77, Fortran 90/95, Fortran 2003/2008, Python, and Java. It won a prestigious R&D 100 award in 2006 for "The world's most rapid communication among many programming languages in a single application.".

Our tool is based on the Scientific Interface Description Language (SIDL). SIDL is tailored to the needs of the scientific community and features support for complex numbers, structs, and dynamic multidimensional arrays. Babel is a compiler that generates glue code from SIDL interface descriptions. SIDL provides a modern object oriented programming model, even on top of traditional procedural languages. This includes automatic reference counting and resource (de)allocation. Code written in one language can be called from any of the supported languages.

Babel focuses on high-performance language interoperability within a single address space. Full support for Remote Method Invocation (RMI) allows for the development of parallel distributed applications.

We also provide tools to automatically generate documentation from SIDL descriptions. An equivalent XML representation eases development of third-party tools. 

https://computation.llnl.gov/projects/babel-high-performance-language-interoperability/#page=home[+https://computation.llnl.gov/projects/babel-high-performance-language-interoperability/#page=home+]

backupninja
~~~~~~~~~~~

Backupninja allows you to coordinate system backup by dropping a few simple configuration files into /etc/backup.d/. Most programs you might use for making backups don't have their own configuration file format. Backupninja provides a centralized way to configure and coordinate many different backup utilities.

The key features of backupninja are:

* easy to read ini style configuration files
* you can drop in scripts to handle new types of backups
* backup actions can be scheduled
* you can choose when status report emails are mailed to you (always, on warning, on error, never)
* console-based wizard (ninjahelper) makes it easy to create backup action configuration files
* passwords are never sent via the command line to helper programs
* works with Linux-Vservers

To preform the actual backup, backupninja processes each configuration file in /etc/backup.d according to the file's suffix:

* .sh: run this file as a shell script.
* .rdiff: filesystem backup (using rdiff-backup)
* .dup: filesystem backup (using duplicity)
* .borg: filesystem backup (using borg)
* .mysql: backup mysql databases
* .pgsql: backup PostgreSQL databases
* .sys: general hardware, partition, and system reports.
* .svn: backup subversion repositories
* .maildir: incrementally backup maildirs (very specialized)

https://0xacab.org/riseuplabs/backupninja[+https://0xacab.org/riseuplabs/backupninja+]

Balsam
~~~~~~

Balsam is a Python-based service that handles the cumbersome process of running many jobs across one or more HPC resources. It runs on the login nodes, keeping track of all your jobs and submitting them to the local scheduler on your behalf.

You could use Balsam as a drop-in replacement for qsub, simply using balsam qsub to submit your jobs with absolutely no restrictions. Let Balsam throttle submission to the local queues, package jobs into ensembles for you, and dynamically size these packages to exploit local scheduling policies.

https://balsam.alcf.anl.gov/[+https://balsam.alcf.anl.gov/+]

https://xgitlab.cels.anl.gov/datascience/balsam[+https://xgitlab.cels.anl.gov/datascience/balsam+]

https://github.com/betterscientificsoftware/python-for-hpc[+https://github.com/betterscientificsoftware/python-for-hpc+]

BART
~~~~

The Berkeley Advanced Reconstruction Toolbox (BART) toolbox is a free and open-source image-reconstruction framework for Computational Magnetic Resonance Imaging.
It consists of a programming library and a toolbox of command-line programs. The library provides common operations on multi-dimensional arrays, Fourier and wavelet transforms, as well as generic implementations of iterative optimization algorithms. The command-line tools provide direct access to basic operations on multi-dimensional arrays as well as efficient implementations of many calibration and reconstruction algorithms for parallel imaging and compressed sensing. 

The features include:

* basic features:
** runs on Linux and Mac OS X
** multi-dimensional operations on arrays
** fast non-uniform Fourier Transform (nuFFT and convolution-based method)
** multi-dimensional (divergence-free) wavelet transform
** parallel computation on multiple cores and with Graphical Processing Units (GPU)
* iterative methods:
** Conjugate Gradients (CG)
** (Fast) Iterative Soft-Thresholding Algorithm (ISTA and FISTA)
** Normalized iterative hard thresholding (NIHT)
** Alternating Direction Method of Multipliers (ADMM)
** Iteratively Regularized Gauss-Newton Method (IRGNM)
** Chambolle-Pock primal-dual algorithm
* calibration methods:
** direct calibration of coil sensitivities from k-space center
** Walsh's method for calibration of coil sensitivities
** ESPIRiT
** (geometric) channel compression and whitening
** RING: estimation of gradient delays for radial MRI
* reconstruction methods for MRI:
** iterative parallel imaging reconstruction: POCSENSE, SENSE
** compressed sensing and parallel imaging
** calibration-less parallel imaging: NLINV and ENLIVE (non-linear optimization) and SAKE (structured low-rank matrix completion)
* regularization (in arbitrary dimensions):
** Tikhonov
** total variation
** l1-wavelet
** (multi-scale) low-rank

http://mrirecon.github.io/bart/[+http://mrirecon.github.io/bart/+]

https://github.com/mrirecon/bart/[+https://github.com/mrirecon/bart/+]

bash
~~~~

Blah.

*Awesome Shell* - https://github.com/alebcay/awesome-shell[+https://github.com/alebcay/awesome-shell+]

*Awesome CLI Apps* - https://github.com/agarrharr/awesome-cli-apps[+https://github.com/agarrharr/awesome-cli-apps+]

*Bash Hackers Wiki* - https://wiki.bash-hackers.org/[+https://wiki.bash-hackers.org/+]

*Advanced Bash Scripting Guide* - http://tldp.org/LDP/abs/html/[+http://tldp.org/LDP/abs/html/+]

*Bash FAQ* - http://mywiki.wooledge.org/BashFAQ[+http://mywiki.wooledge.org/BashFAQ+]

*Bash Programming* - http://mywiki.wooledge.org/BashProgramming[+http://mywiki.wooledge.org/BashProgramming+]

*Bash Pitfalls* - http://mywiki.wooledge.org/BashPitfalls[+http://mywiki.wooledge.org/BashPitfalls+]

Bats
^^^^

Bats is a TAP-compliant testing framework for Bash. It provides a simple way to verify that the UNIX programs you write behave as expected.
A Bats test file is a Bash script with special syntax for defining test cases. Under the hood, each test case is just a function with a description.
Bats is most useful when testing software written in Bash, but you can use it to test any UNIX program.

Test cases consist of standard shell commands. Bats makes use of Bash's errexit (set -e) option when running test cases. If every command in the test case exits with a 0 status code (success), the test passes. In this way, each line is an assertion of truth.

To run your tests, invoke the bats interpreter with a path to a test file. The file's test cases are run sequentially and in isolation. If all the test cases pass, bats exits with a 0 status code. If there are any failures, bats exits with a 1 status code.

https://github.com/sstephenson/bats[+https://github.com/sstephenson/bats+]

baudline
~~~~~~~~

Baudline is a time-frequency browser designed for scientific visualization of the spectral domain.  Signal analysis is performed by Fourier, correlation, and raster transforms that create colorful spectrograms with vibrant detail.  Conduct test and measurement experiments with the built in function generator, or play back audio files with a multitude of effects and filters.  The baudline signal analyzer combines fast digital signal processing, versatile high speed displays, and continuous capture tools for hunting down and studying elusive signal characteristics.

The baudline features include:

* 192 kHz real-time bandwidth
* 96 dB dynamic range
* Real or Quadrature input
* Multiple sound card support
* JACK Audio Connection Kit sound server support
* Input Digital Down Converter (tuner)
* Configurable input channels that can perform various DSP operations
* Fourier, Correlation, Impulse Response, Transfer Function, and Raster transforms
* Channel Equalization
* Frequency, time, amplitude, and sample probability distribution analysis
* Drift Integration "de-chirping"
* High speed displays
* Test signal generation 
* an audio player with looping, speed control, pitch scaling, digital gain boost, etc.
* over a dozen recognized file formats
* up to 9 channels
* measurements of peaks, fundamental frequency, distortion, power, etc.

http://www.baudline.com/[+http://www.baudline.com/+]

https://github.com/kyleterry/awesome-radio[+https://github.com/kyleterry/awesome-radio+]

BayesLite
~~~~~~~~~

Bayeslite is a probabilistic database built on SQLite 3. In addition to SQL queries on conventional SQL tables, it supports probabilistic BQL queries on generative models for data in a table.

BayesDB is a probabilistic programming platform that provides built-in non-parametric Bayesian model discovery. BayesDB makes it easy for users without statistics training to search, clean, and model multivariate databases using an SQL-like language.

BayesDB is based on probabilistic programming, an emerging field based on the insight that probabilistic models and inference algorithms are a new kind of software, and therefore amenable to radical improvements in accessibility, productivity, and scale. Unfortunately, most probabilistic programming systems require users to write probabilistic programs by hand. Instead, BayesDB provides a built-in probabilistic program synthesis system that builds generative models for multivariate databases via inference over programs given a non-parametric Bayesian prior. BayesDB also enables statisticians to override these programs with custom statistical models when appropriate.

The source code for bayeslite, a python/sqlite3 implementation of BayesDB, is available on Github. We recommend installing BayesDB using the Open Probabilistic Programming Stack.

http://probcomp.csail.mit.edu/software/bayesdb/[+http://probcomp.csail.mit.edu/software/bayesdb/+]

http://probcomp.csail.mit.edu/dev/bayesdb/doc/[+http://probcomp.csail.mit.edu/dev/bayesdb/doc/+]

http://probcomp.csail.mit.edu/software/open-probabilistic-programming-stack/[+http://probcomp.csail.mit.edu/software/open-probabilistic-programming-stack/+]

Bazel
~~~~~

Bazel is a free software tool that allows for the automation of building and testing of software. The company Google uses the build tool Blaze internally and released and open-sourced part of the Blaze tool as Bazel, named as an anagram of Blaze.

Similar to build tools like Make, Apache Ant, or Apache Maven, Bazel builds software applications from source code using a set of rules. Rules and macros are created in the Starlark language (previously called Skylark), a dialect of Python. There are built-in rules for building software written in the programming languages of Java, C, Cxx, Go, Python, Objective-C and Bourne shell scripts. Bazel can produce software application packages suitable for deployment for the Android and iOS operating systems.

In designing Bazel, emphasis has been placed on build speed, correctness, and reproducibility. The tool uses parallelization to speed up parts of the build process. It includes a Bazel Query language that can be used to analyze build dependencies in complex build graphs.

Build systems most similar to Bazel are Pants, Buck, and Please. Pants and Buck both aim for similar technical design goals as Bazel and were inspired by the Blaze build system used internally at Google. Blaze is also the predecessor to Bazel. Bazel, Pants, Buck, and Please adopted Starlark as BUILD file parser, respective its BUILD file syntax. Independently developed build systems with similar goals of efficient dependency graph analysis and automated build artifact tracking have been implemented in build systems such as tup.

https://en.wikipedia.org/wiki/Bazel_(software)[+https://en.wikipedia.org/wiki/Bazel_(software)+]

http://gittup.org/tup/[+http://gittup.org/tup/+]

https://please.build/[+https://please.build/+]

https://www.pantsbuild.org/index.html[+https://www.pantsbuild.org/index.html+]

https://buckbuild.com/[+https://buckbuild.com/+]

bbcp
~~~~

bbcp is a point-to-point network file copy application written by Andy Hanushevsky at SLAC as a tool for the BaBar collaboration. It is capable of transferring files at approaching line speeds in the WAN.

A very nice feature of bbcp is the ease with which it can be installed and used. Installation basically involves placing the bbcp executable in your path on all the systems you want to use it on. All standard methods of authentication can be used: passwords and certificates. The latter are most convenient in most situations.

bbcp is a peer-to-peer application. No server process is required - you just invoke bbcp on a source machine and in response a bbcp process is started on the target machine. You can also do this as a third party: the source and target machines do not need to be the same machine that you initiate the file transfer from.

Before using bbcp, it is worthwhile checking that the TCP/IP stack parameters that are set on your sender and receiver computers are suitable for high speed transfer.

http://pcbunn.cithep.caltech.edu/bbcp/using_bbcp.htm[+http://pcbunn.cithep.caltech.edu/bbcp/using_bbcp.htm+]

http://www.slac.stanford.edu/%7Eabh/bbcp/[+http://www.slac.stanford.edu/%7Eabh/bbcp/+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

BEE
~~~

The goal of BEE (Build and Execution Environment) is to create a unified software stack to containerize HPC applications. A container is a package of code (usually binaries) and all of that code's dependencies (libraries, etc.). Once built, this container can be run on many different platforms. The execution environment on each platform will download and install (for this application only) all of the applications and dependencies into an isolated user environment and then execute the code. Containers provide many benefits:

* Users can choose their own software stack (libraries, compilers, etc.) and not be bound by the currently installed environment on any one machine.
* Codes can be run portably across numerous platforms--all dependencies will be downloaded and installed at run time.
* Entire workflow environments can be built into one or more containers. A user can include visualization and analysis tools along with the application. They will all work together as the application runs.
* Provenance and history can be tracked by storing containers in a historical repository. At any time, an older container can be rerun (all of its dependencies are stored with it). Execution is repeatable and interactions between software components can be tracked.
* Functional testing can be performed on smaller, dissimilar machines--there is no real need to test on the actual HPC platform (performance testing obviously requires target hardware).

The BEE project uses Docker to containerize applications. Docker has become the de facto standard container system and is used widely in cloud and web environments. Continuous integration services have been built on Docker, allowing application developers to describe compile and execution environments with Docker. When code is checked into a repository, it can be automatically tested across a suite of different software environments.

The BEE project supports launching applications using the Charliecloud HPC container runtime. These applications can be executed on a traditional HPC cluster or an OpenStack cloud cluster.

https://github.com/lanl/BEE[+https://github.com/lanl/BEE+]

https://github.com/hpc/charliecloud[+https://github.com/hpc/charliecloud+]

https://www.exascaleproject.org/project/lanl-atdm-software-ecosystem-delivery-projects/[+https://www.exascaleproject.org/project/lanl-atdm-software-ecosystem-delivery-projects/+]

Befunge
~~~~~~~

Befunge is a stack-based, reflective, esoteric programming language. It differs from conventional languages in that programs are arranged on a two-dimensional grid. "Arrow" instructions direct the control flow to the left, right, up or down, and loops are constructed by sending the control flow in a cycle. It has been described as "a cross between Forth and Lemmings."

The language was originally created by Chris Pressey[3] in 1993 for the Amiga, as an attempt to devise a language which is as hard to compile as possible. Note that the p command allows for self-modifying code. Nevertheless, a number of compilers have subsequently been written. A number of extensions to the original "Befunge-93" specification also exist, most notably Funge-98, which extends the concept to an arbitrary number of dimensions and can be multithreaded, with multiple instruction pointers operating simultaneously on the same space. Befunge-extensions and variants are called Fungeoids or just Funges.

The Befunge-93 specification restricts each valid program to a grid of 80 instructions horizontally by 25 instructions vertically. Program execution which exceeds these limits "wraps around" to a corresponding point on the other side of the grid; a Befunge program is in this manner topologically equivalent to a torus. Since a Befunge-93 program can only have a single stack and its storage array is bounded, the Befunge-93 language is not Turing-complete (however, it has been shown that Befunge-93 is Turing Complete with unbounded stack word size).[4] The later Funge-98 specification provides Turing completeness by removing the size restrictions on the program; rather than wrapping around at a fixed limit, the movement of a Funge-98 instruction pointer follows a model dubbed "Lahey-space" after its originator, Chris Lahey. In this model, the grid behaves like a torus of finite size with respect to wrapping, while still allowing itself to be extended indefinitely. 

https://en.wikipedia.org/wiki/Befunge[+https://en.wikipedia.org/wiki/Befunge+]

https://esolangs.org/wiki/Befunge[+https://esolangs.org/wiki/Befunge+]

BeStMan
~~~~~~~

BeStMan2 is a full implementation of SRM v2.2, developed by Lawrence Berkeley National Laboratory, for disk based storage systems and mass storage systems such as HPSS. End users may have their own personal BeStMan2 that manages and provides an SRM interface to their local disks or storage systems. It works on top of existing disk-based unix file system, and has been reported so far to work on file systems such as NFS, PVFS, AFS, GFS, GPFS, PNFS, HFS+, LustrE, Ibrix, and XrootdFS. It also works with any file transfer service, such as gsiftp, http, https and ftp, as supported file transfer protocols. It requires minimal administrative efforts on the deployment and maintenance. BeStMan2 also has a gateway mode by configuration that would provide a full SRM interface with small footprints on any existing file system without queuing or space management.

BeStMan2 is a Jetty based implementation of SRM v2.2, as opposed to globus container based implementation in the previous BeStMan. All the rest of functionalities and features are the same.

https://sdm.lbl.gov/bestman/[+https://sdm.lbl.gov/bestman/+]

https://sdm.lbl.gov/bestman/client/index.html[+https://sdm.lbl.gov/bestman/client/index.html+]

https://codeforge.lbl.gov/projects/bestman[+https://codeforge.lbl.gov/projects/bestman+]

Bifrost
~~~~~~~

A stream processing framework, created to ease the development of high-throughput processing CPU/GPU pipelines. It is specifically designed for digital signal processing (DSP) applications within radio astronomy. A portable C API is provided, along with Cxx and Python wrappers.

The heart of bifrost is a flexible ring buffer implementation that allows different signal processing blocks to be connected to form a pipeline. Each block may be assigned to a CPU core, and the ring buffers are used to transport data to and from blocks. Processing blocks may be run on either the CPU or GPU, and the ring buffer will take care of memory copies between the CPU and GPU spaces.

The purpose of bifrost is to allow rapid development of streaming DSP pipelines; that is, it is designed for stream-like data. A simple example of a data stream is the time series voltage data from a radio telescope’s digitizer card. Unlike file-like data, stream-like data has no well defined start and stop points. One can of course take a series of files, each containing a chunk of a time stream, and treat them as a stream.

http://ledatelescope.github.io/bifrost/intro.html[+http://ledatelescope.github.io/bifrost/intro.html+]

https://arxiv.org/abs/1708.00720[+https://arxiv.org/abs/1708.00720+]

https://github.com/ledatelescope/bifrost[+https://github.com/ledatelescope/bifrost+]

BIRD
~~~~

The name `BIRD' is actually an acronym standing for `BIRD Internet Routing Daemon'.
It's a program (well, a daemon, as you are going to discover in a moment) which works as a dynamic router in an Internet type network (that is, in a network running either the IPv4 or the IPv6 protocol). 

Routers are devices which forward packets between interconnected networks in order to allow hosts not connected directly to the same local area network to communicate with each other. They also communicate with the other routers in the Internet to discover the topology of the network which allows them to find optimal (in terms of some metric) rules for forwarding of packets (which are called routing tables) and to adapt themselves to the changing conditions such as outages of network links, building of new connections and so on. Most of these routers are costly dedicated devices running obscure firmware which is hard to configure and not open to any changes (on the other hand, their special hardware design allows them to keep up with lots of high-speed network interfaces, better than general-purpose computer does). Fortunately, most operating systems of the UNIX family allow an ordinary computer to act as a router and forward packets belonging to the other hosts, but only according to a statically configured table. 

A Routing Daemon is in UNIX terminology a non-interactive program running on background which does the dynamic part of Internet routing, that is it communicates with the other routers, calculates routing tables and sends them to the OS kernel which does the actual packet forwarding. 

BIRD is an Internet Routing Daemon designed to avoid all of these shortcomings, to support all the routing technology used in the today's Internet or planned to be used in near future and to have a clean extensible architecture allowing new routing protocols to be incorporated easily. Among other features, BIRD supports:

* both IPv4 and IPv6 protocols
* multiple routing tables
* the Border Gateway Protocol (BGPv4)
* the Routing Information Protocol (RIPv2)
* the Open Shortest Path First protocol (OSPFv2, OSPFv3)
* the Router Advertisements for IPv6 hosts
* a virtual protocol for exchange of routes between different routing tables on a single host
* a command-line interface allowing on-line control and inspection of status of the daemon
* soft reconfiguration (no need to use complex online commands to change the configuration, just edit the configuration file and notify BIRD to re-read it and it will smoothly switch itself to the new configuration, not disturbing routing protocols unless they are affected by the configuration changes)
* a powerful language for route filtering

BIRD has been designed to work on all UNIX-like systems. It has been developed and tested under Linux 2.0 to 2.6, and then ported to FreeBSD, NetBSD and OpenBSD, porting to other systems (even non-UNIX ones) should be relatively easy due to its highly modular architecture. 

https://bird.network.cz/[+https://bird.network.cz/+]

https://gitlab.labs.nic.cz/labs/bird[+https://gitlab.labs.nic.cz/labs/bird+]

https://opensource.com/article/19/1/faucet-open-source-sdn-controller[+https://opensource.com/article/19/1/faucet-open-source-sdn-controller+]

BitBake
~~~~~~~

BitBake is a make-like build tool with the special focus of distributions and packages for embedded Linux cross compilation, although it is not limited to that. It is inspired by Portage,[3] which is the package management system used by the Gentoo Linux distribution. BitBake existed for some time in the OpenEmbedded project until it was separated out into a standalone, maintained, distribution-independent tool. BitBake is co-maintained by the Yocto Project and the OpenEmbedded project. 

BitBake recipes specify how a particular package is built.[4] Recipes consist of the source URL (http, https, ftp, cvs, svn, git, local file system) of the package, dependencies and compile or install options. They also store the metadata for the package in standard variables.[5] During the build process, recipes are used to track dependencies, performing native or cross-compilation of the package and package it so that it is suitable for installation on the local or a target device. It is also possible to create complete images consisting of a root file system and kernel. As a first step in a cross-build setup, the framework will attempt to create a cross-compiler toolchain suited for the target platform. 

https://www.yoctoproject.org/software-item/bitbake/[+https://www.yoctoproject.org/software-item/bitbake/+]

https://www.yoctoproject.org/docs/current/bitbake-user-manual/bitbake-user-manual.html[+https://www.yoctoproject.org/docs/current/bitbake-user-manual/bitbake-user-manual.html+]

https://www.openhub.net/p/bitbake[+https://www.openhub.net/p/bitbake+]

https://a4z.bitbucket.io/docs/BitBake/guide.html[+https://a4z.bitbucket.io/docs/BitBake/guide.html+]

Black
~~~~~

An uncompromising code formatter.
By using Black, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters.

Black makes code review faster by producing the smallest diffs possible. Blackened code looks the same regardless of the project you’re reading. Formatting becomes transparent after a while and you can focus on the content instead.

https://black.readthedocs.io/en/stable/[+https://black.readthedocs.io/en/stable/+]

https://github.com/ambv/black[+https://github.com/ambv/black+]

https://black.now.sh/[+https://black.now.sh/+]

BladeX
~~~~~~

BladeX (Python Blade Morphing) is a Python package for geometrical parametrization and bottom-up construction of propeller blades. It allows to generate and deform a blade based on the radial distribution of its parameters.

BladeX is a Python package for geometrical parametrization and bottom-up construction of propeller blades. It allows to generate and deform a blade based on the radial distribution of its parameters such as pitch, rake, skew, and the sectional foils' parameters such as chord and camber. The package is ideally suited for parametric simulations on large number of blade deformations. It provides an automated procedure for the CAD generation, hence reducing the time and effort required for modelling. The main scope of BladeX is to deal with propeller blades, however it can be flexible to be applied on further applications with analogous geometrical structures such as aircraft wings, turbomachinery, or wind turbine blades.

https://github.com/mathLab/BladeX[+https://github.com/mathLab/BladeX+]

https://github.com/mathLab/BladeX/blob/master/tutorials/README.md[+https://github.com/mathLab/BladeX/blob/master/tutorials/README.md+]

https://github.com/tpaviot/pythonocc[+https://github.com/tpaviot/pythonocc+]

BLAS
~~~~

Basic Linear Algebra Subprograms (BLAS) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. They are the de facto standard low-level routines for linear algebra libraries; the routines have bindings for both C and Fortran. Although the BLAS specification is general, BLAS implementations are often optimized for speed on a particular machine, so using them can bring substantial performance benefits. BLAS implementations will take advantage of special floating point hardware such as vector registers or SIMD instructions. 

https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms[+https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms+]

http://www.netlib.org/blas/[+http://www.netlib.org/blas/+]

https://github.com/Reference-LAPACK[+https://github.com/Reference-LAPACK+]

BLASFEO
^^^^^^^

BLASFEO (as BLAS For Embedded Optimization) provides a set of basic linear algebra routines, performance-optimized for matrices of moderate size (up to a couple hundreds elements in each dimension), as typically encountered in embedded optimization applications.
In the target matrix size range, the optimized version of BLASFEO outperforms both open-source (e.g. OpenBLAS, BLIS, ATLAS) and proprietary (e.g. MKL) BLAS and LAPACK implementations.

The BLASFEO backend provides three possible implementations of each linear algebra routine (LA):

* HIGH_PERFORMANCE: target-tailored; performance-optimized for cache resident matrices; panel-major matrix format
* REFERENCE: target-unspecific lightly-optimizated; small code footprint; column-major matrix format
* BLAS_WRAPPER: call to external BLAS and LAPACK libraries; column-major matrix format

https://github.com/giaf/blasfeo[+https://github.com/giaf/blasfeo+]

https://blasfeo.syscop.de/overview/[+https://blasfeo.syscop.de/overview/+]

https://arxiv.org/abs/1704.02457[+https://arxiv.org/abs/1704.02457+]

BLIS
^^^^

BLIS is a portable software framework for instantiating high-performance BLAS-like dense linear algebra libraries. The framework was designed to isolate essential kernels of computation that, when optimized, immediately enable optimized implementations of most of its commonly used and computationally intensive operations. BLIS is written in ISO C99 and available under a new/modified/3-clause BSD license. While BLIS exports a new BLAS-like API, it also includes a BLAS compatibility layer which gives application developers access to BLIS implementations via traditional BLAS routine calls. An object-based API unique to BLIS is also available.

It is our belief that BLIS offers substantial benefits in productivity when compared to conventional approaches to developing BLAS libraries, as well as a much-needed refinement of the BLAS interface, and thus constitutes a major advance in dense linear algebra computation. While BLIS remains a work-in-progress, we are excited to continue its development and further cultivate its use within the community.

BLIS offers several advantages over traditional BLAS libraries:

* Portability that doesn't impede high performance. Portability was a top priority of ours when creating BLIS. With virtually no additional effort on the part of the developer, BLIS is configurable as a fully-functional reference implementation. But more importantly, the framework identifies and isolates a key set of computational kernels which, when optimized, immediately and automatically optimize performance across virtually all level-2 and level-3 BLIS operations.

* Generalized matrix storage. The BLIS framework exports interfaces that allow one to specify both the row stride and column stride of a matrix. This allows one to compute with matrices stored in column-major order, row-major order, or by general stride. (This latter storage format is important for those seeking to implement tensor contractions on multidimensional arrays.)

* Rich support for the complex domain. BLIS operations are developed and expressed in their most general form, which is typically in the complex domain. These formulations then simplify elegantly down to the real domain, with conjugations becoming no-ops. Unlike the BLAS, all input operands in BLIS that allow transposition and conjugate-transposition also support conjugation (without transposition), which obviates the need for thread-unsafe workarounds. Also, where applicable, both complex symmetric and complex Hermitian forms are supported. (BLAS omits some complex symmetric operations, such as symv, syr, and syr2.)

* Advanced multithreading support. BLIS allows multiple levels of symmetric multithreading for nearly all level-3 operations. (Currently, users may choose to obtain parallelism via either OpenMP or POSIX threads). This means that matrices may be partitioned in multiple dimensions simultaneously to attain scalable, high-performance parallelism on multicore and many-core architectures.

*  The BLIS framework, and the library of routines it generates, are easy to use for end users, experts, and vendors alike. An optional BLAS compatibility layer provides application developers with backwards compatibility to existing BLAS-dependent codes.

* Multilayered API, exposed kernels, and sandboxes. The BLIS framework exposes its implementations in various layers, allowing expert developers to access exactly the functionality desired. This layered interface includes that of the lowest-level kernels, for those who wish to bypass the bulk of the framework. Optimizations can occur at various levels, in part thanks to exposed packing and unpacking facilities, which by default are highly parameterized and flexible.

* Functionality that grows with the community's needs. As its name suggests, the BLIS framework is not a single library or static API, but rather a nearly-complete template for instantiating high-performance BLAS-like libraries.

* Code re-use. Auto-generation approaches to achieving the aforementioned goals tend to quickly lead to code bloat due to the multiple dimensions of variation supported: operation (i.e. gemm, herk, trmm, etc.); parameter case (i.e. side, [conjugate-]transposition, upper/lower storage, unit/non-unit diagonal); datatype (i.e. single-/double-precision real/complex); matrix storage (i.e. row-major, column-major, generalized); and algorithm (i.e. partitioning path and kernel shape). These "brute force" approaches often consider and optimize each operation or case combination in isolation, which is less than ideal when the goal is to provide entire libraries. BLIS was designed to be a complete framework for implementing basic linear algebra operations, but supporting this vast amount of functionality in a manageable way required a holistic design that employed careful abstractions, layering, and recycling of generic (highly parameterized) codes, subject to the constraint that high performance remain attainable.

* A foundation for mixed domain and/or mixed precision operations. BLIS was designed with the hope of one day allowing computation on real and complex operands within the same operation. Similarly, we wanted to allow mixing operands' numerical domains, floating-point precisions, or both domain and precision, and to optionally compute in a precision different than one or both operands' storage precisions. This feature has been implemented for the general matrix multiplication (gemm) operation, providing 128 different possible type combinations, which, when combined with existing transposition, conjugation, and storage parameters, enables 55,296 different gemm use cases.

https://github.com/flame/blis[+https://github.com/flame/blis+]

CombBLAS
^^^^^^^^

The Combinatorial BLAS (CombBLAS) is an extensible distributed-memory parallel graph library offering a small but powerful set of linear algebra primitives specifically targeting graph analytics.
The Combinatorial BLAS development influences the Graph BLAS standardization process.

This is a beta implementation of the Combinatorial BLAS Library in written in Cxx with MPI and OpenMP for parallelism. It is purposefully designed for distributed memory platforms though it also runs in uniprocessor and shared-memory (such as multicores) platforms. It contains efficient implementations of novel data structures/algorithms as well as reimplementations of some previously known data structures/algorithms for convenience.

For I/O purposes, the implementation supports both a tuples format very similar to the Matrix Market and the Matrix Market format itself. We recommend using the Matrix Market version and associated ParallelReadMM() functions.

The main data structure is a distributed sparse matrix.
Sparse and dense vectors are distributed along all processors. This is very space efficient and provides good load balance for SpMSV (sparse matrix-sparse vector multiplication).

The supported operations are:

* Sparse matrix-matrix multiplication on a semiring SR: PSpGEMM()
* Elementwise multiplication of sparse matrices (A .* B and A .* not(B) in Matlab): EWiseMult()
* Unary operations on nonzeros: SpParMat::Apply()
* Matrix-matrix and matrix-vector scaling (the latter scales each row/column with the same scalar of the vector)
* Reductions along row/column: SpParMat::Reduce()
* Sparse matrix-dense vector multiplication on a semiring, SpMV()
* Sparse matrix-sparse vector multiplication on a semiring, SpMV()
* Generalized matrix indexing: SpParMat::operator(const FullyDistVec & ri, const FullyDistVec & ci)
* Generalized sparse matrix assignment: SpParMat::SpAsgn (const FullyDistVec & ri, const FullyDistVec &ci, SpParMat & B)
* Numeric type conversion through conversion operators
* Elementwise operations between sparse and dense matrices: SpParMat::EWiseScale() and operator+=()
* BFS specific optimizations inside BFSFriends.h

All the binary operations can be performed on matrices with different numerical value representations. The type-traits mechanism will take care of the automatic type promotion, and automatic MPI data type determination.

https://people.eecs.berkeley.edu/\~aydin/CombBLAS/html/index.html[+https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html+]

CUTLASS
^^^^^^^

CUTLASS is a collection of CUDA Cxx template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement similar to those used to implement cuBLAS. CUTLASS decomposes these "moving parts" into reusable, modular software components abstracted by Cxx template classes. These thread-wide, warp-wide, block-wide, and device-wide primitives can be specialized and tuned via custom tiling sizes, data types, and other algorithmic policy. The resulting flexibility simplifies their use as building blocks within custom kernels and applications.

To support a wide variety of applications, CUTLASS provides extensive support for mixed-precision computations, providing specialized data-movement and multiply-accumulate abstractions for 8-bit integer, half-precision floating point (FP16), single-precision floating point (FP32), and double-precision floating point (FP64) types. Furthermore, CUTLASS demonstrates CUDA's WMMA API for targeting the programmable, high-throughput Tensor Cores provided by NVIDIA's Volta architecture and beyond.

https://github.com/NVIDIA/cutlass[+https://github.com/NVIDIA/cutlass+]

https://github.com/NVIDIA/cutlass/blob/master/CUTLASS.md[+https://github.com/NVIDIA/cutlass/blob/master/CUTLASS.md+]

PSBLAS
^^^^^^

The major goal of the Parallel Sparse Basic Linear Algebra Subroutines (PSBLAS) project is to provide a framework to enable easy, efficient and portable implementations of iterative solvers for linear systems, while shielding the user from most details of their parallelization. The interface is designed keeping in view a Single Program Multiple Data programming model on distributed memory machines.

Parallel Sparse BLAS (PSBLAS) is a library of Basic Linear Algebra Subroutines for parallel sparse applications that facilitates the porting of complex computations on multicomputers. The project has been prompted by the appearance of a proposal for serial sparse BLAS that are flexible and powerful enough to be used as the building blocks of more complex applications, especially on parallel machines. The PSBLAS library includes routines for multiplying sparse matrices by dense matrices, solving sparse triangular systems, preprocessing sparse matrices and additional routines for dense matrix operations. The PSBLAS interface aims at the maximal flexibility with respect to the data distribution strategy. PSBLAS supports totally arbitrary data distributions under the control of the user. Moreover, it has also some predefined distributions that are popular choices for ScaLAPACK and HPF users. The current version of the PSBLAS library is implemented in Fortran 2003. The object-oriented features of the language enable a very convenient level of abstraction for the target applications of PSBLAS. The library can take care of runtime memory requirements that can be quite difficult or even impossible to predict at implementation or compilation time. Sparse iterative solvers are employed in many software systems covering a broad area of applications, such as the simulation of internal combustion engines, oil reservoirs, semiconductor devices, structural analysis, electromagnetic scattering. 

The tar files do not include the BLAS and METIS libraries, which can be obtained from their respective URLs. For the BLAS, if you do not have a vendor version, consider using ATLAS.

We provide extended storage formats. In this context we also support computations on GPU cards from NVIDIA[+We provide extended storage formats. In this context we also support computations on GPU cards from NVIDIA.

http://people.uniroma2.it/salvatore.filippone/psblas/[+http://people.uniroma2.it/salvatore.filippone/psblas/+]

https://github.com/sfilippone/psblas3[+https://github.com/sfilippone/psblas3+]

https://github.com/sfilippone/psblas3-ext[+https://github.com/sfilippone/psblas3-ext+]

https://dl.acm.org/citation.cfm?doid=365723.365732[+https://dl.acm.org/citation.cfm?doid=365723.365732+]

https://github.com/davidebarbieri/spgpu/[+https://github.com/davidebarbieri/spgpu/+]

ReproBLAS
^^^^^^^^^

ReproBLAS stands for Reproducible Basic Linear Algebra Subprograms.

By reproducibility, we mean getting bitwise identical results from multiple runs of a program on the same input. Floating point summation is not associative because of roundoff errors, so the computed sum depends on the order of summation. Modern processors, which may dynamically assign resources, such as variable numbers of processors, do not guarantee the same order of summation from run-to-run of the same program or subroutine. Reproducibility is needed for debugging, verification and validation, correctness, and even contractual obligations.

ReproBLAS aims at providing users with a set of (Parallel and Sequential) Basic Linear Algebra Subprograms that guarantee reproducibility regardless of the number of processors, of the data partitioning, of the way reductions are scheduled, and more generally of the order in which the sums are computed.

The BLAS are commonly used in scientific programs, and the reproducible versions provided in the ReproBLAS will provide high performance while reducing user effort for debugging, correctness checking and understanding the reliability of programs. 

http://bebop.cs.berkeley.edu/reproblas/[+http://bebop.cs.berkeley.edu/reproblas/+]

http://icl.utk.edu/bblas/[+http://icl.utk.edu/bblas/+]

https://github.com/peterahrens/ReproBLAS[+https://github.com/peterahrens/ReproBLAS+]

SYCL BLAS
^^^^^^^^^

SYCL BLAS implements BLAS - Basic Linear Algebra Subroutines - using SYCL 1.2, the Khronos abstraction layer for OpenCL.
It is written using modern Cxx. The current implementation uses Cxx11 features but we aim to move to Cxx14 in the short term. 

SYCL BLAS uses Cxx Expression Tree templates to generate SYCL Kernels via kernel composition. Expression Tree templates are a widely used technique to implement expressions on Cxx, that facilitate development and composition of operations. In particular, Kernel composition in SYCL has been used in various projects to create efficient domain specific embedded languages that enable users to easily fuse GPU kernels.

SYCL-BLAS is a header-only library. All the relevant files can be found in the include directory. There are four components in SYCL-BLAS, the View, the Operations, the Executors and the Interface itself.

https://github.com/codeplaysoftware/sycl-blas[+https://github.com/codeplaysoftware/sycl-blas+]

Blaze
~~~~~

Blaze translates a subset of modified NumPy and Pandas-like syntax to databases and other computing systems. Blaze allows Python users a familiar interface to query data living in other data storage systems.

Blaze is a high-level user interface for databases and array computing systems. It consists of the following components:

* A symbolic expression system to describe and reason about analytic queries
* A set of interpreters from that query system to various databases / computational engines

This architecture allows a single Blaze code to run against several computational backends. Blaze interacts rapidly with the user and only communicates with the database when necessary. Blaze is also able to analyze and optimize queries to improve the interactive experience.

The Blaze ecosystem is a set of libraries that help users store, describe, query and process data. It is composed of the following core projects:

* Blaze: An interface to query data on different storage systems
* Dask: Parallel computing through task scheduling and blocked algorithms
* Datashape: A data description language
* DyND: A Cxx library for dynamic, multidimensional arrays
* Odo: Data migration between different storage systems

https://github.com/blaze/blaze[+https://github.com/blaze/blaze+]

https://blaze.readthedocs.io/en/latest/index.html[+https://blaze.readthedocs.io/en/latest/index.html+]

http://blaze.pydata.org/[+http://blaze.pydata.org/+]

blaze_tensor
^^^^^^^^^^^^

This project implements 3D datastructures (tensors) that integrate well with the Blaze library.

All the highlights listed for Blaze apply to BlazeTensor as well:

* high performance through the integration of BLAS libraries and manually tuned HPC math kernels
* vectorization by SSE, SSE2, SSE3, SSSE3, SSE4, AVX, AVX2, AVX-512, FMA, and SVML
* parallel execution by OpenMP, HPX, Cxx11 threads and Boost threads
* the intuitive and easy to use API of a domain specific language
* unified arithmetic with dense 3D tensors
* thoroughly tested 3D tensor arithmetic
* completely portable, high quality Cxx source code

The implemented facilities are verified using a thorough testing environment. 

https://github.com/STEllAR-GROUP/blaze_tensor[+https://github.com/STEllAR-GROUP/blaze_tensor+]

BLOPEX
~~~~~~

Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) is a package, written in C and MATLAB/OCTAVE, that includes an eigensolver implemented with the Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG). Its main features are: a matrix-free iterative method for computing several extreme eigenpairs of symmetric positive generalized eigenproblems; a user-defined symmetric positive preconditioner; robustness with respect to random initial approximations, variable preconditioners, and ill-conditioning of the stiffness matrix; and apparently optimal convergence speed.

BLOPEX supports parallel MPI-based computations. BLOPEX is incorporated in the HYPRE package and is available as an external block to the SLEPc package. PHAML has an interface to call BLOPEX eigensolvers, as well as DevTools by Visual Kinematics.

This code does NOT compute ALL eigenvalues. It is designed to compute no more than ~20%, i.e., if the matrix size is 100, do NOT attempt to compute more that ~20 eigenpairs, otherwise, the code will crash. 

https://bitbucket.org/joseroman/blopex[+https://bitbucket.org/joseroman/blopex+]

BOAST
~~~~~

A metaprogramming framework dedicated to computing kernels. BOAST allows the description of a kernel and its possible optimizations using a domain-specific language. BOAST runtime will then compare the different versions’performance as well as verify their exactness. BOAST is applied to three use cases: a Laplace kernel in OpenCL and two HPC applications BigDFT (electronic density computation) and SPECFEM3D (seismic and wave propagation).

https://github.com/Nanosim-LIG/boast[+https://github.com/Nanosim-LIG/boast+]

https://journals-sagepub-com/doi/full/10.1177/1094342017718068[+https://journals-sagepub-com/doi/full/10.1177/1094342017718068+]

Bob
~~~

Bob is a free signal-processing and machine learning toolbox originally developed by the Biometrics group at Idiap Research Institute, Switzerland.

The toolbox is written in a mix of Python and Cxx and is designed to be both efficient and reduce development time. It is composed of a reasonably large number of packages that implement tools for image, audio & video processing, machine learning & pattern recognition, and a lot more task specific packages.

The fundamental data structure of Bob is a multi-dimensional array. In signal- processing and machine learning, arrays are a suitable representation for many different types of digital signals such as images, audio data and extracted features. Python is the working environment selected for this library and so when using Python we have relied on the existing NumPy multi- dimensional arrays numpy.ndarray. This provides with greater flexibility within the Python environment.

At the Cxx level, the Blitz++ library is used to handle arrays. Bob provides internal conversion routines to transparently and efficiently convert NumPy ndarrays to/from Blitz++. As they are done implicitly, the user has no need to care about this aspect and should just use NumPy ndarrays everywhere while inside Python code.

The default way to read and write data from and to files with Bob is using the binary HDF5 format which has several tools to inspect those files. Bob’s support for HDF5 files is given through the Bob’s Core I/O Routines package.

On the other hand, loading and writing of different kinds of data is provided in other Packages of Bob using a plug-in strategy. Many image types can be read using Bob’s I/O Routines for Images of Various type, and many video codecs are supported through the Bob’s Video I/O Routines plug-in. Also, a comprehensive support for MatLab files is given through the Matlab(R) I/O Support for Bob interface.

https://gitlab.idiap.ch/bob/bob/wikis/Home[+https://gitlab.idiap.ch/bob/bob/wikis/Home+]

https://www.idiap.ch/software/bob/[+https://www.idiap.ch/software/bob/+]

https://github.com/bioidiap/bob[+https://github.com/bioidiap/bob+]

Bohrium
~~~~~~~

Bohrium provides automatic acceleration of array operations in Python/NumPy, C, and Cxx targeting multi-core CPUs and GP-GPUs. Forget handcrafting CUDA/OpenCL to utilize your GPU and forget threading, mutexes and locks to utilize your multi-core CPU, just use Bohrium!

The features include:

* Lazy Evaluation, Bohrium will lazy evaluate all Python/NumPy operations until it encounters a “Python Read” such a printing an array or having a if-statement testing the value of an array.
* Views Bohrium supports NumPy views fully thus operating on array slices does not involve data copying.
* Loop Fusion, Bohrium uses a fusion algorithm that fuses (or merges) array operations into the same computation kernel that are then JIT-compiled and executed. However, Bohrium can only fuse operations that have some common sized dimension and no horizontal data conflicts.
* Lazy CPU/GPU Communication, Bohrium only moves data between the host and the GPU when the data is accessed directly by Python or a Python C-extension.
* python -m bohrium, automatically makes import numpy use Bohrium.
* Jupyter Support, you can use the magic command %%bohrium to automatically use Bohrium as NumPy.

https://bohrium.readthedocs.io/[+https://bohrium.readthedocs.io/+]

https://github.com/bh107/bohrium[+https://github.com/bh107/bohrium+]

BOLT
~~~~

BOLT is a recursive acronym that stands for “BOLT is OpenMP over Lightweight Threads”.

BOLT targets a high-performing OpenMP implementation, especially specialized for fine-grain parallelism. Unlike other OpenMP implementations, BOLT utilizes a lightweight threading model for its underlying threading mechanism. It currently adopts Argobots, a new holistic, low-level threading and tasking runtime, in order to overcome shortcomings of conventional OS-level threads. The current BOLT implementation is based on the OpenMP runtime in LLVM, and thus it can be used with LLVM/Clang, Intel OpenMP compiler, and GCC.

https://press3.mcs.anl.gov/bolt/[+https://press3.mcs.anl.gov/bolt/+]

https://github.com/pmodels/bolt[+https://github.com/pmodels/bolt+]

https://github.com/llvm-mirror/openmp[+https://github.com/llvm-mirror/openmp+]

https://press3.mcs.anl.gov/argobots/[+https://press3.mcs.anl.gov/argobots/+]

bolt
~~~~

Multidimensional arrays are core to a wide variety of applications. Some are well-suited to single machines, especially when datasets fit in memory. Others can benefit from distributed computing, especially for out-of-memory datasets and complex workflows.

Bolt is an Python project designed to faciliate working with ndarrays in local and distributed settings. It exposes array operations through either local or distributed implementations, and makes it easy to switch between them. The distributed implementation is currently backed by Spark, but can support other backends in the future. The distributed operations leverage efficient data structures for multidimensional array manipulation, and aim to cover most of NumPy’s ndarray interface in a distributed setting.

http://bolt-project.github.io/[+http://bolt-project.github.io/+]

https://github.com/bolt-project/bolt[+https://github.com/bolt-project/bolt+]

BorgBackup
~~~~~~~~~~

BorgBackup (short: Borg) is a deduplicating backup program. Optionally, it supports compression and authenticated encryption.

The main goal of Borg is to provide an efficient and secure way to backup data. The data deduplication technique used makes Borg suitable for daily backups since only changes are stored. The authenticated encryption technique makes it suitable for backups to not fully trusted targets.

Deduplication based on content-defined chunking is used to reduce the number of bytes stored: each file is split into a number of variable length chunks and only chunks that have never been seen before are added to the repository.
To deduplicate, all the chunks in the same repository are considered, no matter whether they come from different machines, from previous backups, from the same backup or even from the same single file.

All data can be protected using 256-bit AES encryption, data integrity and authenticity is verified using HMAC-SHA256. Data is encrypted clientside.

https://borgbackup.readthedocs.io/en/stable/[+https://borgbackup.readthedocs.io/en/stable/+]

https://github.com/borgbackup/borg[+https://github.com/borgbackup/borg+]

BPAS
~~~~

The Basic Polynomial Algebra Subprograms (BPAS) library provides support for arithmetic operations with polynomials on modern computer architectures, in particular hardware accelerators. Typical operations are polynomial multiplication, multi-point evaluation and interpolation, real root isolation for both univariate and multivariate systems. Its code is written in C++ with CilkPlus extension targeting multi-core processors. 

Polynomial multiplication and matrix multiplication are at the core of many algorithms in symbolic computation.
Algebraic complexity is often estimated in terms of multiplication time. At the software level, this reduction to multiplication is also common (Magma, NTL, FLINT, ...).
BPAS design follows the principle reducing everything to multiplication.

http://www.bpaslib.org/[+http://www.bpaslib.org/+]

https://arxiv.org/abs/1811.01490[+https://arxiv.org/abs/1811.01490+]

Brainfuck
~~~~~~~~~

Brainfuck is an esoteric programming language created in 1993 by Urban Müller, and notable for its extreme minimalism.

The language consists of only eight simple commands and an instruction pointer. While it is fully Turing complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps.

The language's name is a reference to the slang term brainfuck, which refers to things so complicated or unusual that they exceed the limits of one's understanding. 

The language consists of eight commands, listed below. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.

The brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as an array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).

As the name suggests, Brainfuck programs tend to be difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands, and partly it is because the program's text gives no direct indications of the program's state. These, as well as Brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing complete language, Brainfuck is theoretically capable of computing any computable function or simulating any other computational model, if given access to an unlimited amount of memory. A variety of Brainfuck programs have been written. Although Brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for Brainfuck in a more typical language such as C due to its simplicity. There even exist Brainfuck interpreters written in the Brainfuck language itself.

Brainfuck is an example of a so-called Turing tarpit: It can be used to write any program, but it is not practical to do so, because Brainfuck provides so little abstraction that the programs get very long or complicated. 

https://en.wikipedia.org/wiki/Brainfuck[+https://en.wikipedia.org/wiki/Brainfuck+]

https://esolangs.org/wiki/brainfuck[+https://esolangs.org/wiki/brainfuck+]

Brave
~~~~~

Brave is a free and open-source web browser developed by Brave Software Inc. based on the Chromium web browser.[6] The browser blocks ads and website trackers. In a future version of the browser, the company intends to adopt a pay-to-surf business model. 

Brave Software has announced that it is developing a feature allowing users to opt in to receiving ads sold by the company in place of ads blocked by the browser.[8][9][10] Brave intends to pay content publishers 55% of the replaced ad revenue. Brave Software, ad partners, and browser users would each be allocated 15% of the revenue. Users would be able to donate their revenue share to content publishers through micropayments.[11]

In a testing version of the browser, Brave targets web ads by analyzing users' anonymized browsing history.

The 'Basic Attention Token' (BAT) is an open-source, decentralized ad exchange platform based on Ethereum.[24] The platform is integrated with the Brave web browser, it is not possible to use or access the platform from any other browser. Brave Payments, which formerly used Bitcoin, allows users to tip websites and content creators (like YouTubers and Twitch streamers)[25] with BAT tokens, akin to patronage services like Patreon.[26]

Integration of BAT into an application involves implementing BAT Ads, a system that displays ads to users based on locally stored data. Ad targeting is performed locally, removing the need for third-party tracking.

https://en.wikipedia.org/wiki/Brave_(web_browser)[+https://en.wikipedia.org/wiki/Brave_(web_browser)+]

https://brave.com/[+https://brave.com/+]

https://scottlocklin.wordpress.com/2018/05/24/i-dont-want-to-work-on-your-shitty-blockchain-project-especially-you-facebook/[+https://scottlocklin.wordpress.com/2018/05/24/i-dont-want-to-work-on-your-shitty-blockchain-project-especially-you-facebook/+]

Browserify
~~~~~~~~~~

Browserify is an open-source JavaScript tool that allows developers to write Node.js-style modules that compile for use in the browser.[1]

Browserify lets you use require in the browser, the same way you'd use it in Node. It's not just syntactic sugar for loading scripts on the client. It's a tool that brings all the resources of the NPM ecosystem off of the server, and into the client. 

The features include:

* Use many of the tens of thousands of modules on NPM in the browser
* Use watchify, a browserify compatible caching bundler, for super-fast bundle rebuilds as you develop.
* Use tinyify for optimized, tree-shaked bundles in production environments.
* Use --debug when creating bundles to have Browserify automatically include Source Maps for easy debugging.
* Check out tools like Beefy or run-browser which make automating browserify development easier.
* process.nextTick(), __dirname, and __filename node-isms work
* Get browser versions of the node core libraries events, stream, path, url, assert, buffer, util, querystring, http, vm, and crypto when you require() them 

http://browserify.org/[+http://browserify.org/+]

https://github.com/browserify[+https://github.com/browserify+]

https://github.com/browserify/browserify-handbook[+https://github.com/browserify/browserify-handbook+]

http://didact.us/beefy/[+http://didact.us/beefy/+]

BSD
~~~

Blah.

ElectroBSD
^^^^^^^^^^

The Electro Beer Software Distribution (ElectroBSD) is an experimental operating system designed to be used in hostile environments like Germany.
ElectroBSD is (supposed to be) free software but due to unresolved license issues it is currently only released as patchset against the non-free uptream. 

Electro beer is a dual-use technology commonly used to resist threats like the computer police ("Computerbullen"). It's also used by the computer police itself (to wash down transistor doughnuts). 

ElectroBSD is based on FreeBSD but compiled with KERNCONF=ELECTRO_BEER (alcohol-free as in no alcohol) and a couple of customizations. The goal is to make free penetration tests from the computer police slightly less annoying for the test subjects and more expensive for the police (ElectroBSD experts unfortunately aren't cheap). 

https://www.fabiankeil.de/gehacktes/electrobsd/[+https://www.fabiankeil.de/gehacktes/electrobsd/+]

https://archive.fosdem.org/2016/schedule/event/electrobsd/[+https://archive.fosdem.org/2016/schedule/event/electrobsd/+]

Buildah
~~~~~~~

A tool that facilitates building Open Container Initiative (OCI) container images.  It can
be used to:

* create a working container, either from scratch or using an image as a starting point
* create an image, either from a working container or via the instructions in a Dockerfile
* images can be built in either the OCI image format or the traditional upstream docker image format
* mount a working container's root filesystem for manipulation
* unmount a working container's root filesystem
* use the updated contents of a container's root filesystem as a filesystem layer to create a new image
* delete a working container or an image
* rename a local container

https://buildah.io/[+https://buildah.io/+]

https://github.com/containers/buildah[+https://github.com/containers/buildah+]

https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons/[+https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons/+]

https://github.com/containers/libpod[+https://github.com/containers/libpod+]

Buildroot
~~~~~~~~~

Buildroot is a set of Makefiles and patches that simplifies and automates the process of building a complete and bootable Linux environment for an embedded system, while using cross-compilation to allow building for multiple target platforms on a single Linux-based development system. Buildroot can automatically build the required cross-compilation toolchain, create a root file system, compile a Linux kernel image, and generate a boot loader for the targeted embedded system, or it can perform any independent combination of these steps. For example, an already installed cross-compilation toolchain can be used independently, while Buildroot only creates the root file system.

Buildroot is primarily intended to be used with small or embedded systems based on various computer architectures and instruction set architectures (ISAs), including x86, ARM, MIPS and PowerPC. Numerous architectures and their variants are supported; Buildroot also comes with default configurations for several off-the-shelf available embedded boards, such as Cubieboard, Raspberry Pi and SheevaPlug. Several third-party projects and products use Buildroot as the basis for their build systems, including the OpenWrt project that creates an embedded operating system, and firmware for the customer-premises equipment (CPE) used by the Google Fiber broadband service.

Multiple C standard libraries are supported as part of the toolchain, including the GNU C Library, uClibc and musl, as well as the C standard libraries that belong to various preconfigured development environments, such as those provided by Linaro. Buildroot's build configuration system internally uses Kconfig, which provides features such as a menu-driven interface, handling of dependencies, and contextual help; Kconfig is also used by the Linux kernel for its source-level configuration. Buildroot is organized around numerous automatically downloaded packages, which contain the source code of various userspace applications, system utilities, and libraries. Root file system images, which are the final results, may be built using various file systems, including cramfs, JFFS2, romfs, SquashFS and UBIFS.

https://buildroot.org/[+https://buildroot.org/+]

https://en.wikipedia.org/wiki/Buildroot[+https://en.wikipedia.org/wiki/Buildroot+]

Bullet
~~~~~~

Bullet is a physics engine which simulates collision detection, soft and rigid body dynamics. It has been used in video games as well as for visual effects in movies.

The features include:

* Rigid body and soft body simulation with discrete and continuous collision detection
* Collision shapes include: sphere, box, cylinder, cone, convex hull using GJK, non-convex and triangle mesh
* Soft body support: cloth, rope and deformable objects
* A rich set of rigid body and soft body constraints with constraint limits and motors
* Plugins for Maya, Softimage, integrated into Houdini, Cinema 4D, LightWave 3D and Blender and import of COLLADA 1.4 physics content
* Optional optimizations for PlayStation 3 Cell SPU, CUDA and OpenCL

The Bullet website also hosts a Physics Forum for general discussion around Physics Simulation for Games and Animation. 

https://pybullet.org/wordpress/[+https://pybullet.org/wordpress/+]

https://github.com/bulletphysics/bullet3[+https://github.com/bulletphysics/bullet3+]

https://en.wikipedia.org/wiki/Bullet_(software)[+https://en.wikipedia.org/wiki/Bullet_(software)+]

NTRT
^^^^

Tensegrity Robots are a biologically inspired approach to building robots based on the tension networks of tensegrity structures, which have no rigid connections between elements. The NTRT was created to enable: the rapid co-exploration of structures and controls in a physics based simulation environment; the development of tensegrity robotics algorithms such as structural analysis, kinematics, and motion planning; and the validation of the algorithms and controls on hardware prototypes of the tensegrity robots.

The NTRT Simulator is a tensegrity-specific simulator built to run ontop of the Bullet Physics Engine, version 2.82. The NTRTsim includes a set of builder tools for specifying rods and strings as a set of points in Cartesian coordinates. Structures built out of these rods and strings can be specified as a tree of substructures, and can be rotated and moved, which greatly simplifies the task of creating new tensegrity structures. The NTRTsim also includes libraries for controllers such as Central Pattern Generators and a machine learning framework, which allows users to specify their own learning algorithms. For strings, instead of the default Bullet softbodies, which are not physically accurate, we used a two point linear string model using Hooke's law forces with a linear damping term. We also have a contact dynamics module for the cables, allowing them to interact with the structure and the environment. Finally, terrains can be created, and the performance of the controller can be tested as the tensegrity robot moves through the simulated world.

https://github.com/NASA-Tensegrity-Robotics-Toolkit/NTRTsim[+https://github.com/NASA-Tensegrity-Robotics-Toolkit/NTRTsim+]

https://github.com/bulletphysics/bullet3/releases/tag/2.82[+https://github.com/bulletphysics/bullet3/releases/tag/2.82+]

Byfl
~~~~

A program analysis tool based on software performance counters.
Byfl helps application developers understand code performance in a hardware-independent way. The idea is that it instruments your code at compile time then gathers and reports data at run time.

Byfl doesn't do source-to-source transformations (unlike, for example, ROSE).
 Instead, it integrates into the LLVM compiler infrastructure as an LLVM compiler pass. Because Byfl instruments code in LLVM's intermediate representation (IR), not native machine code, it outputs the same counter values regardless of target architecture. In contrast, binary-instrumentation tools such as Pin may tally operations differently on different platforms.

https://github.com/lanl/Byfl[+https://github.com/lanl/Byfl+]

https://www.lanl.gov/software/open-source-software.php[+https://www.lanl.gov/software/open-source-software.php+]

#CCCC

Cadabra
~~~~~~~

A computer algebra system (CAS) designed specifically for the solution of
problems encountered in field theory. It has extensive functionality for
tensor computer algebra, tensor polynomial simplification including multi-term
symmetries, fermions and anti-commuting variables, Clifford algebras and Fierz
transformations, implicit coordinate dependence, multiple index types and many
more. The input format is a subset of TeX. Both a command-line and a graphical
interface are available.

http://cadabra.phi-sci.com/[+http://cadabra.phi-sci.com/+]

http://cadabra.phi-sci.com/cadabra_hep.pdf[+http://cadabra.phi-sci.com/cadabra_hep.pdf+]

https://cadabra.science/[+https://cadabra.science/+]

https://github.com/kpeeters/cadabra2[+https://github.com/kpeeters/cadabra2+]

http://joss.theoj.org/papers/50097b222eb0c6fed339e8a24196dd75[+http://joss.theoj.org/papers/50097b222eb0c6fed339e8a24196dd75+]

Caffe2
~~~~~~

Caffe2 is a deep learning framework that provides an easy and straightforward way for you to experiment with deep learning and leverage community contributions of new models and algorithms. You can bring your creations to scale using the power of GPUs in the cloud or to the masses on mobile with Caffe2’s cross-platform libraries.

The original Caffe framework was useful for large-scale product use cases, especially with its unparalleled performance and well tested Cxx codebase. Caffe has some design choices that are inherited from its original use case: conventional CNN applications. As new computation patterns have emerged, especially distributed computation, mobile, reduced precision computation, and more non-vision use cases, its design has shown some limitations.

Caffe2 improves Caffe 1.0 in a series of directions:

* first-class support for large-scale distributed training
* mobile deployment
* new hardware support (in addition to CPU and CUDA)
* flexibility for future directions such as quantized computation
* stress tested by the vast scale of Facebook applications

One of the basic units of computation in Caffe2 are the Operators. You can think of these as a more flexible version of the layers from Caffe. Caffe2 comes with over 400 different operators and provides guidance for the community to create and contribute to this growing resource.

https://caffe2.ai/[+https://caffe2.ai/+]

https://github.com/pytorch/pytorch/[+https://github.com/pytorch/pytorch/+]

https://github.com/BVLC/caffe[+https://github.com/BVLC/caffe+]

HWGQ
^^^^

Caffe implementation of accurate low-precision neural networks.
HWGQ-Net is a low-precision neural network with 1-bit binary weights and 2-bit quantized activations. It can be applied to many popular network architectures, including AlexNet, ResNet, GoogLeNet, VggNet, and achieves closer performance to the corresponding full-precision networks than previously available low-precision networks. Theorectically, HWGQ-Net has ~32x memory and ~32x convoluational computation savings, suggesting that it can be very useful for the deployment of state-of-the-art neural networks in real world applications. 

https://github.com/zhaoweicai/hwgq[+https://github.com/zhaoweicai/hwgq+]

https://arxiv.org/abs/1702.00953[+https://arxiv.org/abs/1702.00953+]

Calibre
~~~~~~~

Calibre (stylised calibre) is a cross-platform open-source suite of e-book software. Calibre supports organizing existing e-books into virtual libraries, displaying, editing, creating and converting e-books, as well as syncing e-books with a variety of e-readers. Editing books is supported for EPUB and AZW3 formats. Books in other formats like MOBI must first be converted to those formats, if they are to be edited. 

Calibre supports many file formats and reading devices. Most e-book formats can be edited, for example, by changing the font, font size, margins, and metadata, and by adding an auto-generated table of contents. Conversion and editing are easily applied to appropriately licensed digital books, but commercially purchased e-books may need to have digital rights management (DRM) restrictions removed. Calibre does not natively support DRM removal, but may allow DRM removal after installing plug-ins with such a function.[4][5]

Calibre allows users to sort and group e-books by metadata fields. Metadata can be pulled from many different sources, e.g., ISBNdb.com; online booksellers; and providers of free e-books and periodicals in the US and elsewhere, such as the Internet Archive, Munsey's Magazine, and Project Gutenberg; and social networking sites for readers, such as Goodreads and LibraryThing). It is possible to search the Calibre library by various fields, such as author, title, or keyword; as of 2016, full-text search was unimplemented.[6][7]

E-books can be imported into the Calibre library, either by sideloading files manually or by wirelessly syncing an e-book reading device with the cloud storage service in which the Calibre library is backed up, or with the computer on which Calibre resides. Also, online content-sources can be harvested and converted to e-books. This conversion is facilitated by so-called recipes, short programs written in a Python-based domain-specific language. E-books can then be exported to all supported reading devices via USB, Calibre's integrated mail server, or wirelessly. Mailing e-books enables, for example, sending personal documents to the Amazon Kindle family of e-readers and tablet computers.

https://github.com/kovidgoyal/calibre[+https://github.com/kovidgoyal/calibre+]

https://calibre-ebook.com/[+https://calibre-ebook.com/+]

https://en.wikipedia.org/wiki/Calibre_(software)[+https://en.wikipedia.org/wiki/Calibre_(software)+]

CamFort
~~~~~~~

CamFort is a multi-feature tool for improving the quality of Fortran code. Its features are primarily aimed at programming patterns found in numerical modelling code e.g., in computational science.

CamFort is free and open-source. It currently supports Fortran 66, 77, 90, and 95 language standards. Support for Fortran 2003, and 2008 is in progress.

CamFort provides lightweight verification features. Source-code annotations (comments) provide specifications of certain aspects of a program's meaning or behaviour. CamFort can then check that code conforms to these specifications. CamFort can also suggest places to insert specifications, and in some cases case infer the specifications of existing code.

Our current specification and verification features provide:

* Units-of-measure typing allows you to annotate Fortran source code with units of variables and can automatically check whether units are consistently used and report back where it went wrong if they are inconsistent.

* Array access shape allows you to describe and verify the access patterns your code makes over arrays in order to catch array indexing errors.

Many language features of older Fortran standards (pre Fortran 90) are known to be a ready source of programming error. CamFort provides some facilities for automatically refactoring deprecated or dangerous programming patterns, with the goal of helping to meet core quality requirements, such as maintainability. For example, our tool eliminates EQUIVALENCE and COMMON blocks. These refactorings also helps to expose any programming bugs arising from bad programming practices. 

https://camfort.github.io/[+https://camfort.github.io/+]

https://github.com/camfort/camfort[+https://github.com/camfort/camfort+]

https://www.cl.cam.ac.uk/\~acr31/pubs/orchard-camfort.pdf[+https://www.cl.cam.ac.uk/~acr31/pubs/orchard-camfort.pdf+]

http://www.fortran.bcs.org/2017/fortran_verification_cam.pdf[+http://www.fortran.bcs.org/2017/fortran_verification_cam.pdf+]

camiba
~~~~~~

We aim at providing a collection of non tested but heavily used algorithms, which revolve around compressed sensing and sparse recovery in the widest sense.

Camiba provides methods for:

* sparse recovery
* sensing matrix design
* performance metric estimation
* an abstract wrapper to describe CS scenarios
* methods for sparsity order estimation

https://sebastiansemper.github.io/camiba/[+https://sebastiansemper.github.io/camiba/+]

https://github.com/SebastianSemper/camiba[+https://github.com/SebastianSemper/camiba+]

Cantera
~~~~~~~

Cantera is an open-source suite of tools for problems involving chemical kinetics, thermodynamics, and transport processes.

Cantera automates the chemical kinetic, thermodynamic, and transport calculations so that the users can efficiently incorporate detailed chemical thermo-kinetics and transport models into their calculations. 

Cantera utilizes object-oriented concepts for robust yet flexible phase models, and algorithms are generalized so that users can explore different phase models with minimal changes to their overall code. 

Cantera can be used from Python and Matlab, or in applications written in C/Cxx and Fortran 90. 

https://cantera.org/[+https://cantera.org/+]

capnproto
~~~~~~~~~

Cap’n Proto is an insanely fast data interchange format and capability-based RPC system. Think JSON, except binary. Or think Protocol Buffers, except faster. In fact, in benchmarks, Cap’n Proto is INFINITY TIMES faster than Protocol Buffers.

This benchmark is, of course, unfair. It is only measuring the time to encode and decode a message in memory. Cap’n Proto gets a perfect score because there is no encoding/decoding step. The Cap’n Proto encoding is appropriate both as a data interchange format and an in-memory representation, so once your structure is built, you can simply write the bytes straight out to disk!

The encoding is defined byte-for-byte independent of any platform. However, it is designed to be efficiently manipulated on common modern CPUs. Data is arranged like a compiler would arrange a struct – with fixed widths, fixed offsets, and proper alignment. Variable-sized elements are embedded as pointers. Pointers are offset-based rather than absolute so that messages are position-independent. Integers use little-endian byte order because most CPUs are little-endian, and even big-endian CPUs usually have instructions for reading little-endian data.

https://capnproto.org/[+https://capnproto.org/+]

Cartagen
~~~~~~~~

Cartagen is a vector-based, client-side framework for rendering maps in native HTML 5. Maps are styled in GSS, a cascading stylesheet specification for geospatial information – a decision which leverages literacy in CSS to make map styling more accessible. 

https://github.com/jywarren/cartagen/[+https://github.com/jywarren/cartagen/+]

http://cartagen.org/[+http://cartagen.org/+]

CARTO
~~~~~

CARTO is an open, powerful, and intuitive platform for discovering and predicting the key insights underlying the location data in our world.

Empower organizations to optimize operational performance, strategic investments, and everyday decisions with CARTO Engine—our embeddable platform for web and mobile apps—and the new CARTO Builder, a drag and drop analysis tool.

It was built to make it easier for people to tell their stories by providing them with flexible and intuitive ways to create maps and design geospatial applications. CARTO can be installed on your own server.

With CARTO, you can upload your geospatial data (Shapefiles, GeoJSON, etc) using a web form and then make it public or private.

After it is uploaded, you can visualize it in a dataset or on a map, search it using SQL, and apply map styles using CartoCSS. You can even access it using the CARTO APIs, or export it to a file.

In other words, with CARTO you can make awesome maps and build powerful geospatial applications.

https://github.com/CartoDB/cartodb[+https://github.com/CartoDB/cartodb+]

CARTOframes
^^^^^^^^^^^

A Python package for integrating CARTO maps, analysis, and data services into data science workflows.

Python data analysis workflows often rely on the de facto standards pandas and Jupyter notebooks. Integrating CARTO into this workflow saves data scientists time and energy by not having to export datasets as files or retain multiple copies of the data. Instead, CARTOframes give the ability to communicate reproducible analysis while providing the ability to gain from CARTO's services like hosted, dynamic or static maps and Data Observatory augmentation.

The features include:

* Write pandas DataFrames to CARTO tables
* Read CARTO tables and queries into pandas DataFrames
* Create customizable, interactive CARTO maps in a Jupyter notebook
* Interact with CARTO's Data Observatory
* Use CARTO's spatially-enabled database for analysis
* Try it out without needing a CARTO account by using the Examples functionality

Common usage includes:

* Visualize spatial data programmatically as matplotlib images or embedded interactive maps
* Perform cloud-based spatial data processing using CARTO's analysis tools
* Extract, transform, and Load (ETL) data using the Python ecosystem for getting data into and out of CARTO
* Data Services integrations using CARTO's Data Observatory 

https://cartoframes.readthedocs.io/en/latest/[+https://cartoframes.readthedocs.io/en/latest/+]

https://github.com/CartoDB/cartoframes[+https://github.com/CartoDB/cartoframes+]

carto-python
^^^^^^^^^^^^

carto-python is a full, backwards incompatible rewrite of the deprecated cartodb-python SDK. Since the initial rewrite, carto-python has been loaded with a lot of new features, not present in old cartodb-python.

https://github.com/CartoDB/carto-python[+https://github.com/CartoDB/carto-python+]

Cartopy
~~~~~~~

Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.

Cartopy makes use of the powerful PROJ.4, NumPy and Shapely libraries and includes a programmatic interface built on top of Matplotlib for the creation of publication quality maps.

Key features of cartopy are its object oriented projection definitions, and its ability to transform points, lines, vectors, polygons and images between those projections.

You will find cartopy especially useful for large area / small scale data, where Cartesian assumptions of spherical data traditionally break down. If you’ve ever experienced a singularity at the pole or a cut-off at the dateline, it is likely you will appreciate cartopy’s unique features.

The cartopy.crs.CRS class is the very core of cartopy, all coordinate reference systems in cartopy have CRS as a parent class, meaning all projections have the same interface.

Cartopy has exposed an interface to enable easy map creation using matplotlib. Creating a basic map is as simple as telling Matplotlib to use a specific map projection, and then adding some coastlines to the axes.

https://scitools.org.uk/cartopy/docs/latest/[+https://scitools.org.uk/cartopy/docs/latest/+]

https://github.com/SciTools/cartopy[+https://github.com/SciTools/cartopy+]

https://uoftcoders.github.io/studyGroup/lessons/python/cartography/lesson/[+https://uoftcoders.github.io/studyGroup/lessons/python/cartography/lesson/+]

https://github.com/SciTools/cartopy-tutorial[+https://github.com/SciTools/cartopy-tutorial+]

CartoType Maps
~~~~~~~~~~~~~~

CartoType Maps is a multi-platform desktop application for viewing maps, calculating routes, finding places, creating map images, and adding your own map data. It demonstrates the main features of the CartoType library. It comes with some sample maps. If you like, you can download extra maps, or make your own. We also offer a custom map creation service. Please contact us to discuss your requirements.

The current version was built from CartoType 5.4.120 or later. The source code of the Maps App is available under the MIT license. It's in the CartoType public repository.

The Maps App supports graphics accelerated rendering, which is also now available as an integral part of the CartoType SDK. You can turn graphics acceleration on and off using the View menu. To see 3D buildings, load the map of Manhattan (manhattan.ctm1), turn both graphics acceleration and perspective mode on, and zoom in until the buildings appear. You can rotate the map using the rotator tool in the top left corner, and in perspective mode you can adjust the perspective angle from 20 degrees to 90 degrees (overhead view) using the slider just below the rotator.

http://www.cartotype.com/the-maps-app[+http://www.cartotype.com/the-maps-app+]

https://github.com/CartoType/[+https://github.com/CartoType/+]

http://www.cartotype.com/developers/documentation/50-how-to-create-map-files-for-cartotype-ctm1-files[+http://www.cartotype.com/developers/documentation/50-how-to-create-map-files-for-cartotype-ctm1-files+]

http://www.cartotype.com/assets/downloads/maps/[+http://www.cartotype.com/assets/downloads/maps/+]

CasADi
~~~~~~

CasADi is an open-source tool for nonlinear optimization and algorithmic differentiation.
It facilitates rapid — yet efficient — implementation of different methods for numerical optimal control, both in an offline context and for nonlinear model predictive control (NMPC).

CasADi's backbone is a symbolic framework implementing forward and reverse mode of AD on expression graphs to construct gradients, large-and-sparse Jacobians and Hessians. These expression graphs, encapsulated in Function objects, can be evaluated in a virtual machine or be exported to stand-alone C code. 

Initial value problems in ordinary or differential-algebraic equations (ODE/DAE) can be calculated using explicit or implicit Runge-Kutta methods or interfaces to IDAS/CVODES from the SUNDIALS suite. Derivatives are calculated using sensitivity equations, up to arbitrary order.

Nonlinear programs (NLPs), possibly with integer variables (MINLP), can be solved using block structure or general sparsity exploiting sequential quadratic programming (SQP) or interfaces to IPOPT/BONMIN, BlockSQP, WORHP, KNITRO and SNOPT. Solution sensitivities, up to arbitrary order, can be calculated analytically. Quadratic programs (QPs), possibly with integer variables (MIQP), can be solved using a primal-dual active-set method [3] or interfaces to CPLEX, GUROBI, HPMPC, OOQP or qpOASES.

CasADi offers a rich set of differentiable operations for its matrix-valued expression graphs, including common matrix-valued operations, serial or parallel function calls, implicit functions, integrators, spline-based lookup tables, and external codes.
These building blocks allow the user to code a wide variety of optimal control problem (OCP) formulations.

https://web.casadi.org/[+https://web.casadi.org/+]

https://github.com/casadi/casadi[+https://github.com/casadi/casadi+]

http://www.optimization-online.org/DB_HTML/2018/01/6420.html[+http://www.optimization-online.org/DB_HTML/2018/01/6420.html+]

CASCLIK
^^^^^^^

CasADi-based closed-loop inverse kinematics.
This is a library for rapid prototyping of constraint-based, task-priority closed-loop inverse kinematics. In essence this means you feed it an arbitrary expression with robot variables, inputs, virtual variables, and the controllers try to give you exponential convergence to the desired expression.

https://github.com/mahaarbo/casclik[+https://github.com/mahaarbo/casclik+]

https://arxiv.org/abs/1901.06713[+https://arxiv.org/abs/1901.06713+]

MPCTools
^^^^^^^^

Nonlinear model predictive tools for CasADi.

https://bitbucket.org/rawlings-group/mpc-tools-casadi[+https://bitbucket.org/rawlings-group/mpc-tools-casadi+]

omg-tools
^^^^^^^^^

Optimal Motion Generation-tools is a Python software toolbox facilitating the modeling, simulation and embedding of motion planning problems. Its main goal is to collect research topics concerning (spline-based) motion planning into a user-friendly package in order to enlarge its visibility towards the scientific and industrial world.

https://github.com/meco-group/omg-tools[+https://github.com/meco-group/omg-tools+]

RTC-Tools
^^^^^^^^^

The Deltares toolbox for control and optimization of environmental systems.

https://gitlab.com/deltares/rtc-tools[+https://gitlab.com/deltares/rtc-tools+]

https://rtc-tools.readthedocs.io/en/latest/[+https://rtc-tools.readthedocs.io/en/latest/+]

casync
~~~~~~

A tool for distributing file system images.
casync takes inspiration from the popular rsync file synchronization tool as well as the probably even more popular git revision control system. It combines the idea of the rsync algorithm with the idea of git-style content-addressable file systems, and creates a new system for efficiently storing and delivering file system images, optimized for high-frequency update cycles over the Internet. Its current focus is on delivering IoT, container, VM, application, portable service or OS images.

The key parts of casync are:

* While encoding, casync takes a large linear data stream, splits it into variable-sized chunks (the size of each being a function of the chunk's contents), and stores these chunks in individual, compressed files in some directory, each file named after a strong hash value of its contents, so that the hash value may be used to as key for retrieving the full chunk data.
Let's call this directory a "chunk store". At the same time, generate a "chunk index" file that lists these chunk hash values plus their respective chunk sizes in a simple linear array. 

* When decoding, casync takes the chunk index file, and reassembles the large linear data stream by concatenating the uncompressed chunks retrieved from the chunk store, keyed by the listed chunk hash values.

* Introduce a well-defined, reproducible, random-access serialization format for file trees (think: a more modern tar), to permit efficient, stable storage of complete file trees in the system, simply by serializing them and then passing them into the encoding step explained above.

* Put all this on the network: for each image you want to deliver, generate a chunk index file and place it on an HTTP server. Do the same with the chunk store, and share it between the various index files you intend to deliver.

Streams with similar contents will result in mostly the same chunk files in the chunk store. This means it is very efficient to store many related versions of a data stream in the same chunk store, thus minimizing disk usage. Moreover, when transferring linear data streams chunks already known on the receiving side can be made use of, thus minimizing network traffic.

https://github.com/systemd/casync[+https://github.com/systemd/casync+]

http://0pointer.net/blog/casync-a-tool-for-distributing-file-system-images.html[+http://0pointer.net/blog/casync-a-tool-for-distributing-file-system-images.html+]

CDAT
~~~~

CDAT is a powerful and complete front-end to a rich set of visual-data exploration and analysis capabilities well suited for data analysis problems.

CDAT builds on the following key technologies:

* Python and its ecosystem (e.g. NumPy, Matplotlib);
* Jupyter Notebooks and iPython;
* A toolset developed at LLNL for the analysis, visualization, and management of large-scale distributed climate data;
* VTK, the Visualization Toolkit, which is open source software for manipulating and displaying scientific data.

These combined tools, along with others such as the R open-source statistical analysis and plotting software and custom packages (e.g. DV3D), form CDAT and provide a synergistic approach to climate modeling, allowing researchers to advance scientific visualization of large-scale climate data sets. The CDAT framework couples powerful software infrastructures through two primary means:

* Tightly coupled integration of the CDAT Core with the VTK infrastructure to provide high-performance, parallel-streaming data analysis and visualization of massive climate-data sets (other tighly coupled tools include VCS, DV3D, and ESMF/ESMP);
* Loosely coupled integration to provide the flexibility of using tools quickly in the infrastructure such as ViSUS or R for data analysis and visualization as well as to apply customized data analysis applications within an integrated environment.

Within both paradigms, CDAT will provide data-provenance capture and mechanisms to support data analysis. 

https://uvcdat.llnl.gov/[+https://uvcdat.llnl.gov/+]

https://github.com/CDAT/cdat[+https://github.com/CDAT/cdat+]

https://uvcdat.llnl.gov/documentation/vcs/vcs.html[+https://uvcdat.llnl.gov/documentation/vcs/vcs.html+]

https://uvcdat.llnl.gov/documentation/utilities/utilities.html[+https://uvcdat.llnl.gov/documentation/utilities/utilities.html+]

https://uvcdat.llnl.gov/documentation/cdms/cdms.html[+https://uvcdat.llnl.gov/documentation/cdms/cdms.html+]

vCDAT
^^^^^

vCDAT is a desktop application that provides the graphical frontend for the CDAT package. It uses CDAT's VCS and CDMS modules to render high quality visualizations within a browser.

https://github.com/CDAT/vcdat[+https://github.com/CDAT/vcdat+]

https://github.com/CDAT/jupyter-vcdat[+https://github.com/CDAT/jupyter-vcdat+]

CDO
~~~

CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.
Supported data formats are GRIB 1/2, netCDF 3/4, SERVICE, EXTRA and IEG. There are more than 600 operators available.

The Climate Data Operators (CDO) software is a collection of many operators for standard processing of climate and
forecast model data. The operators include simple statistical and arithmetic functions, data selection and subsampling
tools, and spatial interpolation. CDO was developed to have the same set of processing functions for GRIB [GRIB] and NetCDF [NetCDF] datasets in one package.

https://code.mpimet.mpg.de/projects/cdo/[+https://code.mpimet.mpg.de/projects/cdo/+]

https://code.mpimet.mpg.de/projects/cdo/wiki/Cdo%7Brbpy%7D[+https://code.mpimet.mpg.de/projects/cdo/wiki/Cdo%7Brbpy%7D+]

CEED
~~~~

The Center for Efficient Exascale Discretizations (CEED) is a co-design center within the U.S. Department of Energy (DOE) Exascale Computing Project (ECP).

CEED is a research partnership involving 30+ computational scientists from two DOE labs and five universities, including members of the Nek5000, MFEM, MAGMA, OCCA and PETSc projects.

CEED is producing a range of software products supporting general finite element algorithms on triangular, quadrilateral, tetrahedral and hexahedral meshes in 3D, 2D and 1D. We target the whole de Rham complex: H1, H(curl), H(div) and L2/DG spaces and discretizations, including conforming and non-conforming unstructured adaptive mesh refinement (AMR).

Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation for any order space on any order mesh, including high-order curved meshes and all geometries in the de Rham complex. The CEED software will also include optimized assembly support for low-order methods.

The CEED distribution is a collection of software packages that can be integrated together to enable efficient discretizations in a variety of high-order applications on unstructured grids.

CEED is using the Spack package manager for compatible building and installation of its software components.

In this initial version, CEED 1.0, the CEED software suite consists of the following 12 packages, plus the CEED meta-package:

* https://github.com/gslib/gslib[GSLIB] - scalable Many-to-Many (sparse) gather-scatter collectives, and robust interpolation for hexahedral spectral element meshes
* https://github.com/CEED/HPGMG[HPGMG] -  implements full multigrid (FMG) algorithms using finite-volume and finite-element methods
* https://github.com/CEED/Laghos[Laghos] - a miniapp that solves the time-dependent Euler equations of compressible gas dynamics in a moving Lagrangian frame using unstructured high-order finite element spatial discretization and explicit high-order time-stepping
* https://github.com/CEED/libCEED[libCEED] - an initial low-level API library for efficient high-order discretization methods 
* https://github.com/CEED/MAGMA[MAGMA] - next-generation linear algebra libraries for heterogeneous architectures
* https://github.com/CEED/MFEM[MFEM] - a lightweight, general, scalable Cxx library for finite element methods
* https://github.com/CEED/Nek5000[Nek5000] - a spectral element CFD solver
* https://github.com/CEED/Nekbone[Nekbone] -  solves a standard Poisson equation using a conjugate gradient iteration with a simple or spectral element multigrid preconditioner on a block or linear geometry, exposing the principal computational kernel to reveal the essential elements of the algorithmic- architectural coupling that is pertinent to Nek5000
* https://github.com/CEED/NekCEM[NekCEM] - a spectral-element solver for Maxwell's equations, Schrödinger equation, and more
* https://github.com/CEED/PETSc[PETSc] - a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations
* https://github.com/CEED/PUMI[PUMI] -  libraries for unstructured mesh simulations on supercomputers
* https://github.com/CEED/OCCA[OCCA] - library implementing a portable approach for parallel architectures

https://ceed.exascaleproject.org/ceed-1.0/[+https://ceed.exascaleproject.org/ceed-1.0/+]

CEM
~~~

The Coastline Evolution Model (CEM) addresses predominately sandy, wave-dominated coastlines on time-scales ranging from years to millenia and on spatial scales ranging from kilometers to hundreds of kilometers. Shoreline evolution results from gradients in wave-driven alongshore sediment transport. At its most basic level, the model follows the standard 'one-line' modeling approach, where the cross-shore dimension is collapsed into a single data point. However, the model allows the plan-view shoreline to take on arbitrary local orientations, and even fold back upon itself, as complex shapes such as capes and spits form under some wave climates (distributions of wave influences from different approach angles). The model can also represent the geology underlying the sandy coastline and shoreface in a simplified manner and enables the simulation of coastline evolution when sediment supply from an eroding shoreface may be constrained. CEM also supports the simulation of human manipulations to coastline evolution through beach nourishment or hard structures. 

https://csdms.colorado.edu/wiki/Model:CEM[+https://csdms.colorado.edu/wiki/Model:CEM+]

https://github.com/csdms-contrib/cem[+https://github.com/csdms-contrib/cem+]

CernVM-FS
~~~~~~~~~

The CernVM File System provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications. CernVM-FS is implemented as a POSIX read-only file system in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs.

Internally, CernVM-FS uses content-addressable storage and Merkle trees in order to maintain file data and meta-data. CernVM-FS uses outgoing HTTP connections only, thereby it avoids most of the firewall issues of other network file systems. It transfers data and meta-data on demand and verifies data integrity by cryptographic hashes.

By means of aggressive caching and reduction of latency, CernVM-FS focuses specifically on the software use case. Software usually comprises many small files that are frequently opened and read as a whole. Furthermore, the software use case includes frequent look-ups for files in multiple directories when search paths are examined.

CernVM-FS is actively used by small and large HEP collaborations. In many cases, it replaces package managers and shared software areas on cluster file systems as means to distribute the software used to process experiment data. For the experiments at the LHC, CernVM-FS hosts several hundred million files and directories that are distribute to the order of hundred thousand client computers.

https://cernvm.cern.ch/portal/filesystem[+https://cernvm.cern.ch/portal/filesystem+]

https://github.com/cvmfs[+https://github.com/cvmfs+]

http://iopscience.iop.org/article/10.1088/1742-6596/898/6/062031/pdf[+http://iopscience.iop.org/article/10.1088/1742-6596/898/6/062031/pdf+]

Cesium
~~~~~~

CesiumJS is an open-source JavaScript library for world-class 3D globes and maps. Our mission is to create the leading 3D globe and map for static and time-dynamic content, with the best possible performance, precision, visual quality, platform support, community, and ease of use. 

The features include:

* Use 3D Tiles to stream, style, and interact with heterogeneous 3D data, including photogrammetry models, 3D buildings, CAD and BIM exterior and interiors, and point clouds. 
*  Visualize high-resolution global terrain. Optionally exaggerate terrain. Apply procedural materials such as height- or slope-based color ramps.
* Layer imagery from multiple sources, including WMS, TMS, WMTS with time-dynamic imagery, Cesium ion, Bing Maps, Mapbox, GEE, OpenStreetMap, ArcGIS MapServer, standard image files, and custom tiling schemes. Each layer can be alpha-blended with the layers below it, and its brightness, contrast, gamma, hue, and saturation can be dynamically changed. Two layers can be split across the screen.
* Stream terrain, imagery, 3D Tiles, and glTF assets from Cesium ion.
* Use legacy Google Earth Enterprise terrain and imagery.
* Industry standard vector formats, such as KML, GeoJSON, and TopoJSON, including terrain clamping.
* Draw 3D models using glTF 2.0 with Physically-Based Rendering (PBR) materials, animations, skins, and morph targets. Clamp models to terrain and highlight their silhouette. Convert COLLADA and OBJ to glTF using the online converter. Use the binary glTF, Google Draco, and WEB3D_quantized_attributes extensions to reduce the file size. 
* Create data-driven time-dynamic scenes using CZML and stream massive amounts of time-dynamic data with multi-part CZML.
* Draw and style a wide range of geometries
* Clamp polylines, polygons, labels, billboards, and ground-based primitives to the ground/terrain.
* Layer and z-order ground-based primitives.
* Create visual effects
* Apply clipping planes to 3D tilesets, terrain, and 3D models.
* Individual object picking and terrain picking.
* Camera navigation with mouse and touch handlers for rotate, zoom, pan with inertia, flights, free look, and terrain collision detection.
* Batching, culling, and JavaScript and GPU optimizations for performance. Optimized to reduce CPU and power usage when not animating.
* Precision handling for large view distances (logarithmic depth buffer and multiple frusta to avoid z-fighting) and large world coordinates (emulated GPU double precision to avoid jitter ).
* A 3D globe, 2D map, and Columbus view (2.5D) with the same API. 3D views can use a perspective or orthographic projection.
* Display military symbology, such as MIL-STD-2525 and STANAG APP6, by integrating with milsymbol
* Cluster points, labels and billboards. 
* Reference frames such as World Geodetic System (WGS84), International Celestial Reference Frame (ICRF), and east-north-up.
* Equidistant Cylindrical and Mercator 2D map projections.
* Conversions such as longitude/latitude/height to Cartesian.
* Fast Cartesian, spherical, cartographic, matrix, and quaternion types.
* Julian dates, leap seconds, and UTC and TAI time standards.

https://cesiumjs.org/[+https://cesiumjs.org/+]

Cesium-NcWMS
^^^^^^^^^^^^

Cesium (cesiumjs.org) based visualization using ncWMS to serve NetCDF data and D3 (d3js.org) to display graphs.
A live running version of this software can be found here: http://forecast.ewatercycle.org

https://github.com/eWaterCycle/Cesium-NcWMS[+https://github.com/eWaterCycle/Cesium-NcWMS+]

CGAL
~~~~

CGAL is a software project that provides easy access to efficient and reliable geometric algorithms in the form of a Cxx library. CGAL is used in various areas needing geometric computation, such as geographic information systems, computer aided design, molecular biology, medical imaging, computer graphics, and robotics.

The library offers data structures and algorithms like triangulations, Voronoi diagrams, Boolean operations on polygons and polyhedra, point set processing, arrangements of curves, surface and volume mesh generation, geometry processing, alpha shapes, convex hull algorithms, shape reconstruction, AABB and KD trees.

https://www.cgal.org/[+https://www.cgal.org/+]

https://www.cgal.org/projects.html[+https://www.cgal.org/projects.html+]

SFCGAL
^^^^^^

SFCGAL is a Cxx wrapper library around CGAL with the aim of supporting ISO 19107:2013 and OGC Simple Features Access 1.2 for 3D operations.

SFCGAL provides standard compliant geometry types and operations, that can be accessed from its C or Cxx APIs. PostGIS uses the C API, to expose some SFCGAL's functions in spatial databases (cf. PostGIS manual). 

http://www.sfcgal.org/[+http://www.sfcgal.org/+]

T2
^^
In the Institute of Computing at UNICAMP (State University of Campinas), we have extended the Computational Geometry Algorithms Library (CGAL) to allow for the implementation of geometric algorithms on the Oriented Projective Plane T2.

The oriented projective plane T2 is an extension of the Euclidean plane E2 and comprises a number of advantages for algorithm design and implementation. It consists of an alternative geometric model that combines the elegance and efficiency of projective geometry with the consistent handling of oriented lines and planes, signed angles, segments, convex sets, and many other concepts that the classical theory doesn't support. The value of this model for practical computing is well known.

In order to benefit from the extension of CGAL to the oriented projective plane in the context of the classroom, the need for visualization arose. So, we developed a dynamic visualization system, T2Viewer.

http://www.ic.unicamp.br/\~rezende/T2inCGALandT2Viewer.htm[+http://www.ic.unicamp.br/~rezende/T2inCGALandT2Viewer.htm+]

http://www.ic.unicamp.br/%7Erezende/T2Viewer/vid9-rezende.pdf[+http://www.ic.unicamp.br/%7Erezende/T2Viewer/vid9-rezende.pdf+]

http://www.ic.unicamp.br/\~rezende/ExtensionCGALT2.htm[+http://www.ic.unicamp.br/~rezende/ExtensionCGALT2.htm+]

CGD
~~~

Computation Graph Toolkit (CGT) is a library for evaluation and differentiation of functions of multidimensional arrays. 
The core features are as follows:

* Automatic differentiation of functions involving multidimensional arrays, using computation graphs
* Compile fast implementations of array computations that can be run in parallel on multiple CPUs and GPUs. (GPU and multi-GPU support is currently in work-in-progress)
* A compilation process that simplifies your function through arithmetic identities and in-place optimizations, which readily handles extremely large graphs.

CGT is motivated by large-scale machine learning and AI problems, however, the core library will focus on the more abstract problems of evaluating and differentiating mathematical expressions. This will ensure that CGT is flexible enough to handle use-cases that are completely unanticipated by the software authors. Libraries for numerical optimization and convenient construction of neural networks will be built on top of CGT’s core functionality.

With regard to previous work, CGT is most similar to Theano. In fact, CGT aims to mostly replicate Theano’s API. However, CGT makes some core architectural changes that necessitated a new codebase:

* Cxx/CUDA implementations of ops don’t use the Python C-API. Furthermore, the computation graph is compiled into a data-structure that can be executed by Cxx code independently of python. Hence, multithreaded execution is possible.
* Internally, CGT substantially reimagines the data-structures and compilation pipeline, which (in our view) leads to a cleaner codebase and makes ultra-fast compilation possible.

CGT aims to make it easy to to construct large and complicated models, while ensuring that the resulting code is concise and closely resembles the underlying mathematical expressions.

http://rll.berkeley.edu/cgt/[+http://rll.berkeley.edu/cgt/+]

https://github.com/joschu/cgt[+https://github.com/joschu/cgt+]

Chapel
~~~~~~

Chapel, the Cascade High Productivity Language, is a parallel programming language developed by Cray.[3] It is being developed as part of the Cray Cascade project, a participant in DARPA's High Productivity Computing Systems (HPCS) program, which had the goal of increasing supercomputer productivity by the year 2010.

Chapel aims to improve the programmability of parallel computers in general and the Cascade system in particular, by providing a higher level of expression than current programming languages do and by improving the separation between algorithmic expression and data structure implementation details.

The language designers aspire for Chapel to bridge the gap between current HPC programming practitioners, who they describe as Fortran, C or Cxx users writing procedural code using technologies like OpenMP and MPI on one side, and newly graduating computer programmers who tend to prefer Java, Python or Matlab with only some of them having experience with Cxx or C. Chapel should offer the productivity advances offered by the latter suite of languages while not alienating the users of the first.

Chapel supports a multithreaded parallel programming model at a high level by supporting abstractions for data parallelism, task parallelism, and nested parallelism. It enables optimizations for the locality of data and computation in the program via abstractions for data distribution and data-driven placement of subcomputations. It allows for code reuse and generality through object-oriented concepts and generic programming features. For instance, Chapel allows for the declaration of locales.[5]

While Chapel borrows concepts from many preceding languages, its parallel concepts are most closely based on ideas from High Performance Fortran (HPF), ZPL, and the Cray MTA's extensions to Fortran and C. 

https://chapel-lang.org/[+https://chapel-lang.org/+]

https://en.wikipedia.org/wiki/Chapel_(programming_language)[+https://en.wikipedia.org/wiki/Chapel_(programming_language)+]

Charliecloud
~~~~~~~~~~~~

Charliecloud provides user-defined software stacks (UDSS) for high-performance computing (HPC) centers. This “bring your own software stack” functionality addresses needs such as:

* software dependencies that are numerous, complex, unusual, differently configured, or simply newer/older than what the center provides;
* build-time requirements unavailable within the center, such as relatively unfettered internet access;
* validated software stacks and configuration to meet the standards of a particular field of inquiry;
* portability of environments between resources, including workstations and other test and development system not managed by the center;
* consistent environments, even archivally so, that can be easily, reliabily, and verifiably reproduced in the future; and/or
* usability and comprehensibility.

Charliecloud uses Linux user namespaces to run containers with no privileged operations or daemons and minimal configuration changes on center resources. This simple approach avoids most security risks while maintaining access to the performance and functionality already on offer.

Container images can be built using Docker or anything else that can generate a standard Linux filesystem tree.

https://github.com/hpc/charliecloud[+https://github.com/hpc/charliecloud+]

https://hpc.github.io/charliecloud/[+https://hpc.github.io/charliecloud/+]

https://dl.acm.org/citation.cfm?id=3126925[+https://dl.acm.org/citation.cfm?id=3126925+]

https://www.usenix.org/publications/login/fall2017/priedhorsky[+https://www.usenix.org/publications/login/fall2017/priedhorsky+]

Charm
~~~~~

Charm is a parallel programming framework in Cxx supported by an adaptive runtime system, which enhances user productivity and allows programs to run portably from small multicore computers (your laptop) to the largest supercomputers.

It enables users to easily expose and express much of the parallelism in their algorithms while automating many of the requirements for high performance and scalability. It permits writing parallel programs in units that are natural to the domain, without having to deal with processors and threads.

The capabilities include:

* the runtime system can seamlessly provide overlap of communication and computation as an application runs
* an entire suite of load balancers, which can be selected at runtime
* automatic checkpointing and fault tolerance
* the runtime can reduce total energy consumption i.e. both machine and cooling energy consumptions, by combining control over the processor operating frequency/voltage with object migration.
* it comes pre-packaged with many machine layers that are tuned to the latest supercomputer architectures, ranging from Blue Gene/Q to Cray XK6.
* independent modules with interleaved execution
* interoperable with MPI, OpenMP and CUDA
* an ecosystem of tools ranging from a parallel debugger to performance visualization
* a set of benchmark applications

http://charmplusplus.org/[+http://charmplusplus.org/+]

Chebfun
~~~~~~~

Chebfun is an open-source software system for numerical computing with functions. The mathematical starting point of Chebfun is piecewise polynomial interpolation implemented with what we call “Chebyshev technology”. The foundations are described, with Chebfun examples, in Approximation Theory and Approximation Practice. Chebfun has extensive capabilities for dealing with linear and nonlinear differential and integral operators, and it also includes continuous analogues of linear algebra notions like QR and singular value decomposition. The Chebfun2, Chebfun3, Spherefun and Diskfun extensions generalize much of this functionality to other domains.

http://www.chebfun.org/[+http://www.chebfun.org/+]

https://github.com/chebfun/chebfun[+https://github.com/chebfun/chebfun+]

http://www.chebfun.org/docs/guide/[+http://www.chebfun.org/docs/guide/+]

http://www.chebfun.org/examples/[+http://www.chebfun.org/examples/+]

Chef
~~~~

A systems integration framework, built to bring the benefits of configuration management to your entire infrastructure. 

Chef is a powerful automation platform that transforms infrastructure into code. Whether you’re operating in the cloud, on-premises, or in a hybrid environment, Chef automates how infrastructure is configured, deployed, and managed across your network, no matter its size.

You create and test your code on your workstation before you deploy it to other environments. Your workstation is the computer where you author your cookbooks and administer your infrastructure. It’s typically the machine you use everyday. It can be any OS you choose, whether it’s Linux, macOS, or Windows.

You’ll need to install a text editor (whichever you like) to write code and the Chef Development Kit (ChefDK) to get the tools to test your code. The primary testing tools you’ll use are Cookstyle, Foodcritic, ChefSpec, InSpec, and Test Kitchen. With them, you can make sure your Chef code does what you intended before you deploy it to environments used by others, such as staging or production.

https://github.com/chef/chef[+https://github.com/chef/chef+]

https://www.chef.io/[+https://www.chef.io/+]

Chirp
~~~~~

Chirp is a system for performing input and output across the Internet. Using Chirp, an ordinary user can share storage space and data with friends and colleagues without requiring any sort of administrator privileges anywhere.

Chirp is like a distributed filesystem (such as NFS) except that it can be run over wide area networks and requires no special privileges on either the client or the server end. Chirp allows the end user to set up fine-grained access control so that data can be shared (or not shared) with the right people.

Chirp is also like a file transfer system (such as FTP) that provides streaming point-to-point data transfer over the Internet. However, Chirp also provides fine-grained Unix-like data access suitable for direct access by ordinary programs.

Chirp also includes several advanced features for authentication tickets, space allocation, and more. However, each of these features must be explicitly enabled, so you don't have to worry about them if all you want is simple storage access.

Running a Chirp server is easy. You may run a Chirp server as any ordinary user, and you do not need to install the software or even run the programs as root. To run a Chirp server, you must do three things: pick a storage directory, run the server, and then adjust the access control.

The easiest way to access Chirp servers is by using a tool called Parrot. Parrot is a personal virtual filesystem: it "speaks" remote I/O operations on behalf of ordinary programs.

You can also attach to Chirp filesystems by using the FUSE package to attach Chirp as a kernel filesystem module. Unlike Parrot, this requires superuser privileges to install the FUSE package, but will likely work more reliably on a larger number of programs. You can do this with either Linux FUSE or MacFuse. Once you have downloaded and installed FUSE, simply run chirp_fuse with the name of a directory on which the filesystem should be mounted.

For more portable, explicit control of a Chirp server, use the Chirp command line tool. This allows you to connect to a server, copy files, and manage directories, much like an FTP client.

http://ccl.cse.nd.edu/software/chirp/[+http://ccl.cse.nd.edu/software/chirp/+]

Chisel
~~~~~~

Chisel is an open-source hardware construction language developed at UC Berkeley that supports advanced hardware design using highly parameterized generators and layered domain-specific hardware languages. 

https://chisel.eecs.berkeley.edu/[+https://chisel.eecs.berkeley.edu/+]

https://github.com/freechipsproject/chisel3/wiki/Short-Users-Guide-to-Chisel[+https://github.com/freechipsproject/chisel3/wiki/Short-Users-Guide-to-Chisel+]

CHOMBO
~~~~~

The Chombo package provides a set of tools for implementing finite difference methods for the solution of partial differential equations on block-structured adaptively refined rectangular grids. Both elliptic and time-dependent modules are included. Support for parallel platforms and standardized self-describing file formats are included.

Chombo provides a distributed infrastructure for parallel calculations over block-structured, adaptively refined grids. Chombo's design is uniquely flexible and accessible. Any collaborator will be able to develop parallel applications to solve the partial differential equations in which she is interested with far shorter development times than would be possible without the infrastructure. Very careful design and documentation allows said collaborator to enter the software at many levels. She will be able to use Chombo to investigate deep technical issues of adaptive mesh refinement algorithms or to simply adapt the example applications to solve different scientific problems.

Chombo is built using a combination of Cxx and Fortran and uses MPI message-passing for parallel execution. The code is simple to build by todays standards, using just regular GNU Make. It has been ported to several supercomputers and runs fine on laptops and workstations and Macs.   

The features of the Chombo AMR framework include:

* Chombo is the public open-source library from ANAG.
* Chombo supports a wide variety of applications that use AMR by means of a common software framework.
* Mixed-language programming: Cxx for high-level abstractions, Fortran for calculations on regular patches.
* Reusable components, based on mapping of mathematical abstractions to classes.
* Layered architecture that hides different levels of detail behind interfaces.
** Layer 4: Complete parallel applications. AMRSelfGravity, AMRMHD, AMRINS, EBAMRINS, AMRCharm. 100K lines of code.
** Layer 3: Solver libraries: geometric multigrid solvers on unions of rectangles and AMR hierarchies. Hyperbolic solvers. 70K lines of code.
** Layer 2: Tools for managing interactions between different levels of refinement in an AMR calculation. These include interpolation operators, averaging operators, and coarse-fine boundary conditions. 50K lines of code.
** Layer 1: Data and operations on unions of rectangles. This includes set calculus and a rectangular array library, data on unions of rectangles with SPMD parallelism implemented by distributing boxes to processors and load balancing tools. 80K lines of code.
** Utility Layer: Code instrumentation, interoperability libraries. This also has an API for HDF5 I/O and performance and debugging tools. 20K lines of code.

https://commons.lbl.gov/display/chombo/[+https://commons.lbl.gov/display/chombo/+]

CHORDS
~~~~~~

Cloud-Hosted Real-time Data Services for the Geosciences (CHORDS) is a real-time data services infrastructure that will provide an easy-to-use system to acquire, navigate and distribute real-time data streams via cloud services and the Internet. It will lower the barrier to these services for small instrument teams, employ data and metadata formats that adhere to community accepted standards, and broaden access to real-time data for the geosciences community.

CHORDS consists of two components:

* a real-time instrument data management server (Portal)
* a collection of higher level web-services that provide advanced, standards based processing (Services).

A CHORDS portal is a:

* web server and database that accepts real-time data from distributed instruments, and serves the measurements to anyone on the Internet. The data streams are pushed to and pulled from the Portal using .simple HTTP requests.
* management tool that allows you to monitor your remote instruments, insure correct operation, and maximize data collection.
* rolling archive from which scientists and analysts can easily fetch the data in real-time, delivered directly to browsers, programs and mobile apps. It will only hold a certain amount of data, but usually enough to give you plenty of time (e.g. months) to transfer to your own archive system. One click brings you a CSV file. A few lines of code brings data directly into your analysis programs.
* entry point to sophisticated real-time web services that can convert your data streams to standardized formats such as OGC, and provide mapping, visualization, discovery, aggregation and many other web-enabled functions.

Note that a CHORDS Portal is not meant to be a permanent archive. Users should plan to download and save data that they want to keep perminently.

http://ncar.github.io/chords/[+http://ncar.github.io/chords/+]

https://github.com/NCAR/chords[+https://github.com/NCAR/chords+]

https://www.earthcube.org/[+https://www.earthcube.org/+]

Chromium
~~~~~~~~

Chromium OS is an open-source operating system designed for running web applications and browsing the World Wide Web. It is the development version of Chrome OS, a Linux distribution made by Google.

Like Chrome OS, Chromium OS is based on the Linux kernel, but its principal user interface is the Chromium web browser rather than the Google Chrome browser. Chromium also includes the Portage package manager, which was originally developed for Gentoo Linux.[4] Because Chromium OS and Chrome OS use a web browser engine for the user interface, they are oriented toward web applications rather than desktop applications or mobile apps.


https://www.chromium.org/chromium-os[+https://www.chromium.org/chromium-os+]

https://www.chromium.org/chromium-os/quick-start-guide[+https://www.chromium.org/chromium-os/quick-start-guide+]

https://chromium.googlesource.com/chromiumos/docs/+/master/developer_guide.md[+https://chromium.googlesource.com/chromiumos/docs/+/master/developer_guide.md+]

https://memcpy.io/building-chromiumos-for-qemu.html[+https://memcpy.io/building-chromiumos-for-qemu.html+]

https://arnoldthebat.co.uk/wordpress/chromiumos-special-builds/[+https://arnoldthebat.co.uk/wordpress/chromiumos-special-builds/+]

https://nayuos.nexedi.com/[+https://nayuos.nexedi.com/+]

https://www.chromic-os.com/[+https://www.chromic-os.com/+]

https://github.com/FydeOS/chromium_os_for_raspberry_pi[+https://github.com/FydeOS/chromium_os_for_raspberry_pi+]

Cinder
~~~~~~

Cinder is a Cxx library for programming with aesthetic intent - the sort of development often called creative coding. This includes domains like graphics, audio, video, and computational geometry. Cinder is cross-platform, with official support for macOS, Windows, Linux, iOS, and Windows UWP.

Cinder is production-proven, powerful enough to be the primary tool for professionals, but still suitable for learning and experimentation.

https://libcinder.org/about[+https://libcinder.org/about+]

https://github.com/cinder/Cinder[+https://github.com/cinder/Cinder+]

Cinema Toolkit
~~~~~~~~~~~~~~

Cinema is an innovative way of capturing, storing, and exploring extreme scale scientific data – either simulation data or experimental data. It is a highly interactive approach to data analysis and visualization that promotes flexible investigation of large scientific datasets. The Cinema ecosystem consists of database specifications, writers, viewers, and algorithms. Cinema databases consist of data abstracts that are accessed via Cinema viewers in a browser-based approach. While earlier versions of Cinema focussed on images, the current version of Cinema expands the concept of data abstracts to include images, variables, parameters, metadata, mesh files, and csv files.

Cinema enables a multitude of analysis approaches. At the simplest level, a Cinema viewer can be used to visualize a Cinema database, exploring the data through pre-rendered images as if one were using the original 3D representation – but rendering the visualization much faster than possible on a full 3D simulation. Beyond exploring the data, sophisticated analysis algorithms can be applied to the saved raw data variables. The results of that analysis could be a new visualization or it could be a single measurement abstracted from the data at each time step. Computer vision techniques can be used on the raw data to detect, measure, and track features in the data, revealing the scientifically meaningful temporal or spatial evolution of those features. Image processing techniques can be applied to clean up noisy experimental images. Sampling or change detection techniques can be used to identify interesting time steps or spatial regions of a simulation. Cinema can also be used to curate parameters from experimental or simulation runs. Used in conjunction with analysis approaches, the scientist can explore and identify run parameters of particular interest, potentially saving time spent on experimental runs or driving the next round of simulation runs.

Cinema writers are available through common visualization applications. A Cinema database export wizard is accessible interactively in ParaView and will be available in the upcoming VisIt release. Or a Cinema database can be produced through ParaView’s Catalyst or VisIt’s LibSim in situ libraries. The Ascent infrastructure can also be used to export Cinema databases. A Cinema python library, cinema_lib, a command line tool is also available. Lastly, application-specific writers to create Cinema databases are also straightforward to implement in Python or other scripting languages.

https://cinemasciencewebsite.readthedocs.io/en/latest/introduction.html[+https://cinemasciencewebsite.readthedocs.io/en/latest/introduction.html+]

https://github.com/cinemascience[+https://github.com/cinemascience+]

https://cinemaviewer.org/[+https://cinemaviewer.org/+]

https://cinemascience.github.io/[+https://cinemascience.github.io/+]

https://cinemascience.github.io/downloads.html[+https://cinemascience.github.io/downloads.html+]

https://ascent.readthedocs.io/en/latest/[+https://ascent.readthedocs.io/en/latest/+]

Circos
~~~~~~

Circos is a software package for visualizing data and information. It visualizes data in a circular layout — this makes Circos ideal for exploring relationships between objects or positions. There are other reasons why a circular layout is advantageous, not the least being the fact that it is attractive.

Circos is ideal for creating publication-quality infographics and illustrations with a high data-to-ink ratio, richly layered data and pleasant symmetries. You have fine control each element in the figure to tailor its focus points and detail to your audience. 

Circos is flexible. Although originally designed for visualizing genomic data, it can create figures from data in any field—from genomics to visualizing migration to mathematical art. If you have data that describes relationships or multi-layered annotations of one or more scales, Circos is for you.

Circos can be automated. It is controlled by plain-text configuration files, which makes it easily incorporated into data acquisition, analysis and reporting pipelines (a data pipeline is a multi-step process in which data is analyzed by multiple and typically independent tools, each passing their output as the input to the next step). 

http://circos.ca/[+http://circos.ca/+]

CIS
~~~

CIS is an open source command-line tool for easy collocation, visualization, analysis, and comparison of diverse gridded and ungridded datasets used in the atmospheric sciences.

CIS is intended to facilitate a number of oft repeated operations on scientific data. Before analysis, the user is likely to want to subset and aggregate their dataset and possibly collocate it with another dataset. Once this is done, visualisation and statistical analysis help the user make sense of the dataset(s).

A subset of the original dataset is created by restricting its spatio-temporal extent. This command can be used to select e.g. all data for June 2007; all data at the surface or all data over a longitude/latitude box encompassing the Sahara. Restrictions can be combined e.g. all data for 2007 at the surface in a longitude/latitude box encompassing the Sahara. The subset is written to file in NetCDF format.

The original dataset is aggregated by spatio-temporal averaging. This can serve several purposes: it regularizes the data grid; it reduces overall data size; it smoothes the data; it makes the data more representative of large length and/or time-scales. Spatial and temporal averaging can be applied individually or together. This command can be used e.g. to aggregate satellite measurements over 1 by 1 degree grid-boxes or to produce yearly, zonal model averages. The aggregated result is written to file in NetCDF format.

One dataset is resampled to the spatio-temporal grid of another dataset. Depending on the grid structures of these datasets, different resampling methods are available: Nearest Neighbour selection, linear interpolation or area averages. For the latter method, the user may specify a weighting function. Using collocation, satellite data can be collocated with ground site data for evaluation of the satellite product or model data can be collocated with satellite data for evaluation of the model. The collocated result is written to file in NetCDF format.

Arbitrary mathematical functions can be applied to the contents of one or more datasets. This command assumes all data is on the same spatio-temporal grid (either originally, or as a result of collocation). It can be used to calculate e.g. the difference or ratio of two datasets. It also allows the user to mask out data based on other data. The evaluated result is written to file in NetCDF format.

Comprehensive summary statistics for variables in one or more datasets can be calculated. The command is meant to compare two variables on the same spatio-temporal grid (either originally, or as a result of collocation). The statistics include individual means and standard deviations, mean and standard deviation of the difference (both relative and absolute) as well as correlation coefficients and linear regression parameters. The evaluated result is written to screen and may also be written to file in NetCDF format.

he plot command is by far the most versatile CIS command as it allows for a large ranges of plot types: line graphs, scatter plots, heat maps, contour plots and all with arbitrarily specified axes. Plots may be overlaid on top of one another. This command may be used to plot e.g. a global map of yearly averaged model data; a time-series of station data; a scatter plot to compare two collocated datasets; The plot will be produced in a new window but can also be written to a number of graphical file formats (including png, jpg, gif and eps).

http://www.cistools.net/[+http://www.cistools.net/+]

https://esticc.net/data/etoolsdata-and-data-analysis-tools/[+https://esticc.net/data/etoolsdata-and-data-analysis-tools/+]

CISM
~~~~

CISM is a parallel, 3-D thermomechanical model, written mainly in Fortran, that solves equations for the momentum balance and the thickness and temperature evolution of ice sheets. CISM's velocity solver incorporates a hierarchy of Stokes flow approximations, including shallow-shelf, depth-integrated higher order, and 3-D higher order. CISM also includes a suite of test cases, links to third-party solver libraries, and parameterizations of physical processes such as basal sliding, iceberg calving, and sub-ice-shelf melting. The model has been verified for standard test problems, including the Ice Sheet Model Intercomparison Project for Higher-Order Models (ISMIP-HOM) experiments, and has participated in the initMIP-Greenland initialization experiment. In multimillennial simulations with modern climate forcing on a 4km grid, CISM reaches a steady state that is broadly consistent with observed flow patterns of the Greenland ice sheet. CISM has been integrated into version 2.0 of the Community Earth System Model, where it is being used for Greenland simulations under past, present, and future climates. The code is open-source with extensive documentation and remains under active development.

https://cism.github.io/[+https://cism.github.io/+]

https://github.com/CISM/cism/[+https://github.com/CISM/cism/+]

https://cism.github.io/data/cism_documentation_v2_1.pdf[+https://cism.github.io/data/cism_documentation_v2_1.pdf+]

https://www.geosci-model-dev.net/12/387/2019/[+https://www.geosci-model-dev.net/12/387/2019/+]

Clickhouse
~~~~~~~~~~

ClickHouse is an open source column-oriented database management system capable of real time generation of analytical data reports using SQL queries.

ClickHouse's performance exceeds comparable column-oriented DBMS currently available on the market. It processes hundreds of millions to more than a billion rows and tens of gigabytes of data per single server per second.

ClickHouse scales well both vertically and horizontally. ClickHouse is easily adaptable to perform either on cluster with hundreds of nodes, or on a single server or even on a tiny virtual machine. Currently there are installations with more than two trillion rows per single node, as well as installations with 100Tb of storage per single node.

ClickHouse features a user-friendly SQL query dialect with a number of built-in analytics capabilities. For example, it includes probabilistic data structures for fast and memory-efficient calculation of cardinalities and quantiles. There are functions for working dates, times and time zones, as well as some specialized ones like addressing URLs and IPs (both IPv4 and IPv6) and many more.

Data organizing options available in ClickHouse, such as arrays, array joins, tuples and nested data structures, are extremely efficient for managing denormalized data.

Using ClickHouse allows joining both distributed data and co-located data, as the system supports local joins and distributed joins. It also offers an opportunity to use external dictionaries, dimension tables loaded from an external source, for seamless joins with simple syntax.

ClickHouse supports approximate query processing – you can get results as fast as you want, which is indispensable when dealing with terabytes and petabytes of data.

The system's conditional aggregate functions, calculation of totals and extremes, allow getting results with a single query without having to run a number of them.

https://clickhouse.yandex/[+https://clickhouse.yandex/+]

https://github.com/yandex/ClickHouse[+https://github.com/yandex/ClickHouse+]

https://tech.marksblogg.com/billion-nyc-taxi-rides-clickhouse-cluster.html[+https://tech.marksblogg.com/billion-nyc-taxi-rides-clickhouse-cluster.html+]

Climata
~~~~~~~

Climata is a pythonic interface for loading and processing time series data from climate and flow monitoring stations and observers. climata leverages a number of webservices as listed below. climata is powered by wq.io, and shares its goal of maximizing the reusability of data parsing code, by smoothing over some of the differences between various data formats.

Python library for loading and iterating over climate and flow time series data (from ACIS/NOAA RCCs, CoCoRaHS, Hydromet/USBR, CNRFC ESP/NWS, SNOTEL/AWDB/NRCS, and NWIS/USGS).

https://github.com/heigeo/climata[+https://github.com/heigeo/climata+]

http://climata.houstoneng.net/[+http://climata.houstoneng.net/+]

climlab
~~~~~~~

climlab is a flexible engine for process-oriented climate modeling. It is based on a very general concept of a model as a collection of individual, interacting processes. climlab defines a base class called Process, which can contain an arbitrarily complex tree of sub-processes (each also some sub-class of Process). Every climate process (radiative, dynamical, physical, turbulent, convective, chemical, etc.) can be simulated as a stand-alone process model given appropriate input, or as a sub-process of a more complex model. New classes of model can easily be defined and run interactively by putting together an appropriate collection of sub-processes.

Currently, climlab has out-of-the-box support and documented examples for:

* Radiative and radiative-convective column models, with various radiation schemes:
** RRTMG (a widely used radiative transfer code)
** CAM3 (from the NCAR GCM)
** Grey Gas
** Simplified band-averaged models (4 bands each in longwave and shortwave)
* Convection schemes:
** Emanuel moist convection scheme
** Hard convective adjustment (to constant lapse rate or to moist adiabat)
* Diffusion solvers for moist and dry Energy Balance Models
* Flexible insolation including: - Seasonal and annual-mean models - Arbitrary orbital parameters
* Boundary layer scheme including sensible and latent heat fluxes
* Arbitrary combinations of the above, e.g.
2D latitude-pressure models with radiation, horizontally-varying diffusion, and fixed relative humidity

https://github.com/brian-rose/climlab/[+https://github.com/brian-rose/climlab/+]

http://joss.theoj.org/papers/6831d26a8ae32c11b8991bb848d0d4e2[+http://joss.theoj.org/papers/6831d26a8ae32c11b8991bb848d0d4e2+]

climt
~~~~~

climt is a Toolkit for building Earth system models in Python. climt stands for Climate Modelling and Diagnostics Toolkit -- it is meant both for creating models and for generating diagnostics (radiative fluxes for an atmospheric column, for example). However, since it might eventually include model components for purposes other than climate modelling (local area models, large-eddy simulation), we prefer to keep the abbreviation un-expanded!

climt hopes to enable researchers to easily perform online analysis and make modifications to existing models by increasing the ease with which models can be understood and modified. It also enables educators to write accessible models that serve as an entry point for students into Earth system modeling, while also containing state-of-the-art components.

Initially climt contains only components for the atmosphere, and does not yet include a coupler. But there are plans to extend climt to a fully coupled Earth system model in the future. The toolkit is also written in such a way that it could enable the development of non-climate models (e.g. weather prediction, large-eddy simulation). To do so requires only that the prognostic and diagnostic schemes are wrapped into the correct Python-accessible interface.

climt builds on sympl, which provides the base classes and array and constants handling functionality. Thanks to sympl and Pint, climt is also a fully units aware model. It is useful to know how sympl works to use climt better.

https://github.com/climt/climt[+https://github.com/climt/climt+]

https://github.com/mcgibbon/sympl[+https://github.com/mcgibbon/sympl+]

https://www.geosci-model-dev.net/11/3781/2018/[+https://www.geosci-model-dev.net/11/3781/2018/+]

CloudFusion
~~~~~~~~~~~

CloudFusion lets you access a multitude of cloud storages from Linux like any file on your desktop. Work with files from Dropbox, Sugarsync, Amazon S3, Google Storage, Google Drive, and WebDAV storages like any other file on your desktop.

http://joe42.github.io/CloudFusion/[+http://joe42.github.io/CloudFusion/+]

clustershell
~~~~~~~~~~~~

ClusterShell is an event-driven open source Python library, designed to run local or distant commands in parallel on server farms or on large Linux clusters. It will take care of common issues encountered on HPC clusters, such as operating on groups of nodes, running distributed commands using optimized execution algorithms, as well as gathering results and merging identical outputs, or retrieving return codes. ClusterShell takes advantage of existing remote shell facilities already installed on your systems, like SSH.

ClusterShell's primary goal is to improve the administration of high- performance clusters by providing a lightweight but scalable Python API for developers. It also provides clush, clubak and cluset/nodeset, convenient command-line tools that allow traditional shell scripts to benefit from some of the library features.

https://github.com/cea-hpc/clustershell[+https://github.com/cea-hpc/clustershell+]

CLUTO
~~~~~

CLUTO is a software package for clustering low- and high-dimensional datasets and for analyzing the characteristics of the various clusters. CLUTO is well-suited for clustering data sets arising in many diverse application areas including information retrieval, customer purchasing transactions, web, GIS, science, and biology.

CLUTO's distribution consists of both stand-alone programs and a library via which an application program can access directly the various clustering and analysis algorithms implemented in CLUTO. 

The features include:

* Multiple classes of clustering algorithms:
partitional, agglomerative, & graph-partitioning based.
* Multiple similarity/distance functions:
Euclidean distance, cosine, correlation coefficient, extended Jaccard, user-defined.
* Numerous novel clustering criterion functions and agglomerative merging schemes.
* Traditional agglomerative merging schemes:
single-link, complete-link, UPGMA
* Extensive cluster visualization capabilities and output options:
postscript, SVG, gif, xfig, etc.
* Multiple methods for effectively summarizing the clusters:
most descriptive and discriminating dimensions, cliques, and frequent itemsets.
* Can scale to very large datasets containing hundreds of thousands of objects and tens of thousands of dimensions.

http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview[+http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview+]

http://glaros.dtc.umn.edu/gkhome/software[+http://glaros.dtc.umn.edu/gkhome/software+]

Clyther
~~~~~~~

CLyther is a Python tool similar to Cython and PyPy. CLyther is a just-in-time specialization engine for OpenCL. The main entry points for CLyther are its clyther.task and clyther.kernel decorators. Once a function is decorated with one of these the function will be compiled to OpenCL when called.

CLyther is a Python language extension that makes writing OpenCL code as easy as Python itself. CLyther currently only supports a subset of the Python language definition but adds many new features to OpenCL.

CLyther exposes both the OpenCL C library as well as the OpenCL language to python.

http://srossross.github.io/Clyther/[+http://srossross.github.io/Clyther/+]

https://github.com/srossross/Clyther[+https://github.com/srossross/Clyther+]

CMake
~~~~~

https://github.com/onqtam/awesome-cmake[+https://github.com/onqtam/awesome-cmake+]

https://github.com/scivision/cmake-utils[+https://github.com/scivision/cmake-utils+]

CMLIB
~~~~~

A  collection  of  non-proprietary,   easily   transportable   Fortran
subprogram packages solving a variety of mathematical and  statistical
problems.

Although most applications will only  use  a  small  number  of  CMLIB
modules, there are no name conflicts within the library and  thus  all
of CMLIB can easily be installed. All  the  documentation  is  machine
readable.

All of the contents of CMLIB are available in Netlib. In fact, CMLIB pretty much contains all of Netlib as of several years ago.

https://github.com/jacobwilliams/CMLIB[+https://github.com/jacobwilliams/CMLIB+]

ftp://ftp.nist.gov/pub/cmlib/[+ftp://ftp.nist.gov/pub/cmlib/+]

CNI
~~~

CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted. Because of this focus, CNI has a wide range of support and the specification is simple to implement.

As well as the specification, this repository contains the Go source code of a library for integrating CNI into applications and an example command-line tool for executing CNI plugins. A separate repository contains reference plugins and a template for making new plugins.

The template code makes it straight-forward to create a CNI plugin for an existing container networking project. CNI also makes a good framework for creating a new container networking project from scratch.

Application containers on Linux are a rapidly evolving area, and within this area networking is not well addressed as it is highly environment-specific. We believe that many container runtimes and orchestrators will seek to solve the same problem of making the network layer pluggable.

To avoid duplication, we think it is prudent to define a common interface between the network plugins and container execution: hence we put forward this specification, along with libraries for Go and a set of plugins.

https://github.com/containernetworking/cni[+https://github.com/containernetworking/cni+]

https://github.com/containernetworking/plugins[+https://github.com/containernetworking/plugins+]

https://github.com/intel/multus-cni[+https://github.com/intel/multus-cni+]

https://github.com/Huawei-PaaS/CNI-Genie[+https://github.com/Huawei-PaaS/CNI-Genie+]

https://github.com/intel/userspace-cni-network-plugin[+https://github.com/intel/userspace-cni-network-plugin+]

https://github.com/ZTE/Knitter[+https://github.com/ZTE/Knitter+]

Collective Knowledge
~~~~~~~~~~~~~~~~~~~~

We designed Collective Knowledge (CK) as a very small, stable and portable framework with minimal dependencies to help researchers and developers quickly create, share and reuse extensible Python APIs and JSON meta descriptions to abstract any (evolving) code, data and hardware.

Though seemingly simple, such approach helps the community implement customizable workflows to automate, crowdsource and reproduce complex experiments such as AI/SW/HW autotuning and co-design while automatically adapting to any rapidly evolving software and hardware without the need for virtualization!

Unified CK APIs and JSON meta descriptions allow easy integration of CK workflows with popular tools and services such as GitHub, AWS, Docker, Singularity, Jupyter notebooks, Jenkins, Travis, etc. This, in turn, help to enable collaborative and reproducible R&D based on agile, DevOps, FAIR and Wikipedia principles.

https://github.com/ctuning/ck[+https://github.com/ctuning/ck+]

http://cknowledge.org/[+http://cknowledge.org/+]

https://fosdem.org/2019/schedule/event/collective_knowledge/[+https://fosdem.org/2019/schedule/event/collective_knowledge/+]

colour
~~~~~~

A color space is a specific organization of colors. In combination with physical device profiling, it allows for reproducible representations of color, in both analog and digital representations. A color space may be arbitrary, with particular colors assigned to a set of physical color swatches and corresponding assigned color names or numbers such as with the Pantone collection, or structured mathematically as with the NCS System, Adobe RGB and sRGB. A "color model" is an abstract mathematical model describing the way colors can be represented as tuples of numbers (e.g. triples in RGB or quadruples in CMYK); however, a color model with no associated mapping function to an absolute color space is a more or less arbitrary color system with no connection to any globally understood system of color interpretation. Adding a specific mapping function between a color model and a reference color space establishes within the reference color space a definite "footprint", known as a gamut, and for a given color model this defines a color space. For example, Adobe RGB and sRGB are two different absolute color spaces, both based on the RGB color model. When defining a color space, the usual reference standard is the CIELAB or CIEXYZ color spaces, which were specifically designed to encompass all colors the average human can see. 

https://en.wikipedia.org/wiki/Color_space[+https://en.wikipedia.org/wiki/Color_space+]

http://poynton.ca/ColorFAQ.html[+http://poynton.ca/ColorFAQ.html+]

cmocean
^^^^^^^

This package contains colormaps for commonly-used oceanographic variables. Most of the colormaps started from matplotlib colormaps, but have now been adjusted using the viscm tool to be perceptually uniform.

These colormaps were chosen to be perceptually uniform and to reflect the data they are representing in terms of being sequential, divergent, or cyclic (phase colormap), and to be intuitive. For example, the algae colormap is shades of green which could represent chlorophyll.

https://matplotlib.org/cmocean/[+https://matplotlib.org/cmocean/+]

colorspacious
^^^^^^^^^^^^^

Colorspacious is a powerful, accurate, and easy-to-use library for performing colorspace conversions.

In addition to the most common standard colorspaces (sRGB, XYZ, xyY, CIELab, CIELCh), we also include: color vision deficiency ("color blindness") simulations using the approach of Machado et al (2009); a complete implementation of CIECAM02; and the perceptually uniform CAM02-UCS / CAM02-LCD / CAM02-SCD spaces.

https://github.com/njsmith/colorspacious[+https://github.com/njsmith/colorspacious+]

https://colorspacious.readthedocs.io/en/latest/[+https://colorspacious.readthedocs.io/en/latest/+]

cpt-city
^^^^^^^^

An archive of colour gradient for cartography, technical illustration and design. The archive supports several formats and is organised by author.   The available formats are:

* Colour palette tables for use with the Generic Mapping Tools, GMT (cpt)
* CSS3 gradients (c3g)
* Gradients for the GNU image manipulation program, GIMP (ggr)
* Gnuplot palette files (gpf)
* POV-Ray colour map headers (inc)
* Colour-maps used by PostGIS, in particular those for the ST_ColorMap function. It should also be possible to use the maps in the GDAL program gdaldem, the GRASS r.colors utility and in ESRI products which call for a HDR color table (pg)
* PaintShop Pro's native format (having the extension PspGradient), which can also be read by Photoshop (psp)
* QGIS style colour-ramps (qgs)
* The SAO format used by the astronomical image viewer, DS9 (sao)
* Scalar vector graphics gradients (svg)

http://soliton.vm.bytemark.co.uk/pub/cpt-city/[+http://soliton.vm.bytemark.co.uk/pub/cpt-city/+]

cptutils
^^^^^^^^

The GMT package implements colour gradients with the cpt (colour palette) file format, and provides some tools for creating and manipulating them. The cptutils package contains a number of additional utilities, mostly for translation to and from other formats.

The cptutils package was written to aid the construction of the cpt archive cpt-city (where thousands of gradients can be downloaded). The Unix source distribution for version 1.69 of the package can be downloaded here (you will need to have libxml2 and libpng installed to compile it). 

http://soliton.vm.bytemark.co.uk/pub/jjg/en/code/cptutils/[+http://soliton.vm.bytemark.co.uk/pub/jjg/en/code/cptutils/+]

http://soliton.vm.bytemark.co.uk/pub/cptutils-online/select.html[+http://soliton.vm.bytemark.co.uk/pub/cptutils-online/select.html+]

viscm
^^^^^

A tool for visualizing and designing colormaps using colorspacious and matplotlib.

https://github.com/matplotlib/viscm[+https://github.com/matplotlib/viscm+]

https://bids.github.io/colormap/[+https://bids.github.io/colormap/+]

COMCOT
~~~~~~

COMCOT (Cornell Multi-grid Coupled Tsunami Model) is a tsunami
modeling package, capable of simulating the entire lifespan of a tsunami, from
its generation, propagation and runup/rundown in coastal regions.

Waves can be generated via incident wave maker, fault model,
landslide, or even customized profile. Flexible nested grid setup allows for
the balance between accuracy and efficiency.

http://223.4.213.26/archive/tsunami/cornell/comcot.htm[+http://223.4.213.26/archive/tsunami/cornell/comcot.htm+]

https://github.com/AndybnACT/GPU-comcot[+https://github.com/AndybnACT/GPU-comcot+]

https://github.com/AndybnACT/comcot-gfortran[+https://github.com/AndybnACT/comcot-gfortran+]

https://github.com/AndybnACT/COMCOT-Utilities[+https://github.com/AndybnACT/COMCOT-Utilities+]

compiler-explorer
~~~~~~~~~~~~~~~~~

Compiler Explorer is an interactive compiler. The left-hand pane shows editable C, Cxx, Rust, Go, D, Haskell, Swift and Pascal code. The right, the assembly output of having compiled the code with a given compiler and settings. Multiple compilers are supported, and the UI layout is configurable (thanks to GoldenLayout). There is also an ispc compiler ? for a C variant with extensions for SPMD.

https://github.com/mattgodbolt/compiler-explorer[+https://github.com/mattgodbolt/compiler-explorer+]

https://godbolt.org/[+https://godbolt.org/+]

ComPlot
~~~~~~~

ComPlot compares (gridded) model results with observational (point) data by plotting the data on top of the model output as dots. It is meant for oceanographers who want to compare ocean model output with observations.
It is programmed almost exclusively in the classic Ferret scripting language, but several standard Unix commands (like sed) are used to overcome limitations in Ferret. Ferret is a visualisation and analysis environment for oceanographers and meteorologists.

http://joss.theoj.org/papers/10.21105/joss.00368[+http://joss.theoj.org/papers/10.21105/joss.00368+]

http://www.nongnu.org/complot/[+http://www.nongnu.org/complot/+]

http://savannah.nongnu.org/projects/complot[+http://savannah.nongnu.org/projects/complot+]

COMPOSE
~~~~~~~

COMPOSE provides libraries for semi-Lagrangian transport and, together or separately, property preservation:

* CEDR: Communication-Efficient Constrained Density Reconstructors

* SIQK: Spherical Polygon Intersection and Quadrature

https://github.com/E3SM-Project/COMPOSE[+https://github.com/E3SM-Project/COMPOSE+]

compression
~~~~~~~~~~~

Blah.

https://ethw.org/History_of_Lossless_Data_Compression_Algorithms[+https://ethw.org/History_of_Lossless_Data_Compression_Algorithms+]

http://mattmahoney.net/dc/dce.html[+http://mattmahoney.net/dc/dce.html+]

7-Zip
^^^^^

7-Zip is a file archiver with a high compression ratio.
The features include:

* High compression ratio in 7z format with LZMA and LZMA2 compression
* Supported packing/unpacking formats: 7z, XZ, BZIP2, GZIP, TAR, ZIP and WIM
* Supported unpacking only formats:  AR, ARJ, CAB, CHM, CPIO, CramFS, DMG, EXT, FAT, GPT, HFS, IHEX, ISO, LZH, LZMA, MBR, MSI, NSIS, NTFS, QCOW2, RAR, RPM, SquashFS, UDF, UEFI, VDI, VHD, VMDK, WIM, XAR and Z.
* For ZIP and GZIP formats, 7-Zip provides a compression ratio that is 2-10 % better than the ratio provided by PKZip and WinZip
* Strong AES-256 encryption in 7z and ZIP formats
* Self-extracting capability for 7z format

The p7zip package is the port of the command line version of 7-Zip to Linux/Posix.

https://sourceforge.net/projects/p7zip/[+https://sourceforge.net/projects/p7zip/+]

https://www.7-zip.org/[+https://www.7-zip.org/+]

Bitshuffle
^^^^^^^^^^

Filter for improving compression of typed binary data.
Bitshuffle is an algorithm that rearranges typed, binary data for improving compression, as well as a python/C package that implements this algorithm within the Numpy framework.

The library can be used along side HDF5 to compress and decompress datasets and is integrated through the dynamically loaded filters framework.
Algorithmically, Bitshuffle is closely related to HDF5's Shuffle filter except it operates at the bit level instead of the byte level. Arranging a typed data array in to a matrix with the elements as the rows and the bits within the elements as the columns, Bitshuffle "transposes" the matrix, such that all the least-significant-bits are in a row, etc. This transpose is performed within blocks of data roughly 8kB long.

https://github.com/kiyo-masui/bitshuffle[+https://github.com/kiyo-masui/bitshuffle+]

bloscpack
^^^^^^^^^

Command line interface to and serialization format for Blosc, a high
performance, multi-threaded, blocking and shuffling compressor. Uses
python-blosc bindings to interface with Blosc. Also comes with native support
for efficiently serializing and deserializing Numpy arrays.

https://github.com/esc/bloscpack[+https://github.com/esc/bloscpack+]

https://pypi.python.org/pypi/bloscpack[+https://pypi.python.org/pypi/bloscpack+]

http://arxiv.org/abs/1404.6383[+http://arxiv.org/abs/1404.6383+]

http://nbviewer.ipython.org/github/esc/euroscipy2013-talk-bloscpack/blob/master/presentation.ipynb[+http://nbviewer.ipython.org/github/esc/euroscipy2013-talk-bloscpack/blob/master/presentation.ipynb+]

brotli
^^^^^^

Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression.

https://github.com/google/brotli[+https://github.com/google/brotli+]

bzip2
^^^^^

bzip2 is a free and open-source file compression program that uses the Burrows–Wheeler algorithm. It only compresses single files and is not a file archiver.
It  compresses most files more effectively than the older LZW (.Z) and Deflate (.zip and .gz) compression algorithms, but is considerably slower. LZMA is generally more space-efficient than bzip2 at the expense of even slower compression speed, while having much faster decompression.

bzip2 compresses data in blocks of size between 100 and 900 kB and uses the Burrows–Wheeler transform to convert frequently-recurring character sequences into strings of identical letters. It then applies move-to-front transform and Huffman coding. bzip2's ancestor bzip used arithmetic coding instead of Huffman. The change was made because of a software patent restriction.

http://www.bzip.org/[+http://www.bzip.org/+]

CharLs
^^^^^^

CharLS is a Cxx implementation of the JPEG-LS standard for lossless and near-lossless image compression and decompression. JPEG-LS is a low-complexity image compression standard that matches JPEG 2000 compression ratios.

JPEG-LS (ISO/IEC 14495-1:1999 / ITU-T.87) is an image compression standard derived from the Hewlett Packard LOCO algorithm. JPEG-LS has low complexity (meaning fast compression) and high compression ratios, similar to the JPEG 2000 lossless ratios. JPEG-LS is more similar to the old Lossless JPEG than to JPEG 2000, but interestingly the two different techniques result in vastly different performance characteristics. Wikipedia on lossless JPEG and JPEG-LS: http://en.wikipedia.org/wiki/Lossless_JPEG Tip: the ITU makes their version of the JPEG-LS standard (ITU-T.87) freely available for download, the text is identical with the ISO version.

https://github.com/team-charls/charls[+https://github.com/team-charls/charls+]

fpc
^^^

fpc is a Go implementation of Burtscher and Ratanaworabhan's 'FPC' algorithm for compressing a stream of floating point data.

The FPC algorithm can losslessly encode and decode huge amounts of floating-point data very quickly. It scales well to gigabyte-per-second streams. Compression ratios are better than just about any generic compressor like gzip or bzip, and compression and decompression throughput are much better (like, 8x to 300x faster) than other algorithms.

https://github.com/spenczar/fpc[+https://github.com/spenczar/fpc+]

http://cs.txstate.edu/\~burtscher/papers/dcc07a.pdf[+http://cs.txstate.edu/~burtscher/papers/dcc07a.pdf+]

ISABELA
^^^^^^^

Modern large-scale scientific simulations running on HPC systems generate voluminous amounts of data during a single run. To lessen the I/O load during a simulation run, scientists are forced to capture data infrequently, thereby making data collection an intrinsically lossy process. Yet, most lossless compression techniques are hardly suitable for large-scale reduction of floating-point datasets from scientific simulations as the data tends to be inherently random and hard-to-compress.

We introduce an effective method for In-situ Sort-And-B-spline Error-bounded Lossy Abatement (ISABELA) of scientific data. ISABELA is particularly designed for compressing spatio-temporal scientific data that is characterized as being inherently noisy and random-like, and thus commonly believed to be incompressible. With ISABELA, we apply a preconditioner to seemingly random and noisy data along spatial resolution to achieve an accurate fitting model that achieve a very high correlation (≥ 0.99) with the original data.

http://freescience.org/cs/ISABELA/ISABELA.html[+http://freescience.org/cs/ISABELA/ISABELA.html+]

JHPCN-DF
^^^^^^^^

Data compression library based on Jointed Hierarchical Precision Compression Number - Data Format

JHPCN-DF is a novel lossy compression algorithm taylored for floating point dataset. The algorithm enhances the effect of employing standard compression algorithms like deflate because this approach makes the occurence rate of the same byte pattern in data stream higher owing to truncating some lower bits of significand.

https://github.com/avr-aics-riken/JHPCN-DF[+https://github.com/avr-aics-riken/JHPCN-DF+]

Lizard
^^^^^^

Lizard (formerly LZ5) is a lossless compression algorithm which contains 4 compression methods:

* fastLZ4 : compression levels -10...-19 are designed to give better decompression speed than LZ4 i.e. over 2000 MB/s
* LIZv1 : compression levels -20...-29 are designed to give better ratio than LZ4 keeping 75% decompression speed
* fastLZ4 + Huffman : compression levels -30...-39 add Huffman coding to fastLZ4
* LIZv1 + Huffman : compression levels -40...-49 give the best ratio (comparable to zlib and low levels of zstd/brotli) at decompression speed of 1000 MB/s

Lizard library is based on frequently used LZ4 library by Yann Collet but the Lizard compression format is not compatible with LZ4. Lizard library is provided as open-source software using BSD 2-Clause license. The high compression/decompression speed is achieved without any SSE and AVX extensions.

https://github.com/inikep/lizard[+https://github.com/inikep/lizard+]

LossyWave
^^^^^^^^^

LossyWave is a cubic B-Spline wavelet compressor for regular grid datasets (1D,2D,3D). The goal for this method is to provide a low cost, low overhead compression method for smart data reduction via a series of parameters achieving targetted levels of lossyness.

https://github.com/lanl/VizAly-LossyWave[+https://github.com/lanl/VizAly-LossyWave+]

lz4
^^^

LZ4 is lossless compression algorithm, providing compression speed > 500 MB/s per core, scalable with multi-cores CPU. It features an extremely fast decoder, with speed in multiple GB/s per core, typically reaching RAM speed limits on multi-core systems.

Speed can be tuned dynamically, selecting an "acceleration" factor which trades compression ratio for faster speed. On the other end, a high compression derivative, LZ4_HC, is also provided, trading CPU time for improved compression ratio. All versions feature the same decompression speed.

LZ4 is also compatible with dictionary compression, and can ingest any input file as dictionary, including those created by Zstandard Dictionary Builder. (note: only the final 64KB are used).

https://github.com/lz4/lz4[+https://github.com/lz4/lz4+]

https://lz4.github.io/lz4/[+https://lz4.github.io/lz4/+]

lzbench
^^^^^^^

lzbench is an in-memory benchmark of open-source LZ77/LZSS/LZMA compressors. It joins all compressors into a single exe. At the beginning an input file is read to memory. Then all compressors are used to compress and decompress the file and decompressed file is verified. This approach has a big advantage of using the same compiler with the same optimizations for all compressors. The disadvantage is that it requires source code of each compressor (therefore Slug or lzturbo are not included).

https://github.com/inikep/lzbench[+https://github.com/inikep/lzbench+]

MAFISC
^^^^^^

MAFISC is a newly developed compression algorithm that aims to compress multidimensional scientific data well. Currently, it uses lzma as its compressing backend. It is currently provided as an HDF5 filter, which allows it to work transparently. 
+]
https://wr.informatik.uni-hamburg.de/research/projects/icomex/mafisc[+https://wr.informatik.uni-hamburg.de/research/projects/icomex/mafisc+]

https://link.springer.com/article/10.1007%2Fs00450-012-0222-4[+https://link.springer.com/article/10.1007%2Fs00450-012-0222-4

PAQ
^^^

PAQ is a series of open source data compression archivers that have evolved through collaborative development to top rankings on several benchmarks measuring compression ratio (although at the expense of speed and memory usage).

http://mattmahoney.net/dc/paq.html[+http://mattmahoney.net/dc/paq.html+]

http://mattmahoney.net/dc/[+http://mattmahoney.net/dc/+]

QccPack
^^^^^^^

QccPack provides an open-source collection of library routines and
utility programs for quantization, compression, and coding of data.  QccPack
has been written to provide very flexible and general implementations of
procedures commonly used in coding and compression applications.
  
The essential component of the QccPack collection is a library (a static
library, libQccPack.a, and, if supported on your system, a dynamic library,
libQccPack.so) of procedures implementing a large variety of compression
and coding algorithms.  Application programs may make use of the QccPack
library routines by linking the application against the library during 
compilation.  Each library function is very general in its implementation 
so to be useful in a large variety of applications.  
  
Additionally, much of the functionality of the library routines has been 
provided in the form of stand-alone executable programs.  Probably the prime
importance these utility programs is that they provide examples of how to
interface with many of the QccPack library routines.  The utility programs
could also be called from scripts to simulate the operation of complex
coding and compression systems before implementing all the system 
functionality into one stand-alone program. 

http://qccpack.sourceforge.net/[+http://qccpack.sourceforge.net/+]

https://www.linuxjournal.com/article/9692[+https://www.linuxjournal.com/article/9692+]

https://nar.ucar.edu/2015/cisl/scientific-data-compression-and-visualizing-large-datasets[+https://nar.ucar.edu/2015/cisl/scientific-data-compression-and-visualizing-large-datasets+]

SCIL
^^^^

With the Scientific Compression Library (SCIL), we are developing a meta-compressor that allows users to set various quantities that define the acceptable error and the expected performance behavior. The library then aims to choose the appropriate chain of algorithms to yield the users requirements (this feature is still under development). This approach is a crucial step towards a scientifically safe use of much-needed lossy data compression, because it disentangles the tasks of determining scientific ground characteristics of tolerable noise, from the task of determining an optimal compression strategy given target noise levels and constraints. Future algorithms are used without change in the application code, once they are integrated into SCIL. SCIL also comes with a pattern library to generate various relevant synthetic test patterns. Further tools are provided to plot, add noise or to compress CSV and NetCDF3 files. Internally, support functions simplify the development of new algorithms and the testing.

https://github.com/JulianKunkel/scil[+https://github.com/JulianKunkel/scil+]

http://www.sppexa.de/sppexa-activities/software.html[+http://www.sppexa.de/sppexa-activities/software.html+]

TurboPFor
^^^^^^^^^

The fastest integer compression.

https://github.com/powturbo/TurboPFor[+https://github.com/powturbo/TurboPFor+]

https://github.com/powturbo/TurboTranspose[+https://github.com/powturbo/TurboTranspose+]

https://sites.google.com/site/powturbo/[+https://sites.google.com/site/powturbo/+]

XZ
^^

XZ Utils is free general-purpose data compression software with a high compression ratio. XZ Utils were written for POSIX-like systems, but also work on some not-so-POSIX systems. XZ Utils are the successor to LZMA Utils.

The core of the XZ Utils compression code is based on LZMA SDK, but it has been modified quite a lot to be suitable for XZ Utils. The primary compression algorithm is currently LZMA2, which is used inside the .xz container format. With typical files, XZ Utils create 30 % smaller output than gzip and 15 % smaller output than bzip2. 

XZ Utils consist of several components:

* liblzma is a compression library with an API similar to that of zlib.
* xz is a command line tool with syntax similar to that of gzip.
* xzdec is a decompression-only tool smaller than the full-featured xz tool.
* A set of shell scripts (xzgrep, xzdiff, etc.) have been adapted from gzip to ease viewing, grepping, and comparing compressed files.
* Emulation of command line tools of LZMA Utils eases transition from LZMA Utils to XZ Utils.

https://tukaani.org/xz/[+https://tukaani.org/xz/+]

SZ
^^

Today’s HPC applications are producing extremely large amounts of data, thus it is necessary to use an efficient compression before storing them to parallel file systems.

We developed the error-bounded HPC data compressor, by proposing a novel HPC data compression method that works very effectively on compressing large-scale HPC data sets.

The key features of SZ are:

* Compression: Input: a data set (or a floating-point array) with error-bound requirements; Output: the compressed byte stream
* Decompression: input: the compressed byte stream ; Output: the original data set with the compression error of each data point being within a pre-specified error bound ∆.
* SZ supports C and Fortran.
* SZ supports two types of error bounds.

The users can set either absolute error bound or relative error bound, or a combination of the two bounds.

https://github.com/disheng222/SZ[+https://github.com/disheng222/SZ+]

XWRT
^^^^

XWRT (XML-WRT) is an efficient XML/HTML compressor (actually it works well with all textual files). It transforms XML to more compressible form and uses zlib (default), LZMA, PPMd, or lpaq6 as back-end compressor. This idea is based on well-known XML compressor: XMill. Moreover XML-WRT creates a semi-dynamic dictionary and replaces frequently used words with shorter codes. 

https://github.com/inikep/XWRT[+https://github.com/inikep/XWRT+]

Z-checker
^^^^^^^^^

A library to characterize the data and check the compression results of lossy compressors.

https://github.com/CODARcode/Z-checker[+https://github.com/CODARcode/Z-checker+]

https://arxiv.org/abs/1707.09320[+https://arxiv.org/abs/1707.09320+]

zfp
^^^

High-precision numerical data from computer simulations, observations, and experiments is often represented in floating point, and can easily reach terabytes to petabytes of storage. Moving such large data sets to and from disk, across the internet, between compute nodes, and even through the memory hierarchy presents a significant performance bottleneck. To address this problem, we have developed lossy and lossless high-speed data compressors that can greatly reduce the amount of data stored and moved.

For lossless compression, where each and every bit of each floating-point number has to be exactly preserved without any loss in accuracy, our memory efficient streaming fpzip compressor usually provides 1.5x-4x data reduction, depending on data precision and smoothness.

To achieve much higher compression ratios, lossy compression is needed, where small, often imperceptible or numerically negligible errors may be introduced. Our zfp compressor for floating-point and integer data often achieves compression ratios on the order of 100:1, i.e., to less than 1 bit per value of compressed storage.

zfp can achieve an exact bit rate, ensure that reconstructed values are within an absolute error tolerance, or meet a specified precision requirement. zfp also comes with C and Cxx compressed array classes that support random access and can be used in place of conventional C arrays or STL vectors, e.g. for numerical computations.

zfp and fpzip were both designed for compressing logically regular 1D, 2D, 3D, or 4D arrays of single- or double-precision floating-point numbers that exhibit spatial correlation (e.g., regularly sampled continuous functions), and should not be used to compress unstructured data such as triangle mesh geometry, unorganized point sets, or streams of unrelated numbers. Think of fpzip as the floating-point analogue to PNG image compression, and zfp as advanced JPEG for floating-point arrays.

https://computation.llnl.gov/projects/floating-point-compression[+https://computation.llnl.gov/projects/floating-point-compression+]

https://github.com/LLNL/zfp[+https://github.com/LLNL/zfp+]

Zopfli
^^^^^^

Zopfli is data compression software that encodes data into DEFLATE, gzip and zlib formats.[1] It achieves higher compression than other DEFLATE/zlib implementations, but takes much longer to perform the compression.
Zopfli can output either a raw DEFLATE data stream or DEFLATE data encapsulated into gzip or zlib formats. It can be configured to do more or fewer iterations than the default 15 to trade processing time for compression efficiency.

Under default settings, the output of Zopfli is typically 3–8% smaller than zlib's maximum compression, but takes around 80 times longer.[3][2] The speed of decompressing Zopfli's output versus zlib's output is practically unaffected.[5]
Due to its significantly slower compression speed, zopfli is less suited for on-the-fly compression and is typically used for one-time compression of static content.[6][7] This is typically true for web content that is served with DEFLATE-based HTTP compression or web content in a DEFLATE-based file format such as PNG or WOFF font files.[8] Another use case is software updates or downloads with software package files that have a zip-based format such as Android application packages (APK) or Java Archives (JAR), especially over mobile connections. 

https://github.com/google/zopfli[+https://github.com/google/zopfli+]

ZS
^^

ZS is a simple, read-only, binary file format designed for distributing, querying, and archiving arbitarily large data sets (up to tens of terabytes and beyond) – so long as those data sets can be represented as a set of arbitrary binary records. Of course it works on small data sets too. You can think of it as an alternative to storing data in tab- or comma-separated files – each line in such a file becomes a record in a ZS file. But ZS has a number of advantages over these traditional formats:

* ZS files are small: ZS files (optionally) store data in compressed form. The 3-gram counts from the 2012 US English release of the Google N-grams are distributed as a set of gzipped text files in tab-separated format, and take 1.3 terabytes of space. Uncompressed, this data set comes to more than 10 terabytes (and would be even more if loaded into a database). The same data in a ZS file with the default settings (LZMA compression) takes just 0.75 terabytes.

* Nonetheless, ZS files are fast: Decompression is an inherently slow and serial operation, which means that reading compressed files can easily become the bottleneck in an analysis.  The LZMA compression used in our ZS file is, on its own, slower than gzip. If we restrict ourselves to a single core, then we can only read our ZS file at ~50 MB/s. However, ZS files allow for multithreaded decompression.

* In fact, ZS files are really, REALLY fast: Suppose we want to know how many different Google-scanned books published in the USA in 1955 used the phrase “this is fun”. ZS files have a limited indexing ability that lets you quickly locate any arbitrary span of records that fall within a given sorted range, or share a certain textual prefix. This isn’t as nice as a full-fledged database system that can query on any column, but it can be extremely useful for data sets where the first column (or first several columns) are usually used for lookup. Using our example file, finding the “this is fun” entry takes 5 disk seeks and ~25 milliseconds of CPU time – something like 85 ms all told.

* ZS files contain rich metadata: In addition to the raw data records, every ZS file contains a set of structured metadata in the form of an arbitrary JSON document. You can use this to store information about this file’s record format (e.g., column names), notes on data collection or preprocessing steps, recommended citation information, or whatever you like, and be confident that it will follow your data where-ever it goes.

* ZS files are network friendly: Suppose you know you just want to look up a few individual records that are buried inside that 0.75 terabyte file, or want a large span of records that are still much smaller than the full file (e.g., all 3-grams that begin “this is”). With ZS, you don’t have to actually download the full 0.75 terabytes of data. Given a URL to the file, the ZS tools can find and fetch just the parts of the file you need, using nothing but standard HTTP.

* ZS files are splittable: If you’re using a big distributed data processing system (e.g. Hadoop), then it’s useful to split up your file into pieces that approximately match the underlying storage chunks, so each CPU can work on locally stored data. This is only possible, though, if your file format makes it possible to efficiently start reading near arbitrary positions in a file. With ZS files, this is possible (though because this requires multiple index lookups, it’s not as convenient as in file formats designed with this as a primary consideration).

* ZS files are ever-vigilant: Computer hardware is simply not reliable, especially on scales of years and terabytes. I’ve dealt with RAID cards that would occasionally flip a single bit in the data that was being read from disk. How confident are you that this won’t be a key bit that totally changes your results? Standard text files provide no mechanism for detecting data corruption. Gzip and other traditional compression formats provide some protection, but it’s only guaranteed to work if you read the entire file from start to finish and then remember to check the error code at the end, every time. But ZS is different: it protects every bit of data with 64-bit CRC checksums, and the software we distribute will never show you any data that hasn’t first been double-checked for correctness. 

* Relying on the ZS format creates minimal risk: The ZS file format is simple and fully documented; it’s not hard to write an implementation for your favorite language. In an emergency, an average programmer with access to standard libraries could write a minimal but working decompressor in just an hour or two.

https://zs.readthedocs.io/en/latest/[+https://zs.readthedocs.io/en/latest/+]

https://github.com/njsmith/zs[+https://github.com/njsmith/zs+]

zstandard
^^^^^^^^^

Zstandard, or zstd as short version, is a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios. It's backed by a very fast entropy stage, provided by Huff0 and FSE library.

The project is provided as an open-source dual BSD and GPLv2 licensed C library, and a command line utility producing and decoding .zst, .gz, .xz and .lz4 files.

https://github.com/facebook/zstd[+https://github.com/facebook/zstd+]

COMPSs
~~~~~~

The COMP Superscalar (COMPSs) framework is mainly compose of a programming model which aims to ease the development of applications for distributed infrastructures, such as Clusters, Grids and Clouds and a runtime system that exploits the inherent parallelism of applications at execution time. The framework is complemented by a set of tools for facilitating the development, execution monitoring and post-mortem performance analysis.

With the objective of offering a programming environment with high productivity and portability, the COMPSs Programming model has the following key characteristics:

* Sequential programming: COMPSs programmers can develop their applications following the sequential programming paradigm. In this sense, the programmer does not need to take care of the parallelization and distribution aspects, such as thread creation and synchronization, data distribution, messaging or fault tolerance. 

* Infrastructure agnosticism: COMPSs abstracts applications from the underlying distributed infrastructure. COMPSs programs do not include any detail that could tie them to a particular platform, like deployment or resource management. This makes applications portable between infrastructures with diverse characteristics.

* Standard programming languages: COMPSs applications can be developed in Java, Python and C/Cxx. The use of a general purpose programming language facilitates adoption, since these languages are between the more common and popular between programmers.

* APIs: In the case of COMPSs applications in Java, the model does not require to use any special API call, pragma or construct in the application; everything is pure standard Java syntax and libraries. With regard the Python and C/Cxx bindings, a small set of API calls should be used on the COMPSs applications. 

https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/[+https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/+]

https://github.com/bsc-wdc/compss[+https://github.com/bsc-wdc/compss+]

https://github.com/bsc-wdc/apps[+https://github.com/bsc-wdc/apps+]

https://hbp-hpc-platform.fz-juelich.de/?hbp_software=pycompss[+https://hbp-hpc-platform.fz-juelich.de/?hbp_software=pycompss+]

https://journals.sagepub.com/doi/abs/10.1177/1094342015594678[+https://journals.sagepub.com/doi/abs/10.1177/1094342015594678+]

pycompss-autoparallel
^^^^^^^^^^^^^^^^^^^^^

The PyCOMPSs AutoParallel module includes:

* The @parallel() decorator for Python to run applications with PyCOMPSs
* Different Translator modules to automatically generate parallel Python code
** Py2Scop Translator: Translation of Python code into OpenScop format
** Scop2PScop2Py Translator: Integration with the PLUTO tool to generate possible parallelizations of the given code. The output is written in Python (by means of a CLooG extension) with parallel annotations following an OMP fashion.
** Py2PyCOMPSs Translator: Translation of parallel annotated Python code into PyCOMPSs task definitions.
* A Code Replacer module to replace the user code by autogenerated code.
* Several example applications (examples/ folder).

https://github.com/cristianrcv/pycompss-autoparallel[+https://github.com/cristianrcv/pycompss-autoparallel+]

Confuga
~~~~~~~

Confuga is an active storage cluster file system designed for executing DAG-structured scientific workflows. It is used as a collaborative distributed file system and as a platform for execution of scientific workflows with full data locality for all job dependencies.

Confuga is composed of a head node and multiple storage nodes. The head node acts as the metadata server and job scheduler for the cluster. Users interact with Confuga using the head node.

A Confuga cluster can be setup as an ordinary user or maintained as a dedicated service within the cluster. The head node and storage nodes run the Chirp file system service. Users may interact with Confuga using Chirp's client toolset chirp(1), Parrot parrot_run(1), or FUSE chirp_fuse(1).

Confuga manages the details of scheduling and executing jobs for you. However, it does not concern itself with job ordering; it appears as a simple batch execution platform. We recommend using a high-level workflow execution system like Makeflow to manage your workflow and to handle the details of submitting jobs.

Confuga is designed to exploit the unique parameters and characteristics of POSIX scientific workflows. Jobs are single task POSIX applications that are expressed with all input files and all output files. Confuga uses this restricted job specification to achieve performance and to control load within the cluster.

http://ccl.cse.nd.edu/software/confuga/[+http://ccl.cse.nd.edu/software/confuga/+]

containers
~~~~~~~~~~

A container is an isolated user space in which computer programs run directly on the host operating system's kernel but have access to a restricted subset of its resources. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside a container can only see the container's portion of the file system and the devices assigned to it. The mechanism by which a host operating system runs programs in isolated user-space environments is called containerization or operating-system-level virtualization. 

On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers.

In Docker, container also refers to a package of software and dependencies that run inside a virtual user space. A file that represents such a package is called a container image.

Depending on the application, containers are also known as virtual environments (VEs), partitions or jails.

https://en.wikipedia.org/wiki/Container_(virtualization)[+https://en.wikipedia.org/wiki/Container_(virtualization)+]

https://en.wikipedia.org/wiki/List_of_Linux_containers[+https://en.wikipedia.org/wiki/List_of_Linux_containers+]

containerd
^^^^^^^^^^

containerd is an industry-standard container runtime with an emphasis on simplicity, robustness and portability. It is available as a daemon for Linux and Windows, which can manage the complete container lifecycle of its host system: image transfer and storage, container execution and supervision, low-level storage and network attachments, etc.

containerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users.

Containerd was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes.

Containerd is an abstraction layer between your management code and the syscalls and duct tape of features to run a container. 
t provides a client layer of types that platforms can build on top of without ever having to drop down to the kernel level. 
It was designed to be used by Docker and Kubernetes as well as any other container platform that wants to abstract away syscalls or OS specific functionality to run containers on linux, windows, solaris, or other OSes.

Containerd also contains a complete storage and distribution system that supports both OCI and Docker image formats.  It is a content addressed storage system across the containerd API that works not only for images but also metadata, checkpoints, and arbitrary data attached to containers.

https://containerd.io/[+https://containerd.io/+]

https://github.com/containerd/containerd[+https://github.com/containerd/containerd+]

https://blog.docker.com/2017/08/what-is-containerd-runtime/[+https://blog.docker.com/2017/08/what-is-containerd-runtime/+]

Docker
^^^^^^

The current elephant in cyberspace.

Docker is a computer program that performs operating-system-level virtualization.
It is used to run software packages called containers. Containers are isolated from each other and bundle their own application,[8] tools, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating-system kernel and are thus more lightweight than virtual machines. Containers are created from images that specify their precise contents. Images are often created by combining and modifying standard images downloaded from public repositories.

Docker is developed primarily for GNU/Linux, where it uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and
others to allow independent containers to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).
The Linux kernel's support for namespaces mostly isolates an application's view of the operating environment, including process trees, network, user IDs and mounted file systems, while the kernel's cgroups provide resource limiting for memory and CPU.
Docker includes the libcontainer library as its own way to directly use virtualization facilities provided by the Linux kernel, in addition to using abstracted virtualization interfaces via libvirt, LXC and systemd-nspawn.

https://www.docker.com/[+https://www.docker.com/+]

https://en.wikipedia.org/wiki/Docker_(software)[+https://en.wikipedia.org/wiki/Docker_(software)+]

LXC
^^^

LXC (Linux Containers) is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel.

The Linux kernel provides the cgroups functionality that allows limitation and prioritization of resources (CPU, memory, block I/O, network, etc.) without the need for starting any virtual machines, and also namespace isolation functionality that allows complete isolation of an applications' view of the operating environment, including process trees, networking, user IDs and mounted file systems.

LXC combines the kernel's cgroups and support for isolated namespaces to provide an isolated environment for applications. 

https://linuxcontainers.org/[+https://linuxcontainers.org/+]

https://en.wikipedia.org/wiki/LXC[+https://en.wikipedia.org/wiki/LXC+]

*LXC Image Server* - https://us.images.linuxcontainers.org/[+https://us.images.linuxcontainers.org/+]

*Everything You Need to Know About Linux Containers*

* *Part I: Linux Control Groups and Process Isolation* - https://www.linuxjournal.com/content/everything-you-need-know-about-linux-containers-part-i-linux-control-groups-and-process[+https://www.linuxjournal.com/content/everything-you-need-know-about-linux-containers-part-i-linux-control-groups-and-process+]

* *Part II: Working with Linux Containers (LXC)* - https://www.linuxjournal.com/content/everything-you-need-know-about-linux-containers-part-ii-working-linux-containers-lxc[+https://www.linuxjournal.com/content/everything-you-need-know-about-linux-containers-part-ii-working-linux-containers-lxc+]

LXD
^^^

LXD is a next generation system container manager. It offers a user experience similar to virtual machines but using Linux containers instead.
LXD is a value-added extension of LXC. It provides features for creating and managing containers that are not available from LXC itself.
It provides extra features for managing containers, like the ability to perform a “live migration” by moving a running container from one host to another without shutting the container down.

The core of LXD is a privileged daemon which exposes a REST API over a local unix socket as well as over the network (if enabled).

Clients, such as the command line tool provided with LXD itself then do everything through that REST API. It means that whether you're talking to your local host or a remote server, everything works the same way.

LXD isn't a rewrite of LXC, in fact it's building on top of LXC to provide a new, better user experience. Under the hood, LXD uses LXC through liblxc and its Go binding to create and manage the containers.

It's basically an alternative to LXC's tools and distribution template system with the added features that come from being controllable over the network.

https://linuxcontainers.org/lxd/introduction/[+https://linuxcontainers.org/lxd/introduction/+]

runC
^^^^

A command-line tool for spawning and running containers according to the Open Container Initiative (OCI) specification.
It is an implementation of the Open Container Initiative (OCI) specification for the
runtime elements of a container.
RunC includes libcontainer, the original lower-layer library interface originally used in the Docker engine, to set up the operating system constructs that we call a container.

RunC is a useful debug platform for finding hard-to-solve bugs that are trickier to debug with the entire Docker stack above the container process.  It also allows a simplified entry point into running and debugging low-level container features without the overhead of the entire Docker daemon interface.

Because libcontainer, that operating system layer library that does the real work of performing the container isolation primitives for your OS, is at the heart of runC, any OS-layer features—such as seccomp and user namespaces—must be implemented first in runC before they can be exposed to higher layers, like the Docker engine. This additional capability—to try out new features in runC that haven't been exposed yet at higher layers—is another attractive draw to runC, and several of the latest features that have been exposed in Docker were available in libcontainer and runC well before they made their way into Docker. This also means that during the development of these isolation features or enhanced security capabilities, runC is a great tool for testing and trying out unique configurations using the JSON configuration file.

https://opensource.com/life/16/8/runc-little-container-engine-could[+https://opensource.com/life/16/8/runc-little-container-engine-could+]

https://github.com/opencontainers/runc[+https://github.com/opencontainers/runc+]

Singularity
^^^^^^^^^^^

Singularity is a free, cross-platform and open-source computer program that performs operating-system-level virtualization also known as containerization.
One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world.
The need for reproducibility requires the ability to use containers to move applications from system to system.
Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms

The significant HPC-related features of Singularity are:

* Singularity is able to support natively high-performance interconnects, such as InfiniBand and Intel Omni-Path Architecture (OPA).
* Similar to the support for InfiniBand and Intel OPA devices, Singularity can support any PCIe-attached device within the compute node, such as graphic accelerators.
* Singularity has also native support for Open MPI library by utilizing a hybrid MPI container approach where OpenMPI exists both inside and outside the container.

These features make Singularity increasingly useful in areas such as Machine learning, Deep learning and most data-intensive workloads where the applications benefit from the high bandwidth and low latency characteristics of these technologies.

HPC systems traditionally already have resource management and job scheduling systems in place, so the container runtime environments must be integrated into the existing system resource manager.
Using other enterprise container solutions like Docker in HPC systems would require modifications to the software.
Singularity seamlessly integrates with many resource managers including SLURM, Torque and SGE Grid Engine.

https://github.com/sylabs/singularity[+https://github.com/sylabs/singularity+]

https://www.sylabs.io/singularity/[+https://www.sylabs.io/singularity/+]

https://www.sylabs.io/docs/[+https://www.sylabs.io/docs/+]

https://cloud.sylabs.io/library[+https://cloud.sylabs.io/library+]

https://www.sdsc.edu/support/user_guides/tutorials/singularity.html[+https://www.sdsc.edu/support/user_guides/tutorials/singularity.html+]

http://www.admin-magazine.com/HPC/Articles/Singularity-A-Container-for-HPC[+http://www.admin-magazine.com/HPC/Articles/Singularity-A-Container-for-HPC+]

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459[+https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459+]

docker2singularity
^^^^^^^^^^^^^^^^^^

A docker image for converting docker images to singularity images.
docker2singularity uses the Docker daemon located on the host system. It will access the Docker image cache from the host system avoiding having to redownload images that are already present locally.

https://github.com/TACC/docker2singularity[+https://github.com/TACC/docker2singularity+]

cookiecutter
~~~~~~~~~~~~

A command-line utility that creates projects from cookiecutters (project templates), e.g. creating a Python package project from a Python package project template.

https://github.com/audreyr/cookiecutter[+https://github.com/audreyr/cookiecutter+]

https://cookiecutter.readthedocs.io/en/latest/[+https://cookiecutter.readthedocs.io/en/latest/+]

https://github.com/MolSSI/cookiecutter-cms[+https://github.com/MolSSI/cookiecutter-cms+]

COPASI
~~~~~~

COPASI is a software application for simulation and analysis of biochemical networks and their dynamics.

COPASI is a stand-alone program that supports models in the SBML standard and can simulate their behavior using ODEs or Gillespie's stochastic simulation algorithm; arbitrary discrete events can be included in such simulations.

Models in COPASI are based on reactions that convert a set of species into another set of species. Each species is located in a compartment, which is a physical location with a size (volume, area, etc). This maps directly to biochemical reaction networks, but can also represent other types of processes (for example, the species could be cell types). COPASI automatically converts the reaction network to a set of differential equations or to a system of stochastic reaction events — the user does not have to write down the math explicitly, the software does that. Models can also have:

* Unlimited number of species, reactions, and compartments.
* Arbitrary discrete events; these can be used to change the model, or just to monitor the progress of simulations.
* Arbitrary differential equations; these have to be added explicitly by the user and can be mapped to species, compartments, or generic variables.
* Compartments can have variable sizes (ie they can be variables of the model).
* The rates of reaction can be picked from a set of predefined kinetic functions (the most common in biochemistry), or arbitrary functions defined by the user.

Models can be visualized through

* the GUI interface, with tables for reactions, species, compartments, etc.;
* an arbitrary number of network diagrams (including an SBGN-compliant option);
* the full set of differential equations; these can be exported in Latex or MathML formats.

COPASI can import and export models in the SBML format (levels 1 to 3). Models can also be exported in XPP format, Berkeley Madonna format, and as C code (in addition to MathML and Latex).

Simulation can be performed either with stochastic kinetics or with differential equations, and the software easily allows switching between them. The software provides an interface to create parameter scans (sweeps), parameter sampling and repeated simulations — including complex simulation scenarios mixing parameter samples with scans and repeats of simulations (or other analyses).

http://copasi.org/[+http://copasi.org/+]

CORE
~~~~

The Core Library is based on a novel number core. This library provides an API which defines four levels of numerical accuracy. The main intent of this implementation was to support the Exact Geometric Computation (EGC) approach to numerically robust algorithms. Nevertheless, it is quite general and can be used in any other application where numerical accuracy must be guaranteed.

Our library provides users with a simple and natural numerical API to support the following concepts of accuracy:

* LEVEL I: Machine Accuracy. This can be identified with the IEEE standard.
* LEVEL II: Arbitrary Accuracy. If you specify 200 bits of accuracy (in relative or absolute terms), this guarantees that you will not experience overflow or underflow until you exceed 200 bits. This is similar to what any computer algebra system (say, Maple) gives you.
* LEVEL III: Guaranteed Accuracy. This is the novel part: thus, you can specify 1 relative bit or 10 absolute bits of accuracy at this level. Note that 1 relative bit ensures that you get exact signs of real numbers, the critical idea in Exact Geometric Computation.
* LEVEL IV: Mixed Accuracy. This localizes the the previous accuracy levels to individual variables, for better control of accuracy and efficiency.

Every C/Cxx program (perhaps with small changes) which can access any of the first 3 levels. This property has many applications, especially for debugging. A program written for Level IV accuracy can also access any of the lower levels.

https://cs.nyu.edu/exact/[+https://cs.nyu.edu/exact/+]

https://cs.nyu.edu/exact/core_pages/downloads.html[+https://cs.nyu.edu/exact/core_pages/downloads.html+]

https://cs.nyu.edu/exact/core_pages/svn-core.html[+https://cs.nyu.edu/exact/core_pages/svn-core.html+]

coreboot
~~~~~~~~

coreboot, formerly known as LinuxBIOS,[4] is a software project aimed at replacing proprietary firmware (BIOS or UEFI) found in most computers with a lightweight firmware designed to perform only the minimum number of tasks necessary to load and run a modern 32-bit or 64-bit operating system. 

Since coreboot initializes the bare hardware, it must be ported to every chipset and motherboard that it supports. As a result, coreboot is available only for a limited number of hardware platforms and motherboard models.

One of the coreboot variants is Libreboot, a variant of coreboot aiming to be fully free of proprietary blobs. 

CPU architectures supported by coreboot include IA-32, x86-64, ARM, ARM64, MIPS and RISC-V. Supported system-on-a-chip (SOC) platforms include AMD Geode, starting with the Geode GX processor developed for the OLPC. Artec Group added Geode LX support for its ThinCan model DBE61; that code was adopted by AMD and further improved for the OLPC after it was upgraded to the Geode LX platform, and is further developed by the coreboot community to support other Geode variants. Coreboot can be flashed onto a Geode platform using Flashrom. 

coreboot typically loads a Linux kernel, but it can load any other stand-alone ELF executable, such as iPXE, gPXE or Etherboot that can boot a Linux kernel over a network, or SeaBIOS[21] that can load a Linux kernel, Microsoft Windows 2000 and later, and BSDs (previously, Windows 2000/XP and OpenBSD support was provided by ADLO[22][23]). coreboot can also load a kernel from any supported device, such as Myrinet, Quadrics, or SCI cluster interconnects. Booting other kernels directly is also possible, such as a Plan 9 kernel. Instead of loading a kernel directly, coreboot can pass control to a dedicated boot loader, such as a coreboot-capable version of GNU GRUB 2. 

https://en.wikipedia.org/wiki/Coreboot[+https://en.wikipedia.org/wiki/Coreboot+]

https://www.coreboot.org/[+https://www.coreboot.org/+]

https://en.wikipedia.org/wiki/Libreboot[+https://en.wikipedia.org/wiki/Libreboot+]

https://github.com/linuxboot/book[+https://github.com/linuxboot/book+]

https://www.pcengines.ch/apu2.htm[+https://www.pcengines.ch/apu2.htm+]

CouchPotato
~~~~~~~~~~~

CouchPotato (CP) is an automatic NZB downloader. You can keep a "movies I want"-list and it will search for NZBs & Torrents of these movies every X minutes. Once a movie is found, it will send it to SABnzbd or download the nzb/torrent to the provided folder.

https://www.openhub.net/p/CouchPotato[+https://www.openhub.net/p/CouchPotato+]

COVISE
~~~~~~

COVISE, the collaborative visualization and simulation environment, is a modular distributed visualization system. As its focus is on visualization of scientific data in virtual environments, it comprises the VR renderer OpenCOVER. COVISE development is headed by HLRS. It is portable to Windows and UNIX.

COVISE stands for COllaborative VIsualization and Simulation Environment. It is an extendable distributed software environment to integrate simulations, postprocessing and visualization functionalities in a seamless manner. From the beginning COVISE was designed for collaborative working allowing engineers and scientists to spread on a network infrastructure.

In COVISE an application is divided into several processing steps, which are represented by COVISE modules. These modules, being implemented as separate processes, can be arbitrarily spread across different heterogeneous machine platforms. Major emphasis was put on the usage of high performance infrastructures such as parallel and vector computers and fast networks.

COVISE Rendering modules support Virtual environments ranging form workbenches over powerwalls, curved screens up to full domes or CAVEs. The users can thus analyze their datasets intuitively in a fully immersive environment through state of the art visualization techniques including Volume rendering and fast sphere rendering. Physical prototypes or experiments can be included into the analysis process through Augmented Reality techniques. 

http://www.hlrs.de/covise/[+http://www.hlrs.de/covise/+]

https://github.com/hlrs-vis/covise[+https://github.com/hlrs-vis/covise+]

Craft
~~~~

CRAFT was my own dissertation project harnessing the Dyninst toolkit to perform various dynamic floating-point analyses, including cancellation detection, dynamic range tracking, and automated mixed-precision analysis. CRAFT can perform an automated search of a program's instruction space, determining the level of precision necessary in the result of each instruction to pass a user-provided verification routine assuming all other operations are done in double precision. This gives a general overview of the approximate precision requirements of the program, providing insights and guidance for mixed-precision implementation.

https://github.com/crafthpc/craft[+https://github.com/crafthpc/craft+]

https://w3.cs.jmu.edu/lam2mo/fpanalysis.html[+https://w3.cs.jmu.edu/lam2mo/fpanalysis.html+]

https://journals-sagepub-com/doi/full/10.1177/1094342016652462[+https://journals-sagepub-com/doi/full/10.1177/1094342016652462+]

CREST
~~~~~

The CREST method [1] is related to a Bayesian approach that combines presence-only occurrence data and modern climatologies to estimate the conditional response of a given taxon to a variable of interest. Taking the form of probability density functions (pdfs), these links are fitted in one or two steps based on the nature of the proxy being studied. In simple cases, where fossils can be identified at species level (e.g. foraminifers, plant macrofossils), the pdfs are defined by unimodal and parametric functions (e.g. normal or log-normal distributions depending of the nature of the studied variable).

The parameters (e.g. a mean and a standard deviation in the case of a normal or log-normal distribution) describing these distributions are estimated from the ensemble of climate values corresponding to the presence records, each being weighted as an inverse function of its abundance in the study area. This correction is needed to remove the influence of the heterogeneously distributed modern climate space and ensure that the optimum exhibited by the pdf truly reflects the climatic preference of the species, rather than the modern abundance of a given climate value.

https://chevaliermanuel.wixsite.com/webpage/crest[+https://chevaliermanuel.wixsite.com/webpage/crest+]

CRI-O
~~~~~

CRI-O is an implementation of the Kubernetes CRI (Container Runtime Interface) to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative to using Docker as the runtime for kubernetes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Clear Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.

CRI-O supports OCI container images and can pull from any container registry. It is a lightweight alternative to using Docker, Moby or rkt as the runtime for Kubernetes.

https://cri-o.io/[+https://cri-o.io/+]

CROCO
~~~~~

CROCO is a new oceanic modeling system built upon ROMS_AGRIF and the non-hydrostatic kernel of SNH (under testing), gradually including algorithms from MARS3D (sediments)  and HYCOM (vertical coordinates). An important objective for CROCO is to resolve very fine scales (especially in the coastal area), and their interactions with larger scales. It is the oceanic component of a complex coupled system including various components, e.g., atmosphere, surface waves, marine sediments, biogeochemistry and ecosystems.

CROCO Version 1.0 official release is now available in the Download section. It includes new capabilities as non-hydrostatic kernel, ocean-wave-atmosphere coupling, sediment transport, new high-order numerical schemes for advection and mixing, and a dedicated I/O server (XIOS). A new version of CROCO_TOOLS accompany this release. CROCO will keep evolving and integrating new capabilities in the following years.

https://www.croco-ocean.org/[+https://www.croco-ocean.org/+]

https://www.croco-ocean.org/download/croco-project/[+https://www.croco-ocean.org/download/croco-project/+]

https://github.com/ducousso/CROCO-NH[+https://github.com/ducousso/CROCO-NH+]

csvkit
~~~~~~

A suite of command-line tools for converting to and working with CSV, the king of tabular file formats.
The capabilities include:

* convert Excel to CSV
* convert JSON to CSV
* print column names
* select a subset of columns
* reorder columns
* find rows with matching cells
* convert to JSON
* generate summary statistics
* query with SQL
* import into PostgreSQL
* extract data from PostgreSQL

https://csvkit.readthedocs.io/en/1.0.3/[+https://csvkit.readthedocs.io/en/1.0.3/+]

https://github.com/wireservice/csvkit[+https://github.com/wireservice/csvkit+]

cuDF
~~~~

The RAPIDS cuDF library is a GPU DataFrame manipulation library based on Apache Arrow that accelerates loading, filtering, and manipulation of data for model training data preparation. The RAPIDS GPU DataFrame provides a pandas-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.

The RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA® CUDA® primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.

https://github.com/rapidsai/cudf[+https://github.com/rapidsai/cudf+]

https://github.com/rapidsai[+https://github.com/rapidsai+]

cURL
~~~~

cURL is a command-line tool for getting or sending files using URL syntax.

Since cURL uses libcurl, it supports a range of common network protocols, currently including HTTP, HTTPS, FTP, FTPS, SCP, SFTP, TFTP, LDAP, DAP, DICT, TELNET, FILE, IMAP, POP3, SMTP and RTSP (the last four only in versions newer than 7.20.0 or 9 February 2010).

cURL supports HTTPS and performs SSL certificate verification by default when a secure protocol is specified such as HTTPS. When cURL connects to a remote server via HTTPS, it will obtain the remote server certificate, then check against its CA certificate store the validity of the remote server to ensure the remote server is the one it claims to be.

libcurl is a free client-side URL transfer library,[6] supporting cookies, DICT, FTP, FTPS, Gopher, HTTP (with HTTP/2 support), HTTP POST, HTTP PUT, HTTP proxy tunneling, HTTPS, IMAP, Kerberos, LDAP, POP3, RTSP, SCP, and SMTP. The library supports the file URI scheme, SFTP, Telnet, TFTP, file transfer resume, FTP uploading, HTTP form-based upload, HTTPS certificates, LDAPS, proxies, and user-plus-password authentication. 

The libcurl library is free, thread-safe and IPv6 compatible. Bindings are available for more than 40 languages, including C/Cxx, Java, PHP and Python.

The libcurl library can support axTLS,[8] GnuTLS, mbed TLS, NSS, QSOSSL on IBM i, SChannel on Windows, Secure Transport on macOS and iOS, SSL/TLS through OpenSSL, and wolfSSL. 

https://github.com/curl/curl[+https://github.com/curl/curl+]

https://en.wikipedia.org/wiki/CURL[+https://en.wikipedia.org/wiki/CURL+]

https://curl.haxx.se/docs/httpscripting.html[+https://curl.haxx.se/docs/httpscripting.html+]

https://idratherbewriting.com/learnapidoc/docapis_understand_curl.html[+https://idratherbewriting.com/learnapidoc/docapis_understand_curl.html+]

https://curl.haxx.se/[+https://curl.haxx.se/+]

https://curl.haxx.se/docs/comparison-table.html[+https://curl.haxx.se/docs/comparison-table.html+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

kurly
^^^^^

kurly is an alternative to the widely popular curl program, written in Golang.

https://gitlab.com/davidjpeacock/kurly[+https://gitlab.com/davidjpeacock/kurly+]

Curv
~~~~

Curv is a programming language for creating art using mathematics. It's a 2D and 3D geometric modelling tool that supports full colour, animation and 3D printing.

Features:

* Curv is a simple, powerful, dynamically typed, pure functional programming language.
* Curv is easy to use for beginners. It has a standard library of predefined geometric shapes, plus operators for transforming and combining shapes. These can be plugged together like Lego to make 2D and 3D models.
* Coloured shapes are represented using Function Representation (F-Rep). They can be infinitely detailed, infinitely large, and any shape or colour pattern that can be described using mathematics can be represented exactly.
* Curv exposes the full power of F-Rep programming to experts. The standard geometry library is written entirely in Curv. Many of the demos seen on shadertoy.com can be reproduced in Curv, using shorter, simpler programs. Experts can package techniques used on shadertoy as high level operations for use by beginners.
* Rendering is GPU accelerated. Curv programs are compiled into fragment shaders which are executed on the GPU.
* Curv can export meshes to STL, OBJ and X3D files for 3D printing. The X3D format supports full colour 3D printing (on Shapeways.com, at least). These meshes are defect free: watertight, manifold, with no self intersections, degenerate triangles, or flipped triangles.

https://github.com/curv3d/curv[+https://github.com/curv3d/curv+]

CutLang
~~~~~~~

A domain specific language that aims to provide a clear, human readable way to define analyses in high energy particle physics (HEP) along with an interpretation framework of that language. A proof of principle (PoP) implementation of the CutLang interpreter, achieved using C as a layer over the CERN data analysis framework ROOT, is presently available. This PoP implementation permits writing HEP analyses in an unobfuscated manner, as a set of commands in human readable text files, which are interpreted by the framework at runtime. We describe the main features of CutLang and illustrate its usage with two analysis examples. Initial experience with CutLang has shown that a just-in-time interpretation of a human readable HEP specific language is a practical alternative to analysis writing using compiled languages such as C.

https://www.sciencedirect.com/science/article/pii/S0010465518302315[+https://www.sciencedirect.com/science/article/pii/S0010465518302315+]

https://cutlang.hepforge.org/[+https://cutlang.hepforge.org/+]

https://arxiv.org/abs/1801.05727[+https://arxiv.org/abs/1801.05727+]

CVPM
~~~~

The Control Volume Permafrost Model (CVPM) is a modular heat-transfer modeling system designed for scientific and engineering studies in permafrost terrain, and as an educational tool. CVPM implements the nonlinear heat-transfer equations in 1-D, 2-D, and 3-D cartesian coordinates, as well as in 1-D radial and 2-D cylindrical coordinates. To accommodate a diversity of geologic settings, a variety of materials can be specified within the model domain, including: organic-rich materials, sedimentary rocks and soils, igneous and metamorphic rocks, ice bodies, borehole fluids, and other engineering materials.

https://github.com/csdms-contrib/CVPM[+https://github.com/csdms-contrib/CVPM+]

https://www.geosci-model-dev.net/11/4889/2018/[+https://www.geosci-model-dev.net/11/4889/2018/+]

CyberConnector
~~~~~~~~~~~~~~

CyberConnector is a NSF-funded open source project aiming to develop an EarthCube building block for facilitating the automatic preparation and feeding of both historic and near-real time Earth Observation customized data and on-demand derived products into Earth science models.

It will automatically process the EO data into the right products in the right form needed for ESM initialization, validation, and inter-comparison. It can support many different ESMs through its standard interfaces under a unified framework.

CyberConnector can also automatically serve the model outputs in interoperable forms through standard data services to facilitate rapid inter-model comparison. It bridges the sensors and earth science models through standard interfaces, such as Web Processing Service, Sensor Planning Service, and Catalogue Service for the Web.

The system is to be demonstrated with actual earth science models: the Cloud-Resolving Model (CRM), the Community Multi-scale Air Quality Model (CMAQ), and the Finite Volume Coastal Ocean Model. The final platform are to be deployed and maintained for operational ingestion, discovery, access, and present geospatial models, data, and information.

CyberConnector consists of a set of modules:

* *COVALI* -  a sub-system for comparison and validation of atmospheric and other Earth science models
* *CyberConnector Searcher* - a sub-system for searching Earth observations, model results or virtual data products (VDP)
* *CyberConnector Orderer* - a sub-system for orderring VDP (customizing existing observations into ready-to-use format)
* *CyberConnector Service Register* - a sub-system for registering and searching geoprocessing web services

The COVALI features include:

* different projection options and 3-D view to enable multi-perspective viewing the data
* visualizing GRIB/NetCDF data on the maps
* a menu to control maps that allows users to manage the added data layers
* a number of tools to facilitate the comparison and validation among the data

https://github.com/CSISS/cc[+https://github.com/CSISS/cc+]

http://cube.csiss.gmu.edu/CyberConnector/web/index[+http://cube.csiss.gmu.edu/CyberConnector/web/index+]

https://www.earthcube.org/[+https://www.earthcube.org/+]

Cython
~~~~~~

Cython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex). It makes writing C extensions for Python as easy as Python itself.

Cython gives you the combined power of Python and C to let you

* write Python code that calls back and forth from and to C or Cxx code natively at any point.
* easily tune readable Python code into plain C performance by adding static type declarations, also in Python syntax.
* use combined source code level debugging to find bugs in your Python, Cython and C code.
* interact efficiently with large data sets, e.g. using multi-dimensional NumPy arrays.
* quickly build your applications within the large, mature and widely used CPython ecosystem.
* integrate natively with existing code and data from legacy, low-level or high-performance libraries and applications.

The Cython language is a superset of the Python language that additionally supports calling C functions and declaring C types on variables and class attributes. This allows the compiler to generate very efficient C code from Cython code. The C code is generated once and then compiles with all major C/Cxx compilers in CPython 2.6, 2.7 (2.4+ with Cython 0.20.x) as well as 3.3 and all later versions.

PyPy support is work in progress (on both sides) and is considered mostly usable since Cython 0.17. The latest PyPy version is always recommended here. 

https://cython.org/[+https://cython.org/+]

http://docs.cython.org/en/latest/[+http://docs.cython.org/en/latest/+]

https://archive.fosdem.org/2018/schedule/event/cython/[+https://archive.fosdem.org/2018/schedule/event/cython/+]

Cytoscape
~~~~~~~~~

Cytoscape is an open source software platform for visualizing molecular interaction networks and biological pathways and integrating these networks with annotations, gene expression profiles and other state data. Although Cytoscape was originally designed for biological research, now it is a general platform for complex network analysis and visualization.   Cytoscape core distribution provides a basic set of features for data integration, analysis, and visualization.   Additional features are available as Apps (formerly called Plugins).   Apps are available for network and molecular profiling analyses, new layouts, additional file format support, scripting, and connection with databases.   They may be developed by anyone using the Cytoscape open API based on Java™ technology and App community development is encouraged. Most of the Apps are freely available from Cytoscape App Store. 

https://cytoscape.org/[+https://cytoscape.org/+]

http://apps.cytoscape.org/[+http://apps.cytoscape.org/+]

CZML
~~~~

CZML is a JSON format for describing a time-dynamic graphical scene, primarily for display in a web browser running Cesium. It describes lines, points, billboards, models, and other graphical primitives, and specifies how they change with time. While Cesium has a rich client-side API, CZML allows it to be data-driven so that a generic Cesium viewer can display a rich scene without the need for any custom code. In many ways, the relationship between Cesium and CZML is similar to the relationship between Google Earth and KML. Both CZML and KML are data formats for describing scenes in their respective clients and are meant to be generated by a wide variety of applications and possibly even written by hand. And both are meant to be sufficiently client-agnostic that other, compatible clients can render the scene described therein.
CZML has a number of important characteristics, some of which distinguish it from KML:

* CZML is based on JSON.
* CZML can accurately describe properties that change value over time. For example, a line can be red for one interval of time and blue for another. Clients are also expected to be able to interpolate over time-tagged samples. If the position of a vehicle is specified at two times, the client can accurately display the location of the vehicle in between those two times by using a CZML-specified interpolation algorithm. Every property is time-dynamic.
* CZML is structured for efficient, incremental streaming to a client. The entire document need not be present on the client before the scene can be displayed. In many cases, individual clients may even join and leave the stream while it is in progress.
* CZML is optimized for client consumption; it aims to be compact and easy to parse. It is also reasonably readable and writable by humans.
* CZML is extensible. While the primary goal of CZML is to communicate a scene to a virtual-globe-like client, the format can easily be extended to communicate additional static or time-dynamic data to a more sophisticated client. For example, time-dynamic data could be displayed on a 2D chart.
* CZML is an open format. We would like as many projects to take advantage of it as possible and hope to one day formalize with a standard body, such as OGC.
* An open-source library, czml-writer, for writing CZML is maintained on Github.

https://github.com/AnalyticalGraphicsInc/czml-writer/wiki/CZML-Guide[+https://github.com/AnalyticalGraphicsInc/czml-writer/wiki/CZML-Guide+]

https://github.com/AnalyticalGraphicsInc/czml-writer[+https://github.com/AnalyticalGraphicsInc/czml-writer+]

https://github.com/cleder/czml[+https://github.com/cleder/czml+]

#DDDD

D3
~~

D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation. 

D3 allows you to bind arbitrary data to a Document Object Model (DOM), and then apply data-driven transformations to the document. For example, you can use D3 to generate an HTML table from an array of numbers. Or, use the same data to create an interactive SVG bar chart with smooth transitions and interaction.

D3 is not a monolithic framework that seeks to provide every conceivable feature. Instead, D3 solves the crux of the problem: efficient manipulation of documents based on data. This avoids proprietary representation and affords extraordinary flexibility, exposing the full capabilities of web standards such as HTML, SVG, and CSS. With minimal overhead, D3 is extremely fast, supporting large datasets and dynamic behaviors for interaction and animation. D3’s functional style allows code reuse through a diverse collection of official and community-developed modules. 

https://d3js.org/[+https://d3js.org/+]

https://github.com/d3/d3[+https://github.com/d3/d3+]

https://github.com/d3/d3/blob/master/API.md[+https://github.com/d3/d3/blob/master/API.md+]

https://www.npmjs.com/search?q=keywords:d3-module[+https://www.npmjs.com/search?q=keywords:d3-module+]

geo-timescale
^^^^^^^^^^^^^

A modular D3.js-based geologic time scale that utilizes data from the Paleobiology Database.

https://github.com/UW-Macrostrat/geo-timescale[+https://github.com/UW-Macrostrat/geo-timescale+]

D3-GEO
~~~~~~

Discrete geometry makes the challenge of projecting from the sphere to the plane much harder. The edges of a spherical polygon are geodesics (segments of great circles), not straight lines. Projected to the plane, geodesics are curves in all map projections except gnomonic, and thus accurate projection requires interpolation along each arc. D3 uses adaptive sampling inspired by a popular line simplification method to balance accuracy and performance.

The projection of polygons and polylines must also deal with the topological differences between the sphere and the plane. Some projections require cutting geometry that crosses the antimeridian, while others require clipping geometry to a great circle.

Spherical polygons also require a winding order convention to determine which side of the polygon is the inside: the exterior ring for polygons smaller than a hemisphere must be clockwise, while the exterior ring for polygons larger than a hemisphere must be anticlockwise. Interior rings representing holes must use the opposite winding order of their exterior ring. This winding order convention is also used by TopoJSON and ESRI shapefiles; however, it is the opposite convention of GeoJSON’s RFC 7946. (Also note that standard GeoJSON WGS84 uses planar equirectangular coordinates, not spherical coordinates, and thus may require stitching to remove antimeridian cuts.)

D3’s approach affords great expressiveness: you can choose the right projection, and the right aspect, for your data. D3 supports a wide variety of common and unusual map projections. For more, see Part 2 of The Toolmaker’s Guide.

D3 uses GeoJSON to represent geographic features in JavaScript. (See also TopoJSON, an extension of GeoJSON that is significantly more compact and encodes topology.) To convert shapefiles to GeoJSON, use shp2geo, part of the shapefile package. See Command-Line Cartography for an introduction to d3-geo and related tools.

https://github.com/d3/d3-geo[+https://github.com/d3/d3-geo+]

D4M
~~~

D4M is a breakthrough in computer programming that combines the advantages of five distinct processing technologies (sparse linear algebra, associative arrays, fuzzy algebra, distributed arrays, and triple-store/NoSQL databases such as Hadoop HBase and Apache Accumulo) to provide a database and computation system that addresses the problems associated with Big Data. D4M significantly improves search, retrieval, and analysis for any business or service that relies on accessing and exploiting massive amounts of digital data. Evaluations have shown D4M to simultaneously increase computing performance and to decrease the effort required to build applications by as much as 100x. Improved performance translates into faster, more comprehensive services provided by companies involved in healthcare, Internet search, network security, and more. Less, and simplified, coding reduces development times and costs. Moreover, the D4M layered architecture provides a robust environment that is adaptable to various databases, data types, and platforms.

http://www.mit.edu/\~kepner/D4M/[+http://www.mit.edu/~kepner/D4M/+]

https://ocw.mit.edu/resources/res-ll-005-d4m-signal-processing-on-databases-fall-2012/index.htm[+https://ocw.mit.edu/resources/res-ll-005-d4m-signal-processing-on-databases-fall-2012/index.htm+]

https://github.com/Accla/d4m[+https://github.com/Accla/d4m+]

https://github.com/Accla/D4M.py[+https://github.com/Accla/D4M.py+]

https://github.com/Accla/D4M.jl[+https://github.com/Accla/D4M.jl+]

https://github.com/dhutchis/d4mBB[+https://github.com/dhutchis/d4mBB+]

https://github.com/medined/D4M_Schema[+https://github.com/medined/D4M_Schema+]

https://arxiv.org/pdf/1708.02934.pdf[+https://arxiv.org/pdf/1708.02934.pdf+]

http://d4m.mit.edu/publications[+http://d4m.mit.edu/publications+]

DAE Tools
~~~~~~~~~

DAE Tools is equation-based object-oriented modelling, simulation and optimisation software. Its Hybrid approach to mathematical modelling lets you easily develop models of complex multiscale/multiphysics processes/phenomena with complex schedules and perform various activities on them such as simulation, sensitivity analysis, optimisation, parameter estimation, code-generation and model exchange. 

The following class of problems can be solved by DAE Tools:

* Initial value problems of implicit form, described by a system of linear, non-linear, and (partial-)differential algebraic equations
* Index-1 DAE systems
* With lumped or distributed parameters: Finite Difference, Finite Volume or Finite Elements Methods
* Steady-state or dynamic
* Continuous with some elements of event-driven systems (discontinuous equations, state transition networks and discrete events)

DAE Tools is initially developed to model and simulate processes in chemical process industry (mass, heat and momentum transfers, chemical reactions, separation processes, thermodynamics). However, DAE Tools can be used to develop high-accuracy models of (in general) many different kind of processes/phenomena, simulate/optimise them, visualise and analyse the results.

The following approaches/paradigms are adopted in DAE Tools:

* A hybrid approach between general-purpose programming languages (such as c++ and Python) and domain-specific modelling languages (such as Modelica, gPROMS, Ascend etc.) (more information: The Hybrid approach).
* An object-oriented approach to process modelling (more information: The Object-Oriented approach).
* An Equation-Oriented (acausal) approach where all model variables and equations are generated and gathered together and solved simultaneously using a suitable mathematical algorithm (more information: The Equation-Oriented approach).
* Separation of the model definition from the activities that can be carried out on that model. The structure of the model (parameters, variables, equations, state transition networks etc.) is given in the model class while the runtime information in the simulation class. This way, based on a single model definition, one or more different simulation/optimisation scenarios can be defined.
* Core libraries are written in standard c++, however Python is used as the main modelling language (more information: Programming language).

http://www.daetools.com/[+http://www.daetools.com/+]

http://www.daetools.com/opencs.html[+http://www.daetools.com/opencs.html+]

https://peerj.com/articles/cs-160/[+https://peerj.com/articles/cs-160/+]

DAKOTA
~~~~~~

The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models. The Dakota toolkit provides a flexible, extensible interface between such simulation codes and its iterative systems analysis methods, which include:

* optimization with gradient and nongradient-based methods;
* uncertainty quantification with sampling, reliability, stochastic expansion, and epistemic methods;
* parameter estimation using nonlinear least squares (deterministic) or Bayesian inference (stochastic); and
* sensitivity/variance analysis with design of experiments and parameter study methods.

These capabilities may be used on their own or as components within advanced strategies such as hybrid optimization, surrogate-based optimization, mixed integer nonlinear programming, or optimization under uncertainty.

DAKOTA utilizes the following Sandia-developed optimization, design of experiments, uncertainty quantification, and surrogate modeling libraries:

* https://software.sandia.gov/svn/public/ddace/ddace/[DDACE] (design and analysis of computer experiments; available under GNU LGPL; contact: Patty Hough); DDACE additionally uses the MARS and OA packages from StatLib (conditions of use)
* https://software.sandia.gov/trac/hopspack/wiki[HOPSPACK] (Hybrid Optimization Parallel Search PACKage, which supplies a robust asynchronous implementation of generating set search; available under GNU LGPL)
* JEGA (multiobjective genetic algorithms; available under GNU LGPL; contact: John Eddy)
* LHS (Latin Hypercube and Monte Carlo sampling for uncertainty quantification; as of June 18, 2009, available under GNU LGPL (formerly GNU GPL); contact: Laura Swiler)
* OPTxx (nonlinear and direct search optimization algorithms; available under GNU LGPL; contact: Patty Hough); OPTxx additionally uses Teuchos (see below).
* PECOS: univariate/multivariate orthogonal/interpolation polynomial basis functions, numerical integration drivers (quadrature, cubature, sparse grids), random variable transformations, stochastic process modeling (available separately under GNU LGPL; contact: Mike Eldred). PECOS additionally uses Boost (see below), DFFTPACK (public domain), FFTW (GNU GPL, optional extension), LHS (see above), and Teuchos (see below).
* SCOLIB (formerly known as COLINY; nongradient optimization algorithms; available under BSD; contact: John Siirola)
* Surfpack: global data fit surrogate models (available under GNU LGPL; contact: Surfpack developers)

the following Sandia-funded optimization and design of experiments libraries:

* https://software.sandia.gov/svn/public/tpl/FSUDace/[FSUDace] (quasi-Monte Carlo and centroidal voronoi tessellation sampling algorithms from Florida State; available under GNU LGPL; contact: Laura Swiler)
* Sparse grid (numerical quadrature rules and Smolyak sparse grids from Virginia Tech; available under GNU LGPL; contact: Mike Eldred)

and the following external optimization and design of experiments libraries:

* CONMIN (public domain nonlinear programming algorithms; no license required for inclusion in DAKOTA distribution)
* DOT (nonlinear programming algorithms from Vanderplaats Research and Development; optional extension requiring a separate commercial license)
* NL2SOL (public domain nonlinear least squares algorithm; no license required for inclusion in DAKOTA distribution)
* NLPQLP (nonlinear programming algorithms from Klaus Schittkowski; optional extension requiring a separate commercial license)
* NOMAD (Nonlinear Optimization by Mesh Adaptive Direct Search; available under GNU LGPL v 3.0)
* NPSOL/NLSSOL (nonlinear programming algorithms from Stanford Business Software; optional extension requiring a separate commercial license)
* PSUADE (Morris one-at-a-time sampling algorithm from Charles Tong at LLNL CASC; available under GNU LGPL)
* QUESO (Quantification of Uncertainty for Estimation, Simulation and Optimization: libraries for statisical calibration, validation, and uncertainty propagation; with thanks to Ernesto Prudencio and collaborators at UT Austin. Dakota uses the MCMC algorithms from QUESO.)

https://dakota.sandia.gov/[+https://dakota.sandia.gov/+]

https://github.com/stellarscience/dakota-stellar[+https://github.com/stellarscience/dakota-stellar+]

DALI
~~~~

Today’s deep learning applications include complex, multi-stage pre-processing data pipelines that include compute-intensive steps mainly carried out on the CPU. For instance, steps such as load data from disk, decode, crop, random resize, color and spatial augmentations and format conversions are carried out on the CPUs, limiting the performance and scalability of training and inference tasks. In addition, the deep learning frameworks today have multiple data pre-processing implementations, resulting in challenges such as portability of training and inference workflows and code maintainability.

NVIDIA Data Loading Library (DALI) is a collection of highly optimized building blocks and an execution engine to accelerate input data pre-processing for deep learning applications. DALI provides both performance and flexibility of accelerating different data pipelines, as a single library, that can be easily integrated into different deep learning training and inference applications.

Key highlights of DALI include:

* Full data pipeline accelerated from reading from disk to getting ready for training/inference
* Flexibility through configurable graphs and custom operators
* Support for image classification and segmentation workloads
* Ease of integration through direct framework plugins and open source bindings
* Portable training workflows with multiple input formats - JPEG, LMDB, RecordIO, TFRecord
* Extensible for user specific needs through open source license

DALI is preinstalled in the NVIDIA GPU Cloud TensorFlow, PyTorch, and MXNet containers in versions 18.07 and later.

https://github.com/NVIDIA/DALI[+https://github.com/NVIDIA/DALI+]

DAP
~~~

Blah.

pydap
^^^^^

pydap is an implementation of the Opendap/DODS protocol, written from scratch in pure python. You can use pydap to access scientific data on the internet without having to download it; instead, you work with special array and iterable objects that download data on-the-fly as necessary, saving bandwidth and time. The module also comes with a robust-but-lightweight Opendap server, implemented as a WSGI application.

https://github.com/pydap/pydap[+https://github.com/pydap/pydap+]

http://pydap.org/en/latest/[+http://pydap.org/en/latest/+]

Dap
~~~

Dap is a small statistics and graphics package based on C. Version 3.0 and later of Dap can read SBS programs (based on the utterly famous, industry standard statistics system with similar initials - you know the one I mean)! The user wishing to perform basic statistical analyses is now freed from learning and using C syntax for straightforward tasks, while retaining access to the C-style graphics and statistics features provided by the original implementation. Dap provides core methods of data management, analysis, and graphics that are commonly used in statistical consulting practice (univariate statistics, correlations and regression, ANOVA, categorical data analysis, logistic regression, and nonparametric analyses).

Anyone familiar with the basic syntax of C programs can learn to use the C-style features of Dap quickly and easily from the manual and the examples contained in it; advanced features of C are not necessary, although they are available. (The manual contains a brief introduction to the C syntax needed for Dap.) Because Dap processes files one line at a time, rather than reading entire files into memory, it can be, and has been, used on data sets that have very many lines and/or very many variables.

I wrote Dap to use in my statistical consulting practice because the aforementioned utterly famous, industry standard statistics system is (or at least was) not available on GNU/Linux and costs a bundle every year under a lease arrangement. And now you can run programs written for that system directly on Dap! I was generally happy with that system, except for the graphics, which are all but impossible to use,  but there were a number of clumsy constructs left over from its ancient origins. Thus, I decided to mimic the core of the functionality of that system in the context of the C language, which allows much more programming flexibility. 

https://www.gnu.org/software/dap/[+https://www.gnu.org/software/dap/+]

http://savannah.gnu.org/projects/dap[+http://savannah.gnu.org/projects/dap+]

DAPPER
~~~~~~

DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. The tests provide experimental support and guidance for new developments in DA.

DAPPER enables the numerical investigation of DA methods through a variety of typical test cases and statistics. It (a) reproduces numerical benchmarks results reported in the literature, and (b) facilitates comparative studies, thus promoting the (a) reliability and (b) relevance of the results. DAPPER is (c) open source, written in Python, and (d) focuses on readability; this promotes the (c) reproduction and (d) dissemination of the underlying science, and makes it easy to adapt and extend. In summary, it is well suited for teaching and fundamental DA research.

https://github.com/nansencenter/DAPPER[+https://github.com/nansencenter/DAPPER+]

Dash
~~~~

Dash is a Python framework for building analytical web applications. No JavaScript required.
Built on top of Plotly.js, React, and Flask, Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical python code.

Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements.
While Dash apps are viewed in the web browser, you don’t have to write any Javascript or HTML. Dash provides a Python interface to a rich set of interactive web-based components.

Every aesthetic element of the app is customizable: The sizing, the positioning, the colors, the fonts. Dash apps are built and published in the Web, so the full power of CSS is available. 

Dash provides a simple reactive decorator for binding your custom data analysis code to your Dash user interface.
When an input element changes (e.g. when you select an item in the dropdown or drag a slider), Dash’s decorator provides your Python code with the new value of the input.

Your Python function can do anything that it wants with this input new value: It could filter a Pandas DataFrame, make a SQL query, run a simulation, perform a calculation, or start an experiment. Dash expects that your function will return a new property of some element in the UI, whether that’s a new graph,a new table, or a new text element.

Dash applications are web servers running Flask and communicating JSON packets over HTTP requests. Dash’s frontend renders components using React.js, the Javascript user-interface library written and maintained by Facebook.

Dash ships with a Graph component that renders charts with plotly.js. Plotly.js is a great fit for Dash: it’s declarative, open source, fast, and supports a complete range of scientific, financial, and business charts. Plotly.js is built on top of D3.js (for publication-quality, vectorized image export) and WebGL (for high performance visualization).

Dash’s Graph element shares the same syntax as the open-source plotly.py library, so you can easily to switch between the two. Dash’s Graph component hooks into the plotly.js event system, allowing Dash app authors to write applications that respond to hovering, clicking, or selecting points on a Plotly graph.

https://plot.ly/[+https://plot.ly/+]

https://github.com/plotly[+https://github.com/plotly+]

https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503[+https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503+]

https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e[+https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e+]

DASH
~~~~

DASH is a Cxx Template Library for Distributed Data Structures with Support for Hierarchical Locality for HPC and Data-Driven Science.
 
Exascale systems are scheduled to become available in the coming years and will be characterized by extreme scale and a multilevel hierarchical organization. 

Efficient and productive programming of these systems will be a challenge, especially in the context of data-intensive applications. Adopting the promising notion of Partitioned Global Address Space (PGAS) programming the DASH project develops a data-structure oriented Cxx template library that provides hierarchical PGAS-like abstractions for important data containers (multidimensional arrays, lists, hash tables, etc.) and allows a developer to control (and explicitly take advantage of) the hierarchical data layout of global data structures.

In contrast to other PGAS approaches such as UPC, DASH does not propose a new language or require compiler support to realize global address space semantics. Instead, operator overloading and other advanced Cxx features are used to provide the semantics of data residing in a global and hierarchically partitioned address space based on a runtime system with one-sided messaging primitives provided by MPI or GASNet.

As such, DASH can co-exist with parallel programming models already in widespread use (like MPI) and developers can take advantage of DASH by incrementally replacing existing data structures with the implementation provided by DASH. Efficient I/O directly to and from the hierarchical structures and DASH-optimized algorithms such as map-reduce are also part of the project. Two applications from molecular dynamics and geoscience are driving the project and are adapted to use DASH in the course of the project.

http://www.dash-project.org/[+http://www.dash-project.org/+]

https://github.com/dash-project/pydash[+https://github.com/dash-project/pydash+]

Dask
~~~~

Dask provides ways to scale Pandas, Scikit-Learn, and Numpy workflows with minimal rewriting. It integrates well with these tools so that it copies most of their API and uses their data structures internally. Moreover, Dask is co-developed with these libraries to ensure that they evolve consistently, minimizing friction caused from transitioning from workloads on a local laptop, to a multi-core workstation, and to a distributed cluster. Analysts familiar with Pandas/Scikit-Learn/Numpy will be immediately familiar with their Dask equivalents, and have much of their intuition carry over to a scalable context.

Dask is routinely run on thousand-machine clusters to process hundreds of terabytes of data efficiently. It has utilities and documentation on how to deploy in-house, on the cloud, or on HPC super-computers. It supports encryption and authentication using TLS/SSL certificates. It is resilient and can handle the failure of worker nodes gracefully and is elastic, and so can take advantage of new nodes added on-the-fly. Dask includes several user APIs that are used and smoothed over by thousands of researchers across the globe working in different domains.

Dask can enable efficient parallel computations on single machines by leveraging their multi-core CPUs and streaming data efficiently from disk. It can run on a distributed cluster, but it doesn’t have to. Dask allows you to swap out the cluster for single-machine schedulers which are surprisingly lightweight, require no setup, and can run entirely within the same process as the user’s session.

To avoid excess memory use, Dask is good at finding ways to evaluate computations in a low-memory footprint when possible by pulling in chunks of data from disk, doing the necessary processing, and throwing away intermediate values as quickly as possible. This lets analysts perform computations on moderately large datasets (100GB+) even on relatively low-power laptops. This requires no configuration and no setup, meaning that adding Dask to a single-machine computation adds very little cognitive overhead.

https://dask.org/[+https://dask.org/+]

https://github.com/dask/dask[+https://github.com/dask/dask+]

https://github.com/dask/dask-examples[+https://github.com/dask/dask-examples+]

dask-cuda
^^^^^^^^^

Various utilities to improve interoperation between Dask and CUDA-enabled systems.
This repository is designed to be a catch-all for Dask and CUDA utilities. It is experimental and should not be relied upon.

https://github.com/rapidsai/dask-cuda[+https://github.com/rapidsai/dask-cuda+]

dask-cudf
^^^^^^^^^

Dask support for distributed GDF object.  A partitioned gpu-backed dataframe, using Dask.

https://github.com/rapidsai/dask-cudf[+https://github.com/rapidsai/dask-cudf+]

dask-kubernetes
^^^^^^^^^^^^^^^

Dask Kubernetes deploys Dask workers on Kubernetes clusters using native Kubernetes APIs. It is designed to dynamically launch short-lived deployments of workers during the lifetime of a Python process.

Currently, it is designed to be run from a pod on a Kubernetes cluster that has permissions to launch other pods. However, it can also work with a remote Kubernetes cluster (configured via a kubeconfig file), as long as it is possible to open network connections with all the workers nodes on the remote cluster.

http://kubernetes.dask.org/en/latest/[+http://kubernetes.dask.org/en/latest/+]

https://github.com/dask/dask-kubernetes[+https://github.com/dask/dask-kubernetes+]

https://docs.dask.org/en/latest/setup/kubernetes.html[+https://docs.dask.org/en/latest/setup/kubernetes.html+]

https://www.informaticslab.co.uk/home/2018/02/07/instant-access-to-auto-scaling-personal-python-clusters[+https://www.informaticslab.co.uk/home/2018/02/07/instant-access-to-auto-scaling-personal-python-clusters+]

dask-xgboost
^^^^^^^^^^^^

Distributed training with XGBoost and Dask.distributed.
This repository enables you to perform distributed training with XGBoost on Dask.array and Dask.dataframe collections.

https://github.com/rapidsai/dask-xgboost/tree/dask-cudf[+https://github.com/rapidsai/dask-xgboost/tree/dask-cudf+]

distributed
^^^^^^^^^^^

Dask.distributed is a lightweight library for distributed computing in Python. It extends both the concurrent.futures and dask APIs to moderate sized clusters.

https://distributed.readthedocs.io/en/latest/[+https://distributed.readthedocs.io/en/latest/+]

dat
~~~

Ever tried moving large files and folders to other computers? Usually this involves one of a few strategies: being in the same location (usb stick), using a cloud service (Dropbox), or using old but reliable technical tools (rsync). None of these easily store, track, and share your data over time. People often are stuck choosing between security, speed, or ease of use. Dat provides all three by using a state of the art technical foundation and user friendly tools for fast and encrypted file sharing that you control.

Dat is free software built for the public by Code for Science & Society, a nonprofit. Researchers, analysts, libraries, and universities are already using Dat to archive and distribute scientific data. Developers are building applications on Dat for browsing peer-to-peer websites and offline editable maps. Anyone can use Dat to backup files or share those cute cat pictures with a friend. Install and get started today by using the desktop application, command line, or JavaScript library.

Cloud services, such as Dropbox or GitHub, force users to store data on places outside of their control. Until now, it has been very difficult to avoid centralized servers without major sacrifices. Dat's unique distributed network allows users to store data where they want. By decentralizing storage, Dat also increases speeds by downloading from many sources at the same time.

Having a history of how files have changed is essential for effective collaboration and reproducibility. Git has been promoted as a solution for history, but it becomes slow with large files and a high learning curve. Git is designed for editing source code, while Dat is designed for sharing files. With a few simple commands, you can version files of any size. People can instantly get the latest files or download previous versions.

In sum, we've taken the best parts of Git, BitTorrent, and Dropbox to design Dat.

https://datproject.org/[+https://datproject.org/+]

https://dat.land/[+https://dat.land/+]

archipel
^^^^^^^^

archipel is a decentralized archiving and media library system. It's the centerpiece of Archipel: Somoco, a project to preserve grassroots history. archipel is based on the Dat project and is written in JavaScript.

So far, archipel basically is a peer-to-peer shared filesystem (like Dropbox or Google Drive, but completely decentralized). Once you have the app running, you can create archives, add files, share these archives with others and give selected people write access.

Each archive has a public key, which is its address. The private key, which only the creator of the archive has, gives you write access. You share the public key with anyone to give read access (e.g. by publishing it on a website or by sharing it privately via encrypted email).

When adding an existing archive (by copying its public key) a new local key is created. The original creator of the archive may authorize these local keys to give them write access to the archive.

https://github.com/arso-project/archipel[+https://github.com/arso-project/archipel+]

Beaker
^^^^^^

Beaker is an experimental peer-to-peer browser with support for the dat protocol.
Beaker is an experimental browser for exploring and building the peer-to-peer Web. 

https://beakerbrowser.com/[+https://beakerbrowser.com/+]

ScienceFair
^^^^^^^^^^^

The futuristic, fabulous and free desktop app for working with scientific literature.
Discover, collect, organise, read and analyse scientific papers.
Be part of a movement to make science fair.

ScienceFair uses blazing-fast search and a clean user interface to help you find and filter the literature you need. No hidden menus or complex settings.

Instead of static PDFs, ScienceFair uses the eLife Lens reader for a rich reading experience that helps you navigate and interpret scientific papers better.

Search your own library and any number of distributed literature collections simultaneously - the results are seamlessly merged as they stream in from the peer-to-peer network.

Results are automatically data-mined in real-time, giving you a live updating dashboard you can use to analyse the literature and refine your discovery process.

http://sciencefair-app.com/[+http://sciencefair-app.com/+]

http://lens.elifesciences.org/about/[+http://lens.elifesciences.org/about/+]

https://elifesciences.org/labs/88b45406/sciencefair-a-new-desktop-science-library[+https://elifesciences.org/labs/88b45406/sciencefair-a-new-desktop-science-library+]

dataClay
~~~~~~~~

dataClay is a distributed object-oriented data store that enables programmers to handle persistence using the same model they use in their object-oriented applications (written in Java or Python), thus avoiding time-consuming transformations between persistent and non-persistent data models.
In addition, dataClay enables the execution of code next to the data. By moving computation close to the data, dataClay reduces the amount and size of data transfers between the application and the data store, thus improving performance of applications.
 
The features include:

* Single data model: dataClay manages persistent objects using the same abstractions than the supported programming languages, avoiding the time-consuming task of writing mapping code between the program's form of data and the one used by DBMSs or files.
* Distribution: in dataClay data can be distributed among several backends to provide scalability in the amount of data that can be handled, and to exploit parallelism in data-intensive applications.
* Computation close to data: dataClay stores object methods together with the data, thus being able to execute them in the backed where the data resides, instead of moving the data to the application address space.
* In-memory data store: dataClay exploits memory usage as much as possible to improve performance of client applications by keeping objects, and the references between them, instantiated as native language objects ready to be used.
* Replica management: dataClay offers a simple, customizable, and fine-grained mechanism to synchronize replicas in different backends. It enables the application of different synchronization policies (or none at all) depending on the type of data, thus paying the overhead of synchronization only when it is required by the applications.
* Memory and disk management: dataClay takes care of the unreachable objects that may be generated by applications by means of a garbage collector that frees the space they take both from memory and from disk.
* Integration with COMPSs: dataClay is fully integrated with the COMPSs parallel programming model and runtime, thus easing the development of applications that take advantage of data distribution and data locality.

https://www.bsc.es/research-and-development/software-and-apps/software-list/dataclay[+https://www.bsc.es/research-and-development/software-and-apps/software-list/dataclay+]

DataFS
~~~~~~

DataFS provides an abstraction layer for data storage systems. You can combine versioning, metadata management, team permissions and logging, and file storage all in a simple, intuitive interface.

Our goal is to make reading and writing large numbers of files in a team in a distributed, performance-critical computing environment feel as easy and intuitive as working with a handful of files on a local machine.

https://datafs.readthedocs.io/en/latest/[+https://datafs.readthedocs.io/en/latest/+]

https://github.com/ClimateImpactLab/DataFS[+https://github.com/ClimateImpactLab/DataFS+]

DataMelt
~~~~~~~~

DataMelt ("DMelt") is a free software for numeric computation, mathematics, statistics, symbolic calculations, data analysis and data visualization. This program combines the simplicity of scripting languages, such as Python, Ruby, Groovy (and others), with hundreds of Java numerical and graphical packages. Unlike other similar programs, DataMelt can be used for scientific computations using Java, word's most-popular enterprise programming language. The program runs on Windows/Mac OS/Linux and Android platform.

The features include:

* DataMelt, or DMelt, is a software for numeric computation, statistics, analysis of large data volumes ("big data") and scientific visualization. The program can be used in many areas, such as natural sciences, engineering, modeling and analysis of financial markets.
* DMelt is a computational platform. It can be used with different programming languages on different operating systems. Unlike other statistical programs, it is not limited by a single programming language.
* DMelt runs on the Java platform, but can be used with the Python language too. Thus this software combines the word's most-popular enterprise language with the most popular scripting language used in data science.
* DataMelt allows the usage of scripting languages which are significantly faster than the standard Python implemented in C. For example,  DataMelt provides Groovy scripting which is a factor 10 faster than Python. Note algorithms implemented in Java are also significantly faster than those in Python.
* Python programming can use more than 40,000 Java classes for numeric computation and scientific visualization. In addition, more than 4000 classes come with Java API, plus 500 native Python modules. Not to mention modules of Groovy and Ruby.
* DMelt creates high-quality vector-graphics images (SVG, EPS, PDF etc.) that can be included in LaTeX and other text-processing systems.

DataMelt can be used with several scripting languages for the Java platform: Jython (Python programming language), Groovy, JRuby (Ruby programming language) and BeanShell. All scripting languages use common DMelt JAVA API. Data analyses and statistical computations can be done in JAVA. Finally, symbolic calculations can be done using Matlab/Octave high-level interpreted language integrated with JAVA.

https://jwork.org/home/datamelt/[+https://jwork.org/home/datamelt/+]

DATAPAC
~~~~~~~

The DATAPAC library was written by James Filliben of the Statistical Engineering Division. After these routines were incorporated into the Dataplot program, development of DATAPAC stopped. However, there are some subroutines here that may still be of interest. In particular, there are a number of routines for computing various probability functions.

This software is not formally supported and is not being further developed. It is provided on an "as is" basis. There is no formal documentation for the subroutines. However, most of the subroutines contain usage instructions in the comments in the source code.

These routines are written in Fortran 77 and should be portable to most Fortran 77 compilers.

https://www.nist.gov/itl/sed/datapac[+https://www.nist.gov/itl/sed/datapac+]

Dataplot
~~~~~~~~

Dataplot® is a free, public-domain, multi-platform (Unix, Linux, Mac OS X, Windows XP/VISTA/7) software system for scientific visualization, statistical analysis, and non-linear modeling. The target Dataplot user is the researcher and analyst engaged in the characterization, modeling, visualization, analysis, monitoring, and optimization of scientific and engineering processes. The original version was released by James J. Filliben in 1978 with continual enhancements to present. 

https://www.itl.nist.gov/div898/software/dataplot/homepage.html[+https://www.itl.nist.gov/div898/software/dataplot/homepage.html+]

https://www.itl.nist.gov/div898/handbook/toolaids/pff/index.htm[+https://www.itl.nist.gov/div898/handbook/toolaids/pff/index.htm+]

Datasette
~~~~~~~~~

Datasette is a tool for exploring and publishing data. It helps people take data of any shape or size and publish that as an interactive, explorable website and accompanying API.

Datasette is aimed at data journalists, museum curators, archivists, local governments and anyone else who has data that they wish to share with the world.

https://github.com/simonw/datasette[+https://github.com/simonw/datasette+]

https://datasette.readthedocs.io/en/stable/[+https://datasette.readthedocs.io/en/stable/+]

https://github.com/PyTables/datasette-pytables[+https://github.com/PyTables/datasette-pytables+]

Datashader
~~~~~~~~~~

Datashader is a graphics pipeline system for creating meaningful representations of large datasets quickly and flexibly. Datashader breaks the creation of images into a series of explicit steps that allow computations to be done on intermediate representations. This approach allows accurate and effective visualizations to be produced automatically without trial-and-error parameter tuning, and also makes it simple for data scientists to focus on particular data and relationships of interest in a principled way.

The computation-intensive steps in this process are written in Python but transparently compiled to machine code using Numba and flexibly distributed across cores and processors using Dask , providing a highly optimized rendering pipeline that makes it practical to work with extremely large datasets even on standard hardware. 

http://datashader.org/[+http://datashader.org/+]

https://github.com/pyviz/datashader[+https://github.com/pyviz/datashader+]

Dataverse
~~~~~~~~~

The Dataverse is an open source web application to share, preserve, cite, explore and analyze research data.[1][2] Researchers, data authors, publishers, data distributors, and affiliated institutions all receive appropriate credit via a data citation with a persistent identifier (e.g., DOI, or Handle).

A Dataverse repository hosts multiple dataverses. Each dataverse contains dataset(s) or other dataverses, and each dataset contains descriptive metadata and data files (including documentation and code that accompany the data).

Enjoy full control over your data. Receive web visibility, academic credit, and increased citation counts. A personal dataverse is easy to set up, allows you to display your data on your personal website, can be branded uniquely as your research program, makes your data more discoverable to the research community, and satisfies data management plans.

Seamlessly manage the submission, review, and publication of data associated with published articles. Establish an unbreakable link between articles in your journal and associated data. Participate in the open data movement by using Dataverse as part of your journal data policy or list of repository recommendations.

Establish a research data management solution for your community. Federate with a growing list of Dataverse repositories worldwide for increased discoverability of your community’s data. Participate in the drive to set norms for sharing, preserving, citing, exploring, and analyzing research data.

https://dataverse.org/[+https://dataverse.org/+]

https://en.wikipedia.org/wiki/Dataverse[+https://en.wikipedia.org/wiki/Dataverse+]

http://www.dlib.org.ezproxy.library.tamu.edu/dlib/january11/crosas/01crosas.html[+http://www.dlib.org.ezproxy.library.tamu.edu/dlib/january11/crosas/01crosas.html+]

DataWarrior
~~~~~~~~~~~

DataWarrior combines dynamic graphical views and interactive row filtering with chemical intelligence. Scatter plots, box plots, bar charts and pie charts not only visualize numerical or category data, but also show trends of multiple scaffolds or compound substitution patterns. Chemical descriptors encode various aspects of chemical structures, e.g. the chemical graph, chemical functionality from a synthetic chemist’s point of view or 3-dimensional pharmacophore features. These allow for fundamentally different types of molecular similarity measures, which can be applied for many purposes including row filtering and the customization of graphical views. DataWarrior supports the enumeration of combinatorial libraries as the creation of evolutionary libraries. Compounds can be clustered and diverse subsets can be picked. Calculated compound similarities can be used for multidimensional scaling methods, e.g. Kohonen nets. Physicochemical properties can be calculated, structure activity relationship tables can be created and activity cliffs be visualized.

http://www.openmolecules.org/datawarrior/index.html[+http://www.openmolecules.org/datawarrior/index.html+]

DBCSR
~~~~~

DBCSR is a library designed to efficiently perform sparse matrix matrix multiplication, among other operations. It is MPI and OpenMP parallel and can exploit GPUs via CUDA.

https://github.com/cp2k/dbcsr[+https://github.com/cp2k/dbcsr+]

https://www.cp2k.org/dbcsr[+https://www.cp2k.org/dbcsr+]

dCache
~~~~~~

dCache is a distributed storage system proven to scale to hundreds of Petabytes. Originally conceived as a disk cache (hence the name) in front of a tertiary storage to provide efficient data access for data intensive scientific experiments in the field of High Energy Physics (HEP) it has evolved into highly scalable general-purpose open source storage solution.

A dCache instance will generally consist of many storage (or "pool") nodes. On those nodes, normal Linux filesystems (btrfs, ext4, XFS, ZFS) are used to store data.

Alternatively, dCache pools can use storage space provided by a Ceph object storage system.

In addition to those possibilities, dCache can use its hierarchical storage management capabilities to transparently use storage systems with different characteristics (like tape libraries for lower-cost, but higher-latency storage). Built-in mechanisms can be used to increase performance and balance loads, increase resilience and availability. dCache also supplies advanced control systems to manage data as well as data flows.

dCache supports the following I/O (and data management) protocols:

* dCap
* FTP (including GridFTP)
* HTTP (and WEBDAV)
* NFS (parallel NFSv4)
* SRM
* XRootD

dCache supports X.509 certificate based authentication through the Grid Security Infrastructure used as well as username/password authentication and LDAP. For some workloads, users can also authenticate using macaroons or OpenID Connect.

dCache provides fine-grained POSIX and NFS-style access control list (ACLs) based file/directory authorization.

Other features include:

* Resilience and high availability can be configured by enabling multiple file replicas and flexible variety of replica placement policies.
* Easy migration of data via the migration module.
* The "billing" system provides a powerful cost calculation system that allows to control the data flow (reading and writing from/to data servers, between data servers and also between data servers and tape).
* Load balancing and performance tuning by hot pool replication (via cost calculation and replicas created by pool-to-pool-transfers).
* Space management and support for space tokens.
* Detailed logging and debugging as well as accounting and statistics.
* XML information provider with detailed live information about the system.
* Scriptable adminstration interface with a terminal-based front-end.
* Web-interface with live information of the most important information.
* Automatic checksumming for data integrity.

https://www.dcache.org/[+https://www.dcache.org/+]

https://github.com/dCache/dcache[+https://github.com/dCache/dcache+]

https://www.dcache.org/manuals/publications.shtml[+https://www.dcache.org/manuals/publications.shtml+]

https://github.com/dCache/dcache/blob/master/docs/TheBook/src/main/markdown/index.md[+https://github.com/dCache/dcache/blob/master/docs/TheBook/src/main/markdown/index.md+]

https://github.com/dCache/dcap[+https://github.com/dCache/dcap+]

http://xrootd.org/[+http://xrootd.org/+]

dCpp
~~~~

Automatic differentiation in Cxx; infinite differentiability of conditionals, loops, recursion and all things Cxx

dCpp is a tool for automatic differentiation made to be intuitive to the mind of a Cxx programmer and non-invasive to his process. Despite its easy to use nature it retains flexibility, allowing implementations of differentiable (sub) programs operating on differentiable derivatives of other (sub) programs, where the entire process may again be differentiable. This allows trainable training processes, and flexible program analysis through operational calculus.

dCpp was originally developed as an example of how automatic differentiation can be viewed through Tensor and Operational Calculus. It has since been applied to a variety of tasks from dynamical systems analysis and digital geometry, to general program analysis and optimization by various parties.

https://github.com/ZigaSajovic/dCpp[+https://github.com/ZigaSajovic/dCpp+]

https://arxiv.org/abs/1610.07690[+https://arxiv.org/abs/1610.07690+]

https://arxiv.org/abs/1612.02731[+https://arxiv.org/abs/1612.02731+]

deal.II
~~~~~~~

deal.II — a name that originally meant to indicate that it is the successor to the Differential Equations Analysis Library — is a Cxx program library targeted at the computational solution of partial differential equations using adaptive finite elements. It uses state-of-the-art programming techniques to offer you a modern interface to the complex data structures and algorithms required.

The main aim of deal.II is to enable rapid development of modern finite element codes, using among other aspects adaptive meshes and a wide array of tools classes often used in finite element program. Writing such programs is a non-trivial task, and successful programs tend to become very large and complex. We believe that this is best done using a program library that takes care of the details of grid handling and refinement, handling of degrees of freedom, input of meshes and output of results in graphics formats, and the like. Likewise, support for several space dimensions at once is included in a way such that programs can be written independent of the space dimension without unreasonable penalties on run-time and memory consumption. 

https://www.dealii.org/[+https://www.dealii.org/+]

DEAP
~~~~

DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. It seeks to make algorithms explicit and data structures transparent. It works in perfect harmony with parallelisation mechanisms such as multiprocessing and SCOOP.

DEAP includes the following features:

* Genetic algorithm using any imaginable representation
** List, Array, Set, Dictionary, Tree, Numpy Array, etc.
* Genetic programing using prefix trees
** Loosely typed, Strongly typed
** Automatically defined functions
* Evolution strategies (including CMA-ES)
* Multi-objective optimisation (NSGA-II, SPEA2, MO-CMA-ES)
* Co-evolution (cooperative and competitive) of multiple populations
* Parallelization of the evaluations (and more)
* Hall of Fame of the best individuals that lived in the population
* Checkpoints that take snapshots of a system regularly
* Benchmarks module containing most common test functions
* Genealogy of an evolution (that is compatible with NetworkX)
* Examples of alternative algorithms : Particle Swarm Optimization, Differential Evolution, Estimation of Distribution Algorithm

https://github.com/DEAP/deap[+https://github.com/DEAP/deap+]

https://deap.readthedocs.io/en/master/[+https://deap.readthedocs.io/en/master/+]

https://github.com/DEAP/notebooks[+https://github.com/DEAP/notebooks+]

Delta3D
~~~~~~~

Delta3d is an open source software gaming/simulation engine API.
It is a widely used, community-supported, open-source game and simulation engine. delta3d is appropriate for a wide variety of uses including training, education, visualization, and entertainment. Delta3d is unique, because it offers features specifically suited to the Modeling, Simulation and DoD communities, such as the High Level Architecture (HLA), After Action Review (AAR), large scale terrain support, and SCORM Learning Management System (LMS) integration. 

Delta3d is an Open Source engine which can be used for games, simulations, or other graphical applications. Its modular design integrates other well-known Open Source projects such as Open Scene Graph, Open Dynamics Engine, Character Animation Library(CAL3D), and OpenAL. Rather than bury the underlying modules, Delta3D integrates them together in an easy-to-use Application programming interface (API) -- always allowing access to the important underlying components. This provides a high-level API, while still allowing the end user the option of having low-level functionality. The Delta3D engine renders using the Open Graphics Library (OpenGL) that imports a whole list of diverse file formats.

https://sourceforge.net/projects/delta3d/[+https://sourceforge.net/projects/delta3d/+]

https://en.wikipedia.org/wiki/Delta3D[+https://en.wikipedia.org/wiki/Delta3D+]

https://github.com/delta3d/delta3d/tree/master/delta3d[+https://github.com/delta3d/delta3d/tree/master/delta3d+]

http://www.deltaengine.net/home[+http://www.deltaengine.net/home+]

https://delta3d.sourceforge.io/API/html/index.html[+https://delta3d.sourceforge.io/API/html/index.html+]

https://www.computer.org/csdl/magazine/cg/2005/03/mcg2005030010/13rRUILLkpJ[+https://www.computer.org/csdl/magazine/cg/2005/03/mcg2005030010/13rRUILLkpJ+]

DERT
~~~~

Desktop Exploration of Remote Terrain (DERT) is a software tool for exploring large Digital Terrain Models (DTMs) in 3D. It aids in understanding topography and spatial relationships of terrain features, as well as performing simple analysis tasks relevant to the planetary science community.

DERT was developed by the Autonomous Systems and Robotics Area of the Intelligent Systems Division at NASA Ames Research Center. It leverages techniques implemented for science planning support applications provided to a number of NASA missions including Phoenix Mars Lander (PML) and Mars Science Laboratory (MSL).

DERT was funded by the Mars Reconnaissance Orbiter (MRO) mission and developed in collaboration with members of the MRO Context Camera (CTX) science team. DERT is licensed under the NASA Open Source Agreement (NOSA).

DERT constructs a virtual world from a DTM, attempting to stay true to dimension, light, and color. Using a mouse, the user may freely navigate throughout this world, viewing the terrain from any location. In addition to visualization, DERT provides:

* Measurement tools for distance, slope, area, and volume
* Artificial and solar light with positioning feature
* Shadows
* Multiple orthoimage overlays with adjustable transparency
* Landmarks
* Elevation profile
* Cutting plane with terrain difference map
* Through-the-lens view from a camera located on the terrain surface
* Terrain height exaggeration

The term Digital Terrain Model refers to the combination of regularly sampled digital terrain elevation data, a Digital Elevation Model (DEM), with one or more co-registered orthogonally projected digital image overlays, or "ortho-images". Such models are typically generated photogrammetrically from orbital imagery, or directly from orbital lidar and radar altimetry data. Available data sets include those from NASA planetary missions such as Mars Reconnaissance Orbiter (MRO), Mars Global Surveyor (MGS), and Lunar Reconnaissance Orbiter (LRO), as well as those from the Landsat and Shuttle Radar Topography terrestrial missions.

To maintain the interactivity of the virtual world, DERT uses a multi-resolution file structure called a landscape. A landscape is a directory of co-registered layers, each of which contains a tiled pyramid created from the original DTM rasters. This pyramid consists of a quad-tree of tiles representing a raster file. Each branch of the quad-tree contains a tile covering one quarter of the area of its parent and at 4 times the detail. As the user navigates through the world, near tiles are replaced with those of higher resolution while far tiles are replaced with those of less detail. Tile edges are stitched together before rendering. LayerFactory, a companion application, is provided to create landscape layer pyramids from raster files.

https://github.com/nasa/DERT/wiki[+https://github.com/nasa/DERT/wiki+]

https://github.com/nasa/DERT/wiki/Examples[+https://github.com/nasa/DERT/wiki/Examples+]

https://github.com/nasa/DERT[+https://github.com/nasa/DERT+]

DevIL
~~~~~

Developer's Image Library (DevIL) is a programmer's library to develop applications with very powerful image loading capabilities, yet is easy for a developer to learn and use. Ultimate control of images is left to the developer, so unnecessary conversions, etc. are not performed. DevIL utilizes a simple, yet powerful, syntax. DevIL can load, save, convert, manipulate, filter and display a wide variety of image formats.

Currently, DevIL can load .bmp, .cut, .dds, .doom, .exr, .hdr, .gif, .ico, .jp2, .jpg,.lbm, .mdl, .mng, .pal, .pbm, .pcd, .pcx, .pgm, .pic, .png, .ppm, .psd, .psp, .raw, .sgi, .tga and .tif files.
Formats supported for saving include .bmp, .dds, .h, .jpg, .pal, .pbm, .pcx, .pgm, .png, .ppm, .raw, .sgi, .tga and .tif.

DevIL currently supports the following APIs for display: OpenGL, Windows GDI, SDL, DirectX and Allegro. Compilers that can compile DevIL or use it include Djgpp, MSVCxx, Linux gcc, Delphi, Visual Basic, Power Basic and Dev-Cxx.

http://openil.sourceforge.net/[+http://openil.sourceforge.net/+]

https://github.com/DentonW/DevIL[+https://github.com/DentonW/DevIL+]

https://github.com/pitzer/SiftGPU[+https://github.com/pitzer/SiftGPU+]

DFILE Tools
~~~~~~~~~~~

DFILE Tools are reusable software utilities and libraries for batch applications. Their primary function is to process files containing variable length text data. These software tools will generally batch process business rules faster and with fewer operational support issues than SQL databases.
While DFILE Tools utilities can be used to perform common data processing tasks through use of command line arguments and control files, sometimes business rules require custom programming. A C language API is available to access software libraries for reading and writing data files. 

The following are DFILE Tools features:

* Applications reference data fields by name instead of field position within record.
* Meta-data information is kept in configuration files.
* Sort and join utilities have control file interfaces to support processing multiple data files at a time.
* Parallel processing support is available in job stream utility. This includes checkpoint tracking for failure recovery.
* Utilities can directly process data stored in GZIP format.
* Utilities have predicate scripting language for discarding unwanted records. 

A notable weakness associated with standard UNIX tools is their variable length record format. Using special characters for delimiting fields and records is convenient for configuration information but inadequate for processing actual data. Using printable characters as delimiter characters introduces a risk that the delimiter character may exist as data. On the other hand, non-printable delimiter characters are inconvenient to use with some UNIX tools. In either case, processing delimiter formats are inefficient. A better alternative is to store the length of each field in one byte prior to the actual value. This limits field values to 255 characters but is significantly more efficient than processing the delimiter format. For flexibility, both methods are supported.

The utilities in DFILE tools are:

* *dcat* - for ad hoc instances to view data
* *dfile_exec* - a scheduler with job stream management features
* *dfile_partition* - for splitting large file into smaller ones
* *dfile_sort* - for ordering data records, including sorting unordered records and merging already ordered records
* *dfile_cache_create* - for loading sorted dfiles into a UNIX shared memory segment
* *dfile_join* - for joining records between dfiles
* *dfile_agfunc* - for performing summarization operations using aggregate functions
* *dfile_unique* - for purging duplicate records
* *dfile_transform* - for making simple data value changes
* *dfile_diff* - for comparing dfiles
* *fixed2dfile* - for converting fixed length record files to dfiles
* *nsplit* - for efficiently splitting large ASCII data files into smaller files

https://sites.google.com/site/dfiletools/[+https://sites.google.com/site/dfiletools/+]

http://savannah.nongnu.org/projects/dfiletools[+http://savannah.nongnu.org/projects/dfiletools+]

DGGRID
~~~~~~

DGGRID is a public domain software program for creating and manipulating
Discrete Global Grids.
A Discrete Global Grid (DGG) consists of a set of regions that form a
partition of the Earth’s surface, where each region has a single point
contained in the region associated with it. Each region/point combination is a
called a cell. Depending on the application, data objects or values may be
associated with the regions, points, or cells of a DGG. A Discrete Global Grid
System (DGGS) is a series of discrete global grids, usually consisting of
increasingly finer resolution grids (though the term DGG is often used
interchangeably with the term DGGS).

http://discreteglobalgrids.org/[+http://discreteglobalgrids.org/+]

DGtal
~~~~~

The collaborative project DGtal aims at developing generic, efficient and reliable digital geometry data structures, algorithms and tools. It takes the form of an open-source Cxx library DGtal and a set of tools and binaries DGtalTools.

Digital geometry aims at defining proper geometric models and properties onto subsets of the integer plane/space (). It is also interested in defining efficient algorithms for digital object topology and geometry processing. Natural applications of digital geometry are found in image analysis, since images are digital by nature. Digital geometry finds also applications in pattern recognition, computer graphics, biomedical image analysis, OCR, 3D imaging.

DGtal offers concepts, data structures, algorithms for the following tasks: digital spaces and sets (integer plane and subsets, cellular grid space and subsets); integers and fractions (irreducible and continued fractions, Stern-Brocot tree); digital straightness (patterns, digital straight lines and subsegments); grid curve representation (points, pixel, interpixel, Freeman chaincode) and analysis (segmentation, covering); primitives (arithmetic, geometric and combinatorial digital straight segments, digital circular arc, digital planes); Euclidean and digital multigrid shape generation (parametric and implicit curves and surfaces, Gauss digitization); volumetric analysis with distance transformation (DT and reverse DT, medial axis, digital Voronoi diagram); geometric estimators (tangent and curvature estimators along digital curves and surfaces); digital topology (adjacencies, objects, borders, simpleness, homotopic thinning); digital surfaces (implicit and explicit containers, neighborhood, tracking, dual surfaces, marching-cubes); multi-variate polynomials; nD image processing (readers, writers, vector, map and tree containers, ITK bridge); export and visualization (2D and 3D export and display stream mechanism; 3D interaction mechanism).

https://dgtal.org/[+https://dgtal.org/+]

https://github.com/DGtal-team/DGtal[+https://github.com/DGtal-team/DGtal+]

https://github.com/DGtal-team/DGtalTools[+https://github.com/DGtal-team/DGtalTools+]

diffoscope
~~~~~~~~~~

diffoscope will try to get to the bottom of what makes files or directories different. It will recursively unpack archives of many kinds and transform various binary formats into more human readable form to compare them. It can compare two tarballs, ISO images, or PDF just as easily.

Supported file formats: Android APK files, Android boot images, Berkeley DB database files, ColorSync colour profiles (.icc), Coreboot CBFS filesystem images, Dalvik .dex files, Debian .buildinfo files, Debian .changes files, Debian source packages (.dsc), Device Tree Compiler blob files, ELF binaries, FreeDesktop Fontconfig cache files, FreePascal files (.ppu), GHC Haskell .hi files, GIF image files, GNU R Rscript files (.rds), GNU R database files (.rdb), Gettext message catalogues, Git repositories, Gnumeric spreadsheets, Gzipped files, ISO 9660 CD images, JPEG images, JSON files, Java .class files, JavaScript files, LLVM IR bitcode files, MacOS binaries, Microsoft Windows icon files, Microsoft Word .docx files, Mono ‘Portable Executable’ files, Ogg Vorbis audio files, OpenOffice .odt files, OpenSSH public keys, OpenWRT package archives (.ipk), PDF documents, PGP signed/encrypted messages, PNG images, PostScript documents, RPM archives, Rust object files (.deflate), SQLite databases, SquashFS filesystems, TrueType font files, XML binary schemas (.xsb), XML files, XZ compressed files, ar(1) archives, bzip2 archives, character/block devices, cpio archives, directories, ext2/ext3/ext4/btrfs filesystems, statically-linked binaries, symlinks, tape archives (.tar), tcpdump capture files (.pcap) and text files.

https://diffoscope.org/[+https://diffoscope.org/+]

https://reproducible-builds.org/[+https://reproducible-builds.org/+]

DiffSharp
~~~~~~~~~

DiffSharp is a functional automatic differentiation (AD) library.

AD allows exact and efficient calculation of derivatives, by systematically invoking the chain rule of calculus at the elementary operator level during program execution. AD is different from numerical differentiation, which is prone to truncation and round-off errors, and symbolic differentiation, which is affected by expression swell and cannot fully handle algorithmic control flow.

Using the DiffSharp library, differentiation (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) is applied using higher-order functions, that is, functions which take other functions as arguments. Your functions can use the full expressive capability of the language including control flow. DiffSharp allows composition of differentiation using nested forward and reverse AD up to any level, meaning that you can compute exact higher-order derivatives or differentiate functions that are internally making use of differentiation.

The features include:

* Functional nested differentiation with linear algebra primitives, supporting forward and reverse AD, or any combination thereof, up to any level
* Matrix-free Jacobian- and Hessian-vector products
* OpenBLAS backend for highly optimized native BLAS and LAPACK operations
* Parallel implementations of non-BLAS operations (e.g. Hadamard products, matrix transpose)
* Support for 32- and 64-bit floating point precision (32 bit float operations run significantly faster on many systems)
* GPU backend using CUDA/OpenCL
* Generalization to tensors/multidimensional arrays
* Improved Hessian calculations exploiting sparsity structure (e.g. matrix-coloring)
* AD via syntax tree transformation, using code quotations

http://diffsharp.github.io/DiffSharp/[+http://diffsharp.github.io/DiffSharp/+]

https://github.com/DiffSharp/DiffSharp[+https://github.com/DiffSharp/DiffSharp+]

https://arxiv.org/abs/1806.02136[+https://arxiv.org/abs/1806.02136+]

dislib
~~~~~~

The Distributed Computing Library (dislib) provides distributed algorithms ready to use as a library. So far, dislib is highly focused on machine learning algorithms, and is greatly inspired by Scikit-learn. However, other types of numerical algorithms might be added in the future. Dislib has been implemented on top of PyCOMPSs programming model, and it is being developed by the Workflows and Distributed Computing group of the Barcelona Supercomputing Center. The library is designed to allow easy local development through docker. Once the code is finished, it can be run directly on any distributed platform without any further changes. This includes clusters, supercomputers, clouds, and containerized platforms.

https://github.com/bsc-wdc/dislib[+https://github.com/bsc-wdc/dislib+]

dispel4py
~~~~~~~~~

dispel4py is a free and open-source Python library for describing abstract stream-based workflows for distributed data-intensive applications. It enables users to focus on their scientific methods, avoiding distracting details and retaining flexibility over the computing infrastructure they use. It delivers mappings to diverse computing infrastructures, including cloud technologies, HPC architectures and specialised data-intensive machines, to move seamlessly into production with large-scale data loads. The dispel4py system maps workflows dynamically onto multiple enactment systems, such as MPI, STORM and Multiprocessing, without users having to modify their workflows.

https://github.com/dispel4py/dispel4py[+https://github.com/dispel4py/dispel4py+]

DisPerSE
~~~~~~~~

DisPerSE stands for "Discrete Persistent Structures Extractor" and it is an open source software for the identification of persistent topological features such as peaks, voids, walls and in particular filamentary structures within noisy sampled distributions in 2D, 3D. In DisPerSE, structure identification is achieved through the computation of the discrete Morse-Smale complex it can deal directly with noisy datasets via the concept of persistence (a measure of the robustness of topological features).

Although it was initially developed with cosmology in mind (for the study of the properties of filamentary structures in the so called comic web of galaxy distribution over large scales in the Universe), the present version is quite versatile and should be useful for any application where a robust structure identification is required, such as for segmentation or for studying the topology of sampled functions (like computing persistent Betti numbers for instance). Currently, it can be applied indifferently to many kinds of cell complex (such as structured and unstructured grids, 2D manifolds embedded within a 3D space, discrete point samples using delaunay tesselation, Healpix tesselations of the sphere, ...). 

http://www2.iap.fr/users/sousbie/disperse.html[+http://www2.iap.fr/users/sousbie/disperse.html+]

http://www2.iap.fr/users/sousbie/web/html/disperse_latest.tar.html[+http://www2.iap.fr/users/sousbie/web/html/disperse_latest.tar.html+]

https://github.com/thierry-sousbie/DisPerSE[+https://github.com/thierry-sousbie/DisPerSE+]

dit
~~~

Information theory is a powerful extension to probability and statistics, quantifying dependencies among arbitrary random variables in a way tha tis consistent and comparable across systems and scales. Information theory was originally developed to quantify how quickly and reliably information could be transmitted across an arbitrary channel. The demands of modern, data-driven science have been coopting and extending these quantities and methods into unknown, multivariate settings where the interpretation and best practices are not known. For example, there are at least four reasonable multivariate generalizations of the mutual information, none of which inherit all the interpretations of the standard bivariate case. Which is best to use is context-dependent. dit implements a vast range of multivariate information measures in an effort to allow information practitioners to study how these various measures behave and interact in a variety of contexts. We hope that having all these measures and techniques implemented in one place will allow the development of robust techniques for the automated quantification of dependencies within a system and concrete interpretation of what those dependencies mean.

http://docs.dit.io/en/latest/[+http://docs.dit.io/en/latest/+]

https://github.com/dit/dit[+https://github.com/dit/dit+]

http://joss.theoj.org/papers/ba78ef2d389a4954aab904b5fb53f18d[+http://joss.theoj.org/papers/ba78ef2d389a4954aab904b5fb53f18d+]

DIVA
~~~~

DIVA allows the spatial interpolation of data (analysis) in an optimal way, comparable to optimal interpolation (OI). In comparison to OI, it takes into account coastlines, sub-basins and advection. Calculations are highly optimized and rely on a finite element resolution.

Tools to generate the finite element mesh are provided as well as tools to optimize the parameters of the analysis. Quality control of data can be performed and error fields can be calculated. In addition, detrending of data is possible. Finally 3D and 4D extensions are included with emphasis on direct computations of climatologies from Ocean Data View (ODV) spreadsheet files.

https://github.com/gher-ulg/DIVA[+https://github.com/gher-ulg/DIVA+]

DIVAnd.jl
~~~~~~~~~

DIVAnd performs an n-dimensional variational analysis of arbitrarily located observations.
It allows the interpolation and analysis of observations on curvilinear orthogonal grids in an arbitrary high dimensional space by minimizing a cost function. This cost function penalizes the deviation from the observations, the deviation from a first guess and abruptly varying fields based on a given correlation length (potentially varying in space and time). Additional constraints can be added to this cost function such as an advection constraint which forces the analysed field to align with the ocean current. The method decouples naturally disconnected areas based on topography and topology. This is useful in oceanography where disconnected water masses often have different physical properties. Individual elements of the a priori and a posteriori error covariance matrix can also be computed, in particular expected error variances of the analysis. A multidimensional approach (as opposed to stacking two-dimensional analysis) has the benefit of providing a smooth analysis in all dimensions, although the computational cost is increased.

Primal (problem solved in the grid space) and dual formulations (problem solved in the observational space) are implemented using either direct solvers (based on Cholesky factorization) or iterative solvers (conjugate gradient method). In most applications the primal formulation with the direct solver is the fastest, especially if an a posteriori error estimate is needed. However, for correlated observation errors the dual formulation with an iterative solver is more efficient.

https://github.com/gher-ulg/divand.jl[+https://github.com/gher-ulg/divand.jl+]

https://www.geosci-model-dev.net/7/225/2014/[+https://www.geosci-model-dev.net/7/225/2014/+]

https://github.com/gher-ulg/DIVAnd.py[+https://github.com/gher-ulg/DIVAnd.py+]

https://github.com/gher-ulg/DivaPythonTools[+https://github.com/gher-ulg/DivaPythonTools+]

DjVu
~~~~

A  computer file format designed primarily to store scanned documents, especially those containing a combination of text, line drawings, indexed color images, and photographs. It uses technologies such as image layer separation of text and background/images, progressive loading, arithmetic coding, and lossy compression for bitonal (monochrome) images. This allows high-quality, readable images to be stored in a minimum of space, so that they can be made available on the web.

DjVu has been promoted as providing smaller files than PDF for most scanned documents. The DjVu developers report that color magazine pages compress to 40–70 kB, black-and-white technical papers compress to 15–40 kB, and ancient manuscripts compress to around 100 kB; a satisfactory JPEG image typically requires 500 kB. Like PDF, DjVu can contain an OCR text layer, making it easy to perform copy and paste and text search operations. 

https://en.wikipedia.org/wiki/DjVu[+https://en.wikipedia.org/wiki/DjVu+]

djvulibre
^^^^^^^^^

DjVuLibre includes a standalone viewer, a browser plug-in (for Mozilla, Firefox, Konqueror, Netscape, Galeon, and Opera), and command line tools (decoders, encoders, utilities). DjVuLibre works under Unix with X11. 

DjVuLibre includes:

* A standalone DjVu viewer for Unix under X11 (based on the Qt library).
* A browser plugin that works with most Unix browsers, including: Netscape-4.x, Netscape-6.x, Mozilla, Galeon, Konqueror, and Opera.
* A full-fledged wavelet-based compressor for pictures.
* A simple compressor for bitonal (black and white) scanned pages.
* A very simple compressor for scanned color pages.
* A compressor for palettized images (a la GIF/PNG).
* A full set of utilities to manipulate and assemble DjVu images and documents.
* A set of decoders to convert DjVu to a number of other formats.
* An up-to-date version of the Cxx DjVu Reference Library 

http://djvu.sourceforge.net/[+http://djvu.sourceforge.net/+]

dvjusmooth
^^^^^^^^^^

djvusmooth is a graphical editor for DjVu documents.

http://jwilk.net/software/djvusmooth[+http://jwilk.net/software/djvusmooth+]

pdf2djvu
^^^^^^^^

pdf2djvu creates DjVu files from PDF files.
It can extract graphics, the text layer, hyperlinks, a document outline and metadata.

http://jwilk.net/software/pdf2djvu[+http://jwilk.net/software/pdf2djvu+]

python-djvulibre
^^^^^^^^^^^^^^^^

python-djvulibre is a set of Python bindings for the DjVuLibre library, an open source implementation of DjVu.

http://jwilk.net/software/python-djvulibre[+http://jwilk.net/software/python-djvulibre+]

DLMMC
~~~~~

Dynamical Linear Modelling (DLM) regression code in python for analysis of time-series data. The code is targeted at atmospheric time-series analysis, with a detailed worked example (and data) included for stratospheric ozone, but is a fairly general state space model that can be applied or extended to a wide range of problems.

The core of this package is a suite of DLM models implemented in stan, using a combination of HMC sampling and Kalman filtering to infer the DLM model parameters (trend, seasonal cycle, auto-regressive processes etc) given some time-series data. To make the code as accessible as possible, I provide a step-by-step tutorial in python for how to read in your data, run the DLM model(s), and process the outputs to make nice plots. Once you've worked through this tutorial you should have all the tools you need to apply DLM to your own data!

https://github.com/justinalsing/dlmmc[+https://github.com/justinalsing/dlmmc+]

Docker
~~~~~~

Blah.

dex
^^^

A Docker package manager that allows you to run applications without installing them or their dependencies.

https://github.com/dockerland/dex[+https://github.com/dockerland/dex+]

docker-py
^^^^^^^^^

A Python library for the Docker Engine API. It lets you do anything the docker command does, but from within Python apps – run containers, manage containers, manage Swarms, etc.

https://github.com/docker/docker-py[+https://github.com/docker/docker-py+]

nvidia-docker
^^^^^^^^^^^^^

NVIDIA designed NVIDIA-Docker in 2016 to enable portability in Docker images that leverage NVIDIA GPUs. It allowed driver agnostic CUDA images and provided a Docker command line wrapper that mounted the user mode components of the driver and the GPU device files into the container at launch.

https://github.com/NVIDIA/nvidia-docker[+https://github.com/NVIDIA/nvidia-docker+]

https://devblogs.nvidia.com/gpu-containers-runtime/[+https://devblogs.nvidia.com/gpu-containers-runtime/+]

https://docs.nvidia.com/ngc/ngc-user-guide/[+https://docs.nvidia.com/ngc/ngc-user-guide/+]

whalebrew
^^^^^^^^^

Whalebrew creates aliases for Docker images so you can run them as if they were native commands. It's like Homebrew, but with Docker images.

Docker works well for packaging up development environments, but there are lots of tools that aren't tied to a particular project: awscli for managing your AWS account, ffmpeg for converting video, wget for downloading files, and so on. Whalebrew makes those things work with Docker, too.

Whalebrew can run almost any CLI tool, but it isn't for everything (e.g. where commands must start instantly). It works particularly well for:

* Complex dependencies. For example, a Python app that requires C libraries, specific package versions, and other CLI tools that you don't want to clutter up your machine with.
* Cross-platform portability. Package managers tend to be very closely tied to the system they are running on. Whalebrew packages work on any modern version of macOS, Linux, and Windows (coming soon).

https://github.com/bfirsh/whalebrew[+https://github.com/bfirsh/whalebrew+]

https://github.com/whalebrew/whalebrew-packages[+https://github.com/whalebrew/whalebrew-packages+]

DocOnce
~~~~~~~

DocOnce is a modestly tagged (Markdown-like) markup language targeting scientific reports, software documentation, books, blog posts, and slides involving much math and code in the text. From DocOnce source you can generate LaTeX, Sphinx, HTML, IPython notebooks, Markdown, MediaWiki, and other formats. This means that you get the most up-to-date publishing technologies for paper, tablets, and phones. 

The features include:

* DocOnce is a modestly tagged markup language (see syntax example), quite like Markdown, but with many more features, aimed at documents with much math and code in the text (see demo).

* There is extensive support for book projects. In addition to classical LaTeX-based paper books one gets for free fully responsive, modern-looking, HTML-based ebooks for tablets and phones. Parts of books can, e.g., appear in blog posts for discussion and as IPython notebooks for experimentation and annotation.

* For documents with math and code, you can generate clean plain LaTeX (PDF), HTML (with MathJax and Pygments - embedded in your own templates), Sphinx for attractive web design, Markdown, IPython notebooks, HTML for Google or Wordpress blog posts, and MediaWiki. The LaTeX output has many fancy layouts for typesetting of computer code.

* DocOnce can also output other formats (though without support for nicely typeset math and code): plain untagged text, Google wiki, Creole wiki, and reStructuredText. From Markdown or reStructuredText you can go to XML, DocBook, epub, OpenOffice/LibreOffice, MS Word, and other formats.

* The document source is first preprocessed by Preprocess and Mako, which gives you full programming capabilities in the document's text. For example, with Mako it is easy to write a book with all computer code examples in two alternative languages (say Matlab and Python), and you can determine the language at compile time of the document. New user-specific features of DocOnce can also be implemented via Mako.

* DocOnce extends Sphinx, Markdown, and MediaWiki output such that LaTeX align environments with labels work for systems of equations. DocOnce also adjusts Sphinx and HTML code such that it is possible to refer to equations outside the current web page.

* DocOnce makes it very easy to write slides with math and code by stripping down running text in a report or book. LaTeX Beamer slides, HTML5 slides (reveal.js, deck.js, dzslides), and Remark (Markdown) slides are supported. Slide elements can be arranged in a grid of cells to easily control the layout.~~~~

* doit is all about automating task dependency management and execution. Tasks can execute external shell commands/scripts or python functions (actually any callable). So a task can be anything you can code,
Tasks are defined in plain python module with some conventions.
 
DocOnce looks similar to Markdown, Pandoc-extended Markdown, and in particular MultiMarkdown. The main advantage of DocOnce is the richer support for writing large documents (books) with much math and code and with tailored output both in HTML and LaTeX. DocOnce also has special support for exercises, quizzes, and admonitions, three very desired features when developing educational material. Books can be composed of many smaller documents that may exist independently of the book, thus lowering the barrier of writing books.

http://hplgit.github.io/doconce/doc/web/index.html[+http://hplgit.github.io/doconce/doc/web/index.html+]

https://github.com/hplgit/doconce[+https://github.com/hplgit/doconce+]

doit
~~~~

doit comes from the idea of bringing the power of build-tools to execute any kind of task.
People often compare doit to tools like make, grunt, rake, scons, snakemake.
They appreciate doit strong features, flexibility, simplicity of authoring and ease of use.

http://pydoit.org/[+http://pydoit.org/+]

https://github.com/pydoit/doit[+https://github.com/pydoit/doit+]

https://news.ycombinator.com/item?id=13865400[+https://news.ycombinator.com/item?id=13865400+]

DPDK
~~~~

DPDK is the Data Plane Development Kit that consists of libraries to accelerate packet processing workloads running on a wide variety of CPU architectures.

In a world where the network is becoming fundamental to the way people communicate, performance, throughput, and latency are increasingly important for applications like wireless core and access, wireline infrastructure, routers, load balancers, firewalls, video streaming, VoIP, and more. By enabling very fast packet processing, DPDK is making it possible for the telecommunications industry to move performance-sensitive applications like the backbone for mobile networks and voice to the cloud.

https://www.dpdk.org/[+https://www.dpdk.org/+]

DPM
~~~

The Disk Pool Manager (DPM) is a storage system for grid sites. It offers a simple way to create a disk-based grid storage element composed by many disk servers.

DPM supports the data and metadata access protocols that are relevant for file management and access in the Grid environment (HTTP, WebDAV, Xrootd, SRM, gridFTP, RFIO).

In the case of flexible protocols like HTTP and Xrootd, it focuses in giving the advanced features that enhance the performance of the analysis applications.

Its focus is on manageability (ease of installation, configuration, low effort of maintenance), while providing all required functionality for a grid storage solution, for example:

* strong X509 authentication
* proxy certificates
* Support for multiple Virtual Organizations
* support for multiple disk server nodes
* different space types
* multiple file replicas in disk pools

http://lcgdm.web.cern.ch/dpm[+http://lcgdm.web.cern.ch/dpm+]

dask-cudf
~~~~~~~~~

Dask support for distributed GDF object.

https://github.com/rapidsai/dask-cudf[+https://github.com/rapidsai/dask-cudf+]

Dionysus
~~~~~~~~

Dionysus is a Cxx library for computing persistent homology. It provides implementations of the following algorithms:

* Persistent homology computation
* Vineyards
* Persistent cohomology computation
* Zigzag persistent homology

http://mrzv.org/software/dionysus/[+http://mrzv.org/software/dionysus/+]

http://mrzv.org/software/dionysus2/[+http://mrzv.org/software/dionysus2/+]

https://github.com/mrzv[+https://github.com/mrzv+]

https://github.com/mrzv/dionysus[+https://github.com/mrzv/dionysus+]

Docker
~~~~~~

Blah.

https://hub.docker.com/[+https://hub.docker.com/+]

*eddy4R* - https://hub.docker.com/r/stefanmet/eddy4r/[+https://hub.docker.com/r/stefanmet/eddy4r/+]

*GeoNetwork* - https://hub.docker.com/_/geonetwork[+https://hub.docker.com/_/geonetwork+]

*geoscience-notebook* - https://hub.docker.com/r/brunorpinho/geoscience-notebook[+https://hub.docker.com/r/brunorpinho/geoscience-notebook+]

*geospatial* - https://github.com/rocker-org/geospatial[+https://github.com/rocker-org/geospatial+]

*OceanBrowser* - https://hub.docker.com/r/abarth/oceanbrowser[+https://hub.docker.com/r/abarth/oceanbrowser+]

*stoqs* - https://hub.docker.com/r/mbarimike/stoqs[+https://hub.docker.com/r/mbarimike/stoqs+]

Compose
^^^^^^^

Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.

The features of Compose that make it effective are:

* Multiple isolated environments on a single host
* Preserve volume data when containers are created
* Only recreate containers that have changed
* Variables and moving a composition between environments

Using Compose is basically a three-step process.

* Define your app's environment with a Dockerfile so it can be reproduced anywhere.
* Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.
* Lastly, run docker-compose up and Compose will start and run your entire app.

https://github.com/docker/compose[+https://github.com/docker/compose+]

https://docs.docker.com/compose/[+https://docs.docker.com/compose/+]

Docker CE
~~~~~~~~~

Docker Community Edition (CE) is ideal for developers and small teams looking to get started with Docker and experimenting with container-based apps.

https://docs.docker.com/install/[+https://docs.docker.com/install/+]

DOOM
~~~~

Doomschtoff.

Freedoom
^^^^^^^^

The Freedoom project aims to create a complete free content first person shooter game. But Freedoom by itself is just the raw material for a game: it must be paired with a compatible game engine to be played. The download page lists some recommended ones.

id Software released the source code to their classic game, Doom, under the GNU General Public License. This means that the program code that powers Doom is free; Freedoom complements this with free levels, artwork, sound effects and music to make a completely free game.

Freedoom is actually three games in one, consisting of two single player campaigns and one set of levels intended for multiplayer deathmatch (FreeDM).

There is a massive decades-long back catalog containing thousands of Doom levels and other “mods” made by fans of the original game. Freedoom aims to be compatible with these and allows most to be played without the need to use non-free software. 

https://freedoom.github.io/index.html[+https://freedoom.github.io/index.html+]

ZDoom
^^^^^

ZDoom is a family of enhanced ports of the Doom engine for running on modern operating systems. It runs on Windows, Linux, and OS X, and adds new features not found in the games as originally published by id Software. 

https://zdoom.org/index[+https://zdoom.org/index+]

dplyr
~~~~~

dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:

* mutate() adds new variables that are functions of existing variables
* select() picks variables based on their names.
* filter() picks cases based on their values.
* summarise() reduces multiple values down to a single summary.
* arrange() changes the ordering of the rows.

These all combine naturally with group_by() which allows you to perform any operation “by group”. You can learn more about them in vignette("dplyr"). As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in vignette("two-table").

dplyr is designed to abstract over how the data is stored. That means as well as working with local data frames, you can also work with remote database tables, using exactly the same R code.

https://github.com/tidyverse/dplyr[+https://github.com/tidyverse/dplyr+]

https://github.com/tidyverse/dplyr[+https://github.com/tidyverse/dplyr+]

https://r4ds.had.co.nz/transform.html[+https://r4ds.had.co.nz/transform.html+]

Draco
~~~~~

Draco is an open-source library for compressing and decompressing 3D geometric meshes and point clouds. It is intended to improve the storage and transmission of 3D graphics.

3D graphics are a fundamental part of many applications, including gaming, design and data visualization. As graphics processors and creation tools continue to improve, larger and more complex 3D models will become commonplace and help fuel new applications in immersive virtual reality (VR) and augmented reality (AR). Because of this increased model complexity, storage and bandwidth requirements are forced to keep pace with the explosion of 3D data.

With Draco, applications using 3D graphics can be significantly smaller without compromising visual fidelity. For users this means apps can now be downloaded faster, 3D graphics in the browser can load quicker, and VR and AR scenes can now be transmitted with a fraction of the bandwidth, rendered quickly and look fantastic.

Draco is a library for compressing and decompressing 3D geometric meshes and point clouds. It is intended to improve the storage and transmission of 3D graphics.

Draco was designed and built for compression efficiency and speed. The code supports compressing points, connectivity information, texture coordinates, color information, normals, and any other generic attributes associated with geometry. Draco is released as Cxx source code that can be used to compress 3D graphics as well as Cxx and Javascript decoders for the encoded data.

https://google.github.io/draco/[+https://google.github.io/draco/+]

https://github.com/google/draco[+https://github.com/google/draco+]

Drake
~~~~~

Drake (“dragon” in Middle English) is a Cxx toolbox started by the Robot Locomotion Group at the MIT Computer Science and Artificial Intelligence Lab (CSAIL). The development team has now grown significantly, with core development led by the Toyota Research Institute. It is a collection of tools for analyzing the dynamics of our robots and building control systems for them, with a heavy emphasis on optimization-based design/analysis.

While there are an increasing number of simulation tools available for robotics, most of them function like a black box: commands go in, sensors come out. Drake aims to simulate even very complex dynamics of robots (e.g. including friction, contact, aerodynamics, …), but always with an emphasis on exposing the structure in the governing equations (sparsity, analytical gradients, polynomial structure, uncertainty quantification, …) and making this information available for advanced planning, control, and analysis algorithms. Drake provides interfaces to high-level languages (MATLAB, Python, …) to enable rapid-prototyping of new algorithms, and also aims to provide solid open-source implementations for many state-of-the-art algorithms.

The dynamic systemm modeling capabilities of Drake include:

* Cxx block-diagram modeling environment with support for:
** Continuous, discrete, hybrid, event-triggered, and multi-rate systems
** Stochastic systems
** System constraints
* Rigid-body kinematics and dynamics
** Rigorously designed and tested, well-documented multi-body library
** Load from SDF / URDF models (+ a few custom tags)
** Contact/collisions modeled with either compliant or rigid contact using continuous (differential equation) and discrete (difference equation, i.e., time stepping) models; Coming soon: hybrid models for rigid contact
** Geometry queries (e.g. collision detection, contact queries, and sensor queries) for simple geometries and convex meshes; Coming soon: non-convex meshes and multi-contact
** Rich library of kinematic and dynamic queries (e.g. Centroidal dynamics, Center of Pressure, Kinematic Jacobians, …)
* Sensor models (lidar, RGB-D camera, imu, contact force/torque)
* Hand-derived models for many canonical control dynamical systems
* Easily add your own models/components
* For nearly all of the above we aim to expose sparsity in the governing equations and provide analytical gradients / symbolic analysis

https://drake.mit.edu/[+https://drake.mit.edu/+]

Dremio
~~~~~~

Dremio is a data-as-a-service platform that empowers users to discover, curate, accelerate, and share any data at any time, regardless of location, volume, or structure.

Modern data is managed by a wide range of technologies, including relational databases, NoSQL datastores, file systems, Hadoop, and others. Many of the newer datastores are often more agile and provide improved scalability, but at a cost to speed and ease of access via traditional SQL-based analysis tools. Additionally, raw data found in these stores is often too complex or inconsistent for analysis to use with business intelligence tools.

ETL pipelines that load a subset of data into relational databases provide one answer, but aside from the burden these solutions impose on data engineers and IT staff, data becomes stale by the time it is available to analysts and data scientists.

https://docs.dremio.com/[+https://docs.dremio.com/+]

https://github.com/dremio/dremio-oss[+https://github.com/dremio/dremio-oss+]

DSpace
~~~~~~

DSpace is the software of choice for academic, non-profit, and commercial organizations building open digital repositories.  It is free and easy to install “out of the box” and completely customizable to fit the needs of any organization.

DSpace preserves and enables easy and open access to all types of digital content including text, images, moving images, mpegs and data sets.  And with an ever-growing community of developers, committed  to continuously expanding and improving the software, each DSpace installation benefits from the next.

https://duraspace.org/dspace/[+https://duraspace.org/dspace/+]

https://github.com/DSpace/DSpace[+https://github.com/DSpace/DSpace+]

DTFE
~~~~

The Delaunay Tessellation Field Interpolation (DTFE) method represents the natural way of going from discrete samples/measurements to values on a periodic grid using the maximum of information contained in the points set (that is the measurements points or particle distribution).

The DTFE method is especially suitable for astronomical data due to the following reasons:

* Preserves the multi-scale character of the point distribution. This is the case in numerical simulations of large scale structure (LSS) where the density varies over more than 6 orders of magnitude.
* Preserves the local geometry of the point distribution. This is important in recovering sharp features like the different components of the LSS (i.e. clusters, filaments, walls and voids).
* DTFE does not depend on user defined parameters or choices.
* The interpolated fields are volume weighted (versus mass weighted quantities in most other interpolation schemes). This can have a significant effect especially when comparing with analytical predictions which are volume weighted.

The DTFE public code is a Cxx implementation of the DTFE grid interpolation method. The code was written by Marius Cautun at the Kapteyn Astronomical Institute, Netherlands, with the purpose of analyzing cosmological numerical simulations and galaxy redshift survey. Even though the code was designed with astrophysics in mind, it can be used for problems in a wide range of fields where one needs to interpolate from a discrete set of points to a grid.

The code was designed using a modular philosophy and with a wide set of features that can easily be selected using the different program options. The DTFE code is also written using OpenMP directives which allow it to run in parallel on shared-memory architectures.

The code comes with a complete documentation and with a multitude of examples that detail the program features. Moreover, a help desk is available for information and assistance for troubleshooting problems. 

https://www.astro.rug.nl/\~voronoi/DTFE/dtfe.html[+https://www.astro.rug.nl/~voronoi/DTFE/dtfe.html+]

https://arxiv.org/abs/1105.0370[+https://arxiv.org/abs/1105.0370+]

https://github.com/con-f-use/DTFE[+https://github.com/con-f-use/DTFE+]

https://www.youtube.com/watch?v=PuoP3-FUWlA[+https://www.youtube.com/watch?v=PuoP3-FUWlA+]

duplicity
~~~~~~~~~

Duplicity backs directories by producing encrypted tar-format volumes and uploading them to a remote or local file server. Because duplicity uses librsync, the incremental archives are space efficient and only record the parts of files that have changed since the last backup. Because duplicity uses GnuPG to encrypt and/or sign these archives, they will be safe from spying and/or modification by the server.

The duplicity package also includes the rdiffdir utility. Rdiffdir is an extension of librsync's rdiff to directories---it can be used to produce signatures and deltas of directories as well as regular files. These signatures and deltas are in GNU tar format.
Currently duplicity supports deleted files, full unix permissions, directories, and symbolic links, fifos, and device files, but not hard links.

http://duplicity.nongnu.org/[+http://duplicity.nongnu.org/+]

http://duply.net/[+http://duply.net/+]

https://0xacab.org/riseuplabs/backupninja[+https://0xacab.org/riseuplabs/backupninja+]

DynamicalSystems.jl
~~~~~~~~~~~~~~~~~~~

A Julia software library for the exploration of chaos and nonlinear dynamics.
This is divided into a set of modules.

The DynamicalSystems module includes:

* Intuitive, consistent APIs for the definition of general dynamical systems
* Automatic "completion" of the dynamics of the system with numerically computed Jacobians, in case they are not provided by the user
* Robust implementations of all kinds of integrators, that evolve the system, many states of the system, or even deviation vectors
* Library of predefined well-known dynamical systems that have been used extensively in scientific research

The DelayEmbeddings module includes:

* Unified & dedicated interface for numerical data
* Simple and extendable neighborhood estimation by interfacing NearestNeighbors
* Flexible and abstracted reconstruct interface, that creates the delay-coordinates reconstruction of a timeseries efficiently and supports:
** multiple dimensions and multiple timescales.
** Methods that estimate optimal embedding parameters: the delay time (estimate_delay) and the number of temporal neighbors (estimate_dimension)
** Fast calculation of mutual information

The ChaosTools module includes:

* Poincare S.O.S. and orbit diagrams
* Lyapunov Exponents
* Entropies and Dimensions
* Lyapunov exponent of a timeseries (numerical data)
* Finding Fixed Points of Maps
* GALI (Generalized Alignment Index) for distinguishing chaotic and regular behavior
* Nonlinear timeseries analysis

The RecurrenceAnalysis module includes;

* Recurrence, cross-recurrence and joint-recurrence "plots"
* Recurrence quantification analysis including Recurrence rate, determinism, average/maximum diagonal length, divergence, laminarity, trend, entropy, trapping time, average/maximum vertical length

The ultimate goal for DynamicalSystems.jl is to be a useful software library for students and scientists working on chaos, nonlinear dynamics and in general dynamical systems.

https://juliadynamics.github.io/DynamicalSystems.jl/latest/[+https://juliadynamics.github.io/DynamicalSystems.jl/latest/+]

https://github.com/JuliaDynamics/ChaosTools.jl[+https://github.com/JuliaDynamics/ChaosTools.jl+]

DYNAMICO
~~~~~~~~

The DYNAMICO project develops a new dynamical core for LMD-Z, the atmospheric general circulation model (GCM) part of IPSL-CM Earth System Model.

LMDZ4, the current version of LMD-Z, has a shallow-atmosphere, hydrostatic dynamical core. It is based on a latitude-longitude C-grid, a hybrid pressure-based terrain-following vertical coordinate, second-order enstrophy-conserving finite-difference discretization and positive-definite advection. Grid refinement is implemented as a continuous zoom via smooth grid stretching. An extensive package of physical paramererizations is coupled to the dynamical core. IPSL-CM is currently used to produce AR5 simulations. LMD-Z is also at the heart of GCMs of planetary atmospheres (Mars, Venus and Titan).

It is well-known that the latitude-longitude coordinates have a strong singularity at the poles which is undesirable in terms of both numerical stability and computational efficiency. Regular tesselations of the sphere such as a recursively subdivided icosahedron provide an almost-uniform grid and a path to highly parallel computations based on domain decomposition. LMD's logo is itself an icosahedron, evoking the pioneering work of Robert Sadourny on the use of icosahedral grids for solving the equations of atmospheric motion.

The primary goal of DYNAMICO is to re-formulate in LMD-Z the horizontal advection and dynamics on a icosahedral grid, while preserving or improving their qualities with respect to accuracy, conservation laws and wave dispersion. In turn, a new grid refinement strategy is required. A broader goal is to revisit all fundamental features of the dynamical core, especially the shallow-atmosphere/traditional approximation, the vertical coordinate and the coupling with physics. Efficient implementation on present and future supercomputing architectures is also a key issue addressed by DYNAMICO.

DYNAMICO is currently able to solve the hydrostatic primitive equations and participated to the DCMIP workshop held in August 2012 at NCAR. In the near future we will investigate its extension to deep-atmosphere and possibly non-hydrostatic equations following a variational approach that naturally conserves mass, energy and, in a somewhat restricted sense, potential vorticity.

http://forge.ipsl.jussieu.fr/dynamico[+http://forge.ipsl.jussieu.fr/dynamico+]

http://forge.ipsl.jussieu.fr/dynamico/wiki/CompilingDynamico[+http://forge.ipsl.jussieu.fr/dynamico/wiki/CompilingDynamico+]

XIOS
^^^^

A library for the I/O management of climate code.  XIOS uses
and XML configuration file.
It can be used in either attached mode (library) or server
mode (asynchronous transfer).
It can also write to output files in either sequential or parallel mode.

https://forge.ipsl.jussieu.fr/ioserver[+https://forge.ipsl.jussieu.fr/ioserver+]

#EEEE

E3SM
~~~~

E3SM is a state-of-the-art fully coupled model of the Earth's climate including important biogeochemical and cryospheric processes. It is intended to address the most challenging and demanding climate-change research problems and Department of Energy mission needs while efficiently using DOE Leadership Computing Facilities.

https://github.com/E3SM-Project/E3SM[+https://github.com/E3SM-Project/E3SM+]

E4S
~~~

By 2021, the team behind the Exascale Computing Project (ECP) foresees an exascale computing ecosystem capable of 50 times the application performance of the leading 20 petaflops systems, and five times the performance of Summit. With all that prowess on tap, exascale-level systems will be able to solve problems of greater complexity, and support applications that deliver high-fidelity solutions more quickly. However, the applications need a software stack that allows them to access that capability. The new Extreme-Scale Scientific Software Stack (E4S) release represents a major step toward the ECP’s larger goal.

The ECP released the Extreme-Scale Scientific Software Stack (E4S) Release 0.1 to a warm reception at the 2018 Supercomputing Conference in Dallas. The package includes 24 integrated components including libraries, frameworks, and compilers which coax higher performance levels from applications while simplifying the development process with common tools.

The second release of Exascale Computing Project (ECP) Software Technologies (ST) software, Extreme-Scale Scientific Software Stack (E4S) Release 0.2, includes a subset of ECP ST software products, and demonstrates the target approach for future delivery of the full ECP ST software stack. Also available are a number of ECP ST software products that support a Spack package, but are not yet fully interoperable. As the primary purpose of the 0.2 release is demonstrating the ST software stack release approach, not all ECP ST software products were targeted for this release. Software products were targeted primarily based on existing Spack package maturity, location within the scientific software stack, and ECP SDK developer experience with the software. Each release will include additional software products, with the ultimate goal of including all ECP ST software products.

The container release contains binary versions of the Full Release packages listed above. A clone of Spack is also available in the container which can be used to compile the Full Release and Partial Release packages. Example Spack "recipes" (lists of configuration commands) are available in the container. See the README.txt file for more details. This release also includes an OVA file that has Docker, Charliecloud, Shifter, and Singularity preinstalled in it. The Docker container image is also available from Dockerhub: 

`docker pull exascaleproject/sdk:AHM19`

http://e4s.io/[+http://e4s.io/+]

https://www.nextplatform.com/2019/01/22/does-e4s-software-stack-takes-an-extreme-step-towards-exascale/[+https://www.nextplatform.com/2019/01/22/does-e4s-software-stack-takes-an-extreme-step-towards-exascale/+]

https://www.exascaleproject.org/ecp-software-technology-st-capability-assessment-report-car/[+https://www.exascaleproject.org/ecp-software-technology-st-capability-assessment-report-car/+]

earth
~~~~~

Earth is a project to visualize global weather conditions.

https://github.com/cambecc/earth[+https://github.com/cambecc/earth+]

https://earth.nullschool.net/[+https://earth.nullschool.net/+]

EarthSim
~~~~~~~~

EarthSim is a project for developing Python-based workflows for specifying, launching, visualizing, and analyzing environmental simulations, such as those for hydrology modeling. EarthSim is designed to be a lightweight project internally, relying on code and documentation primarily maintained in other, freely available, general-purpose SciPy and PyViz projects:

* Bokeh : Interactive browser-based plotting.
* HoloViews : Easy construction of Bokeh plots for datasets.
* Datashader : Rendering large datasets into images for display in browsers.
* Param : Specifying parameters of interest, e.g. to make widgets.
* XArray : Processing gridded data structures.
* GeoViews : HoloViews with earth-specific projections.

Most of the functionality developed in the EarthSim project is in the various general-purpose open-source Python packages like Bokeh , HoloViews , and Datashader . This User Guide focuses on documenting the small amount of code that’s actually in earthsim itself.

So far, what is documented here shows how to use the drawing/annotation tools that were added to Bokeh and HoloViews as part of this project, along with the specific application of those tools to specifying the generation of irregular triangular meshes used for variable-resolution simulations.

There is also some support in the earthsim module for running the GSSHA hydrology simulator, which is currently illustrated in the separate Topics section. Note that GSSHA and the Filigree and libfiligree packages used in the meshes section below are freely available but currently closed source; the rest of the packages used in EarthSim are all open source. 

Drawing/annotation tools:

* Drawing Tools : Introduction to the drawing tools used to draw, edit and annotate data.
* Adding Annotations : Introduces a number of classes useful for annotating point, line and polygon data.

Making and using triangular meshes:

* Specifying meshes with Filigree : Using draw tools to generate inputs to the Filigree mesh generator.
* Visualizing meshes : Demonstrates how to load large static and time-varying trimesh data and using datashader to interpolate it.
* Analyzing Meshes : Analyzing meshes by plotting data across multi-segment line cross-sections.

http://earthsim.pyviz.org/[+http://earthsim.pyviz.org/+]

https://github.com/pyviz/EarthSim[+https://github.com/pyviz/EarthSim+]

EasyBuild
~~~~~~~~~

EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. It is motivated by the need for a tool that combines the following features:

* a flexible framework for building/installing (scientific) software
* fully automates software builds
* divert from the standard configure / make / make install with custom procedures
* allows for easily reproducing previous builds
* keep the software build recipes/specifications simple and human-readable
* supports co-existence of versions/builds via dedicated installation prefix and module files
* enables sharing with the HPC community (win-win situation)
* automagic dependency resolution
* retain logs for traceability of the build processes

The features of EasyBuild include:

* build & install (scientific) software fully autonomously
* easily configurable: config file/environment/command line
* thorough logging and archiving
* automatic dependency resolution
* building software in parallel
* robust and thoroughly tested code base, fully unit-tested before each release
* thriving, growing community

https://easybuild.readthedocs.io/en/latest/index.html[+https://easybuild.readthedocs.io/en/latest/index.html+]

https://github.com/easybuilders/easybuild[+https://github.com/easybuilders/easybuild+]

ecKit
~~~~~

ecKit is a cross-platform cxx toolkit that supports development of tools and applications at ECMWF. It is based on code developed over the last 20 years within the MARS software and was re-factored out to be reused by other applications. It provides a an abstraction layer on top of the operating system, so it is easier to port code to new architectures. It is developed taking into account the robustness requirements of running production systems at ECMWF. The main focus is UNIX/POSIX systems, and it has been thoroughly tested on AIX, Linux and Mac OSX. Historically, the code base pre-dates and in some way can be seen as a leaner substitute for some 'Boost' libraries.

https://github.com/ecmwf/eckit[+https://github.com/ecmwf/eckit+]

EEMD
~~~~

The ensemble empirical mode decomposition (EEMD) and its complete variant (CEEMDAN) are adaptive, noise-assisted data analysis methods that improve on the ordinary empirical mode decomposition (EMD). All these methods decompose possibly nonlinear and/or nonstationary time series data into a finite amount of components separated by instantaneous frequencies. This decomposition provides a powerful method to look into the different processes behind a given time series data, and provides a way to separate short time-scale events from a general trend. 

https://link.springer.com/article/10.1007%2Fs00180-015-0603-9[+https://link.springer.com/article/10.1007%2Fs00180-015-0603-9+]

https://www.ncl.ucar.edu/Applications/eemd.shtml[+https://www.ncl.ucar.edu/Applications/eemd.shtml+]

libeemd
^^^^^^^

A C library for performing the ensemble empirical mode decomposition (EEMD), its complete variant (CEEMDAN) or the regular empirical mode decomposition (EMD).
While written in C for numerical efficiency, our implementation includes interfaces to the Python and R languages, and interfaces to other languages are straightforward.

https://bitbucket.org/luukko/libeemd[+https://bitbucket.org/luukko/libeemd+]

https://link.springer.com/article/10.1007%2Fs00180-015-0603-9[+https://link.springer.com/article/10.1007%2Fs00180-015-0603-9+]

Rlibeemd
^^^^^^^^

An R interface for libeemd C library for ensemble empirical mode decomposition (EEMD) and its complete variant (CEEMDAN). 

https://github.com/helske/Rlibeemd[+https://github.com/helske/Rlibeemd+]

EFTS
~~~~

The EFTS Agents is a pure-Java framework providing highly efficient and robust software components (agents) to:

* Retrieve remote files from a remote system using various transfer protocols (Poll Agent)

* Replicate and optionally process files via a third-party plug-in (Pre-Processor Agent)

* Push files to a remote system using various transfer protocols (Push Agent)

* Run arbitrary jobs (processes) sequentially or in parallel. Not file driven (Job Agent)

* Check remote system connectivity (Checker Agent)

* Group other EFTS agents into one runtime process (Group Agent)

The agents can be configured to run continually or can be scheduled. They are able to handle flat directory structures or can recurse directory structures, both locally and remotely. Automatic file housekeeping is also provided.

https://www.eumetsat.int/website/home/Data/DataDelivery/SupportSoftwareandTools/index.html[+https://www.eumetsat.int/website/home/Data/DataDelivery/SupportSoftwareandTools/index.html+]

Elektra
~~~~~~~

Elektra serves as a universal and secure framework to access configuration settings in a global, hierarchical key database.

Elektra provides a mature, consistent and easily comprehensible API. Its modularity effectively avoids code duplication across applications and tools concerning their configuration tasks. Elektra abstracts from cross-platform-related issues and enables applications to be aware of other applications' configurations, leveraging easy application integration.

To highlight a few concrete things about Elektra, configuration settings can come from any data source, but usually comes from configuration files that are mounted into Elektra similar to mounting a file system. Elektra is a plugin-based framework, for example, plugins implement various configuration formats like INI, JSON, XML, etc. There is a lot more to discover like executing scripts (python, lua or shell) when a configuration value changes, or, enhanced validation plugins that will not allow corrupted configuration settings to reach your application.

As an application developer you get instant access to various configuration formats and the ability to fallback to default configuration settings without having to deal with this on your own. As an system administrator you can choose your favorite configuration format and mount this configuration for the application. Mounting enables easy application integration as any application using Elektra can access any mounted configuration. You can even mount /etc files such as hosts or fstab, so that there is no need to configure the same values twice in different files.

https://github.com/ElektraInitiative/libelektra[+https://github.com/ElektraInitiative/libelektra+]

https://www.libelektra.org/home[+https://www.libelektra.org/home+]

https://archive.fosdem.org/2018/schedule/event/elektra/[+https://archive.fosdem.org/2018/schedule/event/elektra/+]

Elemental
~~~~~~~~~

Elemental is open-source, openly-developed, software for distributed-memory dense and sparse-direct linear algebra and optimization which supports a wide range of functionality not available elsewhere.

Elemental supports a wide collection of sequential and distributed-memory operations, including support for dense and sparse-direct linear algebra, Linear, Quadratic and Second-Order Cone Programming, and lattice reduction. Furthermore, it supports such functionality for real and complex single-precision, double-precision, “double-double”, “quad-double”, quad-precision, and arbitrary-precision floating-point arithmetic.

The Cxx11 API is by far the most complete, but a large percentage of the library is also exposed to C and Python interfaces. Please see the README for an up-to-date list of unique functionality.

The development of Elemental has led to a number of research articles and is incorporated into a variety of scientific (e.g., libSkylark, PETSc, and CVXPY) and industrial projects.

http://libelemental.org/[+http://libelemental.org/+]

https://epubs.siam.org/doi/10.1137/140993478[+https://epubs.siam.org/doi/10.1137/140993478+]

https://github.com/scibuilder/SciBuilder[+https://github.com/scibuilder/SciBuilder+]

ELKI
~~~~

ELKI is an open source (AGPLv3) data mining software written in Java. The focus of ELKI is research in algorithms, with an emphasis on unsupervised methods in cluster analysis and outlier detection. In order to achieve high performance and scalability, ELKI offers data index structures such as the R*-tree that can provide major performance gains. ELKI is designed to be easy to extend for researchers and students in this domain, and welcomes contributions of additional methods. ELKI aims at providing a large collection of highly parameterizable algorithms, in order to allow easy and fair evaluation and benchmarking of algorithms.

In ELKI, data mining algorithms and data management tasks are separated and allow for an independent evaluation. This separation makes ELKI unique among data mining frameworks like Weka or Rapidminer and frameworks for index structures like GiST. At the same time, ELKI is open to arbitrary data types, distance or similarity measures, or file formats. The fundamental approach is the independence of file parsers or database connections, data types, distances, distance functions, and data mining algorithms. Helper classes, e.g. for algebraic or analytic computations are available for all algorithms on equal terms.

https://elki-project.github.io/[+https://elki-project.github.io/+]

https://github.com/elki-project/elki[+https://github.com/elki-project/elki+]

https://arxiv.org/abs/1902.03616[+https://arxiv.org/abs/1902.03616+]

Elmer
~~~~~

Elmer is an open-source, parallel, Finite Element code, mainly developed by the CSC in Finland. The ice sheet / ice flow model Elmer/Ice is based on Elmer and includes developments related to glaciological problems.

Elmer/Ice includes a large number of dedicated solvers and user functions which are described in these pages.

Elmer/Ice solves the full-Stokes equations for various ice rheologies (classical Glen’s flow law, anisotropic laws and porous compressible firn/snow law). It includes also solvers for the classical asymptotical expansions of the Stokes equations, namely the Shallow Ice Approximation (SIA) and the Shallow Shelf Approximation (SSA). All these equations can be solved diagnostically or in transient, allowing the displacement of the boundaries. By the multi-physics approach of Elmer it is also possible to solve coupled problems, such as thermo-mechanically coupled ice flow.

Elmer/Ice includes solvers for internal variables evolution, such as fabric for the anisotropic rheology or the density for the firn/snow law. Age of ice, temperature, stress and strain-rate fields have also their own solver.

In terms of boundary conditions, Elmer/Ice includes various friction laws (classical Weertman sliding law or effective-pressure dependent friction laws) and two basal hydrology model to evaluate the effective pressure. For the the grounding line dynamics, Elmer/Ice solves the contact problem between ice and bedrock defined by a variational inequality.

Elmer/Ice includes inverse methods to infer largely undetermined parameters such as the basal friction or ice fluidity.

Elmer/Ice also includes dedicated mesh tools specially designed to ice-sheet and glacier type geometries. Three-dimensional meshes are obtained using plane view unstructured mesh constructed using the tool YAMS and then vertically extruded.

http://elmerice.elmerfem.org/[+http://elmerice.elmerfem.org/+]

ELPA
~~~~

The computation of selected or all eigenvalues and eigenvectors of a symmetric (Hermitian) matrix has high relevance for various scientific disciplines. For the calculation of a significant part of the eigensystem typically direct eigensolvers are used. For large problems, the eigensystem calculations with existing solvers can become the computational bottleneck.

As a consequence, the ELPA project was initiated with the aim to develop and implement an efficient eigenvalue solver for petaflop applications.

https://elpa.mpcdf.mpg.de/[+https://elpa.mpcdf.mpg.de/+]

https://gitlab.mpcdf.mpg.de/elpa/elpa[+https://gitlab.mpcdf.mpg.de/elpa/elpa+]

https://arxiv.org/abs/1811.01277[+https://arxiv.org/abs/1811.01277+]

emcee
~~~~~

emcee is an MIT licensed pure-Python implementation of Goodman & Weare’s Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler and these pages will show you how to use it.

We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman & Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to ∼N2 for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort.

https://emcee.readthedocs.io/en/stable/[+https://emcee.readthedocs.io/en/stable/+]

https://github.com/dfm/emcee[+https://github.com/dfm/emcee+]

https://arxiv.org/abs/1202.3665[+https://arxiv.org/abs/1202.3665+]

EMS
~~~

The CSIRO Coastal Environmental Modelling (CEM) team develops, maintains and uses the EMS software that allows investigation of the physical, sediment and biogeochemical processes in marine environments. This is achieved by a ‘driver’ hydrodynamic code into which are linked various libraries to perform sediment transport and biogeochemistry, all supported by a core library. The ‘driver’ may be any model that manages the tracers required for sediments and biogeochemistry. The sediment and biogeochemical libraries are stand-alone modules that are linked to the driver via an interface, and in principle may be linked to any hydrodynamic code. Currently the ‘drivers’ available are a full hydrodynamic mode, a transport model that uses offline data to advect and diffuse sediment / biogeochemical variables, and a box model. The hydrodynamic code may further operate in reduced dimensions of 1D, 2D vertically averaged or 2D laterally averaged. A waves and tracer statistic library also exist; the latter allowing various operations to be performed during run-time on any tracers supported by the driver (e.g. means, fluxes, vertical integrals).

Additional software exists to generate the complex orthogonal curvilinear grids that are typically used for case studies. These grids allow variable resolution over the domain, useful for representing areas of interest with high resolution and less critical regions with coarser resolution. The curvilinear grid may also allow a dimensionality to be reduced from 3-D to 2-D within the same grid. This is useful when representing rivers or narrow estuaries, since the cross-river coordinate becomes very small in these areas and therefore becomes the defining grid size for setting the model time-step. Eliminating these small grid cells by making rivers or estuaries 2-D laterally averaged allows larger time-steps, hence a faster model. The curvilinear grids require dedicated software for visualizisation of model output, and the CEM supports several visualisation platforms to archive this. These software packages allow publication quality images and animations to be produced, and allow exploration of the data in 4 dimensions for analysis purposes.

https://github.com/csiro-coasts/EMS[+https://github.com/csiro-coasts/EMS+]

https://research.csiro.au/cem/software/ems/ems-documentation/[+https://research.csiro.au/cem/software/ems/ems-documentation/+]

Emscripten
~~~~~~~~~~

Emscripten is an Open Source LLVM to JavaScript compiler. Using Emscripten you can:

* Compile C and Cxx code into JavaScript
* Compile any other code that can be translated into LLVM bitcode into JavaScript.
* Compile the C/Cxx runtimes of other languages into JavaScript, and then run code in those other languages in an indirect way (this has been done for Python and Lua)!

Practically any portable C or Cxx codebase can be compiled into JavaScript using Emscripten, ranging from high performance games that need to render graphics, play sounds, and load and process files, through to application frameworks like Qt. Emscripten has already been used to convert a very long list of real-world codebases to JavaScript.

Emscripten generates fast code! Its default output format is asm.js , a highly optimizable subset of JavaScript that can execute at close to native speed in many cases.

The main tool is the Emscripten Compiler Frontend (emcc). This is a drop-in replacement for a standard compiler like gcc.
Emcc uses Clang to convert C/Cxx files to LLVM bitcode, and Fastcomp (Emscripten’s Compiler Core — an LLVM backend) to compile the bitcode to JavaScript. The output JavaScript can be executed by node.js, or from within HTML in a browser.

https://kripken.github.io/emscripten-site/[+https://kripken.github.io/emscripten-site/+]

emulators
~~~~~~~~~



https://en.wikipedia.org/wiki/Emulator[+https://en.wikipedia.org/wiki/Emulator+]

List of emulators written in JavaScript:  https://github.com/fcambus/jsemu[+https://github.com/fcambus/jsemu+]

Bochs
^^^^^

Bochs (pronounced "box") is a portable IA-32 and x86-64 IBM PC compatible emulator and debugger mostly written in Cxx and distributed as free software under the GNU Lesser General Public License. It supports emulation of the processor(s) (including protected mode), memory, disks, display, Ethernet, BIOS and common hardware peripherals of PCs.

Many guest operating systems can be run using the emulator including DOS, several versions of Microsoft Windows, BSDs, Linux, Xenix and Rhapsody (precursor of Mac OS X). Bochs runs on many host operating systems, including Android, iOS, Linux, macOS, PlayStation 2, Windows, and Windows Mobile.

Bochs is mostly used for operating system development (when an emulated operating system crashes, it does not crash the host operating system, so the emulated OS can be debugged) and to run other guest operating systems inside already running host operating systems. It can also be used to run older software – such as PC games – which will not run on non-compatible, or too fast computers. 

Bochs emulates the hardware needed by PC operating systems, including hard drives, CD drives, and floppy drives. It doesn't utilize any host CPU virtualization features, therefore is slower than most virtualization, rather than emulation software. It provides additional security by completely isolating the guest OS from the hardware. Bochs also has extensive debugging features. It is widely used for OS development, as it removes the need for constant system restarts (to test code). 

https://en.wikipedia.org/wiki/Bochs[+https://en.wikipedia.org/wiki/Bochs+]

https://sourceforge.net/projects/bochs/[+https://sourceforge.net/projects/bochs/+]

Citra
^^^^^

Citra is an open-source emulator for the Nintendo 3DS capable of playing many of your favorite games.

https://citra-emu.org/[+https://citra-emu.org/+]

Darkstar
^^^^^^^^

Darkstar provides emulation of the Xerox Dandelion workstation, commonly known as the Star, 8010, or 1108.

Darkstar will also run under Mono (http://www.mono-project.com/) on Unix platforms.

https://github.com/livingcomputermuseum/Darkstar[+https://github.com/livingcomputermuseum/Darkstar+]

https://engblg.livingcomputers.org/index.php/2019/01/19/introducing-darkstar-a-xerox-star-emulator/[+https://engblg.livingcomputers.org/index.php/2019/01/19/introducing-darkstar-a-xerox-star-emulator/+]

Dolphin
^^^^^^^

Dolphin is an emulator for running GameCube and Wii games on Windows, Linux, macOS, and recent Android devices.

https://github.com/dolphin-emu/dolphin[+https://github.com/dolphin-emu/dolphin+]

Gearboy
^^^^^^^

Gearboy is a Nintendo Game Boy / GameBoy Color emulator written in Cxx that runs on iOS, Raspberry Pi, Mac, Windows, Linux and RetroArch.

https://github.com/drhelius/Gearboy[+https://github.com/drhelius/Gearboy+]

GXemul
^^^^^^

GXemul is a framework for full-system computer architecture emulation. Several real machines have been implemented within the framework, consisting of processors (ARM, MIPS, Motorola 88K, PowerPC, and SuperH) and surrounding hardware components such as framebuffers, interrupt controllers, busses, disk controllers, and serial controllers. The emulation is working well enough to allow several unmodified "guest" operating systems to run.

http://gavare.se/gxemul/[+http://gavare.se/gxemul/+]

Hercules
^^^^^^^^

Hercules is a computer emulator allowing software written for IBM mainframe computers (System/370, System/390, and zSeries/System z) and for plug compatible mainframes (such as Amdahl machines) to run on other types of computer hardware, notably on low-cost personal computers. Development started in 1999 by Roger Bowler, a mainframe systems programmer.

Hercules runs under multiple parent operating systems including GNU/Linux, Microsoft Windows, FreeBSD, Solaris, and Mac OS X and is released under the open source software license QPL.[2] It is analogous to Bochs and QEMU in that it emulates CPU instructions and select peripheral devices only. A vendor (or distributor) must still provide an operating system, and the user must install it. Hercules was the first mainframe emulator to incorporate 64-bit z/Architecture support. 

Hercules is technically compatible with all IBM mainframe operating systems, even older versions which no longer run on newer mainframes. However, many mainframe operating systems require vendor licenses to run legally. Newer licensed operating systems, such as OS/390, z/OS, VSE/ESA, z/VSE, VM/ESA, z/VM, TPF/ESA, and z/TPF are technically compatible but cannot legally run on the Hercules emulator except in very limited circumstances, and they must always be licensed from IBM. IBM's Coupling Facility control code, which enables Parallel Sysplex, and UTS also require licenses to run. 

Operating systems which may legally be run, without license costs, on Hercules include:

* Older IBM operating systems including OS/360, DOS/360, DOS/VS, MVS, VM/370, and TSS/370 which are either public domain or "copyrighted software provided without charge."[3]
* The MUSIC/SP operating system may be available for educational and demonstration purposes upon request to its copyright holder, McGill University. Some of MUSIC/SP's features, notably networking, require z/VM (and thus an IBM license). However, a complete demonstration version of MUSIC/SP, packaged with the alternative Sim390 mainframe emulator, is available.
* The Michigan Terminal System (MTS) version 6.0A has been tailored to run under Hercules.[4]
* There is no known legal restriction to running open-source operating systems Linux on z Systems and OpenSolaris for System z on the Hercules emulator. They run well on Hercules, and many Linux on System z developers do their work using Hercules. Several distributors provide 64-bit z/Architecture versions of Linux, and some also provide ESA/390-compatible versions. Mainframe Linux distributions include SUSE Linux Enterprise Server, Red Hat Enterprise Linux, Debian GNU/Linux, CentOS, and Slackware. Sine Nomine Associates brought OpenSolaris to System z, relying on features provided by z/VM. Emulation of those specific z/VM features for OpenSolaris is included starting with Hercules Version 3.07.
* Certain unencumbered editors and utilities which can run on a mainframe without a parent operating system may be available to run on Hercules as well.

http://www.hercules-390.eu/[+http://www.hercules-390.eu/+]

https://github.com/rbowler/spinhawk[+https://github.com/rbowler/spinhawk+]

https://github.com/hercules-390/hyperion[+https://github.com/hercules-390/hyperion+]

http://hercules-390.github.io/html/[+http://hercules-390.github.io/html/+]

https://en.wikipedia.org/wiki/Hercules_(emulator)[+https://en.wikipedia.org/wiki/Hercules_(emulator)+]

http://www.cbttape.org/[+http://www.cbttape.org/+]

http://www.ibiblio.org/jmaynard/[+http://www.ibiblio.org/jmaynard/+]

QB64
^^^^

QB64 is a modern extended BASIC+OpenGL language that retains QB4.5/QBasic compatibility and compiles native binaries for Windows (XP and up), Linux and macOS (up to High Sierra).

https://www.qb64.org/[+https://www.qb64.org/+]

https://github.com/Kroc/PortaDOOM[+https://github.com/Kroc/PortaDOOM+]

RetroArch
^^^^^^^^^

RetroArch is the reference implementation of the libretro API. It is free, open-source, cross-platform software, licensed under the GNU GPLv3.

It is described as a front-end for emulators, game engines, video games, media players and other applications, designed to be fast, lightweight, portable and without dependencies.

RetroArch runs programs converted into dynamic libraries called libretro cores, using several user interfaces such as command-line interface, a few graphical user interfaces (GUI) optimized for gamepads (the most famous one being called XMB, a clone of Sony's XMB), several input, audio and video drivers, plus other sophisticated features like dynamic rate control, audio filters, multi-pass shaders, netplay, gameplay rewinding, cheats etc.

RetroArch has been ported to many platforms. It can run on several PC operating systems (Windows, OS X, GNU/Linux), home consoles (PlayStation 3, Xbox 360, Wii U, etc.), handheld consoles (PlayStation Vita, Nintendo 3DS, etc.), on smartphones (Android, iOS, etc.), embedded systems (Raspberry Pi, ODROID, etc.) and even on web browsers by using the Emscripten compiler. 

https://en.wikipedia.org/wiki/RetroArch[+https://en.wikipedia.org/wiki/RetroArch+]

https://www.retroarch.com/index.php[+https://www.retroarch.com/index.php+]

SimH
^^^^

SIMH is a highly portable, multi-system emulator which runs on Windows, Linux, Mac OS X, FreeBSD, OpenBSD, NetBSD and OpenVMS. It is maintained by Bob Supnik, a former DEC engineer and DEC vice president, and has been in development in one form or another since the 1960s. 

SimH (History Simulator) is a loose Internet-based collective of people interested in restoring historically significant computer hardware and software systems by simulation. The goal of the project is to create highly portable system simulators and to publish them as freeware on the Internet, with freely available copies of significant or representative software. The current, official version of SimH can be found in a GitHub source repository. It includes many additional simulators, as well as more advanced core libraries.

SIMH implements simulators for:

* Data General Nova, Eclipse
* Digital Equipment Corporation PDP-1, PDP-4, PDP-7, PDP-8, PDP-9, PDP-10, PDP-11, PDP-15, VAX11/780, VAX3900
* GRI Corporation GRI-909, GRI-99
* IBM 1401, 1620, 7090/7094, System 3
* Interdata (Perkin-Elmer) 16b and 32b systems
* Hewlett-Packard 2114, 2115, 2116, 2100, 21MX, 1000, 3000
* Honeywell H316/H516
* MITS Altair 8800, with both 8080 and Z80
* Royal-Mcbee LGP-30, LGP-21
* Scientific Data Systems SDS 940
* Xeros Data Systems Sigma 32b systems 

Also available is a collection of tools for manipulating simulator file formats and for cross-assembling code for the PDP-1, PDP-7, PDP-8, and PDP-11.

http://simh.trailing-edge.com/[+http://simh.trailing-edge.com/+]

https://github.com/simh/simh[+https://github.com/simh/simh+]

Unicorn
^^^^^^^

Unicorn is a lightweight multi-platform, multi-architecture CPU emulator framework.

Highlight features:

* Multi-architectures: Arm, Arm64 (Armv8), M68K, Mips, Sparc, & X86 (include X86_64).
* Clean/simple/lightweight/intuitive architecture-neutral API.
* Implemented in pure C language, with bindings for Crystal, Clojure, Visual Basic, Perl, Rust, Haskell, Ruby, Python, Java, Go, .NET, Delphi/Pascal & MSVC available.
* Native support for Windows & *nix (with Mac OSX, Linux, *BSD & Solaris confirmed).
* High performance by using Just-In-Time compiler technique.
* Support fine-grained instrumentation at various levels.
* Thread-safe by design.
* Distributed under free software license GPLv2.

http://www.unicorn-engine.org/[+http://www.unicorn-engine.org/+]

eofs
~~~~

eofs is a Python package for EOF analysis of spatial-temporal data. Using EOFs (empirical orthogonal functions) is a common technique to decompose a signal varying in time and space into a form that is easier to interpret in terms of spatial and temporal variance. Some of the key features of eofs are:

* Suitable for large data sets: computationally efficient for the large output data sets of modern climate models.
* Transparent handling of missing values: missing values are removed automatically during computations and placed back into output fields.
* Automatic metadata: metadata from input fields is used to construct metadata for output fields.
* No Compiler required: a fast implementation written in pure Python using the power of numpy, no Fortran or C dependencies.

https://ajdawson.github.io/eofs/[+https://ajdawson.github.io/eofs/+]

https://github.com/ajdawson/eofs[+https://github.com/ajdawson/eofs+]

ERDDAP
~~~~~~

ERDDAP is a data server that gives you a simple, consistent way to download subsets of gridded and tabular scientific datasets in common file formats and make graphs and maps. This particular ERDDAP installation has oceanographic data (for example, data from satellites and buoys). 

https://coastwatch.pfeg.noaa.gov/erddap/index.html[+https://coastwatch.pfeg.noaa.gov/erddap/index.html+]

https://coastwatch.pfeg.noaa.gov/erddap/information.html[+https://coastwatch.pfeg.noaa.gov/erddap/information.html+]

erddapy
^^^^^^^

erddapy takes advantage of ERDDAP’s RESTful web services and creates the ERDDAP URL for any request like searching for datasets, acquiring metadata, downloading data, etc.

https://ioos.github.io/erddapy/[+https://ioos.github.io/erddapy/+]

https://github.com/ioos/erddapy[+https://github.com/ioos/erddapy+]

http://ioos.github.io/notebooks_demos/notebooks/2018-03-01-erddapy/[+http://ioos.github.io/notebooks_demos/notebooks/2018-03-01-erddapy/+]

ESDM
~~~~

The middleware for earth system data is a prototype to improve I/O performance for earth system simulation as used in climate and weather applications. ESDM exploits structural information exposed by workflows, applications as well as data description formats such as HDF5 and NetCDF to more efficiently organize metadata and data across a variety of storage backends.

https://github.com/ESiWACE/esdm[+https://github.com/ESiWACE/esdm+]

https://www.esiwace.eu/[+https://www.esiwace.eu/+]

https://www.esiwace.eu/results-1/showroom/esiwace-at-the-supercomputing-conference-2016[+https://www.esiwace.eu/results-1/showroom/esiwace-at-the-supercomputing-conference-2016+]

ESMF
~~~~

The Earth System Modeling Framework (ESMF) is high-performance, flexible software infrastructure for building and coupling weather, climate, and related Earth science applications. The ESMF defines an architecture for composing complex, coupled modeling systems and includes data structures and utilities for developing individual models.

The basic idea behind ESMF is that complicated applications should be broken up into coherent pieces, or components, with standard calling interfaces. In ESMF, a component may be a physical domain, or a function such as a coupler or I/O system. ESMF also includes toolkits for building components and applications, such as regridding software, calendar management, logging and error handling, and parallel communications.

The features include:

* Full Fortran 90 interface, partial C/Cxx interface
* Fortran 90 Reference Manual and User's Guide
* Runs on most high performance parallel computing platforms, including IBM, many Linux variants, Cray, Compaq, more
* Supports MPI, OpenMP and hybrid user codes
* 8000+ tests bundled with source distribution
* Free user support
* Active user community

The superstructure for coupling Earth system components includes:

* Component, State and Coupler software that wraps user code with minimal overhead
* Simple drivers that users can modify
* Sequential or concurrent execution of components
* Single executable capability
* Limited multiple executable capability

The infrastructure for building Earth system components includes:

* Time Manager that includes Gregorian, 360 day, no-leap, Julian, and other calendars, as well as a broad range of time functions
* Data structures for storage and manipulation of Arrays, Fields, and Bundles of Fields on the same grid.
* Parallel data communications and regridding software
* Message logging tools
* Resource file manager

https://www.earthsystemcog.org/projects/esmf/[+https://www.earthsystemcog.org/projects/esmf/+]

ESMPy
^^^^^

ESMPy is a Python interface to the Earth System Modeling Framework (ESMF) regridding utility.
It provides a Grid to represent single-tile logically rectangular coordinate data, a Mesh for unstructured coordinates, and a LocStream for collections of unconnected points like observational data streams. ESMPy supports bilinear, nearest neighbor, higher order patch recovery, first-order conservative and second-order conservative regridding. There is also an option to ignore unmapped destination points and mask out points on either the source or destination. Regridding on the sphere takes place in 3D Cartesian space, so the pole problem is not an issue as it commonly is with some Earth system grid remapping software. Grid and Mesh objects can be created in 2D or 3D space, and 3D conservative regridding is fully supported.

https://www.earthsystemcog.org/projects/esmpy/[+https://www.earthsystemcog.org/projects/esmpy/+]

xESMF
^^^^^

xESMF is a Python package for regridding. It is

* Powerful: It uses ESMF/ESMPy as backend and can regrid between general curvilinear grids with all ESMF regridding algorithms, such as bilinear, conservative and nearest neighbour.
* Easy-to-use: It abstracts away ESMF's complicated infrastructure and provides a simple, high-level API, compatible with xarray as well as basic numpy arrays.
* Fast: It is faster than ESMPy's original Fortran regridding engine in serial case, and parallel capability will be added in the next version.

xESMF can regrid between general curvilinear (i.e. quadrilateral or “logically rectilinear”) grids, like

* The Cubed-Sphere grid in GFDL-FV3
* The Latitude-Longitude-Cap grid in MITgcm
* The Lambert Conformal grid in WRF

Current geospatial regridding tools tend to have non-trivial learning curves. xESMF tries to be simple and intuitive. Instead of inventing a new data structure, it relies on well-estabilished standards (numpy and xarray), so users don’t need to learn a bunch of new syntaxes or even a new software stack.

https://github.com/JiaweiZhuang/xESMF[+https://github.com/JiaweiZhuang/xESMF+]

https://xesmf.readthedocs.io/en/latest/[+https://xesmf.readthedocs.io/en/latest/+]

ESMValTool
~~~~~~~~~~

The Earth System Model eValuation Tool (ESMValTool) is a community diagnostics and performance metrics tool for the evaluation of Earth System Models (ESMs) that allows for routine comparison of single or multiple models, either against predecessor versions or against observations. The priority of the effort so far has been to target specific scientific themes focusing on selected Essential Climate Variables, a range of known systematic biases common to ESMs, such as coupled tropical climate variability, monsoons, Southern Ocean processes, continental dry biases and soil hydrology-climate interactions, as well as atmospheric CO2 budgets, tropospheric and stratospheric ozone, and tropospheric aerosols. The tool is being developed in such a way that additional analyses can easily be added. A set of standard namelists for each scientific topic reproduces specific sets of diagnostics or performance metrics that have demonstrated their importance in ESM evaluation in the peer-reviewed literature. The ESMValTool is a community effort open to both users and developers encouraging open exchange of diagnostic source code and evaluation results from the CMIP ensemble. This will facilitate and improve ESM evaluation beyond the state-of-the-art and aims at supporting such activities within the Coupled Model Intercomparison Project (CMIP) and at individual modeling centers. Ultimately, we envisage running the ESMValTool alongside the Earth System Grid Federation (ESGF) as part of a more routine evaluation of CMIP model simulations while utilizing observations available in standard formats (obs4MIPs) or provided by the user. 

The features of ESMValTool include:

* Facilitates the complex evaluation of ESMs and their simulations submitted to international Model Intercomparison Projects (e.g., CMIP).
* Standardized model evaluation can be performed against observations, against other models or to compare different versions of the same model.
* Wide scope: includes many diagnostics and performance metrics covering different aspects of the Earth System (dynamics, radiation, clouds, carbon cycle, chemistry, aerosol, sea-ice, etc.) and their interactions.
* Well-established analysis: standard namelists reproduce specific sets of diagnostics or performance metrics that have demonstrated their importance in ESM evaluation in the peer-reviewed literature.
* Broad documentation: user guide (Eyring et al., 2015); SPHINX; a log-file is written containing all the information of a specific call of the main script: creation date of running the script, version number, analyzed data (models and observations), applied diagnostics and variables, and corresponding references. This helps to increase the traceability and reproducibility of the results.
* High flexibility: new diagnostics and more observational data can be easily added.
* Multi-language support: Python, NCL, R... other open-source languages are possible.
* CF/CMOR compliant: data from many different projects can be handled (CMIP, obs4mips, ana4mips, CCMI, CCMVal, AEROCOM, etc.). Routines are provided to CMOR-ize non-compliant data.
* Integration in modeling workflows: for EMAC, NOAA-GFDL and NEMO, can be easily extended.

https://www.esmvaltool.org/[+https://www.esmvaltool.org/+]

https://github.com/ESMValGroup/ESMValTool[+https://github.com/ESMValGroup/ESMValTool+]

http://www.cesm.ucar.edu/working_groups/CVC/cvdp/[+http://www.cesm.ucar.edu/working_groups/CVC/cvdp/+]

EVSL
~~~~

The EigenValues Slicing Library (EVSL) provides routines for computing eigenvalues located in a given interval, and their associated eigenvectors, of real symmetric - standard or generalized eigenvalue problems. It also provides tools for spectrum slicing, i.e., the technique of subdividing a given interval into p smaller subintervals and computing the eigenvalues in each subinterval independently. EVSL implements a polynomial filtered Lanczos (thick restart, no restart) a rational filtered Lanczos (thick restart, no restart), and a polynomial filtered subspace iteration. 

The features include:

* With EVSL you can compute eigenvalues and vectors in a given (single) arbitrary sub-interval of the spectral interval [the interval containing all eigenvalues].
* You can estimate the spectral density of the matrix and slice the eigenvalue spectrum in roughly equal parts and then invoke one of the filtered routines to compute the spectra in each sub-interval independently. Two methods are provided for this, one based on the Kernel Polynomial Method (KPM) the other on the Lanczos algorithm.
* EVSL uses polynomial and rational filtering. The filter functions are determined and processed automatically and default settings are provided so the user need not provide input for their selection.
* EVSL handles real symmetric standard eigenvalue problems and real symmetric generalized eigenvalue problems.
* Polynomial filtering is recommended mostly for cases where (1) matrix-vector operations are not too expensive and (2) the spectrum is not too irregularly distributed. Rational filtering is useful when factorizations are not too expensive (this essentially excludes very large 3D problems for example).
* Orthogonality within each slice is enforced. However, the codes do not (yet) provide post-processing functions to check and enforce orthogonality across different slices.
* An interface is provided for calling EVSL functions from FORTRAN. In addition, the code allows a matrix-free format, i.e., a procedure in which the user does not provide a matrix but rather his/her own matrix-vector product (matvec) function 
* The code is written in C. Some parallelization is provided with openMP, implementing a trivial parallelism across the slices. [see openMP example ..] A parallel (MPI) version of the code is in the works and will be made available in future releases. 

https://www-users.cs.umn.edu/\~saad/software/EVSL/index.html[+https://www-users.cs.umn.edu/~saad/software/EVSL/index.html+]

https://github.com/eigs/EVSL[+https://github.com/eigs/EVSL+]

ExactPack
~~~~~~~~~

An open-source software package that has been developed for the verification & validation community. The package has two major capabilities:

* The generation of exact solutions for common benchmark problems in computational physics.
* The analysis of computational physics code output to assess accuracy and convergence rates.

ExactPack is designed to be an open project, readily expandable to include new test problems for new physical models. At present, it contains the following modules and solutions (see reference list and documentation for details on these test problems):

* Hydrodynamics
** Noh problem
** Uniform collapse problem (a.k.a. Noh2, Shockless Noh)
** Sedov problem
** Several common shock tube problems (Sod, Einfeldt, Stationary Contact, Slow Shock, Shock Contact Shock, LeBlanc)
** Shock tube problem with JWL equation of state
** Guderley problem
* Reactive flows
** Escape of high-explosive products
** Programmed burn timing
** Steady-detonation reaction-zone
** Mader problem
** Detonation Shock Dynamics problems (beta)
*** Rate stick
*** Cylindrical expansion
*** Explosive arc
* Inviscid hydrodynamics with heat transfer (radiation, conduction)
** Coggeshall problems
** Reinicke Meyer-ter-Vehn problem
** Su-Olson problem
* Solid mechanics
** Blake problem
* Heat Conduction
** Planar sandwich
** Cylindrical sandwich

The analysis capabilities of ExactPack include the import of data from computational physics codes, calculation of error norms between exact solutions and numerical results, and the calculation of spatial convergence rates. These tools follow the standards defined by the American Society of Mechanical Engineers (ASME).

https://github.com/lanl/ExactPack[+https://github.com/lanl/ExactPack+]

https://github.com/lanl/EOSlib[+https://github.com/lanl/EOSlib+]

EXIF
~~~~

Blah.

ExifTool
^^^^^^^^

ExifTool is a platform-independent Perl library plus a command-line application for reading, writing and editing meta information in a wide variety of files. ExifTool supports many different metadata formats including EXIF, GPS, IPTC, XMP, JFIF, GeoTIFF, ICC Profile, Photoshop IRB, FlashPix, AFCP and ID3, as well as the maker notes of many digital cameras by Canon, Casio, DJI, FLIR, FujiFilm, GE, GoPro, HP, JVC/Victor, Kodak, Leaf, Minolta/Konica-Minolta, Motorola, Nikon, Nintendo, Olympus/Epson, Panasonic/Leica, Pentax/Asahi, Phase One, Reconyx, Ricoh, Samsung, Sanyo, Sigma/Foveon and Sony.

https://www.sno.phy.queensu.ca/\~phil/exiftool/[+https://www.sno.phy.queensu.ca/~phil/exiftool/+]

https://hvdwolf.github.io/pyExifToolGUI/[+https://hvdwolf.github.io/pyExifToolGUI/+]

Exiv2
^^^^^

Exiv2 is a Cross-platform Cxx library and a command line utility to manage image metadata. It provides fast and easy read and write access to the Exif, IPTC and XMP metadata and the ICC Profile embedded within digital images in various formats. Exiv2 is available as free software and is used in many projects including KDE and Gnome Desktops as well as many applications including GIMP, darktable, shotwell, GwenView and Luminance HDR. 

https://github.com/Exiv2/exiv2[+https://github.com/Exiv2/exiv2+]

http://www.exiv2.org/[+http://www.exiv2.org/+]

jhead
^^^^^

An EXIF JPEG header manipulation tool.

http://www.sentex.net/\~mwandel/jhead/[+http://www.sentex.net/~mwandel/jhead/+]

libexif
^^^^^^^

libexif is a library for parsing, editing, and saving EXIF data. It is
intended to replace lots of redundant implementations in command-line
utilities and programs with GUIs.

libexif supports parsing, editing and saving of EXIF data. In addition, it
has gettext support. All EXIF tags described in EXIF standard 2.1 (and most
from 2.2) are supported.  Many maker notes from Canon, Casio, Epson,
Fuji, Nikon, Olympus, Pentax and Sanyo cameras are also supported.

https://github.com/libexif/libexif[+https://github.com/libexif/libexif+]

Expokit
~~~~~~~

Expokit is a software package that provides matrix exponential routines for small dense or very large sparse matrices, real or complex. Here you will find the source code in Fortran and Matlab. The native Fortran version is embeddable in C/Cxx.

https://www.maths.uq.edu.au/expokit/[+https://www.maths.uq.edu.au/expokit/+]

https://github.com/weinbe58/expokitpy[+https://github.com/weinbe58/expokitpy+]

#FFFF

FAIR
~~~~

The Finite Amplitude Impulse Response (FaIR) model is a simple emissions-based climate model. It allows the user to input emissions of greenhouse gases and short lived climate forcers in order to estimate global mean atmospheric GHG concentrations, radiative forcing and temperature anomalies.

The original FaIR model (v1.0) was developed to simulate the earth system response to CO2 emissions, with all non-CO2 forcing implemented as an “external” source.

The emissions-based model (v1.3) extends FaIR by replacing all sources of non-CO2 forcing with relationships that are based on the source emissions, with the exception of natural forcings (variations in solar irradiance and volcanic eruptions). It is useful for assessing future policy commitments to anthropogenic emissions (something which we can control) than to radiative forcing (something which is less certain and which we can only partially control).

https://github.com/OMS-NetZero/FAIR[+https://github.com/OMS-NetZero/FAIR+]

https://fair.readthedocs.io/en/latest/[+https://fair.readthedocs.io/en/latest/+]

Faiss
~~~~~

Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in Cxx with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU.

Faiss contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.

Most of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server.

The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace IndexFlatL2 with GpuIndexFlatL2) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported.

https://github.com/facebookresearch/faiss[+https://github.com/facebookresearch/faiss+]

https://github.com/facebookresearch/faiss/wiki/Getting-started[+https://github.com/facebookresearch/faiss/wiki/Getting-started+]

https://arxiv.org/abs/1702.08734[+https://arxiv.org/abs/1702.08734+]

FanStore
~~~~~~~~

FanStore is a transient shared object store designed for disitrbuted deep learning workloads. FanStore stores datasets in across local storage (e.g., RAM, RAM disk, SSD), and use interconnect for remote file access. FanStore maintains a global name space across nodes and exposes a POSIX-compliant access interface as a regular shared file system. FanStore runs in user space, enabled by the funtion interception technique, which is 10x faster than FUSE. FanStore supports compression, which can significantly increase the storage hardware capacity for some applications. FanStore dramatically reduce the I/O traffic between compute nodes and the shared file system.

Emperically, FanStore has enbabled deep learning training with millions of small files (KB--MB), with a total size of ~100GB on hundreds of compute nodes with close-to-perfect scaling performanace. The following figure shows the ResNet-50 performance scalability of a GPU cluster (4 GPUs per node) an a CPU cluster with Keras, TensorFlow, and Horovod.

Technically, FanStore partitions dataset into chunks and stores one or multiple chunks on each node. Metadata of the dataset is replicated across nodes for highly current access. File data is either accessed locally through PCIE or remotely through round-trip MPI messages, as shown in the following figure.

https://github.com/TACC/FanStore[+https://github.com/TACC/FanStore+]

FastBit
~~~~~~~

FastBit is an open-source data processing library following the spirit of NoSQL movement. It offers a set of searching functions supported by compressed bitmap indexes. It treats user data in the column-oriented manner similar to well-known database management systems such as Sybase IQ, MonetDB, and Vertica. It is designed to accelerate user's data selection tasks without imposing undue requirements. In particular, the user data is NOT required to be under the control of FastBit software, which allows the user to continue to use their existing data analysis tools.

The key technology underlying the FastBit software is a set of compressed bitmap indexes. In database systems, an index is a data structure to accelerate data accesses and reduce the query response time. Most of the commonly used indexes are variants of the B-tree, such as B+-tree and B*-tree. FastBit implements a set of alternative indexes called compressed bitmap indexes. Compared with B-tree variants, these indexes provide very efficient searching and retrieval operations, but are somewhat slower to update after a modification of an individual record.

https://sdm.lbl.gov/fastbit/[+https://sdm.lbl.gov/fastbit/+]

https://code.lbl.gov/projects/fastbit/[+https://code.lbl.gov/projects/fastbit/+]

https://github.com/brentp/fastbit-python[+https://github.com/brentp/fastbit-python+]

Fastor
~~~~~~

Fastor [FAst SIMD opTimised tensOR algebra framework]: is a smart stack-based high performance tensor (multi-dimensional array) library written in modern Cxx [Cxx11/14/17] with powerful in-built tensor algebraic functionalities (tensor contraction, permutation, reductions, special tensor groups and much more). Designed as a generic multi-dimensional tensor algebra library, Fastor also incorporates domain specific features for tensor contraction algorithms typically arising in classical mechanics, in particular, in finite element analysis of nonlinear solids, fluids and coupled continua. There are multiple paradigms that Fastor exploits:

* Operation minimisation/Low FLOP/Complexity reducing Algorithms: Fastor relies on an extremely smart and domain-aware Expression Template (ET) engine that can not only perform lazy evaluations and operator chaining but can also perform sophisticated mathematical transformation or compile time graph optimisation or both to reduce the complexity of evaluation of expressions by orders of magnitude. Some of these functionalities are non-existent in other available Cxx ET linear algebra libraries.
* SIMD/Data parallelism/Stream computing Fastor utilises explicit SIMD (SSE/SSE2/SSE3/SSE4/AVX/AVX2/AVX512/FMA) instructions
* Zero overhead tensor algebraic functions statically dispatched bespoke kernels for a variety of tensor products using a priori knowledge of tensors either through template specialisation or advanced topological studies or both

Fastor is essentially designed for small mutlidimensional tensors (for instance, tensors appearing during numerical integration in a finite element framework such as stresses, work conjugates, Hessian or small multi-dimensional arrays useful for 2D/3D graphics).
The dimensions of the tensors must be known at compile time, which is typically the case for the use-cases it is designed for. However one of the strongest features of Fastor is in its in-built template meta-programming engine, in that, it can automatically determine at compile time, the dimensions of the tensors resulting from a complex operation yet to be performed, hence it can always allocate exactly the right amount of stack memory required. This is in contrast to static arrays in C or Fortran where one has to allocate a huge block of memory before hand to avoid stack overflow.

Fastor introduces powerful tensor views which make tensor indexing, slicing and broadcating look and feel native to scientific programmers.
Building upon its domain specific features, Fastor implements the tensor cross product family of algebra recently introduced by Bonet et. al. in the context of numerical analysis of nonlinear classical mechanics which can significantly reduce the amount algebra involved in tensor derivatives of functionals which are forbiddingly complex to derive using a standard approach.

https://github.com/romeric/Fastor[+https://github.com/romeric/Fastor+]

https://www.sciencedirect.com/science/article/pii/S0010465517300681[+https://www.sci/JJJ/
encedirect.com/science/article/pii/S0010465517300681+]

FastR
~~~~~

A high-performance implementation of the R programming language, built on GraalVM.

The goal of FastR is to be a drop-in replacement for GNU-R, the reference implementation of the R language. FastR faithfully implements the R language, and any difference in behavior is considered to be a bug.

FastR is capable of running binary R packages built for GNU-R as long as those packages properly use the R extensions C API (for best results, it is recommended to install R packages from source). FastR supports R graphics via the grid package and packages based on grid (like lattice and ggplot2). We are currently working towards support for the base graphics package. FastR currently supports many of the popular R packages, such as ggplot2, jsonlite, testthat, assertthat, knitr, Shiny, Rcpp, rJava, quantmod and more…

Moreover, support for dplyr and data.table are on the way. We are actively monitoring and improving FastR support for the most popular packages published on CRAN including all the tidyverse packages. However, one should take into account the experimental state of FastR, there can be packages that are not compatible yet, and if you try it on a complex R application, it can stumble on those.

https://github.com/oracle/fastr[+https://github.com/oracle/fastr+]

FATODE
~~~~~~

FATODE is a FORTRAN library for the integration of ordinary differential equations with direct and adjoint sensitivity analysis capabilities. FATODE implements four families of methods -- explicit Runge-Kutta for nonstiff problems and fully implicit Runge-Kutta, singly diagonally implicit Runge-Kutta, and Rosenbrock for stiff problems. Each family contains several methods with different orders of accuracy; users can add new methods by simply providing their coefficients. For each family the forward, adjoint, and tangent linear models are implemented.

General purpose solvers for dense and sparse linear algebra are used; users can easily incorporate problem-tailored linear algebra routines. To the best of our knowledge FATODE is the first publicly available general purpose package that offers forward and adjoint sensitivity analysis capabilities in the context of Runge Kutta methods. A wide range of applications are expected to benefit from its use; examples include parameter estimation, data assimilation, optimal control, and uncertainty quantification. 

The features include:

* Time stepping methods with good stability properties implemented in FATODE. (ERK, FIRK, SDIRK and ROS stand for Explicit Runge-Kutta, Fully Implicit Runge-Kutta, Singly Diagonally Implicit Runge-Kutta and Rosenbrock, respectively.)
* The capabilities to solve general ODEs (both stiff and nostiff problems) and to perform direct and adjoint discrete sensitivity analysis.
* Standalone extendable linear solvers for both full and sparse matrices. 
* Provide fine controls over step size, order of accuracy, termination criteria for Newton iterations and so on.

http://people.cs.vt.edu/\~asandu/Software/FATODE/index.html[+http://people.cs.vt.edu/~asandu/Software/FATODE/index.html+]

FAUST
~~~~~

FAUST (Functional AUdio STream) is a domain-specific purely functional programming language for implementing signal processing algorithms in the form of libraries, audio plug-ins, or standalone applications. A FAUST program denotes a signal processor: a mathematical function that is applied to some input signal and then fed out. 

Faust (Functional Audio Stream) is a functional programming language for sound synthesis and audio processing with a strong focus on the design of synthesizers, musical instruments, audio effects, etc. Faust targets high-performance signal processing applications and audio plug-ins for a variety of platforms and standards.

The core component of Faust is its compiler. It allows to "translate" any Faust digital signal processing (DSP) specification to a wide range of non-domain specific languages such as Cxx, C, JAVA, JavaScript, LLVM bit code, WebAssembly, etc. In this regard, Faust can be seen as an alternative to Cxx but is much simpler and intuitive to learn.

Thanks to a wrapping system called "architectures," codes generated by Faust can be easily compiled into a wide variety of objects ranging from audio plug-ins to standalone applications or smartphone and web apps, etc.

https://faust.grame.fr/[+https://faust.grame.fr/+]

https://faust.grame.fr/doc/manual/index.html[+https://faust.grame.fr/doc/manual/index.html+]

https://en.wikipedia.org/wiki/FAUST_(programming_language)[+https://en.wikipedia.org/wiki/FAUST_(programming_language)+]

fckit
~~~~~

Fortran toolkit for interoperating Fortran with C/Cxx.

In addition useful algorithms from ecKit are wrapped with Fortran.

https://github.com/ecmwf/fckit[+https://github.com/ecmwf/fckit+]

FDT
~~~

FDT is an Application for Efficient Data Transfers which is capable of reading and writing at disk speed over wide area networks (with standard TCP). It is written in Java, runs an all major platforms and it is easy to use.

FDT is based on an asynchronous, flexible multithreaded system and is using the capabilities of the Java NIO libraries. Its main features are:

* Streams a dataset (list of files) continuously, using a managed pool of buffers through one or more TCP sockets.
* Uses independent threads to read and write on each physical device.
* Transfers data in parallel on multiple TCP streams, when necessary.
* Uses appropriate-sized buffers for disk I/O and for the network.
* Restores the files from buffers asynchronously.
* Resumes a file transfer session without loss, when needed.

FDT can be used to stream a large set of files across the network, so that a large dataset composed of thousands of files can be sent or received at full speed, without the network transfer restarting between files.

https://fast-data-transfer.github.io/[+https://fast-data-transfer.github.io/+]

https://github.com/fast-data-transfer[+https://github.com/fast-data-transfer+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

Feather
~~~~~~~

Feather provides binary columnar serialization for data frames. It is designed to
make reading and writing data frames efficient, and to make sharing data across
data analysis languages easy. This initial version comes with bindings for Python and R.

Feather uses the [Apache Arrow](https://arrow.apache.org) columnar memory
specification to represent binary data on disk. This makes read and write
operations very fast. This is particularly important for encoding null/NA values
and variable-length types like UTF8 strings.

Feather is a part of the broader Apache Arrow project. Feather defines its own
simplified schemas and metadata for on-disk representation.

Feather currently supports the following column types:

* A wide range of numeric types (int8, int16, int32, int64, uint8, uint16,
  uint32, uint64, float, double).
* Logical/boolean values.
* Dates, times, and timestamps.
* Factors/categorical variables that have fixed set of possible values.
* UTF-8 encoded strings.
* Arbitrary binary data.

All column types support NA/null values.

https://github.com/wesm/feather[+https://github.com/wesm/feather+]

https://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/[+https://blog.cloudera.com/blog/2016/03/feather-a-fast-on-disk-format-for-data-frames-for-r-and-python-powered-by-apache-arrow/+]

http://wesmckinney.com/blog/feather-arrow-future/[+http://wesmckinney.com/blog/feather-arrow-future/+]

Feather.jl
^^^^^^^^^^

Feather.jl provides a pure Julia library for reading and writing feather-formatted binary files, an efficient on-disk representation of a DataFrame.

http://juliadata.github.io/Feather.jl/stable/[+http://juliadata.github.io/Feather.jl/stable/+]

https://github.com/JuliaData/Feather.jl[+https://github.com/JuliaData/Feather.jl+]

FeatureServer
~~~~~~~~~~~~~

FeatureServer is an implementation of a RESTful Geographic Feature Service. Using standard HTTP methods, you can fetch a representation of a feature or a collection of features, add new data to the service, or delete data from the service. Use it as an aggregator -- post your GeoRSS feeds to it, and then browse them using WFS. Use it as a translator: use the OGR DataSource to load a shapefile and open it in Google Earth.

The available data sources are:

* The DBM datasource uses anydbm combined with pickle to store features in a file on disk.
* Load images from flickr.
* GeoAlchemy datasource for FeatureServer allows you to access features stored in one of the supported spatial databases. 
* The OGR datasource allows you to take any OGR datasource -- such as a shapefile, PostGIS database, GML file, or other formats supported by OGR -- and use it as a backend for a FeatureServer layer.
* Load streets from OpenStreetMap by area, id, or key/value pair.
* The PostGIS datasource implements a direct connection to PostGIS, allowing for full featured editing/updating.
* SpatiaLite is an open source library intended to extend the SQLite core to support fully fledged Spatial SQL capabilities.
* A simple SQLite datasource that can be used on any website with Python support for SQLite. Creates 2 tables for each layer: one for the features, and one for any attributes/properties pertaining to those features.
* Powered by Twittervision, the twitter datasource lets you use twittervision's API to display the current location of a user.
* The WFS datasource implements read-only access to WFS servers.

The services provided by FeatureServer are:

* Outputs each attribute of a feature as a separate column and the geomtry as WKT, i.e. CSV
* DXF - Compatible with release 11 and 12, points, polyline (linestring), polygon
* GeoJSON - input and output in the emerging GeoJSON specification. FeatureServer supports GeoJSON Points, Lines, and Polygons with Rings, as both input and output.
* GeoRSS Atom - input and output of Points/Lines/Polygons (no rings/holes) in GeoRSS Simple (Atom)
* Output-only support of WFS/GML.
* GPX (the GPS Exchange Format) is a light-weight XML data format for the interchange of GPS data (waypoints, routes, and tracks) between applications and Web services on the Internet.
* Output-only support of features as HTML files, powered by Cheetah templates.
* Input and output of Points, Lines, and Polygons from KML.
* The Esri shapefile or simply a shapefile is a popular geospatial vector data format for geographic information systems software.
* SpatiaLite is an open source library intended to extend the SQLite core to support fully fledged Spatial SQL capabilities.

http://featureserver.org/[+http://featureserver.org/+]

Fedora
~~~~~~

Fedora is a robust, modular, open source repository system for the management and dissemination of digital content. It is especially suited for digital libraries and archives, both for access and preservation. It is also used to provide specialized access to very large and complex digital collections of historic and cultural materials as well as scientific data. Fedora has a worldwide installed user base that includes academic and cultural heritage organizations, universities, research institutions, university libraries, national libraries, and government agencies.

The Fedora project is led by the Fedora Leadership Group and is under the stewardship of the DuraSpace not-for-profit organization providing leadership and innovation for open source technology projects and solutions that focus on durable, persistent access to digital data.

In partnership with stakeholder community members DuraSpace has put together global, strategic collaborations to sustain Fedora which is used by more than three hundred institutions. 

https://duraspace.org/fedora/[+https://duraspace.org/fedora/+]

FEHM
~~~~

 FEHM is used to simulate groundwater and contaminant flow and transport in deep and shallow, fractured and un-fractured porous media. FEHM was created in the early 1970s to simulate geothermal and hot dry rock reservoirs and then used in studies of flow fields and mass transport in the saturated and unsaturated zones below the potential Yucca Mountain repository. Today FEHM is used to simulate groundwater and contaminant flow and transport in deep and shallow, fractured and un-fractured porous media throughout the US DOE complex. FEHM has proved to be a valuable asset on a variety of projects of national interest including Environmental Remediation of the Nevada Test Site, the LANL Groundwater Protection Program, geologic CO2 sequestration, Enhanced Geothermal Energy (EGS) programs, Oil and Gas production, Nuclear Waste Isolation, and Arctic Permafrost.

Subsurface physics has ranged from single fluid/single phase fluid flow when simulating basin scale groundwater aquifers to complex multi-fluid/multi-phase fluid flow that includes phase change with boiling and condensing in applications such as unsaturated zone surrounding nuclear waste storage facility or leakage of CO2/brine through faults or wellbores. The numerical method used in FEHM is the control volume method (CV) for fluid flow and heat transfer equations which allows FEHM to exactly enforce energy/mass conservation; while an option is available to use the finite element (FE) method for displacement equations to obtain more accurate stress calculations. In addition to these standard methods, an option to use FE for flow is available, as well as a simple Finite Difference scheme.

* 3-dimensional complex geometries with unstructured grids
* Saturated and unsaturated media
* Simulation of production from gas hydrate reservoirs
* Simulation of geothermal reservoirs
* Non-isothermal, multi-phase flow of gas, water, oil
* Non-isothermal, multi-phase flow of air, water
* Non-isothermal, multi-phase flow of CO2, water
* Multiple chemically reactive and sorbing tracers
* Preconditioned conjugate gradient solution of coupled linear equations
* Fully implicit, fully coupled Newton Raphson solution of nonlinear equations
* Double porosity and double porosity/double permeability capabilities
* Control volume (CV) and finite element method (FE) methods
* Coupled geomechanics (THM) problems (fluid flow and heat transfer coupled with stress/deformation) including non-linear elastic and plastic deformation, nonlinear functional dependence of rock properties (e.g. permeability, porosity, Young's modulus) on pressure, temperature and damage/stress

https://fehm.lanl.gov/[+https://fehm.lanl.gov/+]

https://github.com/lanl/FEHM[+https://github.com/lanl/FEHM+]

FEHM.jl
^^^^^^^

A Julia interface to FEHM.

https://github.com/zemjulia/FEHM.jl[+https://github.com/zemjulia/FEHM.jl+]

PyFEHM
^^^^^^

PyFEHM is an open-source (LGPL 2.1) Python library that provides classes and methods to support a scripting environment for the subsurface heat and mass transfer, and geomechanics code FEHM. 

https://pyfehm.lanl.gov/[+https://pyfehm.lanl.gov/+]

Femtocode
~~~~~~~~~

Femtocode is a language and a system to support fast queries on structured data, such as high-energy physics data.

The goal of this project is to replace the practice of copying and reducing a centrally produced dataset with direct queries on that dataset. Currently, high-energy physicists write programs to extract the attributes and data records of interest— personally managing the storage and versioning of that private copy— just to make plots from the subset in real time. Femtocode will allow users to make plots from the original dataset in real time, which may be as large as petabytes.

Femtocode makes this possible by introducing a novel translation of query semantics into pure array operations, which strips away all unnecessary work at runtime. By dramatically reducing the computation time, the only bottleneck left is the data transfer, so caching is also heavily used to minimize the impact of repeated and similar queries.

https://github.com/diana-hep/femtocode[+https://github.com/diana-hep/femtocode+]

FEniCS
~~~~~~

FEniCS is a popular open-source (LGPLv3) computing platform for solving partial differential equations (PDEs). FEniCS enables users to quickly translate scientific models into efficient finite element code. With the high-level Python and Cxx interfaces to FEniCS, it is easy to get started, but FEniCS offers also powerful capabilities for more experienced programmers. FEniCS runs on a multitude of platforms ranging from laptops to high-performance clusters.

Each component of the FEniCS platform has been fundamentally designed for parallel processing. Executing a FEniCS script in parallel is as simple as calling mpirun -np 64 python script.py. This framework allows for rapid prototyping of finite element formulations and solvers on laptops and workstations, and the same code may then be deployed on large high-performance computers.

FEniCS is a collection of inter-operating modules.  These modules are:

* *DOLFIN* -  The computational backend of FEniCS. It implements the FEniCS Problem Solving Environment in Python and Cxx. 

* *mshr* - The mesh generation component of FEniCS. It generates simplicial DOLFIN meshes in 2D and 3D from geometries described by Constructive Solid Geometry (CSG) or from surface files, utilizing CGAL and Tetgen as mesh generation backends.

* *UFL* - The Unified Form Language (UFL) is a domain specific language for declaration of finite element discretizations of variational forms. More precisely, it defines a flexible interface for choosing finite element spaces and defining expressions for weak forms in a notation close to mathematical notation.

* *FFC* - FFC is a compiler for finite element variational forms. From a high-level description of the form, it generates efficient low-level Cxx code that can be used to assemble the corresponding discrete operator (tensor). In particular, a bilinear form may be assembled into a matrix and a linear form may be assembled into a vector. FFC may be used either from the command line (by invoking the ffc command) or as a Python module.

* *FIAT* - FIAT is a Python package for automatic generation of finite element basis functions. It is capable of generating finite element basis functions for a wide range of finite element families on simplices (lines, triangles and tetrahedra), including the Lagrange elements, and the elements of Raviart-Thomas, Brezzi-Douglas-Marini and Nedelec. It is also capable of generating tensor-product elements and a number more exotic elements, such as the Argyris, Hermite and Morley elements.

* *dijitso* - A Python module for distributed just-in-time shared library building.

https://fenics.readthedocs.io/en/latest/[+https://fenics.readthedocs.io/en/latest/+]

https://fenicsproject.org/[+https://fenicsproject.org/+]

DOLFIN-X
^^^^^^^^

DOLFIN-X is an experimental version of DOLFIN. It is being actively developed, but is not ready for production use. New experimental features may come and go as development proceeds.

DOLFIN is the computational backend of FEniCS and implements the FEniCS Problem Solving Environment in Python and Cxx.

https://github.com/FEniCS/dolfinx[+https://github.com/FEniCS/dolfinx+]

FEniCS Mechanics
^^^^^^^^^^^^^^^^

A python package developed to facilitate the formulation of computational mechanics simulations and make these simulations more accessible to users with limited programming or mechanics knowledge. This is done by using the tools from the FEniCS Project (www.fenicsproject.org). The FEniCS Project libraries provide tools to formulate and solve variational problems. FEniCS Mechanics is built on top of these libraries specifically for computational mechanics problems.

At this point, FEniCS Mechanics can handle problems in fluid mechanics and (hyper)elasticity. Fluid Mechanics problems are formulated in Eulerian coordinates, while problems in elasticity are formulated in Lagrangian coordinates. Further development is required for the implementation of Arbitrary Lagrangian-Eulerian (ALE) formulations. Single domain problems are supported, but interaction problems are not.

https://shaddenlab.gitlab.io/fenicsmechanics/chapters/introduction.html[+https://shaddenlab.gitlab.io/fenicsmechanics/chapters/introduction.html+]

https://arxiv.org/abs/1901.07960[+https://arxiv.org/abs/1901.07960+]

FenicsSolver
^^^^^^^^^^^^

A set of multi-physics FEM solvers based on Fenics with GUI support(via integration Fenics into FreeCAD FemWorkbench and CfdWorkbench), focusing on multi-body, reduced-order nonlinear problem and mutlti-solver coupling.It functions like COMSOL or Moose, but it is free and it is made of Python.

https://github.com/qingfengxia/FenicsSolver[+https://github.com/qingfengxia/FenicsSolver+]

multiphenics
^^^^^^^^^^^^

multiphenics is a python library that aims at providing tools in FEniCS for an easy prototyping of multiphysics problems on conforming meshes. In particular, it facilitates the definition of subdomain/boundary restricted variables and enables the definition of the problem by means of a block structure.

https://github.com/mathLab/multiphenics[+https://github.com/mathLab/multiphenics+]

RNniCS
^^^^^^

RBniCS is an implementation in FEniCS of several reduced order modelling techniques (and, in particular, certified reduced basis method and Proper Orthogonal Decomposition-Galerkin methods) for parametrized problems. It is ideally suited for an introductory course on reduced basis methods and reduced order modelling, thanks to an object-oriented approach and an intuitive and versatile python interface. To this end, it has been employed in several doctoral courses on "Reduced Basis Methods for Computational Mechanics".

RBniCS can also be used as a basis for more advanced projects that would like to assess the capability of reduced order models in their existing FEniCS-based software, thanks to the availability of several reduced order methods (such as reduced basis and proper orthogonal decomposition) and algorithms (such as successive constraint method, empirical interpolation method) in the library.

https://github.com/mathLab/RBniCS[+https://github.com/mathLab/RBniCS+]

Ferret
~~~~~~

Ferret is an interactive computer visualization and analysis environment designed to meet the needs of oceanographers and meteorologists analyzing large and complex gridded data sets. It runs on recent Unix and Mac systems, using X windows for display. PyFerret, introduced in 2012, is a Python module wrapping Ferret. PyFerret is an upgrade to Ferret which runs existing Ferret scripts and includes all Ferret functionality with updated graphics capabilities and additional analysis functions. In addition the pyferret module provides Python functions so Python users can easily take advantage of Ferret's abilities to retrieve, manipulate, visualize, and save data.

Ferret and PyFerret can transparently access extensive remote Internet data sources using OPeNDAP.

Ferret was developed by the Thermal Modeling and Analysis Project (TMAP) at PMEL in Seattle to analyze the outputs of its numerical ocean models and compare them with gridded, observational data. The model data sets are generally multi-gigabyte in size with mixed multi-dimensional variables defined on staggered grids. Ferret offers a Mathematica-like approach to analysis; new variables may be defined interactively as mathematical expressions involving data set variables. Calculations may be applied over arbitrarily shaped regions. Fully documented graphics are produced with a single command.

Many excellent software packages have been developed recently for scientific visualization. The features that make Ferret distinctive among these packages are Mathematica-like flexibility, geophysical formatting, "intelligent" connection to its data base, memory management for very large calculations, and symmetrical processing in 6 dimensions.

https://ferret.pmel.noaa.gov/Ferret/[+https://ferret.pmel.noaa.gov/Ferret/+]

https://github.com/NOAA-PMEL/Ferret[+https://github.com/NOAA-PMEL/Ferret+]

https://github.com/NOAA-PMEL/PyFerret[+https://github.com/NOAA-PMEL/PyFerret+]

ComPlot
^^^^^^^

ComPlot compares (gridded) model results with observational (point) data by plotting the data on top of the model output as dots. It is meant for oceanographers who want to compare ocean model output with observations.
It is programmed almost exclusively in the classic Ferret scripting language, but several standard Unix commands (like sed) are used to overcome limitations in Ferret.

https://savannah.nongnu.org/projects/complot/[+https://savannah.nongnu.org/projects/complot/+]

http://hg.savannah.nongnu.org/hgweb/complot/file/[+http://hg.savannah.nongnu.org/hgweb/complot/file/+]

http://joss.theoj.org/papers/5f86741ca9c53ccaafc0162f681b443f[+http://joss.theoj.org/papers/5f86741ca9c53ccaafc0162f681b443f+]

FESOM
~~~~~

FESOM (Finite-Element/volumE Sea ice-Ocean Model) is a multi-resolution sea ice-ocean model that solves the equations of motion on unstructured meshes. The model is developed and supported by researchers at the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (AWI), in Bremerhaven, Germany.

FESOM implements the idea of using unstructured meshes with variable resolution. This mesh flexibility allows to increase resolution in dynamically active regions, while keep a relatively coarse-resolution setup elsewhere. FESOM allows global multi-resolution simulations without traditional nesting.

The dynamical core of the original version of FESOM employs the finite element method; its successor, FESOM2, uses the finite volume method, which increases the computational efficiency by a factor of 3-5. Both versions include the Finite-Element Sea Ice Model (FESIM).

Excellent scalability characteristics allow to make effective use of massively parallel supercomputers. 

FESOM is also used as the sea ice-ocean component of the AWI Climate Model (AWI-CM), which will contribute to CMIP6.

https://fesom.de/[+https://fesom.de/+]

https://github.com/FESOM[+https://github.com/FESOM+]

https://github.com/FESOM/pyfesom[+https://github.com/FESOM/pyfesom+]

https://fesom.de/models/fesom20/[+https://fesom.de/models/fesom20/+]

FESTUNG
~~~~~~~

FESTUNG is an Open Source toolbox for the discontinuous Galerkin method on unstructured grids, written in Matlab / GNU Octave. It is primarily intended as a fast and flexible prototyping platform and testbed for students and developers. It relies on fully vectorized matrix/vector operations to deliver optimized computational performance combined with a compact, user-friendly interface and a comprehensive documentation.

A selection of the current set of features of FESTUNG includes:

* Generic problem description and coupling framework.

* Unstructured 1D and 2D (triangular, trapezoidal) meshes.

* (L)DG/HDG discretizations up to fourth order (with hierarchichal polynomial basis functions).

* explicit SSP Runge-Kutta (up to 3rd order) and implicit DIRK scheme (up to 4th order) time discretization.

* High-order hierarchical vertex-based slope limiters.

* Fully vectorized assembly.

* VTK- and Tecplot-compatible output.

* Advection- and diffusion-type operators readily available.

https://github.com/FESTUNG/project[+https://github.com/FESTUNG/project+]

https://www.mso.math.fau.de/index.php?id=4944[+https://www.mso.math.fau.de/index.php?id=4944+]

https://arxiv.org/abs/1408.3877[+https://arxiv.org/abs/1408.3877+]

https://arxiv.org/abs/1602.05754[+https://arxiv.org/abs/1602.05754+]

https://arxiv.org/abs/1709.04390[+https://arxiv.org/abs/1709.04390+]

https://arxiv.org/abs/1806.03908[+https://arxiv.org/abs/1806.03908+]

FFmpeg
~~~~~~

FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created. It supports the most obscure ancient formats up to the cutting edge. No matter if they were designed by some standards committee, the community or a corporation. It is also highly portable: FFmpeg compiles, runs, and passes our testing infrastructure FATE across Linux, Mac OS X, Microsoft Windows, the BSDs, Solaris, etc. under a wide variety of build environments, machine architectures, and configurations. 

 The FFmpeg project tries to provide the best technically possible solution for developers of applications and end users alike. To achieve this we combine the best free software options available. We slightly favor our own code to keep the dependencies on other libs low and to maximize code sharing between parts of FFmpeg. Wherever the question of "best" cannot be answered we support both options so the end user can choose. 

FFmpeg is part of the workflow of hundreds of other software projects, and its libraries are a core part of software media players such as VLC, and has been included in core processing for YouTube and the iTunes inventory of files. Codecs for the encoding and/or decoding of most of all known audio and video file formats is included, making it highly useful for the transcoding of common and uncommon media files into a single common format. 

The FFmpeg tools are:

* *ffmpeg* - command line tool to convert multimedia files between formats
* *ffplay* - simple media player based on SDL and the FFmpeg libraries
* *ffprobe* - simple multimedia stream analyzer

The FFmpeg libraries are:

* *libavutil* - a library containing functions for simplifying programming, including random number generators, data structures, mathematics routines, core multimedia utilities, and much more
* *libavcodec* - a library containing decoders and encoders for audio/video codecs
* *libavformat* - a library containing demuxers and muxers for multimedia container formats
* *libavdevice* - is a library containing input and output devices for grabbing from and rendering to many common multimedia input/output software frameworks, including Video4Linux, Video4Linux2, VfW, and ALSA
* *libavfilter* - a library containing media filters
* *libswcale* - a library performing highly optimized image scaling and color space/pixel format conversion operations
* *libswresample* - a library performing highly optimized audio resampling, rematrixing and sample format conversion operations

https://ffmpeg.org/[+https://ffmpeg.org/+]

https://trac.ffmpeg.org/[+https://trac.ffmpeg.org/+]

http://ffmpeg.tv/[+http://ffmpeg.tv/+]

https://wiki.multimedia.cx/[+https://wiki.multimedia.cx/+]

https://github.com/FFmpeg/FFmpeg[+https://github.com/FFmpeg/FFmpeg+]

https://en.wikipedia.org/wiki/FFmpeg[+https://en.wikipedia.org/wiki/FFmpeg+]

https://www.ostechnix.com/20-ffmpeg-commands-beginners/[+https://www.ostechnix.com/20-ffmpeg-commands-beginners/+]

FIM
~~~

FIM is a lightweight universal image viewer, mostly for Linux (but not only).  
FIM aims to be a highly customizable and scriptable image viewer for users who are comfortable with software like the VIM text editor (see this minitutorial) or the Mutt mail user agent (see this minitutorial).  
It has been developed with Linux in mind, but can be built to run on several Unix systems or even on MS-Windows.
 
It is lightweight: it depends on a relatively few libraries, it displays in full screen and you control it using the keyboard.
It is universal: it can open many file formats and it can display pictures in the following video modes: 

* Graphically, with the Linux https://www.kernel.org/doc/Documentation/fb/framebuffer.txt[framebuffer] device
* Graphically, under X/Xorg, using the http://www.libsdl.org/[SDL] library
* Graphically, under X/Xorg, using the https://docs.enlightenment.org/api/imlib2/html/[Imlib2] library
* Rendered as ASCII Art in any textual console, using the http://aa-project.sourceforge.net/aalib/[AAlib] library

The right video mode gets auto-detected or selected at runtime, and may be opted in/out before build at configure time, if desired.

http://www.nongnu.org/fbi-improved/[+http://www.nongnu.org/fbi-improved/+]

http://savannah.nongnu.org/projects/fbi-improved/[+http://savannah.nongnu.org/projects/fbi-improved/+]

http://librsb.sourceforge.net/[+http://librsb.sourceforge.net/+]

Fimex
~~~~~

Fimex is a library and a program to convert gridded geospatial data between different formats and projections.

 Fimex is a the File Interpolation, Manipulation and EXtraction library for gridded geospatial data, written in C/Cxx. It converts between different, extensible dataformats (currently netcdf, NcML, grib1/2 and felt). It enables you to change the projection and interpolation of scalar and vector grids. It makes it possible to subset the gridded data and to extract only parts of the files.

For simple usage, Fimex comes also with the command line program fimex. 

https://github.com/metno/fimex[+https://github.com/metno/fimex+]

https://wiki.met.no/fimex/start[+https://wiki.met.no/fimex/start+]

Firedrake
~~~~~~~~~

Firedrake is an automated system for the solution of partial differential equations using the finite element method (FEM). Firedrake uses sophisticated code generation to provide mathematicians, scientists, and engineers with a very high productivity way to create sophisticated high performance simulations. The features include:

* Expressive specification of any PDE using the Unified Form Language from the FEniCS Project.
* Sophisticated, programmable solvers through seamless coupling with PETSc.
* Triangular, quadrilateral, and tetrahedral unstructured meshes.
* Layered meshes of triangular wedges or hexahedra.
* Vast range of finite element spaces.
* Sophisticated automatic optimisation, including sum factorisation for high order elements, and vectorisation.
* Geometric multigrid.
* Customisable operator preconditioners.
* Support for static condensation, hybridisation, and HDG methods

https://github.com/firedrakeproject/firedrake[+https://github.com/firedrakeproject/firedrake+]

https://www.firedrakeproject.org/[+https://www.firedrakeproject.org/+]

https://github.com/firedrakeproject/gusto[+https://github.com/firedrakeproject/gusto+]

https://github.com/firedrakeproject/flooddrake[+https://github.com/firedrakeproject/flooddrake+]

https://github.com/firedrakeproject/firedrake-bench[+https://github.com/firedrakeproject/firedrake-bench+]

FisicaLab
~~~~~~~~~

FisicaLab (can be pronounced as PhysicsLab) is an educational application to solve physics problems. Its main objective is let the user to focus in physics concepts, leaving aside the mathematical details (FisicaLab take care of them). This allows the user to become familiar with the physical concepts without running the risk of getting lost in mathematical details. And so, when the user gain confidence in applying physical concepts, will be better prepared to solve the problems by hand (with pen and paper). The latest release of FisicaLab have the following modules:
[ FisicaLab ]

* Kinematics of points 2D.
* Circular kinematics of points 2D.
* Static of points 2D.
* Static of rigid bodies 2D.
* Dynamics of points 2D.
* Circular dynamics of points 2D.
* Heat, calorimetry, ideal gas and expansion.

The static and dynamic problems are entered constructing the free body diagrams of the objects. Although FisicaLab is easy and intuitive, we recommend you read the documentation first.

https://www.gnu.org/software/fisicalab/index.en.html[+https://www.gnu.org/software/fisicalab/index.en.html+]

http://savannah.gnu.org/projects/fisicalab[+http://savannah.gnu.org/projects/fisicalab+]

FlatBuffers
~~~~~~~~~~~

FlatBuffers is an efficient cross platform serialization library for Cxx, C#, C, Go, Java, JavaScript, Lobster, Lua, TypeScript, PHP, Python, and Rust. It was originally created at Google for game development and other performance-critical applications.

The advantages of FlatBuffers as a serialization protocol are:

* Access to serialized data without parsing/unpacking - What sets FlatBuffers apart is that it represents hierarchical data in a flat binary buffer in such a way that it can still be accessed directly without parsing/unpacking, while also still supporting data structure evolution (forwards/backwards compatibility).

* Memory efficiency and speed - The only memory needed to access your data is that of the buffer. It requires 0 additional allocations (in Cxx, other languages may vary). FlatBuffers is also very suitable for use with mmap (or streaming), requiring only part of the buffer to be in memory. Access is close to the speed of raw struct access with only one extra indirection (a kind of vtable) to allow for format evolution and optional fields. It is aimed at projects where spending time and space (many memory allocations) to be able to access or construct serialized data is undesirable, such as in games or any other performance sensitive applications. See the benchmarks for details.

* Flexible - Optional fields means not only do you get great forwards and backwards compatibility (increasingly important for long-lived games: don't have to update all data with each new version!). It also means you have a lot of choice in what data you write and what data you don't, and how you design data structures.

* Tiny code footprint - Small amounts of generated code, and just a single small header as the minimum dependency, which is very easy to integrate. Again, see the benchmark section for details.

* Strongly typed - Errors happen at compile time rather than manually having to write repetitive and error prone run-time checks. Useful code can be generated for you.

* Convenient to use - Generated Cxx code allows for terse access & construction code. Then there's optional functionality for parsing schemas and JSON-like text representations at runtime efficiently if needed (faster and more memory efficient than other JSON parsers).

* Cross platform code with no dependencies - Cxx code will work with any recent gcc/clang and VS2010. Comes with build files for the tests & samples (Android .mk files, and cmake for all other platforms).

https://google.github.io/flatbuffers/[+https://google.github.io/flatbuffers/+]

Flatpak
~~~~~~~

Flatpak is a next-generation technology for building and distributing desktop applications on Linux.
Flatpak (formerly xdg-app) is a software utility for software deployment, package management, and application virtualization for Linux desktop computers. It provides a sandbox environment in which users can run applications in isolation from the rest of the system. Applications using Flatpak need permission from the user to control hardware devices or access the user's files.

https://flatpak.org/[+https://flatpak.org/+]

https://flathub.org/home[+https://flathub.org/home+]

Flatkvm
^^^^^^^

Flatkvm is a tool to easily run flatpak apps isolated inside a VM, using QEMU/KVM.

https://github.com/flatkvm/flatkvm[+https://github.com/flatkvm/flatkvm+]

FleCSI
~~~~~~

FleCSI is a compile-time configurable framework designed to support multi-physics application development. As such, FleCSI provides a very general set of infrastructure design patterns that can be specialized and extended to suit the needs of a broad variety of solver and data requirements. FleCSI currently supports multi-dimensional mesh topology, geometry, and adjacency information, as well as n-dimensional hashed-tree data structures, graph partitioning interfaces, and dependency closures.

FleCSI introduces a functional programming model with control, execution, and data abstractions that are consistent both with MPI and with state-of-the-art, task-based runtimes such as Legion and Charm++. The abstraction layer insulates developers from the underlying runtime, while allowing support for multiple runtime systems including conventional models like asynchronous MPI.

The intent is to provide developers with a concrete set of user-friendly programming tools that can be used now, while allowing flexibility in choosing runtime implementations and optimizations that can be applied to future architectures and runtimes.

FleCSI's control and execution models provide formal nomenclature for describing poorly understood concepts such as kernels and tasks. FleCSI's data model provides a low-buy-in approach that makes it an attractive option for many application projects, as developers are not locked into particular layouts or data structure representations.

https://github.com/laristra/flecsi[+https://github.com/laristra/flecsi+]

https://www.lanl.gov/software/open-source-software.php[+https://www.lanl.gov/software/open-source-software.php+]

fleming
~~~~~~~

Fleming contains a set of routines for doing datetime manipulation. Named after Sandford Fleming, the father of worldwide standard timezones, this package is meant to aid datetime manipulations with regards to timezones.

Fleming addresses some of the common difficulties with timezones and datetime objects, such as performing arithmetic and datetime truncation across a Daylight Savings Time border. It also provides utilities for generating date ranges and getting unix times with respect to timezones.

Fleming accepts pytz timezone objects as parameters, and it is assumed that the user has a basic understanding of pytz.

The functions in fleming are:

* convert_to_tz : Converts a datetime object into a provided timezone.
* add_timedelta : Adds a timedelta to a datetime object.
* floor : Rounds a datetime object down to the previous time interval.
* ceil : Rounds a datetime object up to the next time interval.
* intervals : Gets a range of times at a given timedelta interval.
* unix_time : Returns a unix time stamp of a datetime object.

https://fleming.readthedocs.io/en/develop/[+https://fleming.readthedocs.io/en/develop/+]

http://pytz.sourceforge.net/[+http://pytz.sourceforge.net/+]

https://github.com/ambitioninc/fleming[+https://github.com/ambitioninc/fleming+]

FLENS
~~~~~

C++ is a programming language for writing libraries. And FLENS is a C++ library for writing numerical linear algebra libraries.
The combination C++/FLENS allows the implementation of high performance libraries in the field of numerical linear algebra.

The features include:

* a headers-only Cxx library that requires a Cxx11 conformant compiler
* matrix/vector types for dense linear algebra
* low and high level interfaces to BLAS
* a reimplementation of LAPACK
* CXXBLAS, FLENS-BLAS and FLENS-LAPACK support types from the QD library and mpfr
* use of ulmBLAS as the default computational BLAS backend
* use of overloaded operators for BLAS operations

http://apfel.mathematik.uni-ulm.de/\~lehn/FLENS-Trinity/index.html[+http://apfel.mathematik.uni-ulm.de/~lehn/FLENS-Trinity/index.html+]

https://github.com/michael-lehn/FLENS[+https://github.com/michael-lehn/FLENS+]

ulmBLAS
^^^^^^^

ulmBLAS is a high performance C++ implementation of BLAS (Basic Linear Subprograms). Standard conform interfaces for C and Fortran are provided.

BLAS defines a set of low-level linear algebra operations and has become a de facto standard API for linear algebra libraries and routines. Several BLAS library implementations have been tuned for specific computer architectures. Highly optimized implementations have been developed by hardware vendors such as Intel (Intel MKL) or AMD (ACML).

Related to ulmBLAS is the FLENS C++ library. It provides powerful and efficient matrix/vector types for implementing numerical algorithms. In the FLENS library ulmBLAS is used as the default BLAS implementation.

Higher level numerical libraries and applications like LAPACK, SuperLU, Matlab, GNU Octave, Mathematica, etc. use BLAS as computational kernel. Performance of these libraries and applications directly depends on the performance of the underlying BLAS implementation.

http://apfel.mathematik.uni-ulm.de/\~lehn/ulmBLAS/[+http://apfel.mathematik.uni-ulm.de/~lehn/ulmBLAS/+]

https://github.com/michael-lehn/ulmBLAS[+https://github.com/michael-lehn/ulmBLAS+]

FLEXPART
~~~~~~~~

FLEXPART (“FLEXible PARTicle dispersion model”) is a Lagrangian transport and dispersion model suitable for the simulation of a large range of atmospheric transport processes. Applications range from the dispersion of radionuclides or air pollutants, over the establishment of flow climatologies, to the analysis of Earth’s water cycle. FLEXPART also produces output suitable for inverse determination of emission sources, e.g., of greenhouse gases or volcanic ash.

FLEXPART is a further development of its predecessor FLEXTRA (“FLEXible TRAjectory model”). Both FLEXTRA and FLEXPART have been open source models since the beginning and, over time have found a large international user community. In addition to the reference version of FLEXPART which is based on meteorological data from ECMWF or GFS, several branches of FLEXPART have been developed, which can be run with meteorological data from mesoscale models.

With the growing number of FLEXPART users, it has become important to provide a platform where the FLEXPART community can effectively exchange information, further develop the model together, and provide help to new users. The main FLEXPART developers have together chosen this platform for organizing their collaborative work and keeping track of the changes in FLEXPART. We would like to ask also all other users to share not only further developments of FLEXPART but also all other (e.g., post-processing or plotting) tools related to FLEXPART which may be useful for others. 

https://www.flexpart.eu/[+https://www.flexpart.eu/+]

https://github.com/sajinh/flx_wrf2[+https://github.com/sajinh/flx_wrf2+]

https://github.com/henrig/FLEXPART[+https://github.com/henrig/FLEXPART+]

https://www.geosci-model-dev-discuss.net/gmd-2018-333/[+https://www.geosci-model-dev-discuss.net/gmd-2018-333/+]

https://www.geosci-model-dev.net/6/1889/2013/gmd-6-1889-2013.html[+https://www.geosci-model-dev.net/6/1889/2013/gmd-6-1889-2013.html+]

Flexx
~~~~~

Flexx is a pure Python toolkit for creating graphical user interfaces (GUI’s), that uses web technology for its rendering. Apps are written purely in Python; The PScript transpiler generates the necessary JavaScript on the fly.

You can use Flexx to create (cross platform) desktop applications, web applications, and export an app to a standalone HTML document. It also works in the Jupyter notebook.

The primary motivation for Flexx is the undeniable fact that the web (i.e. browser technology) has become an increasingly popular method for delivering applications to users, also for (interactive) scientific content.

The purpose of Flexx is to provide a single application framework to create desktop applications, web apps, and (hopefully soon) mobile apps. By making use of browser technology, the library itself can be relatively small and pure Python, making it widely available and easy to use.

https://flexx.readthedocs.io/en/stable/[+https://flexx.readthedocs.io/en/stable/+]

https://github.com/flexxui/flexx[+https://github.com/flexxui/flexx+]

FLiT
~~~~

Floating-point Litmus Tests (FLiT) is a Cxx test infrastructure for detecting variability in floating-point code caused by variations in compiler code generation, hardware and execution environments.

Originally, FLiT stood for "Floating-point Litmus Tests", but has grown into a tool with much more flexability than to study simple litmus tests. However, it has always been the focus of FLiT to study the variability caused by compilers. That brings us to the other reason for the name, "flit" is defined by the Merriam Webster dictionary as "to pass quickly or abruptly from one place or condition to another". This fits in well with testing for various sources of variability.

Compilers are primarily focused on optimizing the speed of your code. However, when it comes to floating-point, compilers go a little further than some might want, to the point that you may not get the same result from your floating-point algorithms. For many applications, this may not matter as the difference is typically very small. But there are situations where

* The difference is not so small
* Your application cares even about small differences, or
* Your application is so large (such as a weather simulation) that a small change may propagate into very large result variability.

FLiT helps developers determine where reproducibility problems may occur due to compilers. The developer creates reproducibility tests with their code using the FLiT testing framework. Then FLiT takes those reproducibility tests and compiles them under a set of configured compilers and a large range of compiler flags. The results from the tests under different compilations are then compared against the results from a "ground truth" compilation (e.g. a completely unoptimized compilation).

More than simply comparing against a "ground truth" test result, the FLiT framework also measures runtime of your tests. Using this information, you can not only determine which compilations of your code are safe for your specific application, but you can also determine the fastest safe compilation. This ability helps the developer navigate the tradeoff between reproducibility and performance.

https://github.com/PRUNERS/FLiT[+https://github.com/PRUNERS/FLiT+]

FluidFFT
~~~~~~~~

This package provides Cxx classes and their Python wrapper classes useful to perform Fast Fourier Transform (FFT) with different libraries, in particular

* http://www.fftw.org/[fftw3] and fftw3-mpi
* https://github.com/mpip/pfft[pfft]
* https://github.com/sdsc/p3dfft[p3dfft]
* https://developer.nvidia.com/cufft[cufft] (fft library by CUDA running on GPU)

pfft and p3dfft are specialized in computing FFT efficiently on several cores of big clusters. The data is split in pencils and the computations can be distributed on several processes.

FluidFFT provides an unified API to use all these libraries. FluidFFT is not limited to just performing Fourier transforms. It is a complete development framework for codes using (distributed) FFT. A simple API allows the developers to easily perform operations on data in real and spectral spaces (gradient, divergence, rotational, sum over wavenumbers, computation of spectra, etc.) and deal with the data distribution (gather the data on one process and scatter the data to many processes) without having to consider the internal organization of every FFT library.

https://fluidfft.readthedocs.io/en/latest/[+https://fluidfft.readthedocs.io/en/latest/+]

https://arxiv.org/abs/1807.01775[+https://arxiv.org/abs/1807.01775+]

https://bitbucket.org/fluiddyn/fluidfft[+https://bitbucket.org/fluiddyn/fluidfft+]

Fluidity
~~~~~~~~

Fluidity is an open source, general purpose, multiphase computational fluid dynamics code capable of numerically solving the Navier-Stokes equation and accompanying field equations on arbitrary unstructured finite element meshes in one, two and three dimensions. It is parallelised using MPI and is capable of scaling to many thousands of processors. Other innovative and novel features include the use of anisotropic adaptive mesh technology, and a user-friendly GUI and a Python interface which can be used to calculate diagnostic fields, set prescribed fields or set user-defined boundary conditions.

Fluidity is used in a number of different scientific areas including geophysical fluid dynamics, computational fluid dynamics, ocean modelling and mantle convection. It uses a finite element/control volume method which allows arbitrary movement of the mesh with time dependent problems, allowing mesh resolution to increase or decrease locally according to the current simulated state. It has a wide range of element choices including mixed formulations.

http://fluidityproject.github.io/[+http://fluidityproject.github.io/+]

Fluidsim
~~~~~~~~

Fluidsim is a framework for studying fluid dynamics with numerical simulations using Python. It is part of the wider project FluidDyn.

Fluidsim is an object-oriented library to develop solvers (mainly using pseudo-spectral methods) by writing mainly Python code. The result is very efficient even compared to a pure Fortran or C code since the time-consuming tasks are performed by optimized compiled functions.

Fluidsim is a HPC code written mostly in Python. It uses the library fluidfft to use very efficient FFT libraries. Fluidfft is written in C, Cython and Python. Fluidfft and fluidsim take advantage of Pythran, a static Python compiler which produces very efficient binaries by compiling Python via C11.

fluidsim excels in particular in solving equations over a periodic space:

* 2d and 3d incompressible Navier-Stokes equations,
* 2d and 3d incompressible Navier-Stokes equations under the Boussinesq approximation (with a buoyancy variable),
* 2d and 3d stratified Navier-Stokes equations under the Boussinesq approximation with constant Brunt-Väisälä frequency,
* 2d one-layer shallow-water equations + modified versions of these equations,
* 2d Föppl-von Kármán equations (elastic thin plate).

Being a framework, Fluidsim can easily be extended in other packages to develop other solvers (see for example the package fluidsim_ocean).

https://fluidsim.readthedocs.io/en/latest/[+https://fluidsim.readthedocs.io/en/latest/+]

https://bitbucket.org/fluiddyn/fluidsim_ocean[+https://bitbucket.org/fluiddyn/fluidsim_ocean+]

Flux Framework
~~~~~~~~~~~~~~

The Flux framework is a family of projects used to build site-customized resource management systems for High Performance Computing (HPC) data centers.

Current projects include:

* *flux-core* - implements the communication layer and lowest level services and interfaces for the Flux resource manager framework. It consists of a distributed message broker and plug-in comms modules that implement various distributed services.

* *flux-sched* - contains the job scheduling facility for the Flux resource manager framework. It consists of an engine that handles all the functionality common to scheduling. The engine has the ability to load one or more scheduling plugins that provide specific scheduling behavior.

* *capacitor* - implements a bulk execution manager using flux-core. Capacitor provides functionality like that of job arrays or tools like CRAM, except with the full capabilities of flux and its job scheduling facilities like flux-sched. If you need to run and monitor thousands of jobs in an allocation, and don’t want an email from your sysadmins, consider flux and capacitor.

https://flux-framework.org/[+https://flux-framework.org/+]

https://github.com/flux-framework[+https://github.com/flux-framework+]

https://flux-framework.org/papers/Flux-SC18-WORKS.pdf[+https://flux-framework.org/papers/Flux-SC18-WORKS.pdf+]

ForestClaw
~~~~~~~~~~

ForestClaw is a parallel, multi-block adaptive finite volume library for solving PDEs on mapped, logically Cartesian meshes. For solving hyperbolic problems using explicit, single step algorithms, ForestClaw's block-structured adaptive algorithm, including multi-rate time stepping uses the Berger, Oliger and Colella AMR algorithms.
The hyperbolic solvers are currently based on ClawPack (R. J. LeVeque). Future plans include support for general method-of-lines solvers in a multi-rate setting. 

Where ForestClaw departs from the standard Berger-Oliger-Colella block-structured approach is that the multi-resolution grid hierarchy is not stored as overlapping, nested grids but rather as a composite structure of non-overlapping fixed sized grids, each of which is stored as a leaf in a forest of quad- or octrees. The particular code base we use for managing the tree is p4est (www.p4est.org).

The name "ForestClaw" derives from the use of a "forest-of-octrees" paradigm in p4est, and the original set of solvers from the ClawPack family of solvers for hyperbolic systems of conservations laws (CLAW = Conservation LAW). However, the core routines in ForestClaw can be extended to include any grids and solvers suitable for logically Cartesian meshes. Users can create their own grid and solver types and still take complete advantage of the generic adaptive infrastructure layer provided by ForestClaw.

https://math.boisestate.edu/\~calhoun/ForestClaw/[+https://math.boisestate.edu/~calhoun/ForestClaw/+]

https://github.com/ForestClaw/forestclaw[+https://github.com/ForestClaw/forestclaw+]

http://www.clawpack.org/[+http://www.clawpack.org/+]

http://www.p4est.org/[+http://www.p4est.org/+]

Fortran
~~~~~~~

Blah.

http://fortranwiki.org/fortran/show/Compilers[+http://fortranwiki.org/fortran/show/Compilers+]

https://github.com/scivision/fortran2018-examples[+https://github.com/scivision/fortran2018-examples+]

*Zen of Modern Fortran* - zaghi.github.io/zen-of-fortran-talk/index.html[+zaghi.github.io/zen-of-fortran-talk/index.html+]

*DMOZ Fortran Archive* - https://szaghi.github.io/dmoz-fortran-archive/[+https://szaghi.github.io/dmoz-fortran-archive/+]

CLAW
~~~~

The CLAW Project is an open-source project including a directive language specification and a reference compiler targeting performance portability in climate and weather application written in Fortran.

The CLAW Compiler is a source-to-source translator working on the XcodeML intermediate representation. It implements the necessary transformation to the CLAW Directive Language Specifications. Intent of this language is to achieve performance portability on weather and climate code, especially for column- or point-wise computation.

The components are:

* FPP: standard preprocessor.
* OMNI F_Front: FORTRAN front-end. Convert FORTRAN source code into an intermediate representation (XcodeML/F).
* CLAWX2T: CLAW XcodeML to XcodeML translator.
* OMNI F_Back: FORTRAN back-end. Generates FORTRAN code from XcodeML/F intermediate representation.
* OMNI C_Back: C back-end. Generates C code from XcodeML/C intermediate representation.

The CLAW XcodeML to XcodeML translator library stack is divided in three distinct library:

* CLAW-WANI: CLAW translator implementation with all high-level transformation as well as mechanism to plug external translator and transformation.
* CLAW-SHENRON: Define the super classes for any translator or transformation implementation.
* CLAW-TATSU: Define the interface between XcodeML IR and the translation library. Implements all primitive transformations that serve as higher-level transformation building block.

https://github.com/claw-project/claw-compiler[+https://github.com/claw-project/claw-compiler+]

https://claw-project.github.io/[+https://claw-project.github.io/+]

f18
^^^

Flang f18 is a new LLVM Fortran compiler front-end based in Cxx17 instead of the monolithic C code inherited from PGI that become the first-generation Flang.

The code has been compiled and tested with GCC versions 7.2.0, 7.3.0, 8.1.0, and 8.2.0. The code has been compiled and tested with clang 6.0 using either GCC 7.3.0 or 8.1.0 headers; however, the headers needed small patches.

To build and install f18, there are several options for specifying the Cxx compiler. You can have the proper Cxx compiler on your path, or you can set the environment variable CXX, or you can define the variable GCC on the cmake command line.

https://github.com/flang-compiler/f18[+https://github.com/flang-compiler/f18+]

https://www.scivision.co/flang-compiler-build-tips/[+https://www.scivision.co/flang-compiler-build-tips/+]

https://github.com/flang-compiler/flang/releases[+https://github.com/flang-compiler/flang/releases+]

FCM
^^^

The FCM system is designed to simplify the task of managing and building Fortran source code.
The build system has the following features:

* Parallel build.
* Efficient incremental build. Changes to the checksums of source files and/or the build configuration (e.g. changes to the compiler flags) trigger the appropriate re-compilation.
* Inheritance of items from an existing build.
* Build dependency analysis.
* Automatic generation of include files to contain the calling interfaces of standalone functions and subroutinues in Fortran source files.
* Extract of source files from multiple repositories and working copies.
* Extract and merge of source files from different branches of development.
* Minimal configuration.

FCM uses Subversion for version control.  Subversion is a generalised tool which can be used in lots of different ways. This makes some day-to-day tasks more complex than they need be. FCM defines a simplified process and appropriate naming conventions. It then adds a layer on top of Subversion to provide a natural interface which is specifically tailored to this process.
FCM uses Trac to manage software projects.

http://metomi.github.io/fcm/doc/[+http://metomi.github.io/fcm/doc/+]

https://github.com/metomi/fcm[+https://github.com/metomi/fcm+]

Flang
^^^^^

Flang is a Fortran compiler targeting LLVM.
 is a Fortran language front-end designed for integration with LLVM and the LLVM optimizer.

Flang+LLVM is a production-quality Fortran solution designed to be co-installed and is fully interoperable with Clang Cxx.

Flang single-core and OpenMP performance is now on par with GNU Fortran. Flang has implemented Fortran 2003 and has a near full implementation of OpenMP through version 4.5 targeting multicore CPUs.

The goals of Flang are:

* Ensure Flang becomes a self-sustaining open source project
* Attract additional developers from the (broad) community to create a critical mass of contributors
* Deliver single-core CPU performance comparable or better than that of gfortran
* Enable multicore CPU and GPU programming with a robust and performant implementation of OpenMP
* Create a source base that can be readily re-hosted and re-targeted to future systems as easily as Clang and LLVM
* Create a source base in which researchers and developers can ramp up and be productive quickly
* Create a source base that is componentized to a degree that enables re-use of elements of Flang in tools projects

https://github.com/flang-compiler/flang[+https://github.com/flang-compiler/flang+]

https://github.com/flang-compiler/flang/wiki[+https://github.com/flang-compiler/flang/wiki+]

https://github.com/flang-compiler/flang/wiki/Building-Flang[+https://github.com/flang-compiler/flang/wiki/Building-Flang+]

https://www.scivision.co/flang-compiler-build-tips/[+https://www.scivision.co/flang-compiler-build-tips/+]

FLAP
^^^^

A pure Fortran Library for building powerful, easy-to-use, elegant command line interfaces.

https://github.com/szaghi/FLAP[+https://github.com/szaghi/FLAP+]

FLPR
^^^^

FLPR is a general source-to-source translation framework for developing application-specific Fortran source transformation tools. This package contains:

* A "best effort" Fortran 2018 input parser for fixed and free formats.
* Data structures for manipulating source code at the program unit, statement, and physical line levels.
* Sample applications (in the apps/ directory) that illustrate usage and provide some ideas as to how you could use the library.

This implementation is based on the 28-DEC-2017 Draft N2146 of the ISO/IEC JTC1/SC22/WG5 Fortran Language Standard, available from the Fortran Working Group.
FLPR requires C++17 library support: GCC 8+ is probably a good place to start.

https://github.com/lanl/FLPR[+https://github.com/lanl/FLPR+]

FoBIS
^^^^^

A KISS tool for automatic building modern Fortran projects.

https://github.com/szaghi/FoBiS[+https://github.com/szaghi/FoBiS+]

FORD
^^^^

This is an automatic documentation generator for modern Fortran programs. FORD stands for FORtran Documenter. As you may know, "to ford" refers to crossing a river (or other body of water). It does not, in this context, refer to any company or individual associated with cars.

Ford was written due to Doxygen's poor handling of Fortran and the lack of comparable alternatives. ROBODoc can't actually extract any information from the source code and just about any other automatic documentation software I found was either proprietary, didn't work very well for Fortran, or was limited in terms of how it produced its output. f90doc is quite good and I managed to modify it so that it could handle most of Fortran 2003, but it produces rather ugly documentation, can't provide as many links between different parts of the documentation as I'd like, and is written in Perl (which I'm not that familiar with and which lacks the sort of libraries found in Python for producing HTML content).

The goal of FORD is to be able to reliably produce documentation for modern Fortran software which is informative and nice to look at. The documentation should be easy to write and non-obtrusive within the code. While it will never be as feature-rich as Doxygen, hopefully FORD will be able to provide a good alternative for documenting Fortran projects.

https://github.com/Fortran-FOSS-Programmers/ford[+https://github.com/Fortran-FOSS-Programmers/ford+]

ForEx
^^^^^

ForEx is fortran 2003 project taking advance of the C preprocessor capabilities in order to emulate exception handling.

https://github.com/victorsndvg/ForEx[+https://github.com/victorsndvg/ForEx+]

FPL
^^^

FPL is pure fortran 2003 library that can manage the parameters of your program from a single point.

https://github.com/victorsndvg/FPL[+https://github.com/victorsndvg/FPL+]

FTL
^^^

The Fortran Template Library (FTL) is a general purpose library for Fortran 2003. Its intention is to bring all these nice things we take for granted in modern languages like Python and C++ to the Fortran world: Generic containers, versatile algorithms, easy string manipulation, and more. It is heavily inspired by C++'s standard library, especially the part that is commonly referred to as the Standard Template Library (STL).

The components are:

* *ftlDynArray* - A resizeable array container. It can slowly grow in size as elements are added at its end. Note that insertion at the end is an amortized constant time operation.
* *ftlList* - A linked list container that allows constant time insert and erase operations anywhere within the list.
* *ftlHashMap* - An associative containers that stores elements formed by the combination of a key value and a mapped value, and which allows for fast retrieval of individual elements based on their keys. It's basically a dictionary that internally uses a hash table to allow constant time retrieval of elements.
* *ftlHashSet* - A container representing a set of unique elements. It's pretty much like an ftlHashMap where the key is at the same time also the value.
* *ftlHash* - A small utility library that provides hash functions for the Fortran intrinsic types. This allows them to be used as key types in ftlHashMap and as elements in ftlHashSet. Furthermore these basic has functions can be used to implement hash functions for other derived types, so that these can also be used as keys in ftlHashMap.
* *ftlString* - A variable length string type that integrates seamlessly with plain Fortran strings.* *ftlLRegex* - A convenient Fortran wrapper around the POSIX regular expression functionality in the C standard library (aka regex.h) or alternatively the PCRE (Perl Compatible Regular Expressions) library.
* *ftlAlgorithms* - A library of generic algorithms that work on all FTL containers. 
* *ftlArray* - A little module that provides the FTL style container iterators for plain one-dimensional Fortran arrays. This allows the ftlAlgorithms to work on normal Fortran arrays.
* *ftlSharedPtr* - Provides a reference counted ftlSharedPtr in the spirit of C++'s std::shared_ptr.

https://github.com/SCM-NV/ftl[+https://github.com/SCM-NV/ftl+]



functional-fortran
^^^^^^^^^^^^^^^^^^

Functional programming for modern Fortran.
While not designed as a purely functional programming language, modern Fortran goes a long way by letting the programmer use pure functions to encourage good functional discipline, express code in mathematical form, and minimize bug-prone mutable state. This library provides a set of commonly used tools in functional programming, with the purpose to help Fortran programmers be less imperative and more functional.

The following functions are provided:

* arange - returns a regularly spaced array
* complement - returns a set complement of two arrays
* empty - returns an empty array
* filter - filters an array using a logical input function
* foldl - recursively left-folds an array using an input function
* foldr - recursively right-folds an array using an input function
* foldt - recursively tree-folds an array using an input function
* head - returns the first element of an array
* init - returns everything but the last element
* insert - inserts an element into an array, out-of-bound safe
* intersection - returns a set intersection of two arrays
* iterfold - iteratively reduces an array using an input function
* last - returns the last element of an array
* limit - limits a scalar or array by given lower and upper bounds
* map - maps an array with an input function
* set - returns a set given input array
* reverse - returns array in reverse order
* sort - recursive quicksort using binary tree pivot
* split - returns first or second half of an array
* subscript - out-of-bound safe implementation of vector subscript
* tail - returns everything but the first element
* unfold - unfolds an array with an input function
* union - returns a set union of two arrays

All of the above functions are compatible with the standard Fortran 2008 kinds: int8, int16, int32, int64, real32, real64, real128, complex(real32), complex(real64), and complex(real128).

Further, these functions (and their corresponding operators) are compatible with character strings: complement, empty, head, init, intersection, insert, last, reverse, set, sort, split, tail, and union.

Functions that operate on one or two arguments are also available as unary or binary operators, respectively. These are: .complement., .head., .init., .intersection., .last., .reverse., .set., .sort., .tail., and .union..

https://wavebitscientific.github.io/functional-fortran/[+https://wavebitscientific.github.io/functional-fortran/+]

https://github.com/wavebitscientific/functional-fortran[+https://github.com/wavebitscientific/functional-fortran+]

FURY
^^^^

A pure Fortran Library for improving reliability of physical math computations by taking into account units of measure.

https://github.com/szaghi/FURY[+https://github.com/szaghi/FURY+]

OpenCoarrays
^^^^^^^^^^^^

OpenCoarrays supports Fortran 2018 compilers by providing a parallel application binary interface (ABI) that abstracts away the underlying parallel programming model, which can be the Message Passing Interface (MPI) or OpenSHMEM. Parallel Fortran 2018 programs may be written and compiled into object files once and then linked or re-linked to either MPI or OpenSHMEM without modifying or recompiling the Fortran source. Not a single line of source code need change to switch parallel programming models. The default programming model is MPI because it provides the broadest capability for supporting Fortran 2018 features. However, having the option to change parallel programming models at link-time may enhance portability and performance (see Rouson et al. (2017)).

OpenCoarrays provides a compiler wrapper (caf), parallel runtime libraries (libcaf_mpi and libcaf_openshmem), and a parallel executable file launcher (cafrun). The wrapper and launcher provide a uniform abstraction for compiling and executing parallel Fortran 2018 programs without direct reference to the underlying parallel programming model.

The GNU Compiler Collection (GCC) Fortran front end (gfortran) has used OpenCoarrays since the GCC 5.1.0 release . 

https://github.com/sourceryinstitute/OpenCoarrays[+https://github.com/sourceryinstitute/OpenCoarrays+]

PreForM
^^^^^^^

PreForM.py supports the most used cpp pre-processing directives and provides advanced features typical of templating systems. Even if PreForM.py is currently Fortran-agnostic (it being usable within any programming languages) it is focused on Fortran programming language.

https://github.com/szaghi/PreForM[+https://github.com/szaghi/PreForM+]

StringiFor
^^^^^^^^^^

Modern Fortran standards (2003+) have introduced a better support for characters variables, but Fortraners still do not have the power on dealing with strings of other more-rich-programmers, e.g. Pythoners. Allocatable deferred length character variables are now quantum-leap with respect the old inflexible Fortran characters, but it is still not enough for many Fortraners. Moreover, Fortran does not provide builtin methods for widely used strings manipulations offered by other languages, e.g. UPPER/lowercase transformation, tokenization, etc... StringiFor attempts to fill this lack.

https://github.com/szaghi/StringiFor[+https://github.com/szaghi/StringiFor+]

VecFor
^^^^^^

VecFor is a user-friendly and Object-Oriented designed API for handling vectors in a (3D) three dimensional frame of reference. It exposes (among others) the object Vector that posses a far complete set of overloaded operators for performing vectorial calculus algebra.

https://github.com/szaghi/VecFor[+https://github.com/szaghi/VecFor+]

https://github.com/szaghi/VecFor/wiki[+https://github.com/szaghi/VecFor/wiki+]

FreeCAD
~~~~~~~

FreeCAD is an open-source parametric 3D modeler made primarily to design real-life objects of any size. Parametric modeling allows you to easily modify your design by going back into your model history and changing its parameters. 

FreeCAD allows you to sketch geometry constrained 2D shapes and use them as a base to build other objects. It contains many components to adjust dimensions or extract design details from 3D models to create high quality production ready drawings. 

FreeCAD is a multiplatfom (Windows, Mac and Linux), highly customizable and extensible software. It reads and writes to many open file formats such as STEP, IGES, STL, SVG, DXF, OBJ, IFC, DAE and many others, making it possible to seamlessly integrate it into your workflow. 

FreeCAD is designed to fit a wide range of uses including product design, mechanical engineering and architecture. Whether you are a hobbyist, a programmer, an experienced CAD user, a student or a teacher, you will feel right at home with FreeCAD. 

FreeCAD equips you with all the right tools for your needs. You get modern Finite Element Analysis (FEA) tools, experimental CFD, BIM, Geodata workbenches, Path workbench, a robot simulation module that allows you to study robot movements and many more features. FreeCAD really is a Swiss Army knife of general-purpose engineering toolkits. 

https://www.freecadweb.org/[+https://www.freecadweb.org/+]

BIM Workbench
^^^^^^^^^^^^^

This is a workbench for FreeCAD that implements a complete set of Building Information Modeling (BIM) tools and allows a proper BIM workflow similar to professional BIM applications like Revit, ArchiCAD, AllPlan or BricsCAD.

FreeCAD is an open-source generic, parametric 3D modeling application. The BIM workbench, rather than trying to be a all-in-one, independent application and mimic the above applications, integrates into all layers of FreeCAD and allows you to use all the other moldeling and analysis tools of FreeCAD for BIM purposes too. In fact, if you know FreeCAD well already, you can do BIM without the BIM workbench, and only use it at the very end to format your model for BIM.

But one of the specific goals of this particular workbench, is to try to focus on user experience. FreeCAD is a powerful application, but has a rather steep learning curve. In the BIM workbench, we try to ease that curve somehow.

https://github.com/yorikvanhavre/BIM_Workbench[+https://github.com/yorikvanhavre/BIM_Workbench+]

FreedomBone
~~~~~~~~~~~

Mainstream software is so broken and the organizations that develop it so untrustworthy that we are reaching a breaking point. So you want to run your own internet services? Email, chat, VoIP, web sites, file synchronisation, wikis, blogs, social networks, media hosting, backups, VPN. Freedombone is a home server system which enables you to self-host all of these things. You can run Freedombone on an old laptop or single board computer. 

https://freedombone.net/[+https://freedombone.net/+]

https://www.openhub.net/p/freedombone[+https://www.openhub.net/p/freedombone+]

https://www.youtube.com/watch?v=KDH5Uu-ktI8[+https://www.youtube.com/watch?v=KDH5Uu-ktI8+]

FREEWAT
~~~~~~~

The FREEWAT platform is a large plugin integrated into the QGIS GIS desktop.

FREEWAT includes several modules for dealing with water management issues, with particular attention to groundwater. Simulation codes (mainly from the MODFLOW USGS family) for dealing with groundwater-related processes (e.g., groundwater flow, solute transport in aquifers, etc.) constitute the basis of the plugin.

The complete list of modules so far integrated is provided below:

* Observations Analysis Tools (OAT) for time series analysis;
* Tools for the analysis, interpretation and visualization of hydrogeological data (akvaGIS);
* Tools for the analysis of groundwater quality datasets (akvaGIS);
* Groundwater flow modelling (based on MODFLOW-2005);
* Solute transport in the unsaturated zone (based on MT3D-USGS and the USB module);
* Solute transport in the saturated zone (based on MT3DMS);
* Density-dependent groundwater flow (based on SEAWAT);
* Management of water in agriculture (based on the FARM Process);
* Water management and planning (based on MODFLOW-OWHM);
* Crop yield at harvest (based on the Crop Growth Module, belonging to the EPIC family);
* Sensitivity analysis and calibration (based on UCODE_2014).

http://www.freewat.eu/[+http://www.freewat.eu/+]

Frictionaless Data
~~~~~~~~~~~~~~~~~~

Frictionless Data is about removing the friction in working with data through the development of a set of tools, specifications, and best practices for publishing data. The heart of Frictionless Data is Data Package, a containerization format for any kind of data based on existing practices for publishing open-source software.

https://github.com/frictionlessdata[+https://github.com/frictionlessdata+]

https://frictionlessdata.io/[+https://frictionlessdata.io/+]

https://frictionlessdata.io/software/[+https://frictionlessdata.io/software/+]

Fritzing
~~~~~~~~

Fritzing is an Electronic Design Automation software for designers, artists and just anyone who has interest in physical computing and prototyping.

Fritzing's goal is to provide easy tools for documenting and sharing physical computing projects, producing layouts for Printed Circuit Boards (PCB) and teaching electronics.

http://fritzing.org/home/[+http://fritzing.org/home/+]

https://fosdem.org/2019/schedule/event/fritzing/[+https://fosdem.org/2019/schedule/event/fritzing/+]

FROSST
~~~~~~

The Formidable Repository of Open Sparse Tensors and Tools is a collection of publicly available sparse tensor datasets and tools. FROSTT has two primary objectives:

* Foster sparse tensor research. Tensors are found in a variety of data sources and are difficult to collect, pre-process, and analyze. FROSTT addresses these challenges by providing a central location to access datasets and a platform of tools to perform analysis.

* Promote reproducible research. FROSTT provides a globally consistent set of tensors, enabling the reproduction and verification of published results.

http://frostt.io/[+http://frostt.io/+]

https://github.com/frostt-tensor[+https://github.com/frostt-tensor+]

functional-fortran
~~~~~~~~~~~~~~~~~~

While not designed as a purely functional programming language, modern Fortran goes a long way by letting the programmer use pure functions to encourage good functional discipline, express code in mathematical form, and minimize bug-prone mutable state. This library provides a set of commonly used tools in functional programming, with the purpose to help Fortran programmers be less imperative and more functional.

The following functions are provided:

* arange - returns a regularly spaced array
* complement - returns a set complement of two arrays
* empty - returns an empty array
* filter - filters an array using a logical input function
* foldl - recursively left-folds an array using an input function
* foldr - recursively right-folds an array using an input function
* foldt - recursively tree-folds an array using an input function
* head - returns the first element of an array
* init - returns everything but the last element
* insert - inserts an element into an array, out-of-bound safe
* intersection - returns a set intersection of two arrays
* iterfold - iteratively reduces an array using an input function
* last - returns the last element of an array
* limit - limits a scalar or array by given lower and upper bounds
* map - maps an array with an input function
* set - returns a set given input array
* reverse - returns array in reverse order
* sort - recursive quicksort using binary tree pivot
* split - returns first or second half of an array
* subscript - out-of-bound safe implementation of vector subscript
* tail - returns everything but the first element
* unfold - unfolds an array with an input function
* union - returns a set union of two arrays

https://github.com/wavebitscientific/functional-fortran[+https://github.com/wavebitscientific/functional-fortran+]

https://wavebitscientific.github.io/functional-fortran/[+https://wavebitscientific.github.io/functional-fortran/+]

FUSE
~~~~

Filesystem in Userspace (FUSE) is a software interface for Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by running file system code in user space while the FUSE module provides only a "bridge" to the actual kernel interfaces. 

To implement a new file system, a handler program linked to the supplied libfuse library needs to be written. The main purpose of this program is to specify how the file system is to respond to read/write/stat requests. The program is also used to mount the new file system. At the time the file system is mounted, the handler is registered with the kernel. If a user now issues read/write/stat requests for this newly mounted file system, the kernel forwards these IO-requests to the handler and then sends the handler's response back to the user.
Unmounting a FUSE-based file system with the fusermount command

FUSE is particularly useful for writing virtual file systems. Unlike traditional file systems that essentially work with data on mass storage, virtual filesystems don't actually store data themselves. They act as a view or translation of an existing file system or storage device.

In principle, any resource available to a FUSE implementation can be exported as a file system. 

https://github.com/libfuse/libfuse[+https://github.com/libfuse/libfuse+]

https://en.wikipedia.org/wiki/Filesystem_in_Userspace[+https://en.wikipedia.org/wiki/Filesystem_in_Userspace+]

archivemount
^^^^^^^^^^^^

Archivemount is a piece of glue code between libarchive
(http://people.freebsd.org/~kientzle/libarchive/) and FUSE
(http://fuse.sourceforge.net). It can be used to mount a (possibly compressed)
archive (as in .tar.gz or .tar.bz2) and use it like an ordinary filesystem.

https://github.com/cybernoid/archivemount[+https://github.com/cybernoid/archivemount+]

http://libarchive.org/[+http://libarchive.org/+]

EncFS
^^^^^

EncFS provides an encrypted filesystem in user-space. It runs in userspace, using the FUSE library for the filesystem interface. EncFS is open source software, licensed under the LGPL.

EncFS is now over 15 years old (first release in 2003). It was written because older NFS and kernel-based encrypted filesystems such as CFS had not kept pace with Linux development. When FUSE became available, I wrote a CFS replacement for my own use and released the first version to Open Source in 2003.

EncFS encrypts individual files, by translating all requests for the virtual EncFS filesystem into the equivalent encrypted operations on the raw filesystem.

https://github.com/vgough/encfs[+https://github.com/vgough/encfs+]

GlusterFS
^^^^^^^^^

GlusterFS is a scale-out network-attached storage file system. It has found applications including cloud computing, streaming media services, and content delivery networks.
GlusterFS aggregates various storage servers over Ethernet or Infiniband RDMA interconnect into one large parallel network file system. It is free software, with some parts licensed under the GNU General Public License (GPL) v3 while others are dual licensed under either GPL v2 or the Lesser General Public License (LGPL) v3. GlusterFS is based on a stackable user space design. 

GlusterFS has a client and server component. Servers are typically deployed as storage bricks, with each server running a glusterfsd daemon to export a local file system as a volume. The glusterfs client process, which connects to servers with a custom protocol over TCP/IP, InfiniBand or Sockets Direct Protocol, creates composite virtual volumes from multiple remote servers using stackable translators. By default, files are stored whole, but striping of files across multiple remote volumes is also supported. The final volume may then be mounted by the client host using its own native protocol via the FUSE mechanism, using NFS v3 protocol using a built-in server translator, or accessed via gfapi client library. Native-protocol mounts may then be re-exported e.g. via the kernel NFSv4 server, SAMBA, or the object-based OpenStack Storage (Swift) protocol using the "UFO" (Unified File and Object) translator. 

https://github.com/gluster/glusterfs[+https://github.com/gluster/glusterfs+]

https://en.wikipedia.org/wiki/Gluster[+https://en.wikipedia.org/wiki/Gluster+]

IPFS
^^^^

InterPlanetary File System (IPFS) is a protocol and network designed to create a content-addressable, peer-to-peer method of storing and sharing hypermedia in a distributed file system.[2]

IPFS is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. IPFS could be seen as a single BitTorrent swarm, exchanging objects within one Git repository. In other words, IPFS provides a high-throughput, content-addressed block storage model, with content-addressed hyperlinks.[3]

The filesystem can be accessed in a variety of ways, including via FUSE and over HTTP.

https://en.wikipedia.org/wiki/InterPlanetary_File_System[+https://en.wikipedia.org/wiki/InterPlanetary_File_System+]

https://ipfs.io/[+https://ipfs.io/+]

KBFS
^^^^

This client allows you to mount KBFS as a proper filesystem at some mountpoint on your local device (by default, /keybase/). It communicates locally with the Keybase service, and remotely with three types of KBFS servers (block servers, metadata servers, and key servers).

https://github.com/keybase/kbfs[+https://github.com/keybase/kbfs+]

partfs
^^^^^^

partfs allows one to access partitions within a device or file. the main purpose of partfs is to allow the creation of disk images without superuser privileges. this can be useful for the enabling automatic partition discovery for containers or for building disk images for embedded software.

https://github.com/braincorp/partfs[+https://github.com/braincorp/partfs+]

sshfs
^^^^^

SSHFS allows you to mount a remote filesystem using SFTP. Most SSH servers support and enable this SFTP access by default, so SSHFS is very simple to use - there's nothing to do on the server-side.

https://github.com/libfuse/sshfs[+https://github.com/libfuse/sshfs+]


Futhark
~~~~~~~

Futhark is a small programming language designed to be compiled to efficient parallel code. It is a statically typed, data-parallel, and purely functional array language in the ML family, and comes with a heavily optimising ahead-of-time compiler that presently generates GPU code via CUDA and OpenCL, although the language itself is hardware-agnostic.

Futhark is not designed for graphics programming, but instead uses the compute power of the GPU to accelerate data-parallel array computations. The language supports regular nested data-parallelism, as well as a form of imperative-style in-place modification of arrays, while still preserving the purity of the language via the use of a uniqueness type system.

While the Futhark language and compiler is an ongoing research project, it is quite usable for real programming, and can compile nontrivial programs which then run on real GPUs at high speed.

Futhark is a simple language and is designed to be easy to learn, although it omits some common features in order to generate high-performance parallel code. Nevertheless, Futhark can already be used for nontrivial programs. We are actively looking for more potential applications as well as people who are interested in contributing to the language design.

Futhark is not intended to replace existing general-purpose languages. The intended use case is that Futhark is only used for relatively small but compute-intensive parts of an application. The Futhark compiler generates code that can be easily integrated with non-Futhark code. For example, you can compile a Futhark program to a Python module that internally uses PyOpenCL to execute code on the GPU, yet looks like any other Python module from the outside (more on this here). The Futhark compiler will also generate more conventional C code, which can be accessed from any language with a basic FFI.

https://futhark-lang.org/index.html[+https://futhark-lang.org/index.html+]

https://github.com/diku-dk/futhark[+https://github.com/diku-dk/futhark+]

FV3GFS
~~~~~~

NOAA GFDL's Finite Volume Cubed Sphere (FV3) dynamical core was selected for the new NGGPS atmospheric model. FV3 dynamical core implementation includes incorporating FV3 into NEMS, and developing advanced physics and data assimilation techniques to match or exceed the skill of operational Global Forecast System (GFS). In addition, NWS is working with federal partners, universities, and the community to create a fully accessible community model.

NEMS FV3GFS is currently running in experimental mode in real-time using fully cycled 4D-EnVar Hybrid Data Assimilation. It is coupled to the earth system grid through the fv3cap . Current operational GFS physics along with newly included GFDL Microphysics is coupled with FV3 dynamic core through the Inter-operable Physics Driver (Version 4).

https://github.com/NOAA-EMC/fv3gfs[+https://github.com/NOAA-EMC/fv3gfs+]

https://vlab.ncep.noaa.gov/web/fv3gfs[+https://vlab.ncep.noaa.gov/web/fv3gfs+]

cubedsphere
^^^^^^^^^^^

CubedSphere is an xarray-based package for processing the Cubed-Sphere data from GFDL-FV3 and models using FV3 such as GEOS-Chem HP, GEOS5, and CESM.

https://github.com/JiaweiZhuang/cubedsphere[+https://github.com/JiaweiZhuang/cubedsphere+]

FXT
~~~

FXT is a library of low-level algorithms. Its main focus is on bit-manipulations, combinatorial generation, and fast transforms. The library is accompanied by the fxtbook. A strong emphasis is on performance and many of the routines are among the fastest available. 

The routines about combinatorial generation for combinations, subsets, compositions, integer partitions, set partitions, permutations, arrangements, mixed radix numbers, multi sets (subsets and permutations), Catalan objects and Dyck words, necklaces and Lyndon words, ascent sequences, Cayley permutations, Young tableaux, Motzkin paths, various restricted growths strings (RGS), and many other types of objects are documented, and there are over 500 demo programs.

https://www.jjj.de/fxt/[+https://www.jjj.de/fxt/+]

#GGGG

GAGA
~~~~

A software package for solving large compressed sensing problems in miliseconds by exploiting the power of graphics processing units.  The current release consists of five greedy algorithms using five matrix ensembles.  This release is set to compile as Matlab executables to enhance your compressed sensing research and applications.  A user guide is available for download detailing the capabilities inluding simple implementations for large-scale testing at problem sizes previously too computationally expensive.

The greedy algorithms are:

* Thresholding
* Iterative Hard Thresholding
* Normalized Iterative Hard Thresholding
* Hard Thresholding Pursuit
* CoSaMP/Subspace Pursuit

The matrix ensembles are:

* Dense Gaussian Matrices
* Dense Binary Matrices
* Sparse Binary Matrices
* Sparse Expander Matrices
* Subsampled Discrete Cosine Transform

http://www.gaga4cs.org/index.html[+http://www.gaga4cs.org/index.html+]

Galant
~~~~~~

The Graph Algorithm Animation Tool (Galant) challenges and motivates students to engage more deeply with algorithm concepts, without distracting them with programming language details or GUIs. Even though Galant is specifically designed for graph algorithms, it has also been used to animate other algorithms, most notably sorting algorithms.

https://github.com/mfms-ncsu/galant[+https://github.com/mfms-ncsu/galant+]

https://www.computer.org/csdl/magazine/cg/2017/01/mcg2017010008/13rRUwciPhT[+https://www.computer.org/csdl/magazine/cg/2017/01/mcg2017010008/13rRUwciPhT+]

http://snapapps.github.io/[+http://snapapps.github.io/+]

https://www.cs.usfca.edu/\~galles/visualization/Algorithms.html[+https://www.cs.usfca.edu/~galles/visualization/Algorithms.html+]

GAMA
~~~~

GAMA is an AutoML package for end-users and AutoML researchers. It uses genetic programming to efficiently generate optimized machine learning pipelines given specific input data and resource constraints. A machine learning pipeline contains data preprocessing (e.g. PCA, normalization) as well as a machine learning algorithm (e.g. Logistic Regression, Random Forests), with fine-tuned hyperparameter settings (e.g. number of trees in a Random Forest).

GAMA can also combine multiple tuned machine learning pipelines together into an ensemble, which on average should help model performance. At the moment, GAMA is restricted to classification and regression problems on tabular data.

In addition to its general use AutoML functionality, GAMA aims to serve AutoML researchers as well. During the optimization process, GAMA keeps an extensive log of progress made. Using this log, insight can be obtained on the behaviour of the population of pipelines.

https://github.com/PGijsbers/GAMA[+https://github.com/PGijsbers/GAMA+]

https://pgijsbers.github.io/gama/[+https://pgijsbers.github.io/gama/+]

http://joss.theoj.org/papers/74cfa7e5796270cd2a96005370973c71[+http://joss.theoj.org/papers/74cfa7e5796270cd2a96005370973c71+]

GammaRay
~~~~~~~~

GammaRay is a graphical user interface (GUI) that automates geostatistical workflows by driving and coordinating the several modules of the renowned Geostatistical Software Library (GSLib). The main purpose of GammaRay is to add a user-friendly interface layer on top of the scientifically and numerically robust GSLib, greatly automating parameter file editing and module chaining so the practitioner can focus on geostatistics.

https://github.com/PauloCarvalhoRJ/gammaray[+https://github.com/PauloCarvalhoRJ/gammaray+]

http://www.gslib.com/[+http://www.gslib.com/+]

https://opengeostat.github.io/pygslib/index.html[+https://opengeostat.github.io/pygslib/index.html+]

GCAM
~~~~

The Global Change Assessment Model (GCAM) is a global model that represents the behavior of, and interactions between five systems: the energy system, water, agriculture and land use, the economy, and the climate. It is used in a wide range of different applications from the exploration of fundamental questions about the complex dynamics between human and Earth systems to the those associated with response strategies to address important environmental questions. GCAM is a community model stewarded by The Joint Global Change Research Institute (JGCRI). This wiki page provides the documentation for GCAM.

GCAM is an integrated, multi-sector model that explores both human and Earth system dynamics. The role of models like GCAM is to bring multiple human and physical Earth systems together in one place to shed light on system interactions and provide scientific insights that would not otherwise be available from the pursuit of traditional disciplinary scientific research alone. GCAM is constructed to explore these interactions in a single computational platform with a sufficiently low computational requirement to allow for broad explorations of scenarios and uncertainties. Components of GCAM are designed to capture the behavior of human and physical systems, but they do not necessarily include the most detailed process-scale representations of its constituent components. On the other hand, model components in principle provide a faithful representation of the best current scientific understanding of underlying behavior.

The GCAM ecostystem includes a range of different tools, from data manipulation tools, the dynamic core of GCAM, and a range of disaggregation models, visualization tools and emulation tools. The GCAM release includes a data system for creatinig the XML inputs for GCAM and the dynamic core. Additional tools are produced and available separately and not included in the release version of GCAM.

The five systems in then GCAM Core are as follows:

* Macro-economy: This module takes population and labor productivity assumptions as inputs and produces regional Gross Domestic Product and regional populations as inputs for the other modules. The macroeconomy sets the scale of economic activity in GCAM.
* Energy systems: The energy system is a detailed representation of the sources of energy supply, modes of energy transformation, and energy service demands such as passenger and freight transport, industrial energy use, and residential and commercial energy service demands. The module reports demands for and supplies of energy forms as well as emissions of greenhouse gases, aerosols and other short-lived species. Energy systems demand bioenergy from agriculture and land systems and water from water systems.
* Agriculture and Land Systems: The agriculture and land systems provide in information about land use, land cover, carbon stocks and net emissions, the production of bioenergy, food, fiber, and forest products. Demands are driven by the size of the population, their income levels, and commodity prices. The module reports demands for and supplies of agricultural and other commodities, land and emissions of greenhouse gases, aerosols and other short-lived species. The demand for bioenergy is a derived demand by the energy sector. Agriculture and land systems demand water from water systems.
* Water Systems: The water module provides information about water withdrawals and water consumption for energy, agriculture, and municipal uses.
* Physical Earth System: The physical Earth system in GCAM is modeled using Hector, a physical Earth system emulator that provides information about the composition of the atmosphere based on emissions provided by the other modules, ocean acidity, and climate.

http://jgcri.github.io/gcam-doc/[+http://jgcri.github.io/gcam-doc/+]

https://github.com/JGCRI/gcam-core[+https://github.com/JGCRI/gcam-core+]

GDAL
~~~~

GDAL is a translator library for raster and vector geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single raster abstract data model and single vector abstract data model to the calling application for all supported formats. It also comes with a variety of useful command line utilities for data translation and processing.

https://www.gdal.org/[+https://www.gdal.org/+]

https://github.com/OSGeo/gdal[+https://github.com/OSGeo/gdal+]

https://github.com/dwtkns/gdal-cheat-sheet[+https://github.com/dwtkns/gdal-cheat-sheet+]

https://github.com/naturalatlas/node-gdal[+https://github.com/naturalatlas/node-gdal+]

https://github.com/pcjericks/py-gdalogr-cookbook[+https://github.com/pcjericks/py-gdalogr-cookbook+]

https://rasterio.readthedocs.io/en/latest/[+https://rasterio.readthedocs.io/en/latest/+]

https://github.com/yeesian/ArchGDAL.jl[+https://github.com/yeesian/ArchGDAL.jl+]

https://github.com/appelmar/gdalcubes[+https://github.com/appelmar/gdalcubes+]

gdalcubes
^^^^^^^^^

gdalcubes is a library to represent collections of Earth Observation (EO) images as on-demand data cubes (or multidimensional arrays). Users define data cubes by spatiotemporal extent, resolution, and spatial reference system and let gdalcubes read only relevant parts of the data and simultaneously apply reprojection, resampling, and cropping (using gdalwarp). Data cubes may be simply exported as NetCDF files or directly streamed chunk-wise into external software such as R or Python. The library furthermore implements simple operations to reduce data cubes over time, to apply pixel-wise arithmetic expressions, and to filter by space, time, and bands.

gdalcubes is not a database, i.e., it does not need to store additional copies of the imagery but instead simply links to and indexes existing files / GDAL datasets, i.e. it can also directly access data in cloud environments with GDAL virtual file systems. 

https://appelmar.github.io/gdalcubes/[+https://appelmar.github.io/gdalcubes/+]

https://github.com/appelmar/gdalcubes[+https://github.com/appelmar/gdalcubes+]

GEM
~~~

GEM is a GTK+ Graphical User Interface (GUI) for GNU/Linux which allows you to easily manage your emulators. This software aims to stay the simplest.
The features include:

* Numerous emulators are already configured : mednafen, gens, snes9x…).
* The Ability to filter ou games' list to make quick searches.
* A native screenshots viewer.
* A log viewer for each game launch (launch command and its output).
* The ability to specify custom parameters for a game.

https://gem.tuxfamily.org/[+https://gem.tuxfamily.org/+]

https://framagit.org/PacMiam/gem[+https://framagit.org/PacMiam/gem+]

GenASiS
~~~~~~~

GenASiS (General Astrophysical Simulation System) is a new code being developed initially and primarily, though by no means exclusively, for the simulation of core-collapse supernovae on the world's leading capability supercomputers. 

GenASiS is designed for modularity and extensibility of the physics. Presently in use or under development are capabilities for Newtonian self-gravity, Newtonian and special relativistic magnetohydrodynamics (with ‘realistic’ equation of state), and special relativistic energy- and angle-dependent neutrino transport—including full treatment of the energy and angle dependence of scattering and pair interactions. Early version of GenASiS' neutrino transport solvers have been subjected to almost every transport test problem we can ﬁnd in the literature, including some we fashioned.

Written in Modern Fortran (Fortran 2003 and 2008) and designed with object-oriented paradigm, GenASiS is still currently under heavy development. 

http://genasis.xyz/[+http://genasis.xyz/+]

https://github.com/GenASiS[+https://github.com/GenASiS+]

https://www.sciencedirect.com/science/article/pii/S0010465517303429[+https://www.sciencedirect.com/science/article/pii/S0010465517303429+]

GeoDocker
~~~~~~~~~

GeoDocker is a collection of Docker images encapsulating a distributed geo-processing platform based on GeoTrellis, GeoMesa, and GeoWave. The emphasis is on providing integration between these projects and exposing geo-processing functionality in Hadoop ecosystem.

https://github.com/geodocker/geodocker[+https://github.com/geodocker/geodocker+]

GeoGad
~~~~~~

GeoGad is short for "Geometry Gadget" and it is a programming language I wrote and implemented as part of a geometric modeler. When anyone ever creates a new programming language, there is one dominant question which must be answered: For the love of god, why!?

The purpose of this language is primarily to support user interactive input. Having become completely convinced of the utility and brilliance of Hewlett-Packard’s Reverse Polish Notation calculators for user facing calculation work, I thought that such a system would be ideal for technical geometry modeling. Another influence that proves the utility of this approach with geometry is PostScript. Unfortunately, PostScript is not ideal to work with interactively and it is mired in two dimensions. GeoGad seeks to create an RPN/RPL style language like that found on later HP calculators such as the beloved HP-48 and HP-28, for example, which will be ideal for working with accurate technical geometry.

The language is currently implemented in Python and pretty much anything Python can do can be quite easily implemented in this language as a function. It can be thought of as a way to turn Python into an interactive HP Reverse Polish Lisp kind of experience. I have been using it as a very powerful calculator pretty much every day now for 9 years.

http://xed.ch/project/gg/[+http://xed.ch/project/gg/+]

GeoGebra
~~~~~~~~

GeoGebra is an interactive geometry, algebra, statistics and calculus application, intended for learning and teaching mathematics and science from primary school to university level. GeoGebra is available on multiple platforms with its desktop applications for Windows, macOS and Linux, with its tablet apps for Android, iPad and Windows, and with its web application based on HTML5 technology.

GeoGebra is an interactive mathematics software program for learning and teaching mathematics and science from primary school up to university level. Constructions can be made with points, vectors, segments, lines, polygons, conic sections, inequalities, implicit polynomials and functions. All of them can be changed dynamically afterwards. Elements can be entered and modified directly via mouse and touch, or through the Input Bar. GeoGebra has the ability to use variables for numbers, vectors and points, find derivatives and integrals of functions and has a full complement of commands like Root or Extremum. Teachers and students can use GeoGebra to make conjectures and to understand how to prove geometric theorems.

Its main features are:

* Interactive geometry environment (2D and 3D)
* Built-in spreadsheet
* Built-in CAS
* Built-in statistics and calculus tools
* Allows scripting
* Large number of interactive learning and teaching resources at GeoGebra Materials

Dynamic GeoGebra applets can be directly uploaded to the GeoGebra Materials platform,[5] the official cloud service and repository of GeoGebra related and interactive learning and teaching resources.
The service now hosts more than 1 million resources (April 2016), 400,000+ of which are shared publicly as searchable materials - such as interactive worksheets, simulations, games, and e-books created using the GeoGebraBook feature.

GeoGebra materials can be also exported in several formats, including as static images or as Animated GIF. SVG vector images can be further edited using third party software, e.g. Inkscape. EMF vector formats can be directly imported in several Office applications. There are also options for exporting to the system clipboard, PNG, PDF, EPS. GeoGebra can also create code that can be used inside LaTeX files through its PSTricks, PGF/TikZ and Asymptote export options. 

https://www.geogebra.org/[+https://www.geogebra.org/+]

https://en.wikipedia.org/wiki/GeoGebra[+https://en.wikipedia.org/wiki/GeoGebra+]

Geogram
~~~~~~~

Geogram is a programming library of geometric algorithms. It includes a simple yet efficient Mesh data structure (for surfacic and volumetric meshes), exact computer arithmetics (a-la Shewchuck, implemented in GEO::expansion ), a predicate code generator ( PCK : Predicate Construction Kit), standard geometric predicates (orient/insphere), Delaunay triangulation, Voronoi diagram, spatial search data structures, spatial sorting) and less standard ones (more general geometric predicates, intersection between a Voronoi diagram and a triangular or tetrahedral mesh embedded in n dimensions). The latter is used by FWD/WarpDrive, the first algorithm that computes semi-discrete Optimal Transport in 3d that scales up to 1 million Dirac masses.

http://alice.loria.fr/software/geogram/doc/html/index.html[+http://alice.loria.fr/software/geogram/doc/html/index.html+]

https://gforge.inria.fr/frs/?group_id=5833[+https://gforge.inria.fr/frs/?group_id=5833+]

Geoid
~~~~~

An augmented reality globe application in Python and OpenCV, as well as a ocean current visulization project in Javascript/WebGL.

https://github.com/blendmaster/geoid[+https://github.com/blendmaster/geoid+]

http://blendmaster.github.io/geoid/[+http://blendmaster.github.io/geoid/+]

GeoJs
~~~~~

GeoJS is a javascript library for visualizing geospatial data in a browser. Its flexible API provides users with the ability to combine multiple visualizations drawn in WebGL, canvas, and SVG into a single dynamic map.

GeoJS can combine multiple layers of different data types: map tiles, choropleth polygons, vector layers, and more.

GeoJS can handle interactions with large quantities of data, whether it is a million line segments or a grid of scalar values. The visualization is interactive, and can be animated, too.

There are built-in features for many common information visualization methods: heatmaps, pixelmaps, vectors, text labels, contour plots, graphs, plus the usual points, lines, and polygons.

https://opengeoscience.github.io/geojs/[+https://opengeoscience.github.io/geojs/+]

https://github.com/OpenGeoscience/geojs[+https://github.com/OpenGeoscience/geojs+]

GeoMesa
~~~~~~~

GeoMesa is an Apache licensed open source suite of tools that enables large-scale geospatial analytics on cloud and distributed computing systems, letting you manage and analyze the huge spatio-temporal datasets that IoT, social media, tracking, and mobile phone applications seek to take advantage of today.

GeoMesa does this by providing spatio-temporal data persistence on top of the Accumulo, HBase, and Cassandra distributed column-oriented databases for massive storage of point, line, and polygon data. It allows rapid access to this data via queries that take full advantage of geographical properties to specify distance and area. GeoMesa also provides support for near real time stream processing of spatio-temporal data by layering spatial semantics on top of the Apache Kafka messaging system.

Through a geographical information server such as GeoServer, GeoMesa facilitates integration with a wide range of existing mapping clients by enabling access to its databases and streaming capabilities over standard OGC (Open Geospatial Consortium) APIs and protocols such as WFS and WMS. These interfaces also let GeoMesa drive map user interfaces and serve up data for analytics such as queries, histograms, heat maps, and time series analyses.

GeoMesa features include the ability to:

* Store gigabytes to petabytes of spatial data (tens of billions of points or more)
* Serve up tens of millions of points in seconds
* Ingest data faster than 10,000 records per second per node
* Scale horizontally easily (add more servers to add more capacity)
* Support Spark analytics
* Drive a map through GeoServer or other OGC Clients

https://www.geomesa.org/[+https://www.geomesa.org/+]

https://www.geomesa.org/documentation/[+https://www.geomesa.org/documentation/+]

https://github.com/locationtech/geomesa[+https://github.com/locationtech/geomesa+]

geometric algebra
~~~~~~~~~~~~~~~~~

Blah.

https://github.com/ga/Resources[+https://github.com/ga/Resources+]

GMac
^^^^

GMac, short for “Geometric Macro“, is a .NET based software system that allows implementing geometric models and algorithms based on Geometric Algebra (GA) in arbitrary target programming languages.

The core part of GMac is the GMacCompiler, a transcompiler or source-to-source translator that accepts code written in a simple high-level domain-specific language (GMacDSL), and can be configured using GMacAPI to output very efficient low-level code in any desired target programming language. The ultimate goal behind creating GMac is to allow GC implementations to be as flexible, organized, and efficient as possible. Although GMac is originally created using C#, it can be configured to generate a set of textual code files in any desired structure using an API that is accessible through any .NET language including Cxx, C#, VB.NET, F#, and IronPython among others.

GMac is intended to be a sophisticated software system for implementing Geometric Algebra algorithms in an efficient manner to narrow the gap between practical software engineering and GA-based geometric modeling in order to enable GA to gain wider acceptance among software designers, as well as researchers.

The features include:

* A simple yet powerful computational Domain Specific Language, GMacDSL, that exposes the power of Geometric Algebra to geometric computing software applications.
* Using GMac, the user can define any number of fixed interrelated frames up to 15 dimensions of any signature.
* Through GMacDSL, the user can define combinatorial nested structures containing multivectors and scalar values to hold basic geometric computing blocks of data.
* GMacDSL provides a rich set of multivector operators to construct sophisticated computational symbolic expressions.
* GMacDSL gives the user the ability to define Geometric Macros, a simple form of parameterized geometric computing procedures based on Geometric Algebra multivector expressions. Geometric macros can call each other to provide simple encapsulation and code reuse capabilities.
* The GMacDSL user code is automatically parsed to an Abstract Syntax Tree; stored in the GMacAST component classes. The GMacAST structure is specifically designed to be effective when used for later scripting and code generation tasks by including comprehensive information about the semantics and relations of the code elements defined in the GMacDSL code.

https://gmac-guides.netlify.com/[+https://gmac-guides.netlify.com/+]

https://github.com/ga-explorer/GMac[+https://github.com/ga-explorer/GMac+]

https://arxiv.org/abs/1607.04767[+https://arxiv.org/abs/1607.04767+]

geomstats
~~~~~~~~~

A python package that performs computations on manifolds such as hyperspheres, hyperbolic spaces, spaces of symmetric positive definite matrices and Lie groups of transformations. We provide efficient and extensively unit-tested implementations of these manifolds, together with useful Riemannian metrics and associated Exponential and Logarithm maps. The corresponding geodesic distances provide a range of intuitive choices of Machine Learning loss functions. We also give the corresponding Riemannian gradients. The operations implemented in geomstats are available with different computing backends such as numpy, tensorflow and keras. We have enabled GPU implementation and integrated geomstats manifold computations into keras deep learning framework. This paper also presents a review of manifolds in machine learning and an overview of the geomstats package with examples demonstrating its use for efficient and user-friendly Riemannian geometry. 

https://github.com/geomstats/geomstats[+https://github.com/geomstats/geomstats+]

https://arxiv.org/abs/1805.08308[+https://arxiv.org/abs/1805.08308+]

Geomview
~~~~~~~~

Geomview is an interactive 3D viewing program for Unix. Geomview lets you view and manipulate three-dimensional objects: you use the mouse to rotate, translate, zoom in and out, and so on. Geomview can be used as a standalone viewer for static objects or as a display engine for other programs which produce dynamically changing geometry. Geomview can display objects described in a variety of file formats. Geomview comes with a wide selection of example objects, and you can create your own objects too. 

The simplest way to use Geomview is as a standalone viewer to see and manipulate objects. It can display objects described in a variety of file formats. It comes with a wide selection of example objects, and you can easily create your own objects too.

You can also use Geomview to handle the display of data coming from another program that is running simultaneously. As the other program changes the data, the Geomview image reflects the changes. Programs that generate objects and use Geomview to display them are called external modules. External modules can control almost all aspects of Geomview. The idea here is that many aspects of the display and interaction parts of geometry software are independent of the geometric content and can be collected together in a single piece of software that can be used in a wide variety of situations. The author of the external module can then concentrate on implementing the desired algorithm and leave the display aspects to Geomview. Geomview comes with a collection of sample external modules, and the manual describes how to write your own.

Geomview allows multiple independently controllable objects and cameras. It provides interactive control for motion, appearances (including lighting, shading, and materials), picking on an object, edge or vertex level, snapshots in many image file, PostScript, or Renderman RIB format, and adding or deleting objects is provided through direct mouse manipulation, control panels, and keyboard shortcuts.

Geomview supports the following simple data types: polyhedra with shared vertices (.off), quadrilaterals, rectangular meshes, vectors, and Bezier surface patches of arbitrary degree including rational patches. Object hierarchies can be constructed with lists of objects and instances of object(s) transformed by one or many 4x4 matrices. Arbitrary portions of changing hierarchies may be transmitted by creating named references. The full specification for object file formats is in the OOGL (Object Oriented Graphics Language) Reference section of the Geomview manual. If you already have Geomview you might want to try out the OOGL tutorial. 

http://www.geomview.org/[+http://www.geomview.org/+]

GeoPackage
~~~~~~~~~~

GeoPackage is the modern alternative to formats like SDTS and Shapefile. At
its core, GeoPackage is simply a SQLite database schema. If you know SQLite,
you are close to knowing GeoPackage. Install Spatialite – the premiere spatial
extention to SQLite – and you get all the performance of a spatial database
along with the convenience of a file-based data set that can be emailed,
shared on a USB drive or burned to a DVD.

GeoPackage was carefully designed this way to facilitate widespread adoption
and use of a single simple file format by both commercial and open-source
software applications — on enterprise production platforms as well as mobile
hand-held devices. GeoPackage is a standard from the Open Geospatial
Consortium. It was designed and prototyped following a multi-year, open
process of requirements testing and public input. It is designed for
extension. So if you need more than the core GeoPackage feature set, join
OGC's open process to standardize community-tested enhancements.

http://www.geopackage.org/[+http://www.geopackage.org/+]

https://www.sciencedirect.com/science/article/pii/S0098300416303533[+https://www.sciencedirect.com/science/article/pii/S0098300416303533+]

GeoPandas
~~~~~~~~~

GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by pandas to allow spatial operations on geometric types. Geometric operations are performed by shapely. Geopandas further depends on https://fiona.readthedocs.io/en/latest/[fiona] for file access and
https://pypi.org/project/descartes/[descartes] and https://matplotlib.org/[matplotlib] for plotting.

The goal of GeoPandas is to make working with geospatial data in python easier. It combines the capabilities of pandas and shapely, providing geospatial operations in pandas and a high-level interface to multiple geometries to shapely. GeoPandas enables you to easily do operations in python that would otherwise require a spatial database such as https://postgis.net/[PostGIS].

GeoPandas implements two main data structures, a GeoSeries and a GeoDataFrame. These are subclasses of pandas Series and DataFrame, respectively.

A GeoSeries is essentially a vector where each entry in the vector is a set of shapes corresponding to one observation. An entry may consist of only one shape (like a single polygon) or multiple shapes that are meant to be thought of as one observation (like the many polygons that make up the State of Hawaii or a country like Indonesia).

A GeoDataFrame is a tabular data structure that contains a GeoSeries.

http://geopandas.org/[+http://geopandas.org/+]

GEOS-5
~~~~~~

The Goddard Earth Observing System Model, Version 5 (GEOS-5) is a system of models integrated using the Earth System Modeling Framework (ESMF). The GEOS-5 Data Assimilation System (GEOS-5 DAS) integrates the GEOS-5 Atmospheric Global Climate Model (GEOS-5 AGCM) with the Gridpoint Statistical Interpolation (GSI) atmospheric analysis developed jointly with NOAA/NCEP/EMC. The GEOS-5 systems are being developed in the GMAO to support NASA's earth science research in data analysis, observing system modeling and design, climate and weather prediction, and basic research. 

https://geos5.org/[+https://geos5.org/+]

https://gmao.gsfc.nasa.gov/systems/geos5/[+https://gmao.gsfc.nasa.gov/systems/geos5/+]

https://opensource.gsfc.nasa.gov/projects/GEOS-5/[+https://opensource.gsfc.nasa.gov/projects/GEOS-5/+]

GeoSPARQL
~~~~~~~~~

GeoSPARQL is a standard for representation and querying of geospatial linked data for the Semantic Web from the Open Geospatial Consortium (OGC).[1] The definition of a small ontology based on well-understood OGC standards is intended to provide a standardized exchange basis for geospatial RDF data which can support both qualitative and quantitative spatial reasoning and querying with the SPARQL database query language.[2]

The Ordnance Survey Linked Data Platform uses OWL mappings for GeoSPARQL equivalent properties in its vocabulary.[3][4] The LinkedGeoData data set is a work of the Agile Knowledge Engineering and Semantic Web (AKSW) research group at the University of Leipzig,[5] a group mostly known for DBpedia, that uses the GeoSPARQL vocabulary to represent OpenStreetMap data.

In particular, GeoSPARQL provides for:

* a small topological ontology in RDFS/OWL for representation using
** Geography Markup Language (GML) and well-known text representation of geometry (WKT) literals, and
** Simple Features, RCC8, and DE-9IM (a.k.a. Clementini, Egenhofer) topological relationship vocabularies and ontologies for qualitative reasoning, and
* a SPARQL query interface using
** a set of topological SPARQL extension functions for quantitative reasoning, and
** a set of Rule Interchange Format (RIF) Core inference rules for query transformation and interpretation.

http://www.opengeospatial.org/standards/geosparql[+http://www.opengeospatial.org/standards/geosparql+]

https://en.wikipedia.org/wiki/GeoSPARQL[+https://en.wikipedia.org/wiki/GeoSPARQL+]

GeoTriples
^^^^^^^^^^

GeoTriples is a tool for transforming geospatial data from their original formats (e.g., shapefiles or spatially-enabled relational databases) into RDF. The following input formats are supported: spatially-enabled relational databases (PostGIS and MonetDB), ESRI shapefiles and XML, GML, KML, JSON, GeoJSON and CSV documents.

GeoTriples comprises two main components: the mapping generator and the R2RML/RML mapping processor. The mapping generator takes as input a geospatial data source (e.g., a shapefile) and creates automatically an R2RML or RML mapping that can transform the input into an RDF graph which uses the GeoSPARQL vocabulary. The user may edit the generated R2RML/RML mapping document to comply with her requirements (e.g., use a vocabulary different than the one of GeoSPARQL). Then, the mapping processor executes the R2RML/RML mappings to produce the output geospatial RDF graph. The mapping processor of GeoTriples comes in two forms: a single-node implementation and an implementation that uses Apache Hadoop for dealing with big geospatial data.

t is often the case in applications that relevant geospatial data is stored in spatially-enabled relational databases (e.g., PostGIS) or files (e.g., shapefiles), and its owners do not want to explicitly transform it into linked data. For example, this might be because these data sources get frequently updated and/or are very large. If this is the case, GeoTriples is still very useful. GeoTriples users can use the generated mappings in the system Ontop-spatial to view their data sources virtually as linked data. Ontop-spatial is a geospatial Ontology-Based Data Access system which performs on-the-fly GeoSPARQL-to-SQL translation over spatially-enabled relational databases using ontologies and mappings.

http://geotriples.di.uoa.gr/[+http://geotriples.di.uoa.gr/+]

https://github.com/LinkedEOData/GeoTriples[+https://github.com/LinkedEOData/GeoTriples+]

http://ontop-spatial.di.uoa.gr/[+http://ontop-spatial.di.uoa.gr/+]

ontop-spatial
^^^^^^^^^^^^^

Ontop-spatial makes GeoSPARQL-to-SQL translation easy.  You can
create virtual geospatial RDF views on top of your geospatial databases and pose GeoSPARQL queries without converting your data.
The geometries will be mapped to GeoSPARQL geometry literals using ontologies and R2RML/OBDA mappings.

Ontop-spatial supports the following GeoSPARQL components: core, Topology vocabulary, Geometry topology extension, RDFS entailment extension, Query rewrite extension and subset of the Geometry Extension.

As its parent system, Ontop, Ontop-spatial can be used as a standard SPARQL endpoint that can execute GeoSPARQL queries on top of geospatial databases. Therefore, it can be used complementarily with other tools that produce, manage, explore, and visualize geospatial RDF data. For example, R2R2ML mappings generated by GeoTriples can be given as input to Ontop-spatial to create a virtual geospatial repository. Also, the geometries of an Ontop-spatial repository can be visualized using Sextant, a web-based tool for browsing and visualizing linked geospatial data. An Ontop-spatial endpoint can also be used as source endpoint to the linking tool Silk, that has recently been extended with geospatial features. Last but not least, the virtual geospatial RDF graphs produced by Ontop-spatial can be materialized and stored in a geospatial RDF store (e.g., Strabon).

http://ontop-spatial.di.uoa.gr/[+http://ontop-spatial.di.uoa.gr/+]

https://github.com/LinkedEOData/GeoTriples[+https://github.com/LinkedEOData/GeoTriples+]

http://sextant.di.uoa.gr/[+http://sextant.di.uoa.gr/+]

http://silkframework.org/[+http://silkframework.org/+]

http://www.strabon.di.uoa.gr/[+http://www.strabon.di.uoa.gr/+]

Parliament
^^^^^^^^^^

Parliament™ is a high-performance triple store designed for the Semantic Web.
Parliament is an operational, fully featured triple store that runs on a single machine.
SHARD is a proof-of-concept implementation of a distributed triple-store.

Parliament is a complete triple store and data management solution that is compatible with the RDF, RDFS, OWL, SPARQL, and GeoSPARQL standards. Parliament incorporates a number of open source third party packages, including Jetty (a servlet container), Jena and ARQ (a query processor), Joseki (a servlet-based implementation of the SPARQL protocol), and Berkeley DB (used to implement the resource dictionary).

The features of Parliament include:

* Parliament employs an innovative data storage scheme that interweaves the data with a unique index. This keeps the index small and allows Parliament to keep the index up-to-date with very little extra effort.
* Parliament has a temporal index, so that it can efficiently answer queries like "find all events that occurred between times X and Y".
* Parliament supports GeoSPARQL, the newly adopted OGC standard for geospatial semantic data. Using its geospatial index, Parliament can efficiently answer queries like "find all items located within region X".
* Parliament directly supports RDF statement reification, enabling efficient storage and lookup of provenance and other kinds of metadata.
* Parliament includes a high-performance rule engine, which serves as an efficient means of inference. The rule engine applies a set of inference rules to the directed graph of data in the triple store in order to derive new facts. This enables Parliament to automatically and transparently infer additional facts and relationships in the data to enrich query results.

http://parliament.semwebcentral.org/[+http://parliament.semwebcentral.org/+]

Sextant
^^^^^^^

Sextant is a web-based and mobile ready application for exploring, interacting, and visualizing time-evolving linked geospatial data. Sextant has been designed with the aim of being flexible, portable, and interoperable with other GIS tools.

The core feature of Sextant is the ability to create thematic maps by combining geospatial and temporal information that exists in a number of heterogeneous data sources ranging from standard SPARQL endpoints, to SPARQL endpoints following the standard GeoSPARQL defined by the Open Geospatial Consortium (OGC), or well-adopted geospatial file formats, like KML, GML and GeoTIFF. In this manner we overcome the main disadvantage of existing semantic web tools that allow the visualization of a single SPARQL endpoint, and provide functionality to domain experts from different fields in creating thematic maps, which emphasize spatial variation of one or a small number of geographic distributions. Moreover we go beyond and present a map ontology that assists on modeling these maps in RDF and allow for easy sharing, editing and search mechanisms over existing maps.

One of our goals was to build a tool that is interoperable with GIS tools. To achieve that we try to support some of the most promising file formats used in the GIS area. Two are the main categories of these file formats according to the way the information is represented in the files. These are the raster and the vector file formats. In Sextant we currently support the visualization of KML, GML, GeoTIFF and WMS layers and provide tools for interaction with these layers, such as the colorization of geometry features according to specific values to create color maps for better understanding of the various aspects of layers. Another important feature is the utilization of the temporal dimension. Implementation of the valid time component of stRDF and stSPARQL in system Strabon allows us to query both the spatial and the temporal dimension. Enriching our results with temporal information allows us to create layers with valid time. Using the SIMILE Timeline widget we can make these layers appear and disappear from the map according to their valid time. This feature allows the creation of thematic maps that change over time and can assist experts in the fields of agriculture, biodiversity, climate, disasters, ecosystems, energy, water and weather, in visualizing temporal maps that help them understand the evolution of data.

Apart from visualizing the spatial and temporal dimension, statistical charts play an important role in understanding the various measures of datasets. Statistical data is a foundation for policy prediction, planning and adjustments and underpins many of the mash-ups and visualizations we see on the web. There is strong interest in being able to publish statistical data in a web-friendly format to enable it to be linked and combined with related information. At the heart of a statistical dataset is a set of observed values organized along a group of dimensions, together with associated metadata. The Data Cube vocabulary enables such information to be represented using the W3C RDF standard and published following the principles of linked data. We demonstrate how to utilize the Data Cube vocabulary to enhance existing datasets and allow the creation of charts through Sextant in an intuitive way that does not involve the use of SPARQL from the user point of view.

https://github.com/zefyros/Sextant[+https://github.com/zefyros/Sextant+]

http://sextant.di.uoa.gr/[+http://sextant.di.uoa.gr/+]

Strabon
^^^^^^^

Strabon is a spatiotemporal RDF store. You can use it to store linked geospatial data that changes over time and pose queries using two popular extensions of SPARQL. Strabon supports spatial datatypes enabling the serialization of geometric objects in OGC standards WKT and GML. It also offers spatial and temporal selections, spatial and temporal joins, a rich set of spatial functions similar to those offered by geospatial relational database systems and support for multiple Coordinate Reference Systems. Strabon can be used to model temporal domains and concepts such as events, facts that change over time etc. through its support for valid time of triples, and a rich set of temporal functions. Strabon is built by extending the well-known RDF store Sesame (now called RDF4J) and extends RDF4J’s components to manage thematic, spatial and temporal data that is stored in the backend RDBMS.

The first query language supported by Strabon is stSPARQL. stSPARQL can be used to query data represented in an extension of RDF called stRDF. stRDF and stSPARQL have been designed for representing and querying geospatial data that changes over time, e.g., the growth of a city over the years due to new developments can be represented and queried using the valid time dimesion of stRDF and stSPARQL respectively. The expressive power of stSPARQL makes Strabon the only fully implemented RDF store with rich spatial and temporal functionalities available today.

Strabon also supports the querying of static geospatial data expressed in RDF using a subset of the recent OGC standard GeoSPARQL which consists of the core, geometry extension and geometry topology extension. The implementation of the other components of GeoSPARQL is underway.

http://www.strabon.di.uoa.gr/[+http://www.strabon.di.uoa.gr/+]

GeoTrellis
^^^^^^^^^^

GeoTrellis is a geographic data processing engine for high performance applications.

GeoTrellis Server is a set of components designed to simplify viewing, processing, and serving raster data from arbitrary sources with an emphasis on doing so in a functional style. It aims to ease the pains related to constructing complex raster processing workflows which result in TMS-based (z, x, y) and extent-based products.

In addition to providing a story about how sources of imagery can be displayed or returned, this project aims to simplify the creation of dynamic, responsive layers whose transformations can be described in MAML (Map Algebra Modeling Language).

https://github.com/geotrellis/geotrellis-server[+https://github.com/geotrellis/geotrellis-server+]

https://github.com/geotrellis/maml[+https://github.com/geotrellis/maml+]

https://geotrellis.io/[+https://geotrellis.io/+]

GeoViews
~~~~~~~~

GeoViews is a Python library that makes it easy to explore and visualize any data that includes geographic locations. It has particularly powerful support for multidimensional meteorological and oceanographic datasets, such as those used in weather, climate, and remote sensing research, but is useful for almost anything that you would want to plot on a map! You can see lots of example notebooks at geo.holoviews.org, and a good overview is in our blog post announcement.

GeoViews is built on the HoloViews library for building flexible visualizations of multidimensional data. GeoViews adds a family of geographic plot types based on the Cartopy library, plotted using either the Matplotlib or Bokeh packages.

Each of the new GeoElement plot types is a new HoloViews Element that has an associated geographic projection based on cartopy.crs. The GeoElements currently include Feature, WMTS, Tiles, Points, Contours, Image, QuadMesh, TriMesh, RGB, HSV, Labels, Graph, HexTiles, VectorField and Text objects, each of which can easily be overlaid in the same plots. E.g. an object with temperature data can be overlaid with coastline data using an expression like gv.Image(temperature) * gv.Feature(cartopy.feature.COASTLINE). Each GeoElement can also be freely combined in layouts with any other HoloViews Element , making it simple to make even complex multi-figure layouts of overlaid objects.

https://github.com/pyviz/geoviews[+https://github.com/pyviz/geoviews+]

http://geoviews.org/[+http://geoviews.org/+]

GeoWave
~~~~~~~

GeoWave is an open-source library for storage, index, and search of multi-dimensional data on top of sorted key-value datastores and popular big data frameworks. GeoWave includes specific tailored implementations that have advanced support for OGC spatial types (up to 3 dimensions), and both bounded and unbounded temporal values. Both single and ranged values are supported on all axes. GeoWave’s geospatial support is built on top of the GeoTools project extensibility model. This means that it can integrate natively with any GeoTools-compatible project, such as GeoServer and UDig, and can ingest GeoTools compatible data sources.

GeoWave provides out-of-the-box support for distributed key/value stores, as necessary for mission needs. The latest version of GeoWave supports Apache Accumulo and Apache HBase stores, though additional data stores can be implemented as requested or needed.

This guide serves the purpose of focusing on the development side of GeoWave capabilities as well as assisting developers with the GeoWave code surroundings.

GeoWave capabilities include:

* Add multi-dimensional indexing capability to Apache Accumulo and Apache HBase
* Add support for geographic objects and geospatial operators to Apache Accumulo and Apache HBase
* Provide a GeoServer plugin to allow geospatial data in Accumulo and HBase to be shared and visualized via OGC standard services
* Provide Map-Reduce input and output formats for distributed processing and analysis of geospatial data

Geospatial software plugins include the following:
        
* GeoServer plugin to allow geospatial data in Accumulo to be shared and visualized via OGC standard services
* PDAL plugin for working with point cloud data
* Mapnik plugin for generating map tiles and generally making good looking maps.

Basically, GeoWave is working to bridge geospatial software with distributed compute systems and attempting to do for distributed key/value stores what PostGIS does for PostgreSQL.

https://locationtech.github.io/geowave/[+https://locationtech.github.io/geowave/+]

https://github.com/locationtech/geowave[+https://github.com/locationtech/geowave+]

Gephi
~~~~

Gephi is a tool for data analysts and scientists keen to explore and understand graphs. Like Photoshop™ but for graph data, the user interacts with the representation, manipulate the structures, shapes and colors to reveal hidden patterns. The goal is to help data analysts to make hypothesis, intuitively discover patterns, isolate structure singularities or faults during data sourcing. It is a complementary tool to traditional statistics, as visual thinking with interactive interfaces is now recognized to facilitate reasoning. This is a software for Exploratory Data Analysis, a paradigm appeared in the Visual Analytics field of research.

Layout algorithms give the shape to the graph. Gephi provides state-of-the-art algorithms layout algorithms, both for efficiency and quality. The Layout palette allows user to change layout settings while running, and therefore dramatically increase user feedback and experience.

https://gephi.org/[+https://gephi.org/+]

GHOST
~~~~~

The "General, Hybrid, and Optimized Sparse Toolkit" (GHOST) is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the "MPI+X" paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We describe the details of its design with respect to the challenges posed by modern heterogeneous supercomputers and recent algorithmic developments. Implementation details which are indispensable for achieving high efficiency are pointed out and their necessity is justified by performance measurements or predictions based on performance models.

https://bitbucket.org/essex/ghost[+https://bitbucket.org/essex/ghost+]

https://mkreutzer.bitbucket.io/ghost_doc/[+https://mkreutzer.bitbucket.io/ghost_doc/+]

*GHOST: Building blocks for high performance sparse linear algebra on heterogeneous systems* - https://arxiv.org/abs/1507.08101[+https://arxiv.org/abs/1507.08101+]

http://www.sppexa.de/sppexa-activities/software.html[+http://www.sppexa.de/sppexa-activities/software.html+]

GIPPY
~~~~~

Gippy is a Python library for image processing of geospatial raster data. The core of the library is implemented as a Cxx library, libgip, with Python bindings automatically generated with swig. Gippy encapsulates the functionality of GDAL and CImg that automatically handles issues common to geospatial data, such as handling of nodata values and chunking up of very large images by saving chains of functions and only processing the image in pieces upon a read request. In addition to providing a library of image processing functions and algorithms, Gippy can also be used as a simpler interface to GDAL for the opening, creating, reading and writing of geospatial raster files in Python.

https://github.com/gipit/gippy[+https://github.com/gipit/gippy+]

https://gippy.readthedocs.io/en/latest/[+https://gippy.readthedocs.io/en/latest/+]

GLM
~~~

GLM is a 1-dimensional lake water balance and stratification model. It is distributed coupled with a powerful ecological modelling library to also support simulations of lake water quality and ecosystems processes.

GLM is suitable for a wide range of natural and engineered lakes, including shallow (well-mixed) and deep (stratified) systems. The model has been successfully applied to systems from the scale of individual ponds and wetlands to the scale of Great Lakes.

GLM computes vertical profiles of temperature, salinity and density by accounting for the effect of inflows/outflows, mixing and surface heating and cooling, including the effect of ice cover on heating and mixing of the lake. Since the model is one-dimensional it assumes no horizontal variability so users must ensure the lake conditions match this one-dimensional assumption. The model is ideally suited to long-term investigations ranging from seasons to decades, and for coupling with biogeochemical models to explore the role that stratification and vertical mixing play on the dynamics of lake ecosystem.

GLM incorporates a flexible Lagrangian layer structure similar to the approach of several 1-D lake model designs (Imberger and Patterson 1981; Hamilton and Schladow 1997). The Lagrangian approach was originally introduced in the model DYRESM developed by the Centre for Water Research and allows for layers to change thickness by contracting and expanding in response to inflows, outflows, mixing and surface mass fluxes. When sufficient energy becomes available to over come density gradients, two layers will merge thus accounting for the process of mixing. Layer thicknesses are adjusted by the model in order to sufficiently resolve the vertical density gradient. Unlike the fixed grid design where mixing algorithms are typically based on vertical velocities, numerical diffusion of the thermocline is limited, making the GLM approach particularly suited to long-term investigations.

http://aed.see.uwa.edu.au/research/models/GLM/[+http://aed.see.uwa.edu.au/research/models/GLM/+]

http://aed.see.uwa.edu.au/research/models/AED/index.html[+http://aed.see.uwa.edu.au/research/models/AED/index.html+]

https://www.geosci-model-dev.net/12/473/2019/[+https://www.geosci-model-dev.net/12/473/2019/+]

Global Arrays
~~~~~~~~~~~~~

Global Arrays (GA) is a Partitioned Global Address Space (PGAS) programming model. It provides primitives for one-sided communication (Get, Put, Accumulate) and Atomic Operations (read increment). It supports blocking and non-blocking primtives, and supports location consistency. 

The Global Arrays toolkit consists of many useful and related pieces:

* Communication Runtime for Extreme Scale (ComEx) provides vector and strided interfaces to optimize performance of remote memory copy operations for non-contiguous data.
* ChemIO aka Parallel IO (pario) is a package consisting of three independent parallel I/O libraries for high-performance computers. It was designed for computational chemistry; however, the supported abstractions and features are general enough to be of interest to other applications.
** Disk Resident Arrays extend the GA Non-Uniform Memory Access (NUMA) programming model to disk.
** Shared Files to which multiple processors can read and write independently.
** Exclusive Access Files (EAF) per-processor private files.
* Memory Allocator (MA) is a local memory manager/allocator with several useful features not available in Fortran or C languages.
* Task Scheduling Library (tascel)
* TCGMSG is an efficient but limited in functionality (comparing to MPI) message-passing library available on many current (and legacy) systems.
* TCGMSG-MPI is a portability layer between TCGMSG and MPI. It is recommended as a transition library from TCGMSG to MPI for existing TCGMSG codes.

http://hpc.pnl.gov/globalarrays/[+http://hpc.pnl.gov/globalarrays/+]

https://github.com/GlobalArrays/ga[+https://github.com/GlobalArrays/ga+]

https://github.com/GlobalArrays/ga4py[+https://github.com/GlobalArrays/ga4py+]

glom
~~~~

Python's nested data operator (and CLI), for all your declarative restructuring needs.

Real applications have real data, and real data nests. Objects inside of objects inside of lists of objects.

glom is a new and powerful way to handle real-world data, featuring:

* Path-based access for nested data structures
* Readable, meaningful error messages
* Declarative data transformation, using lightweight, Pythonic specifications
* Built-in data exploration and debugging features

All of that and more, available as a fully-documented, pure-Python package, tested on Python 2.7-3.7, as well as PyPy.

https://github.com/mahmoud/glom[+https://github.com/mahmoud/glom+]

https://glom.readthedocs.io/en/latest/[+https://glom.readthedocs.io/en/latest/+]

GLOW
~~~~


The GLobal airglOW Model

The GLobal airglOW model, also known as GLOW, is a toolkit of subroutines and driver programs for performing calculations of optical emissions in the upper atmosphere, particularly the thermosphere and ionosphere, above about 100 km altitude. These emissions are caused by combinations of solar-driven photon processes, auroral precipitation, and chemical reactions. GLOW can calculate various spectral features for any combination of solar and/or auroral inputs, given a specification of the neutral atmosphere and the high-altitude ionosphere.

GLOW is designed to work with either theoretical or empirical models of thermospheric neutral densities, or with measured quantities. It is especially configured to use output from the NCAR Thermosphere-Ionosphere-Electrodynamics General Circulation Model (TIE-GCM) and Thermosphere-Ionosphere-Mesosphere-Electrodynamics General Circulation Model (TIME-GCM) (see http://www.hao.ucar.edu/modeling/tgcm/). It is also configured for use with empirical models such as the NRL Mass Spectrometer Incoherent Scatter - 2000 model (NRLMSISE-00) and the International Reference Ionosphere (IRI). GLOW uses modeled or measured solar extreme-ultraviolet spectra and assumed or measured auroral electrons as inputs, and then calculates energetic electron distributions, ionization and excitation rates, the density of ionized and excited species, and the volume emission rates of various emission lines and bands, as a function of altitude and location. Post-processors can then integrate these volume emission rates to calculate column brightness, or perform radiative transfer calculations.

The source code, data files, and basic documentation are available to the research community under the auspices of an open-source academic research license.

https://www2.hao.ucar.edu/modeling/glow[+https://www2.hao.ucar.edu/modeling/glow+]

https://github.com/NCAR/GLOW[+https://github.com/NCAR/GLOW+]

https://github.com/scivision/NCAR-GLOW[+https://github.com/scivision/NCAR-GLOW+]

glTF
~~~~

glTF (GL Transmission Format) is a file format for 3D scenes and models using the JSON standard. It is described by its creators as the "JPEG of 3D." It is an API-neutral runtime asset delivery format developed by the Khronos Group 3D Formats Working Group and announced at HTML5DevConf 2016. The intention is that glTF be an efficient, interoperable asset delivery format that compresses the size of 3D scenes and minimizes runtime processing by applications using WebGL and other APIs. glTF also defines a common publishing format for 3D content tools and services. 

https://www.khronos.org/gltf/[+https://www.khronos.org/gltf/+]

https://en.wikipedia.org/wiki/GlTF[+https://en.wikipedia.org/wiki/GlTF+]

GMAT
~~~~

GMAT is an open source, platform independent trajectory optimization and design system designed to model and optimize spacecraft trajectories in flight regimes ranging from low Earth orbit to lunar applications, interplanetary trajectories, and other deep space missions. The system supports constrained and unconstrained trajectory optimization and built-in features make defining cost and constraint functions trivial so analysts can determine how their inclusion or exclusion affects solutions.

The system also contains initial value solvers (propagation) and boundary value solvers and efficiently propagates spacecraft either singly or coupled. GMAT's propagators naturally synchronize the epochs of multiple vehicles and shorten run times by avoiding fixed step integration or interpolation to synchronize epochs of spacecraft.

A user can interact with GMAT using either a graphical user interface (GUI) or script language that has a syntax similar to the MathWorks' MATLAB® system. All of the system elements can be expressed through either interface and users can configure elements in the GUI and then view the corresponding script, or write script and load it into GMAT.

Analysts model space missions in GMAT by first creating resources such as spacecraft, propagators, and optimizers to name a few. These resources can be configured to meet the needs of specific applications and missions. After the resources are configured they are used in the mission sequence to model the motion of spacecraft and simulate events in a mission's time evolution. The mission sequence supports commands such as Nonlinear Constraint, Minimize, Propagate, Function Calls, Inline Math, and Script Events among others.

The system can display trajectories in space, plot parameters against one another, and save parameters to files for later processing. The trajectory and plot capabilities are fully interactive, plotting data as a mission is run and allowing users to zoom into regions of interest. Trajectories and data can be viewed in any coordinate system defined in GMAT, and GMAT allows users to rotate the view and set the focus to any object in the display. The trajectory view can be animated so users can watch the evolution of the trajectory over time.

https://opensource.gsfc.nasa.gov/projects/GMAT/index.php[+https://opensource.gsfc.nasa.gov/projects/GMAT/index.php+]

G'MIC
~~~~~

G'MIC is a full-featured open-source framework for image processing, distributed under the CeCILL free software licenses (LGPL-like and/or GPL-compatible). It provides several user interfaces to convert / manipulate / filter / visualize generic image datasets, ranging from 1D scalar signals to 3D+t sequences of multi-spectral volumetric images, hence including 2D color images.

The user interfaces are:

* *gmic*, a command-line interface , to use the G'MIC image processing features from a shell. In this setting, G'MIC may be seen as a friendly companion to the ImageMagick or GraphicsMagick software suites.
* *libgmic*, a small, portable, thread-safe and multi-threaded C++ image processing library to be linked to third-party applications. Its simple API allows programmers to add all G'MIC features in their own software without much efforts (a C API is available as well).
* *G'MIC-Qt*, a plug-in to bring G'MIC capabilities to the image retouching software GIMP, Krita and Paint.NET. More than 500 filters are already available, sorted by category (Artistic, Black & white, Colors, Contours, Deformations, Degradations, Details, Film emulation, Frames, Layers, Light & shadows, Patterns, Rendering, Repair, Sequences, etc.).
* *G'MIIC Online*, a web service to allow users applying image processing algorithms on their images, directly from a web browser.
* *ZArt*, a Qt-based interface for real-time processing of video streaming coming from webcams or video files.

https://framagit.org/dtschump/gmic[+https://framagit.org/dtschump/gmic+]

https://gmic.eu/[+https://gmic.eu/+]

GNU Build System
~~~~~~~~~~~~~~~~

The GNU Build System, also known as the Autotools, is a suite of programming tools designed to assist in making source code packages portable to many Unix-like systems.

It can be difficult to make a software program portable: the C compiler differs from system to system; certain library functions are missing on some systems; header files may have different names. One way to handle this is to write conditional code, with code blocks selected by means of preprocessor directives (#ifdef); but because of the wide variety of build environments this approach quickly becomes unmanageable. Autotools is designed to address this problem more manageably.

Autotools is part of the GNU toolchain and is widely used in many free software and open source packages. Its component tools are free software-licensed under the GNU General Public License with special license exceptions permitting its use with proprietary software.

The GNU Build System makes it possible to build many programs using a two-step process: configure followed by make.

Autotools consists of the GNU utility programs Autoconf, Automake, and Libtool. Other related tools frequently used alongside it include GNU's make program, GNU gettext, pkg-config, and the GNU Compiler Collection, also called GCC.

https://www.sourceware.org/autobook/[+https://www.sourceware.org/autobook/+]

http://freesoftwaremagazine.com/books/autotools_a_guide_to_autoconf_automake_libtool/[+http://freesoftwaremagazine.com/books/autotools_a_guide_to_autoconf_automake_libtool/+]

https://autotools.io/index.html[+https://autotools.io/index.html+]

autoconf
^^^^^^^^

GNU Autoconf is a tool for producing configure scripts for building, installing and packaging software on computer systems where a Bourne shell is available.

Autoconf is agnostic about the programming languages used, but it is often used for projects using C, C++, Fortran, Fortran 77, Erlang or Objective-C.

A configure script configures a software package for installation on a particular target system. After running a series of tests on the target system, the configure script generates header files and a makefile from templates, thus customizing the software package for the target system. Together with Automake and Libtool, Autoconf forms the GNU Build System, which comprises several other tools, notably Autoheader. 

The developer specifies the desired behaviour of the configure script by writing a list of instructions in the GNU m4 language in a file called "configure.ac". A library of pre-defined m4 macros is available to describe common configure script instructions. Autoconf transforms the instructions in "configure.ac" into a portable configure script. The system that will be doing the building need not have autoconf installed: autoconf is needed only to build the configure script, that is usually shipped with the software. 

The Autoconf approach to portability is to test for features, not for versions. For example, the native C compiler on SunOS 4 did not support ISO C. However, it is possible for the user or administrator to have installed an ISO C-compliant compiler. A pure version-based approach would not detect the presence of the ISO C compiler, but a feature-testing approach would be able to discover the ISO C compiler the user had installed. 

https://www.gnu.org/software/autoconf/autoconf.html[+https://www.gnu.org/software/autoconf/autoconf.html+]

https://en.wikipedia.org/wiki/Autoconf[+https://en.wikipedia.org/wiki/Autoconf+]

The GNU Autoconf Archive is a collection of more than 500 macros for GNU Autoconf that have been contributed as free software by friendly supporters of the cause from all over the Internet. Every single one of those macros can be re-used without imposing any restrictions whatsoever on the licensing of the generated configure script. In particular, it is possible to use all those macros in configure scripts that are meant for non-free software. This policy is unusual for a Free Software Foundation project. The FSF firmly believes that software ought to be free, and software licenses like the GPL are specifically designed to ensure that derivative work based on free software must be free as well. In case of Autoconf, however, an exception has been made, because Autoconf is at such a pivotal position in the software development tool chain that the benefits from having this tool available as widely as possible outweigh the disadvantage that some authors may choose to use it, too, for proprietary software. 

https://www.gnu.org/software/autoconf-archive/[+https://www.gnu.org/software/autoconf-archive/+]

https://invisible-island.net/autoconf/autoconf.html[+https://invisible-island.net/autoconf/autoconf.html+]

automake
^^^^^^^^

In software development, GNU Automake is a programming tool to automate parts of the compilation process. It eases usual compilation problems. For example, it points to needed dependencies.

It automatically generates one or more Makefile.in from files called Makefile.am. Each Makefile.am contains, among other things, useful variable definitions for the compiled software, such as compiler and linker flags, dependencies and their versions, etc. The generated Makefile.ins are portable and compliant with the Makefile conventions in the GNU Coding Standards, and may be used by configure scripts to generate a working Makefile.

Automake aims to allow the programmer to write a makefile in a higher-level language, rather than having to write the whole makefile manually. In simple cases, it suffices to give:

* A line that declares the name of the program to build
* A list of source files
* A list of command-line options to be passed to the compiler (for example, in which directories header files will be found)
* A list of command-line options to be passed to the linker (which libraries the program needs and in what directories they are to be found)

Automake also takes care of automatically generating the dependency information,[5] so that when a source file is modified, the next invocation of the make command will know which source files need to be recompiled. If the compiler allows it, Automake tries to make the dependency system dynamic: whenever a source file is compiled, that file's dependencies are updated by asking the compiler to regenerate the file's dependency list. In other words, dependency tracking is a side effect of the compilation process. 

Automake is written in Perl and must be used with GNU Autoconf.[2] Automake contains the following commands:

* aclocal
* automake

aclocal, however, is a general-purpose program that can be useful to autoconf users. The GNU Compiler Collection, for example, uses aclocal even though its makefile is hand written. 

https://www.gnu.org/software/automake/[+https://www.gnu.org/software/automake/+]

https://en.wikipedia.org/wiki/Automake[+https://en.wikipedia.org/wiki/Automake+]

autotoolset
^^^^^^^^^^^

The Autotoolset package (former autotools package) complements the GNU build system by providing the following features:

* Automatic generation of legal notices that are needed in order to apply the GNU GPL license
* Automatic generation of directory trees for new software packages, such that they conform to the GNITS standard (more or less).
* Some rudimentary portability framework for C++ programs. There is a lot of room for improvement here, in the future. Also a framework for embedding text into your executable and handling include files accross multiple directories.
* Support for writing portable software that uses both Fortran and C++.
* Additional support for writing software documentation in Texinfo, but also in LaTeX.
* A manual introducing both Autotools and the GNU build system in a unified task-oriented manner.

http://autotoolset.sourceforge.net/[+http://autotoolset.sourceforge.net/+]

libtool
^^^^^^^

Libtool helps manage the creation of static and dynamic libraries on various Unix-like operating systems. Libtool accomplishes this by abstracting the library-creation process, hiding differences between various systems (e.g. Linux systems vs. Solaris).

GNU Libtool is designed to simplify the process of compiling a computer program on a new system, by "encapsulating both the platform-specific dependencies, and the user interface, in a single script".[3] When porting a program to a new system, Libtool is designed so the porter need not read low-level documentation for the shared libraries to be built, rather just run a configure script (or equivalent).

https://www.gnu.org/software/libtool/[+https://www.gnu.org/software/libtool/+]

https://en.wikipedia.org/wiki/GNU_Libtool[+https://en.wikipedia.org/wiki/GNU_Libtool+]

http://metastatic.org/text/libtool.html[+http://metastatic.org/text/libtool.html+]

GNU make
^^^^^^^^

GNU Make is a tool which controls the generation of executables and other non-source files of a program from the program's source files.

Make gets its knowledge of how to build your program from a file called the makefile, which lists each of the non-source files and how to compute it from other files. When you write a program, you should write a makefile for it, so that it is possible to use Make to build and install the program.

The capabilities of GNU make include:

* Make enables the end user to build and install your package without knowing the details of how that is done -- because these details are recorded in the makefile that you supply.
* Make figures out automatically which files it needs to update, based on which source files have changed. It also automatically determines the proper order for updating files, in case one non-source file depends on another non-source file.
*
* As a result, if you change a few source files and then run Make, it does not need to recompile all of your program. It updates only those non-source files that depend directly or indirectly on the source files that you changed.
* Make is not limited to any particular language. For each non-source file in the program, the makefile specifies the shell commands to compute it. These shell commands can run a compiler to produce an object file, the linker to produce an executable, ar to update a library, or TeX or Makeinfo to format documentation.
* Make is not limited to building a package. You can also use Make to control installing or deinstalling a package, generate tags tables for it, or anything else you want to do often enough to make it worth while writing down how to do it.

https://www.gnu.org/software/make/[+https://www.gnu.org/software/make/+]

Gnumeric
~~~~~~~~

Gnumeric is a spreadsheet program that is part of the GNOME Free Software Desktop Project.
It is intended to replace other spreadsheet programs such as Microsoft Excel (proprietary).

Gnumeric has the ability to import and export data in several file formats, including CSV, Microsoft Excel (write support for the more recent .xlsx format is incomplete), Microsoft Works spreadsheets (*.wks), HTML, LaTeX, Lotus 1-2-3, OpenDocument and Quattro Pro; its native format is the Gnumeric file format (.gnm or .gnumeric), an XML file compressed with gzip. It includes all of the spreadsheet functions of the North American edition of Microsoft Excel and many functions unique to Gnumeric. Pivot tables and Visual Basic for Applications macros are not yet supported.

Gnumeric's accuracy has helped it to establish a niche for statistical analysis and other scientific tasks. For improving the accuracy of Gnumeric, the developers are cooperating with the R Project.

Gnumeric has a different interface for the creation and editing of graphs from other spreadsheet software. For editing a graph, Gnumeric displays a window where all the elements of the graph are listed. Other spreadsheet programs typically require the user to select the individual elements of the graph in the graph itself in order to edit them.

http://www.gnumeric.org/[+http://www.gnumeric.org/+]

http://www.troubleshooters.com/lpm/200306/200306.htm[+http://www.troubleshooters.com/lpm/200306/200306.htm+]

GNUnet
~~~~~~

GNUnet is a software framework for decentralized, peer-to-peer networking and an official GNU package. The framework offers link encryption, peer discovery, resource allocation, communication over many transports (such as TCP, UDP, HTTP, HTTPS, WLAN and Bluetooth) and various basic peer-to-peer algorithms for routing, multicast and network size estimation.

GNUnet's basic network topology is that of a mesh network. GNUnet includes a distributed hash table (DHT) which is a randomized variant of Kademlia that can still efficiently route in small-world networks. GNUnet offers a "F2F topology" option for restricting connections to only the users' trusted friends. The users' friends' own friends (and so on) can then indirectly exchange files with the users' computer, never using its IP address directly.

GNUnet uses Uniform resource identifiers (not approved by IANA, although an application has been made). GNUnet URIs consist of two major parts: the module and the module specific identifier. A GNUnet URI is of form gnunet://module/identifier where module is the module name and identifier is a module specific string.

The primary codebase is written in C, but with gnunet-java there is an effort to produce an API for developing extensions in Java. GNUnet is part of the GNU project. It has gained interest to the hacker community after the PRISM revelations.

GNUnet consists of several subsystems, of which essential ones are Transport and Core subsystems. Transport subsystem provides insecure link-layer communications, while Core provides peer discovery and encryption. On top of the core subsystem various applications are built.

GNUnet includes various P2P applications in the main distribution of the framework, including filesharing, chat and VPN; additionally, a few external projects (such as secushare) are also extending the GNUnet infrastructure. 

https://gnunet.org/[+https://gnunet.org/+]

https://en.wikipedia.org/wiki/GNUnet[+https://en.wikipedia.org/wiki/GNUnet+]

GNU toolchain
~~~~~~~~~~~~~

The GNU toolchain is a broad collection of programming tools produced by the GNU Project. These tools form a toolchain (a suite of tools used in a serial manner) used for developing software applications and operating systems.
The GNU toolchain plays a vital role in development of Linux, some BSD systems, and software for embedded systems. Parts of the GNU toolchain are also directly used with or ported to other platforms.

Projects included in the GNU toolchain are:

* GNU make: an automation tool for compilation and build
* GNU Compiler Collection (GCC): a suite of compilers for several programming languages
* GNU C Library (glibc): core C library including headers, libraries, and dynamic loader
* GNU Binutils: a suite of tools including linker, assembler and other tools
* GNU Bison: a parser generator, often used with the Flex lexical analyser
* GNU m4: an m4 macro processor
* GNU Debugger (GDB): a code debugging tool
* GNU build system (autotools): Autoconf, Automake and Libtool

https://en.wikipedia.org/wiki/GNU_toolchain[+https://en.wikipedia.org/wiki/GNU_toolchain+]

gobot
~~~~~

A framework using the Go programming language for robotics, physical computing, and the IoT.
It provides a simple, yet powerful way to create solutions that incorporate multiple, different hardware devices at the same time.

https://github.com/hybridgroup/gobot/[+https://github.com/hybridgroup/gobot/+]

https://gobot.io/[+https://gobot.io/+]

https://github.com/hybridgroup/gort[+https://github.com/hybridgroup/gort+]

https://cylonjs.com/[+https://cylonjs.com/+]

http://artoo.io/[+http://artoo.io/+]

Golly
~~~~~

Golly is an open source, cross-platform application for exploring Conway's Game of Life and many other types of cellular automata.

http://golly.sourceforge.net/[+http://golly.sourceforge.net/+]

go-ncs
~~~~~~

Go language bindings for the Movidius Neural Compute Stick (NCS).

https://github.com/hybridgroup/go-ncs[+https://github.com/hybridgroup/go-ncs+]

https://software.intel.com/en-us/neural-compute-stick[+https://software.intel.com/en-us/neural-compute-stick+]

https://cybernetist.com/2018/10/07/edge-computing-with-go-and-intel-movidius-neural-compute-stick/[+https://cybernetist.com/2018/10/07/edge-computing-with-go-and-intel-movidius-neural-compute-stick/+]

https://github.com/movidius/ncappzoo[+https://github.com/movidius/ncappzoo+]

GPlates
~~~~~~~

GPlates offers a novel combination of interactive plate-tectonic reconstructions, geographic information system (GIS) functionality and raster data visualisation. GPlates enables both the visualisation and the manipulation of plate-tectonic reconstructions and associated data through geological time. GPlates runs on Windows, Linux and MacOS X. GPlates has an online user manual.

The GPlates Web Portal is a gateway to a series of web pages for the interactive visualisation of cutting-edge geoscience datasets, all possible within freely available web browsers.

The GPlates Python library (pyGPlates) enables access to GPlates functionality via the Python programming language. It allows users to use GPlates in a programmatic way and hence provides much more flexibility than the GPlates desktop interface can offer.

http://www.gplates.org/[+http://www.gplates.org/+]

http://www.gplates.org/docs.html[+http://www.gplates.org/docs.html+]

http://portal.gplates.org/[+http://portal.gplates.org/+]

https://github.com/GPlates[+https://github.com/GPlates+]

GrADS
~~~~~

The Grid Analysis and Display System (GrADS) is an interactive desktop tool that is used for easy access, manipulation, and visualization of earth science data. GrADS has two data models for handling gridded and station data. GrADS supports many data file formats, including binary (stream or sequential), GRIB (version 1 and 2), NetCDF, HDF (version 4 and 5), and BUFR (for station data). GrADS has been implemented worldwide on a variety of commonly used operating systems and is freely distributed over the Internet.

GrADS uses a 5-Dimensional data environment: the four conventional dimensions (longitude, latitude, vertical level, and time) plus an optional 5th dimension for grids that is generally implemented but designed to be used for ensembles. Data sets are placed within the 5-D space by use of a data descriptor file. GrADS handles grids that are regular, non-linearly spaced, gaussian, or of variable resolution. Data from different data sets may be graphically overlaid, with correct spatial and time registration. Operations are executed interactively by entering FORTRAN-like expressions at the command line. A rich set of built-in functions are provided, but users may also add their own functions as external routines written in any programming language.

Data may be displayed using a variety of graphical techniques: line and bar graphs, scatter plots, smoothed contours, shaded contours, streamlines, wind vectors, grid boxes, shaded grid boxes, and station model plots. Graphics may be output in PostScript or image formats. GrADS provides geophysically intuitive defaults, but the user has the option to control all aspects of graphics output.

GrADS has a programmable interface (scripting language) that allows for sophisticated analysis and display applications. Use scripts to display buttons and dropmenus as well as graphics, and then take action based on user point-and-clicks. GrADS can be run in batch mode, and the scripting language facilitates using GrADS to do long overnight batch jobs.

http://cola.gmu.edu/grads/[+http://cola.gmu.edu/grads/+]

http://opengrads.org/[+http://opengrads.org/+]

GraalVM
~~~~~~~

GraalVM is a universal virtual machine for running applications written in JavaScript, Python, Ruby, R, JVM-based languages like Java, Scala, Clojure, Kotlin, and LLVM-based languages such as C and Cxx.

GraalVM removes the isolation between programming languages and enables interoperability in a shared runtime. It can run either standalone or in the context of OpenJDK, Node.js, Oracle Database, or MySQL. 

Zero overhead interoperability between programming languages allows you to write polyglot applications and select the best language for your task. Native images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications.

GraalVM can run in the context of Node.js by replacing V8 with GraalVM for executing JavaScript. The main benefits of doing so is to enable polyglot applications (e.g., use Java, R, or Python libraries), run Node.js with large heap configurations and Java’s garbage collectors, or use GraalVM’s interoperability to define data structures in C/Cxx and use them from JavaScript.

GraalVM enables the use of existing Java libraries or Java frameworks (like Spark or Flink) directly from Node.js. Also, one can use for example R or Python for data science or plotting directly from a JavaScript application. 

http://www.graalvm.org/[+http://www.graalvm.org/+]

https://github.com/oracle/graal[+https://github.com/oracle/graal+]

https://gist.github.com/smarr/d1f8f2101b5cc8e14e12[+https://gist.github.com/smarr/d1f8f2101b5cc8e14e12+]

https://medium.com/graalvm/graalvm-ten-things-12d9111f307d[+https://medium.com/graalvm/graalvm-ten-things-12d9111f307d+]

GraphBLAS
~~~~~~~~~

The GraphBLAS Forum is an open effort to define standard building blocks for graph algorithms in the language of linear algebra.

We believe that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. We believe that it is critical to move quickly and define such a standard, thereby freeing up researchers to innovate and diversify at the level of higher level algorithms and graph analytics applications. This effort was inspired by the Basic Linear Algebra Subprograms (BLAS) of dense Linear Algebra, and hence our working name for this standard is “the GraphBLAS”.

A key insight behind this work is that when a graph is represented by a sparse incidence or adjacency matrix, sparse matrix-vector multiplication is a step of breadth first search. By generalizing the pair of scalar operations involved in the linear algebra computations to define a semiring, we can extend the range of these primitives to support a wide range of parallel graph algorithms. 

http://graphblas.org/index.php[+http://graphblas.org/index.php+]

https://people.eecs.berkeley.edu/\~aydin/CombBLAS/html/index.html[+https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html+]

https://github.com/cmu-sei/gbtl[+https://github.com/cmu-sei/gbtl+]

GBTL
^^^^

This is Version 2.0 of the Cxx implementation that is mathematically equivalent to the GraphBLAS C API. Unlike the first version (which predates the GraphBLAS C API Specification), this only contains the 'sequential' backend (in the platforms directory) written for a single CPU. Support for GPUs that was in version 1.0 is currently not available but can be accessed.

The implementation of the sequential backend is currently focused on correctness over performance. The project also contains implementations of many common graph algorithms using the Cxx API.

https://github.com/cmu-sei/gbtl[+https://github.com/cmu-sei/gbtl+]

GraphIt
~~~~~~~

GraphIt is a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of edge traversal and vertex data layout optimizations.

http://graphit-lang.org/[+http://graphit-lang.org/+]

https://github.com/GraphIt-DSL/graphit[+https://github.com/GraphIt-DSL/graphit+]

Graphite
~~~~~~~~

Graphite is a research platform for computer graphics, 3D modeling and numerical geometry.
Graphite is an experimental 3D modeler, built in top of the Geogram programming library. It has data structures and efficient OpenGL visualization for pointsets, surfacic meshes (triangles and polygons), volumetric meshes (tetrahedra and hybrid meshes). It has state-of-the-art mesh repair, remeshing, reconstruction algorithms. It also has an interface to the Tetgen tetrahedral mesh generator

http://alice.loria.fr/index.php?option=com_content&view=article&id=22[+http://alice.loria.fr/index.php?option=com_content&view=article&id=22+]

https://gforge.inria.fr/frs/?group_id=1465[+https://gforge.inria.fr/frs/?group_id=1465+]

http://alice.loria.fr/software/graphite/doc/html/[+http://alice.loria.fr/software/graphite/doc/html/+]

GREB
~~~~

The Monash simple climate model is based on the Globally Resolved Energy
Balance (GREB) model, which is a climate model published by Dommenget and
Floeter [2011] in the international peer review science journal Climate
Dynamics. The model simulates most of the main physical processes in the
climate system in a very simplistic way and therefore allows very fast and
simple climate model simulations. It can compute global climate simulations of
one year in about 1 second on a normal PC computer. Despite its simplicity the
model simulates the climate response to external forcings, such as doubling of
the CO2 concentrations very realistically (similar to state of the art climate
models).

https://github.com/alex-robinson/greb-ucm[+https://github.com/alex-robinson/greb-ucm+]

http://users.monash.edu.au/\~dietmard/content/GREB/GREB_model.html[+http://users.monash.edu.au/~dietmard/content/GREB/GREB_model.html+]

http://maths-simpleclimatemodel-dev.maths.monash.edu/[+http://maths-simpleclimatemodel-dev.maths.monash.edu/+]

https://blogs.monash.edu/climate/2012/12/13/the-monash-simple-climate-model/[+https://blogs.monash.edu/climate/2012/12/13/the-monash-simple-climate-model/+]

http://users.monash.edu.au/\~dietmard/papers/dommenget.and.floeter.greb.paper.cdym2011.pdf[+http://users.monash.edu.au/~dietmard/papers/dommenget.and.floeter.greb.paper.cdym2011.pdf+]

https://www.geosci-model-dev.net/12/425/2019/[+https://www.geosci-model-dev.net/12/425/2019/+]

Greenplum Database
~~~~~~~~~~~~~~~~~~

Greenplum Database (GPDB) is an advanced, fully featured, open source data warehouse, based on PostgreSQL. It provides powerful and rapid analytics on petabyte scale data volumes. Uniquely geared toward big data analytics, Greenplum Database is powered by the world’s most advanced cost-based query optimizer delivering high analytical query performance on large data volumes.

GPORCA is a cost-based optimizer which is used by Greenplum Database in conjunction with the PostgreSQL planner. It is also known as just ORCA, and Pivotal Query Optimizer (PQO). The code for GPORCA resides in a separate repository, below are steps outlining how to build Greenplum with GPORCA enabled.

https://github.com/greenplum-db/gpdb[+https://github.com/greenplum-db/gpdb+]

https://greenplum.org/[+https://greenplum.org/+]

GRIB
~~~~

GRIB (GRIdded Binary or General Regularly-distributed Information in Binary form[1]) is a concise data format commonly used in meteorology to store historical and forecast weather data. It is standardized by the World Meteorological Organization's Commission for Basic Systems, known under number GRIB FM 92-IX, described in WMO Manual on Codes No.306. Currently there are three versions of GRIB. Version 0 was used to a limited extent by projects such as TOGA, and is no longer in operational use. The first edition (current sub-version is 2) is used operationally worldwide by most meteorological centers, for Numerical Weather Prediction output (NWP). A newer generation has been introduced, known as GRIB second edition, and data is slowly changing over to this format. Some of the second-generation GRIB are used for derived product distributed in Eumetcast of Meteosat Second Generation. Another example is the NAM (North American Mesoscale) model. 

GRIB files are a collection of self-contained records of 2D data, and the individual records stand alone as meaningful data, with no references to other records or to an overall schema. So collections of GRIB records can be appended to each other or the records separated.

Each GRIB record has two components - the part that describes the record (the header), and the actual binary data itself. The data in GRIB-1 are typically converted to integers using scale and offset, and then bit-packed. GRIB-2 also has the possibility of compression. 

https://en.wikipedia.org/wiki/GRIB[+https://en.wikipedia.org/wiki/GRIB+]

cfgrib
^^^^^^

A Python interface to map GRIB files to the NetCDF Common Data Model following the CF Convention using ecCodes.

Python interface to map GRIB files to the Unidata's Common Data Model v4 following the CF Conventions. The high level API is designed to support a GRIB engine for xarray and it is inspired by netCDF4-python and h5netcdf. Low level access and decoding is performed via the ECMWF ecCodes library.  The features include:

* enables the engine='cfgrib' option to read GRIB files with xarray,

* reads most GRIB 1 and 2 files

* supports all modern versions of Python 3.7, 3.6, 3.5 and 2.7, plus PyPy and PyPy3,

* works on most Linux distributions and MacOS, the ecCodes C-library is the only system dependency,

* PyPI package with no install time build (binds with CFFI ABI mode),

* reads the data lazily and efficiently in terms of both memory usage and disk access,

* allows larger-than-memory and distributed processing via dask.

https://github.com/ecmwf/cfgrib[+https://github.com/ecmwf/cfgrib+]

degrib
^^^^^^

The National Digital Forecast Database (NDFD) is a database put together by the National Weather Service (NWS) to provide forecasts of sensible weather elements (e.g., cloud cover, maximum temperature) on a seamless grid.  The NDFD is currently given out to the public as a GRIB2 file.

The problem, when the NDFD first came out, was that the only way to decode a GRIB2 message was to use the GRIB2 library.  This required the user to write code, and then refer to the WMO's specifications to decipher the "meta" data (e.g., variable type, variable unit, reference date time, valid date time, etc).  This defeated the original purpose of the NDFD which was to make the digital data easy to use by the public.

To resolve this the Meteorological Development Laboratory (MDL) created a driver for the GRIB2 library.  The driver, known as "degrib" (aka "NDFD GRIB2 decoder"), was originally designed to:

* Provide an example of how to use the GRIB2 Decoder library by making documented source code available to users.  Since the time that NDFD came out, NCEP has created a more complete GRIB2 library.  Since people were familiar with the degrib driver, MDL continued to maintain the degrib driver program, but has transitioned to using the NCEP library.
     
* Be able to convert from GRIB2 to selected file formats such as ESRI shapefiles (.shp), ASCII comma sepearated files (.csv), NetCDF files, and binary float files (.flt) (useful in conjunction with GrADS, ESRI ArcGIS, or ESRI ArcView 3 + Spatial Analyst extension).
     
* Enable users to understand the "meta" data produced by the GRIB2 library without needing to refer to the WMO's specifications by creating an ASCII (.txt) file that does the necessary lookups in the WMO's GRIB2 specification tables.

To encourage the use of the NDFD, degrib was designed to run either via a Graphical User Interface (GUI), or from the command line, giving the user the flexibility to run the program manually or via a script or program. 

https://www.weather.gov/mdl/degrib_home[+https://www.weather.gov/mdl/degrib_home+]

ecCodes
^^^^^^^

ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats:

* WMO FM-92 GRIB edition 1 and edition 2
* WMO FM-94 BUFR edition 3 and edition 4 
* WMO GTS abbreviated header (only decoding).

A useful set of command line tools provide quick access to the messages. C, Fortran 90 and Python interfaces provide access to the main ecCodes functionality.

ecCodes is an evolution of GRIB-API.  It is designed to provide the user with a simple set of functions to access data from several formats with a key/value approach.

For GRIB encoding and decoding, the GRIB-API functionality is provided fully in ecCodes with only minor interface and behaviour changes. Interfaces for C, Fortran 90 and Python are all maintained as in GRIB-API.  However, the GRIB-API Fortran 77 interface is no longer available.

In addition, a new set of functions with the prefix "codes_" is provided to operate on all the supported message formats. These functions have the same interface and behaviour as the "grib_" functions. 

A selection of GRIB-API tools has been included in ecCodes (ecCodes GRIB tools), while new tools are available for the BUFR (ecCodes BUFR tools) and GTS formats. The new tools have been developed to be as similar as possible to the existing GRIB-API tools maintaining, where possible, the same options and behaviour. A significant difference compared with GRIB-API tools is that bufr_dump produces output in JSON format suitable for many web based applications.

https://confluence.ecmwf.int//display/ECC/ecCodes+Home[+https://confluence.ecmwf.int//display/ECC/ecCodes+Home+]

https://github.com/ecmwf/eccodes-python[+https://github.com/ecmwf/eccodes-python+]

GRIB-API
^^^^^^^^

The ECMWF GRIB-API is an application program interface accessible from C, FORTRAN and Python programs developed for encoding and decoding WMO FM-92 GRIB edition 1 and edition 2 messages. A useful set of command line tools is also provided to give quick access to GRIB messages.

Support for GRIB-API has been discontinued as 0f 2019.
The ecCOdes packages has superseded it.

https://confluence.ecmwf.int/display/GRIB/Home[+https://confluence.ecmwf.int/display/GRIB/Home+]

grib2json
^^^^^^^^^

A command line utility that decodes GRIB2 files as JSON.

This utility uses the netCDF-Java GRIB decoder, part of the THREDDS project by University Corporation for Atmospheric Research/Unidata.

https://github.com/cambecc/grib2json[+https://github.com/cambecc/grib2json+]

PyGRIB
^^^^^^

Python module for reading and writing GRIB (editions 1 and 2) files. GRIB is the World Meterological Organization standard for distributing gridded data. The module is a python interface to the GRIB API C library from the European Centre for Medium-Range Weather Forecasts (ECMWF).

https://jswhit.github.io/pygrib/docs/index.html[+https://jswhit.github.io/pygrib/docs/index.html+]

https://github.com/jswhit/pygrib[+https://github.com/jswhit/pygrib+]

wgrib2
^^^^^^

wgrib2 is not simple upgrade of wgrib for grib2 files. wgrib can slice and dice grib1 files. wgrib2 is more like four drawers of kitchen utensils as well as the microwave and blender. This kitchen miracle was done by a more developer friendly design and the contributions of many people. Some functionality include,

* inventory and read grib2 files
* create subsets
* create regional subsets by cookie cutter or projections
* export to ieee, text, binary, CSV, netcdf and mysql
* write of new grib2 fields
* parallel processing by using threads (OpenMP)
* parallel processing by flow-based programming
* fortran and C interface 

OpenMP is used to speed up wgrib2 by running loops over multiple cores. With gcc and icc, OpenMP is on by default.

http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/index.html[+http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/index.html+]

http://www.nco.ncep.noaa.gov/pmb/codes/GRIB2/[+http://www.nco.ncep.noaa.gov/pmb/codes/GRIB2/+]

http://www.cpc.ncep.noaa.gov/products/wesley/wgrib.html[+http://www.cpc.ncep.noaa.gov/products/wesley/wgrib.html+]

zygrib
^^^^^^

Visualization of weather data from files in GRIB Format 1 and 2.

http://www.zygrib.org/[+http://www.zygrib.org/+]

gridded
~~~~~~~

A single API for accessing / working with gridded model results on multiple grid types.
The goal of this package is to present a single way to work with results from ANY model -- regardless of what type of grid it was computed on. In particular:

* Regular Structured Grids (CF Conventions), with API embedded in Iris and to some degree in xarray
* Unstructured Grids (CF + UGRID Conventions), with nascent API in pyugrid
* Staggered Grids (CF + SGRID Conventions) with nascent API in pysgrid

gridded has been developed because a number of us need to work with multiple model types, and have found ourselves writing a lot of custom code for each type. In particular, inter-comparison of results is an ugly process. To preserve the integrity of the results, it's best to NOT interpolate on to a common grid. gridded lets one work with multiple model types with the same API, while preserving the native grid as much as possible.

https://github.com/NOAA-ORR-ERD/gridded[+https://github.com/NOAA-ORR-ERD/gridded+]

GridFTP
~~~~~~~

GridFTP is a high-performance, secure, reliable data transfer protocol
optimized for high-bandwidth wide-area networks. The GridFTP protocol is based
on FTP, the highly-popular Internet file transfer protocol. We have selected a
set of protocol features and extensions defined already in IETF RFCs and added
a few additional features to meet requirements from current data grid
projects.

GridFTP also addresses the problem of incompatibility between storage and
access systems. Previously, each data provider would make their data available
in their own specific way, providing a library of access functions. This made
it difficult to obtain data from multiple sources, requiring a different
access method for each, and thus dividing the total available data into
partitions. GridFTP provides a uniform way of accessing the data, encompassing
functions from all the different modes of access, building on and extending
the universally accepted FTP standard. FTP was chosen as a basis for it
because of its widespread use, and because it has a well defined architecture
for extensions to the protocol (which may be dynamically discovered).

http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/[+http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/+]

http://en.wikipedia.org/wiki/GridFTP[+http://en.wikipedia.org/wiki/GridFTP+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

GridPACK
~~~~~~~~

GridPACK™ is a software framework consisting of a set of modules designed to simplify the development of programs that model the power grid and run on parallel, high performance computing platforms. The modules are available as a library and consist of components for setting up and distributing power grid networks, support for modeling the behavior of individual buses and branches in the network, converting the network models to the corresponding algebraic equations, and parallel routines for manipulating and solving large algebraic systems. Additional modules support input and output as well as basic profiling and error management.

The GridPACK framework will simplify parallel power grid application development by eliminating many of the index transformations and calculations required to distribute data in parallel applications and also to move data from one processor to another as the calculation proceeds. Application writers can focus primarily on the physics of their problem without worrying about much of the baggage associated with writing applications in parallel. GridPACK supplies core functionalities that will simplify parallel code development. These include a generic framework for setting up power grid networks and applying whatever physical properties to the bus and branches that developers want, functionality for distributing the networks across processors and managing the exchange of network data between processors, generic functionality for mapping data from the networks to matrices and vectors, and libraries of parallel matrix and vector operations, linear and non-linear solvers and preconditioners for solving the equations generated by power grid models. The framework will also supply routines for generating networks from standard format input files that describe power grid networks (e.g. PTI files), writing data from the network back out to files, and additional functionality for profiling and error management. The modules in the framework are being designed to compartmentalize functionality in a way that will simplify future code maintenance. This compartmentalization will also allow modules to be updated more easily without requiring modification of other modules or of the applications. 

https://github.com/GridOPTICS/GridPACK[+https://github.com/GridOPTICS/GridPACK+]

https://www.gridpack.org/wiki/index.php/Main_Page[+https://www.gridpack.org/wiki/index.php/Main_Page+]

GSI
~~~

The Gridpoint Statistical Interpolation (GSI) system is a unified data assimilation (DA) system for both global and regional applications. It was initially developed by the National Centers for Environmental Prediction (NCEP) Environmental Modeling Center (EMC) as a next generation analysis system based on the then operational Spectral Statistical Interpolation (SSI) analysis system (; ; ). Instead of being constructed in spectral space like the SSI, the GSI is constructed in physical space and is designed to be a flexible, state-of-art system that is efficient on available parallel computing platforms. Starting with a three-dimensional variational (3DVar) data assimilation technique, the current GSI can be run as a data assimilation system of 2DVar (for surface data analysis), 3DVar, 3D ensemble-variational (3D EnVar), 4D EnVar, 3D/4D hybrid EnVar, or 4DVar (if coupled with an adjoint model from a GSI supported forecast system).

The GSI features include:

* Combined with an ensemble system, GSI can be used as an 3D/4D ensemble-variational hybrid data assimilation system.
* GSI features capabilities for observation sensitivity calculation. Coupled with its global model, this feature has been used by NASA for its operational data impact study.
* GSI can be used as observation observer to provide O-B for EnKF system or other data analysis systems.

https://dtcenter.org/com-GSI/users/index.php[+https://dtcenter.org/com-GSI/users/index.php+]

://ral.ucar.edu/projects/code-management-and-community-support-for-gsi-and-enkf[+https://ral.ucar.edu/projects/code-management-and-community-support-for-gsi-and-enkf+]

EnKF
^^^^

The community EnKF system is a Monte-Carlo algorithm for data assimilation that uses an ensemble of short-term forecasts to estimate the background-error covariance in the Kalman Filter. It is designed to be flexible, state-of-art, and run efficiently on various parallel computing platforms. The EnKF system is in the public domain and is freely available for community use. 

The Developmental Testbed Center (DTC) maintains and supports a community version of the EnKF system (currently Version 1.3) as well as the community version of the Grid-point Statistical Interpolation (GSI) (now at Version 3.7). The testing and support of this EnKF system at the DTC includes both regional numerical weather prediction (NWP) applications coupled with the Weather Research and Forecasting (WRF) Model, as well as global NWP applications.

The EnKF uses the observation operators in the GSI system to transform model variables to observed variables in observation space. Therefore, the types of observations available for use in the EnKF match those for the GSI. EnKF Version 1.3 has been tested to work with the GSI Version 3.7.

https://dtcenter.org/EnKF/users/[+https://dtcenter.org/EnKF/users/+]

GUDHI
~~~~~

The central goal of the project is to settle the algorithmic foundations of geometry understanding in dimensions higher than 3.

The need to understand geometric structures is ubiquitous in science and has become an essential part of scientific computing and data analysis. Many applications in physics, biology, and engineering involve a variety of higher dimensional spaces such as phase space in particle physics, invariant manifolds in dynamical systems, configuration spaces of mechanical systems, energy landscapes of molecules.

Understanding the geometry of such spaces faces many challenges: choosing appropriate representations of highly non-linear shapes, bypassing the curse of dimensionality, inferring stable properties from data, and providing software that scale with real applications. The GUDHI project aims at advancing the state of the art by developing tight and long-standing interactions between mathematical research, algorithmic design and advanced software development.

The GUDHI library is a generic open source Cxx library, with a Python interface, for Topological Data Analysis (TDA) and Higher Dimensional Geometry Understanding. The library offers state-of-the-art data structures and algorithms to construct simplicial complexes and compute persistent homology.

The library comes with data sets, demos, examples and test suites.

The current release of the GUDHI library includes:

* Data structures to represent, construct and manipulate simplicial complexes.
* Simplification of simplicial complexes by edge contraction.
* Algorithms to compute persistent homology and bottleneck distance.

https://project.inria.fr/gudhi/[+https://project.inria.fr/gudhi/+]

http://gudhi.gforge.inria.fr/[+http://gudhi.gforge.inria.fr/+]

http://gudhi.gforge.inria.fr/python/latest/[+http://gudhi.gforge.inria.fr/python/latest/+]

https://gforge.inria.fr/frs/?group_id=3865[+https://gforge.inria.fr/frs/?group_id=3865+]

https://anaconda.org/conda-forge/gudhi[+https://anaconda.org/conda-forge/gudhi+]

https://project.inria.fr/gudhi/files/2011/12/main-synopsis.pdf[+https://project.inria.fr/gudhi/files/2011/12/main-synopsis.pdf+]

TDA
^^^

Tools for the statistical analysis of persistent homology and for density clustering. For that, this package provides an R interface for the efficient algorithms of the Cxx libraries GUDHI, Dionysus and PHAT.

https://cran.r-project.org/web/packages/TDA/index.html[+https://cran.r-project.org/web/packages/TDA/index.html+]

http://www.mrzv.org/software/dionysus/[+http://www.mrzv.org/software/dionysus/+]

https://bitbucket.org/phat-code/phat/[+https://bitbucket.org/phat-code/phat/+]

Guile
~~~~~

GNU Ubiquitous Intelligent Language for Extensions[2] (GNU Guile) is the preferred extension language system for the GNU Project,[3] which features an implementation of the programming language Scheme. Its first version was released in 1993.[1] In addition to large parts of Scheme standards, Guile Scheme includes modularized extensions for many different programming tasks.

Guile Scheme is a general-purpose, high-level programming language whose flexibility allows expressing concepts in fewer lines of code than would be possible in languages such as C. For example, its hygienic macro system allows adding domain specific syntax-elements without modifying Guile. Guile implements the Scheme standard R5RS, most of R6RS, several Scheme Requests for Implementation (SRFI), and many extensions of its own. 

The core idea of Guile Scheme is that "the developer implements critical algorithms and data structures in C or Cxx and exports the functions and types for use by interpreted code. The application becomes a library of primitives orchestrated by the interpreter, combining the efficiency of compiled code with the flexibility of interpretation."[8] Thus Guile Scheme (and other languages implemented by Guile) can be extended with new data types and subroutines implemented through the C API.

https://www.gnu.org/software/guile/[+https://www.gnu.org/software/guile/+]

8sync
^^^^^

8sync (pronounced "eight-sync") is an asynchronous programming library for GNU Guile. Based on the actor model, it makes use of delimited continuations to avoid a mess of callbacks resulting in clean, easy to read non-blocking code.

https://www.gnu.org/software/8sync/[+https://www.gnu.org/software/8sync/+]

http://savannah.gnu.org/projects/8sync[+http://savannah.gnu.org/projects/8sync+]

Artanis
^^^^^^^

GNU Artanis is a framework for web authoring - for instance, generating HTML pages dynamically. In other words, a WAF (Web Application Framework).

https://web-artanis.com/about.html[+https://web-artanis.com/about.html+]

https://www.gnu.org/software/artanis/[+https://www.gnu.org/software/artanis/+]

GVR
~~~

GVR (Global View Resilience) is a user-level library that enables portable, efficient, application-controlled resilience.  The primary target of GVR is HPC applications that require both extreme scalability and performance as well as resilience.  GVR's key approaches include independent versioning of application arrays, efficient partial or whole restoration, open resilience to maximize the number of errors that can be handled (minimize fail-stop occurrences).  Application knowledge can be exploited to control overhead, maximize error coverage, and maximize recoverable errors.

https://sites.google.com/site/uchicagolssg/lssg/research/gvr/downloads[+https://sites.google.com/site/uchicagolssg/lssg/research/gvr/downloads+]

http://people.cs.uchicago.edu/\~aachien/lssg/research/gvr/[+http://people.cs.uchicago.edu/~aachien/lssg/research/gvr/+]

https://journals-sagepub-com/doi/full/10.1177/1094342016664796[+https://journals-sagepub-com/doi/full/10.1177/1094342016664796+]

gym
~~~

Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.

The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.

https://gym.openai.com/docs/[+https://gym.openai.com/docs/+]

https://github.com/openai/gym[+https://github.com/openai/gym+]

#HHHH

h11
~~~

This is a little HTTP/1.1 library written from scratch in Python, heavily inspired by hyper-h2.

h11’s goal is to be a simple, robust, complete, and non-hacky implementation of the first “chapter” of the HTTP/1.1 spec: RFC 7230: HTTP/1.1 Message Syntax and Routing. That is, it mostly focuses on implementing HTTP at the level of taking bytes on and off the wire, and the headers related to that, and tries to be picky about spec conformance when possible. It doesn’t know about higher-level concerns like URL routing, conditional GETs, cross-origin cookie policies, or content negotiation. But it does know how to take care of framing, cross-version differences in keep-alive handling, and the “obsolete line folding” rule, and to use bounded time and space to process even pathological / malicious input, so that you can focus your energies on the hard / interesting parts for your application. And it tries to support the full specification in the sense that any useful HTTP/1.1 conformant application should be able to use h11.

This is a “bring-your-own-I/O” protocol library; like h2, it contains no I/O code whatsoever. This means you can hook h11 up to your favorite network API, and that could be anything you want: synchronous, threaded, asynchronous, or your own implementation of RFC 6214 – h11 won’t judge you. This is h11’s main feature compared to the current state of the art, where every HTTP library is tightly bound to a particular network framework, and every time a new network API comes along then someone has to start over reimplementing the entire HTTP stack from scratch.

https://github.com/python-hyper/h11[+https://github.com/python-hyper/h11+]

https://h11.readthedocs.io/en/latest/[+https://h11.readthedocs.io/en/latest/+]

https://lukasa.co.uk/2015/10/The_New_Hyper/[+https://lukasa.co.uk/2015/10/The_New_Hyper/+]

h5netcdf
~~~~~~~~

A Python interface for the netCDF4 file-format that reads and writes local or remote HDF5 files directly via h5py or h5pyd, without relying on the Unidata netCDF library.  This was created because:

* We've seen occasional reports of better performance with h5py than netCDF4-python, though in many cases performance is identical. For one workflow, h5netcdf was reported to be almost 4x faster than netCDF4-python.

* It has one less massive binary dependency (netCDF C). If you already have h5py installed, reading netCDF4 with h5netcdf may be much easier than installing netCDF4-Python.

* Anecdotally, HDF5 users seem to be unexcited about switching to netCDF -- hopefully this will convince them that the netCDF4 is actually quite sane!

* Finally, side-stepping the netCDF C library (and Cython bindings to it) gives us an easier way to identify the source of performance issues and bugs.

https://github.com/shoyer/h5netcdf[+https://github.com/shoyer/h5netcdf+]

HADOOP
~~~~~~

A collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Originally designed for computer clusters built from commodity hardware[3]—still the common use—it has also found use on clusters of higher-end hardware.[4][5] All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.

The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality,[6] where nodes manipulate the data they have access to. This allows the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking.
The base Apache Hadoop framework is composed of the following modules:

* Hadoop Common – contains libraries and utilities needed by other Hadoop modules;
* Hadoop Distributed File System (HDFS) – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster;
* Hadoop YARN – introduced in 2012 is a platform responsible for managing computing resources in clusters and using them for scheduling users' applications;[9][10]
* Hadoop MapReduce – an implementation of the MapReduce programming model for large-scale data processing

The term Hadoop has come to refer not just to the previously mentioned base modules and sub-modules, but also to the ecosystem,[11] or collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache ZooKeeper, Cloudera Impala, Apache Flume, Apache Sqoop, Apache Oozie, and Apache Storm.

https://en.wikipedia.org/wiki/Apache_Hadoop[+https://en.wikipedia.org/wiki/Apache_Hadoop+]

https://hadoop.apache.org/[+https://hadoop.apache.org/+]

http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html[+http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html+]

HBase
^^^^^

HBase is an open-source, non-relational, distributed database modeled after Google's Bigtable and written in Java. It is developed as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System), providing Bigtable-like capabilities for Hadoop. That is, it provides a fault-tolerant way of storing large quantities of sparse data (small amounts of information caught within a large collection of empty or unimportant data.

HBase features compression, in-memory operation, and Bloom filters on a per-column basis as outlined in the original Bigtable paper.[1] Tables in HBase can serve as the input and output for MapReduce jobs run in Hadoop, and may be accessed through the Java API but also through REST, Avro or Thrift gateway APIs. HBase is a column-oriented key-value data store and has been idolized widely because of its lineage with Hadoop and HDFS. HBase runs on top of HDFS and is well-suited for faster read and write operations on large datasets with high throughput and low input/output latency. 

https://en.wikipedia.org/wiki/Apache_HBase[+https://en.wikipedia.org/wiki/Apache_HBase+]

https://hbase.apache.org/[+https://hbase.apache.org/+]

Flume
^^^^^

Apache Flume is a distributed, reliable, and available software for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. 

https://en.wikipedia.org/wiki/Apache_Flume[+https://en.wikipedia.org/wiki/Apache_Flume+]

https://flume.apache.org/[+https://flume.apache.org/+]

Hive
^^^^

Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis.[2] Hive gives a SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.

Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides a SQL-like query language called HiveQL with schema on read and transparently converts queries to MapReduce, Apache Tez[8] and Spark jobs. All three execution engines can run in Hadoop's resource negotiator, YARN (Yet Another Resource Negotiator). To accelerate queries, it provides indexes, including bitmap indexes.

By default, Hive stores metadata in an embedded Apache Derby database, and other client/server databases like MySQL can optionally be used.

https://en.wikipedia.org/wiki/Apache_Hive[+https://en.wikipedia.org/wiki/Apache_Hive+]

http://hive.apache.org/[+http://hive.apache.org/+]

Impala
^^^^^^

Apache Impala is a query engine that runs on Apache Hadoop.  Impala brings scalable parallel database technology to Hadoop, enabling users to issue low-latency SQL queries to data stored in HDFS and Apache HBase without requiring data movement or transformation. Impala is integrated with Hadoop to use the same file and data formats, metadata, security and resource management frameworks used by MapReduce, Apache Hive, Apache Pig and other Hadoop software. 

https://en.wikipedia.org/wiki/Apache_Impala[+https://en.wikipedia.org/wiki/Apache_Impala+]

http://impala.apache.org/index.html[+http://impala.apache.org/index.html+]

Oozie
^^^^^

Apache Oozie is a server-based workflow scheduling system to manage Hadoop jobs.

Workflows in Oozie are defined as a collection of control flow and action nodes in a directed acyclic graph. Control flow nodes define the beginning and the end of a workflow (start, end, and failure nodes) as well as a mechanism to control the workflow execution path (decision, fork, and join nodes). Action nodes are the mechanism by which a workflow triggers the execution of a computation/processing task. Oozie provides support for different types of actions including Hadoop MapReduce, Hadoop distributed file system operations, Pig, SSH, and email. Oozie can also be extended to support additional types of actions. 

https://en.wikipedia.org/wiki/Apache_Oozie[+https://en.wikipedia.org/wiki/Apache_Oozie+]

https://oozie.apache.org/[+https://oozie.apache.org/+]

Phoenix
^^^^^^^

Apache Phoenix is an open source, massively parallel, relational database engine supporting OLTP for Hadoop using Apache HBase as its backing store. Phoenix provides a JDBC driver that hides the intricacies of the noSQL store enabling users to create, delete, and alter SQL tables, views, indexes, and sequences; insert and delete rows singly and in bulk; and query data through SQL.[1] Phoenix compiles queries and other statements into native noSQL store APIs rather than using MapReduce enabling the building of low latency applications on top of noSQL stores.

https://en.wikipedia.org/wiki/Apache_Phoenix[+https://en.wikipedia.org/wiki/Apache_Phoenix+]

http://phoenix.apache.org/[+http://phoenix.apache.org/+]

Pig
^^^

Apache Pig is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark. Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy and then call directly from the language. 

https://en.wikipedia.org/wiki/Apache_Pig[+https://en.wikipedia.org/wiki/Apache_Pig+]

https://pig.apache.org/[+https://pig.apache.org/+]

Spark
^^^^^

Apache Spark is an open-source distributed general-purpose cluster-computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. 

Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[8]

Spark facilitates the implementation of both iterative algorithms, that visit their data set multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying of data. The latency of such applications may be reduced by several orders of magnitude compared to a MapReduce implementation (as was common in Apache Hadoop stacks).[2][9] Among the class of iterative algorithms are the training algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.[10]

Apache Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos.[11] For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS),[12] MapR File System (MapR-FS),[13] Cassandra,[14] OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core. 

https://en.wikipedia.org/wiki/Apache_Spark[+https://en.wikipedia.org/wiki/Apache_Spark+]

https://spark.apache.org/[+https://spark.apache.org/+]

Sqoop
^^^^^

Sqoop is a command-line interface application for transferring data between relational databases and Hadoop.

Sqoop supports incremental loads of a single table or a free form SQL query as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in Hive or HBase.[2] Exports can be used to put data from Hadoop into a relational database. Sqoop got the name from "SQL-to-Hadoop".[3] Sqoop became a top-level Apache project in March 2012.

https://en.wikipedia.org/wiki/Sqoop[+https://en.wikipedia.org/wiki/Sqoop+]

https://sqoop.apache.org/[+https://sqoop.apache.org/+]

Storm
^^^^^

Apache Storm is a distributed stream processing computation framework written predominantly in the Clojure programming language. Originally created by Nathan Marz[1] and team at BackType,[2] the project was open sourced after being acquired by Twitter.[3] It uses custom created "spouts" and "bolts" to define information sources and manipulations to allow batch, distributed processing of streaming data. The initial release was on 17 September 2011.[4]

A Storm application is designed as a "topology" in the shape of a directed acyclic graph (DAG) with spouts and bolts acting as the graph vertices. Edges on the graph are named streams and direct data from one node to another. Together, the topology acts as a data transformation pipeline. At a superficial level the general topology structure is similar to a MapReduce job, with the main difference being that data is processed in real time as opposed to in individual batches. Additionally, Storm topologies run indefinitely until killed, while a MapReduce job DAG must eventually end.

https://en.wikipedia.org/wiki/Storm_(event_processor)[+https://en.wikipedia.org/wiki/Storm_(event_processor)+]

http://storm.apache.org/[+http://storm.apache.org/+]

ZooKeeper
^^^^^^^^^

Apache ZooKeeper is a software project of the Apache Software Foundation. It is essentially a centralized service for distributed systems to a hierarchical key-value store, which is used to provide a distributed configuration service, synchronization service, and naming registry for large distributed systems.[2] ZooKeeper was a sub-project of Hadoop but is now a top-level Apache project in its own right.

ZooKeeper's architecture supports high availability through redundant services. The clients can thus ask another ZooKeeper leader if the first fails to answer. ZooKeeper nodes store their data in a hierarchical name space, much like a file system or a tree data structure. Clients can read from and write to the nodes and in this way have a shared configuration service. ZooKeeper can be viewed as an atomic broadcast system, through which updates are totally ordered. The ZooKeeper Atomic Broadcast (ZAB) protocol is the core of the system.

https://en.wikipedia.org/wiki/Apache_ZooKeeper[+https://en.wikipedia.org/wiki/Apache_ZooKeeper+]

https://zookeeper.apache.org/[+https://zookeeper.apache.org/+]

Haiku
~~~~~

HAIKU is an open source operating system currently in development. Specifically targeting personal computing, Haiku is a fast, efficient, simple to use, easy to learn, and yet very powerful system for computer users of all levels. Additionally, Haiku offers something over other open source platforms which is quite unique: The project consists of a single team writing everything from the kernel, drivers, userland services, tool kit, and graphics stack to the included desktop applications and preflets. While numerous open source projects are utilized in Haiku, they are integrated seamlessly. This allows Haiku to achieve a level of consistency that provides many conveniences, and is truly enjoyable to use by both end-users and developers alike.

The key highlights that distinguish Haiku from other operating systems include:

* Specific focus on personal computing
* Custom kernel designed for responsiveness
* Fully threaded design for great efficiency with multi-processor/core CPUs
* Rich object-oriented API for faster development
* Database-like file system (BFS) with support for indexed metadata
* Unified, cohesive interface

The Be Operating System introduced progressive concepts and technologies that we believe represent the ideal means to simple and efficient personal computing. Haiku is the realization of those concepts and technologies in the form of an operating system that is open source and free.

https://www.haiku-os.org/about/[+https://www.haiku-os.org/about/+]

https://www.haiku-os.org/docs/api/index.html[+https://www.haiku-os.org/docs/api/index.html+]

https://en.wikipedia.org/wiki/Haiku_Applications[+https://en.wikipedia.org/wiki/Haiku_Applications+]

https://www.haiku-os.org/guides/virtualizing/virtualbox/[+https://www.haiku-os.org/guides/virtualizing/virtualbox/+]

https://www.haiku-os.org/guides/virtualizing/xen/[+https://www.haiku-os.org/guides/virtualizing/xen/+]

Halide
~~~~~~

 Halide is a programming language designed to make it easier to write high-performance image and array processing code on modern machines. Halide currently targets:

* CPU architectures: X86, ARM, MIPS, Hexagon, PowerPC
* Operating systems: Linux, Windows, macOS, Android, iOS, Qualcomm QuRT
* GPU Compute APIs: CUDA, OpenCL, OpenGL, OpenGL Compute Shaders, Apple Metal, Microsoft Direct X 12

Rather than being a standalone programming language, Halide is embedded in Cxx. This means you write Cxx code that builds an in-memory representation of a Halide pipeline using Halide's Cxx API. You can then compile this representation to an object file, or JIT-compile it and run it in the same process. 

http://halide-lang.org/[+http://halide-lang.org/+]

https://github.com/halide/Halide[+https://github.com/halide/Halide+]

Hamlib
~~~~~~

The Ham Radio Control Libraries, Hamlib for short, is a development effort to provide a consistent interface for programmers wanting to incorporate radio control in their programs. Hamlib is not a complete user application, rather, it is a software layer intended to make controlling various radios and other shack hardware much easier. Hamlib will allow authors of such software as logging programs, digital communications programs, or those wanting to develop the ultimate radio control software to concentrate on the user interface and the basic function of the program rather than radio control.

Hamlib's focus is on controlling rigs that employ a port and command protocol for setting frequency, mode, VFO, PTT, etc. Most VHF/UHF transceivers do not employ such control capability but do provide for cloning the memory contents from radio to another of the same model. A related project, CHIRP, aims to support rigs with such a clone capability. Please contact the CHIRP project for support of such rigs.

Most recent amateur radio transceivers allow external control of their functions through a serial interface. Unfortunately, control commands are not always consistent across a manufacturer's product line and each manufacturer's product line differs greatly from its competitors.

Hamlib attempts to solve this problem by presenting a "virtual radio" to the programmer by providing an API to actions such as setting a given VFO's frequency, setting the operating mode, querying the radio of its current status and settings, and giving the application a list of a given radio's capabilities. Unfortunately, what can be accomplished by Hamlib is limited by the radios themselves and some offer very limited capability.

Other devices, such as antenna rotators, can be placed into the hamlib control scheme. Other recent developments include network interface servers and a USB interface capability. Language bindings are provided for C, Cxx, Perl, Python, and TCL (more to come).

https://github.com/Hamlib/Hamlib/wiki[+https://github.com/Hamlib/Hamlib/wiki+]

https://github.com/Hamlib/Hamlib[+https://github.com/Hamlib/Hamlib+]

https://hamlib.github.io/[+https://hamlib.github.io/+]

Harvey
~~~~~~

Harvey is an effort to provide a modern, distributed, 64 bit operating system. A different environment for researching and finding new lines of work. It can be built with gcc and clang and has an ANSI/POSIX compliant subsystem.

It runs on x86_64 (amd64) machines. Main work is focused in improving the kernel and userland, trying to bring up a full usable operating system with common tools for development and a USB installation image. It collects many different ideas and concepts that, across many platforms and operating systems, influenced the computing world for years.

https://harvey-os.org/[+https://harvey-os.org/+]

https://github.com/Harvey-OS/harvey[+https://github.com/Harvey-OS/harvey+]

HashDist
~~~~~~~~

An environment management system.
HashDist can build complete software stacks across operating systems,
version them, use packages from the system or build them, and customize them.
You can deploy production code or your own customizations.

https://hashdist.github.io/[+https://hashdist.github.io/+]

https://hashdist.readthedocs.io/en/latest/[+https://hashdist.readthedocs.io/en/latest/+]

https://github.com/hashdist/hashdist[+https://github.com/hashdist/hashdist+]

https://github.com/pghysels/hashstack[+https://github.com/pghysels/hashstack+]

HAXM
~~~~

HAXM is a cross-platform hardware-assisted virtualization engine (hypervisor), widely used as an accelerator for Android Emulator and QEMU. It has always supported running on Windows and macOS, and has been ported to other host operating systems as well, such as Linux and NetBSD.

HAXM runs as a kernel-mode driver on the host operating system, and provides a KVM-like interface to user space, thereby enabling applications like QEMU to utilize the hardware virtualization capabilities built into modern Intel CPUs, namely Intel Virtualization Technology.

https://github.com/intel/haxm[+https://github.com/intel/haxm+]

HDF5
~~~~

Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data. Originally developed at the National Center for Supercomputing Applications, it is supported by The HDF Group, a non-profit corporation whose mission is to ensure continued development of HDF5 technologies and the continued accessibility of data stored in HDF.

In keeping with this goal, the HDF libraries and associated tools are available under a liberal, BSD-like license for general use. HDF is supported by many commercial and non-commercial software platforms, including Java, MATLAB, Scilab, Octave, Mathematica, IDL, Python, R, and Julia. The freely available HDF distribution consists of the library, command-line utilities, test suite source, Java interface, and the Java-based HDF Viewer (HDFView).

The current version, HDF5, differs significantly in design and API from the major legacy version HDF4.

https://www.hdfgroup.org/[+https://www.hdfgroup.org/+]

https://support.hdfgroup.org/services/filters.html[+https://support.hdfgroup.org/services/filters.html+]

F90GIO
^^^^^^

Fortran 90 General I/O Interface (F90GIO) provides I/O interface for NetCDF, HDF4, HDF5 reading/writing with Fortran 90 languages. It is developed in the hope of allieviating researchers' burden in atmospheric and oceanic science to deal with either re-analysis, forecast, or remote sensed data. Instead of wasting time learning these techniques, researchers may prefer to focus on the scientific problems. With this thought in mind, I decide to deliever this package to the public. Most of the routines have been tested both on Mac Pro and Linux clusters.

This README file contains information about: I. what compiler flags should be included when compiling NetCDF, HDF4, HDF5 libraries; II. how to compile F90IO library, III. how to use F90IO library.

https://github.com/cd10kfsu/F90GIO[+https://github.com/cd10kfsu/F90GIO+]

h5fs
^^^^

H5FS is a mountable Linux virtual file system that allows to read the content of HDF5 as if they were real files.

H5FS (Hierarchical Data Format FileSystem) is a file system for Linux (and other operating systems with a FUSE implementation, such as Mac OS X or FreeBSD) capable of operations on an HDF5 file.

On the local computer where the HDF5 file is mounted, the implementation makes use of the FUSE (Filesystem in Userspace) kernel module. The practical effect of this is that the end user can interact with the content of a HDF5 file in a natural way as HDF5 uses a filesystem-like data format to access the resources.

GROUPS are exposed as folders and DATASETS are exposed as files.

https://www.logilab.org/project/h5fs[+https://www.logilab.org/project/h5fs+]

H5hut
^^^^^

The H5hut library is an implementation of several data models for particle-based simulations that encapsulates the complexity of parallel HDF5 and is simple to use, yet does not compromise performance.

H5hut is tuned for writing collectively from all processors to a single, shared file. Although collective I/O performance is typically (but not always) lower than that of file-per- processor, having a shared file simplifies scientific workflows in which simulation data needs to be analyzed or visualized. In this scenario, the file-per-processor approach leads to data management headaches because large collections of files are unwieldy to manage from a file system standpoint. On a parallel file system like Lustre, even the ls utility will break when presented with tens of thousands of files, and performance begins to degrade with this number of files because of contention at the metadata server. Often a post-processing step is necessary to refactor file-per-processor data into a format that is readable by the analysis tool. In contrast, H5hut files can be directly loaded in parallel by visualization tools like VisIt and ParaView.

H5hut is a veneer API for HDF5: H5hut files are also valid HDF5 files and are compatible with other HDF5-based interfaces and tools. For example, the h5dump tool that comes standard with HDF5 can export H5hut files to ASCII or XML for additional portability. H5hut also includes tools to convert H5hut data to the Visualization ToolKit (VTK) format and to generate scripts for the GNUplot data plotting tool.

https://gitlab.psi.ch/H5hut/src/wikis/home[+https://gitlab.psi.ch/H5hut/src/wikis/home+]

http://dav.lbl.gov/software/[+http://dav.lbl.gov/software/+]

h5py
^^^^

The h5py package is a Pythonic interface to the HDF5 binary data format. 

It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays. Thousands of datasets can be stored in a single file, categorized and tagged however you want.

H5py uses straightforward NumPy and Python metaphors, like dictionary and NumPy array syntax. For example, you can iterate over datasets in a file, or check out the .shape or .dtype attributes of datasets. You don't need to know anything special about HDF5 to get started.

In addition to the easy-to-use high level interface, h5py rests on a object-oriented Cython wrapping of the HDF5 C API. Almost anything you can do from C in HDF5, you can do from h5py.

http://www.h5py.org/[+http://www.h5py.org/+]

https://github.com/h5py/h5py[+https://github.com/h5py/h5py+]

h5pyd
^^^^^

Python client library for HDF5 REST interface (HSDS).
The library is provides a high-level interface to the REST specification that is generally easier to use than invoking http calls directly.
The package is based on the popular h5py package and aims to be source compatible with the h5py high level interface.

https://github.com/HDFGroup/h5pyd[+https://github.com/HDFGroup/h5pyd+]

H5Z-ZFP
^^^^^^^

A highly flexible floating point and integer compression plugin for the HDF5 library using ZFP compression.

The HDF5 filter plugin code here is also part of the Silo library. However, we have made an effort to also support it as a stand-alone package due to the likely broad appeal and utility of the ZFP compression library.

This plugin supports all 4 modes of the ZFP compression library, rate, accuracy, precision and expert. It supports 1, 2 and 3 dimensional datasets of single and double precision integer and floating point data. It can be applied to HDF5 datasets of more than 3 dimensions as long as no more than 3 dimensions of the HDF5 dataset chunking are of size greater than 1. For datasets greater than 3 dimensions, a good strategy is to select non-unity chunk dimensions in decreasing order of dimensions expressing the spatial correlation.

https://github.com/LLNL/H5Z-ZFP[+https://github.com/LLNL/H5Z-ZFP+]

hfs3
^^^^

h5s3 is a driver for hdf5 1.8 and 1.10 which allows you to efficiently work with large hdf5 files by transparently storing them in Amazon s3. h5s3 allows you to fetch only the subset of a file that you need, facilitating distributed computation against a large hdf5 file.

https://h5s3.github.io/h5s3/[+https://h5s3.github.io/h5s3/+]

http://matthewrocklin.com/blog/work/2018/02/06/hdf-in-the-cloud[+http://matthewrocklin.com/blog/work/2018/02/06/hdf-in-the-cloud+]

hdf5-json
^^^^^^^^^

Specification and tools for representing HDF5 in JSON.

https://hdf5-json.readthedocs.io/en/latest/[+https://hdf5-json.readthedocs.io/en/latest/+]

hdf5r
^^^^^

hdf5r is an R interface to the HDF5 library. It is implemented using R6 classes based on the HDF5-C-API. The package supports all data-types as specified by HDF5 (including references) and provides many convenience functions yet also an extensive selection of the native HDF5-C-API functions. hdf5r is available on Github and has already been released on CRAN for all major platforms (Windows, OS X, Linux). It is also tested using several hundred assertions.

https://github.com/hhoeflin/hdf5r[+https://github.com/hhoeflin/hdf5r+]

hdf5-json
^^^^^^^^^

This repository contains a specification, library, and utilities for describing HDF5 content in JSON. The utilities can be used to convert any HDF5 file to JSON or from a JSON file (using the convention described here to HDF5).

The library is useful for any Python application that needs to translate between HDF5 objects and JSON serializations. In addition to the utilities provided in this repository, the library is used by HDF Server (a RESTful web service for HDF5), and HDF Product Designer (an application for creating product designs).

https://github.com/HDFGroup/hdf5-json[+https://github.com/HDFGroup/hdf5-json+]

hdf5storage
^^^^^^^^^^^

This Python package provides high level utilities to read/write a variety of Python types to/from HDF5 (Heirarchal Data Format) formatted files. This package also provides support for MATLAB MAT v7.3 formatted files, which are just HDF5 files with a different extension and some extra meta-data.

All of this is done without pickling data. Pickling is bad for security because it allows arbitrary code to be executed in the interpreter. One wants to be able to read possibly HDF5 and MAT files from untrusted sources, so pickling is avoided in this package.

https://pythonhosted.org/hdf5storage/[+https://pythonhosted.org/hdf5storage/+]

https://github.com/frejanordsiek/hdf5storage[+https://github.com/frejanordsiek/hdf5storage+]

HDFql
^^^^^

Scientists, students, engineers and data professionals currently waste a lot of unnecessary time managing the data format HDF5. That’s because the interfaces for HDF are highly complex (for instance, the C API contains more than 400 low-level functions that are far from easy to use!). With HDF becoming increasingly common in the big data arena, a faster and simpler solution is needed.

HDFql is the first high-level language to manage HDF data. Designed to be as simple and powerful as SQL, HDFql dramatically reduces the learning effort and time needed to handle HDF5. Built on intuitive syntax, the tool offers a clean interface which reads and writes HDF data across programming languages and platforms.

The features include:

* Designed to be as simple as SQL. Hides complex operations and frees users from low-level details
* Offers a clean interface requiring just a few intuitive statements even for complex operations. Gone are the days where HDF5 required innumerous lines of code
* Processes data with superior velocity by using all CPU cores available. This means much higher volumes of data are processed in the same amount of time
* Based on models of human cognition and natural language. Fast learning curve
* Unlike other tools, HDFql not only reads HDF5 but also allows
you to write HDF5 data
* Portable across C, Cxx, Java, Python, C#, Fortran and R using
one uniform high-level language

http://www.hdfql.com/[+http://www.hdfql.com/+]

HDFView
^^^^^^^

HDFView software consists of the HDFView utility and the Java HDF Object Package.

HDFView is a visual tool written in Java for browsing and editing HDF (HDF5 and HDF4) files. Using HDFView, you can:

    View a file hierarchy in a tree structure
    Create new files, add or delete groups and datasets
    View and modify the content of a dataset
    Add, delete and modify attributes

HDFView uses the Java HDF Object Package, which implements HDF4 and HDF5 data objects in an object-oriented form.

https://www.hdfgroup.org/downloads/hdfview/[+https://www.hdfgroup.org/downloads/hdfview/+]

Hickle
^^^^^^

hickle is a Python 2/3 package for quickly dumping and loading python data structures to Hierarchical Data Format 5 (HDF5) files. When dumping to HDF5, hickle automatically convert Python data structures (e.g. lists, dictionaries, numpy arrays) into HDF5 groups and datasets. When loading from file, hickle automatically converts data back into its original data type. A key motivation for hickle is to provide high-performance loading and storage of scientific data in the widely-supported HDF5 format.

hickle is designed as a drop-in replacement for the Python pickle package, which converts Python object hierarchies to and from Python-specific byte streams (processes known as 'pickling' and 'unpickling' respectively). Several different protocols exist, and files are not designed to be compatible between Python versions, nor interpretable in other languages. In contrast, hickle stores and loads files from HDF5, for which application programming interfaces (APIs) exist in most major languages, including C, Java, R, and MATLAB.

Python data structures are mapped into the HDF5 abstract data model in a logical fashion, using the h5py package. Metadata required to reconstruct the hierarchy of objects, and to allow conversion into Python objects, is stored in HDF5 attributes. Most commonly used Python iterables (dict, tuple, list, set), and data types (int, float, str) are supported, as are numpy N-dimensional arrays. Commonly-used astropy data structures and scipy sparse matrices are also supported.

https://github.com/telegraphic/hickle[+https://github.com/telegraphic/hickle+]

http://joss.theoj.org/papers/0c6638f84a1a574913ed7c6dd1051847[+http://joss.theoj.org/papers/0c6638f84a1a574913ed7c6dd1051847+]

HighFive
^^^^^^^^

HighFive is a modern Cxx/Cxx11 friendly interface for libhdf5.

HighFive supports STL vector/string, Boost::UBLAS and Boost::Multi-array. It handles Cxx from/to HDF5 automatic type mapping. HighFive does not require an additional library and supports both HDF5 thread safety and Parallel HDF5.

https://github.com/BlueBrain/HighFive[+https://github.com/BlueBrain/HighFive+]

HSDS
^^^^

HSDS is a web service that implements a REST-based web service for HDF5 data stores.
HDF Server is a freely available service (implemented in Python using the Tornado framework) that enables remote access to HDF5 content using a RESTful API.

h5serv uses a REST interface to support CRUD (create, read, update, delete) operations on the full spectrum of HDF5 objects including: groups, links, datasets, attributes, and committed data types. As a REST-based service a variety of clients can be developed in JavaScript, Python, C, and other common languages.

REST is an architectural pattern, typically HTTP-based, that has the following properties:

* Stateless – the server does not maintain any client state (this enables better scalability).
* Resources are identified in the request. For example, an operation on an attribute should identify that attribute via a URI (Uniform Resource Identifier).
* Uses all the HTTP methods: GET, PUT, DELETE, POST in ways that align with their semantic intent (e.g. don’t use GET to invoke an operation that modifies a resource).
* Provides hypermedia links to aid discovery of resources vended by the API (this is known by the awkward acronym, HATEOAS for Hypermedia as the Engine Of Application State).

The beauty of this is that the basic patterns of any REST-based service are well known to developers (e.g. that PUT operations are idempotent), and that a lot of the machinery of the Internet has been built around the specifics of how HTTP works (e.g. a caching system can use ETags to determine if content needs to be refreshed or not).

Also, note that a common use of REST is for developing applications that run within a browser (think Google Maps); it is also effective for connecting services with desktop applications or command line scripts. For C-based applications the libcurl library can be used to create HTTP requests, while for Python the requests module has similar functionality.

https://github.com/HDFGroup/hsds[+https://github.com/HDFGroup/hsds+]

https://h5serv.readthedocs.io/en/latest/index.html[+https://h5serv.readthedocs.io/en/latest/index.html+]

https://www.hdfgroup.org/2015/04/hdf5-for-the-web-hdf-server[+https://www.hdfgroup.org/2015/04/hdf5-for-the-web-hdf-server+]

N5
^^

The N5 API specifies the primitive operations needed to store large chunked n-dimensional tensors, and arbitrary meta-data in a hierarchy of groups similar to HDF5.

Other than HDF5, N5 is not bound to a specific backend. This repository includes a simple file-system backend. There are also an HDF5 backend, a Google Cloud backend, and an AWS-S3 backend.

At this time, N5 supports:

* arbitrary group hierarchies
* arbitrary meta-data (stored as JSON or HDF5 attributes)
* chunked n-dimensional tensor datasets
* value-datatypes: [u]int8, [u]int16, [u]int32, [u]int64, float32, float64
* compression: raw, gzip, bzip2, xz

Chunked datasets can be sparse, i.e. empty chunks do not need to be stored.

N5 group is not a single file but simply a directory on the file system. Meta-data is stored as a JSON file per each group/ directory. Tensor datasets can be chunked and chunks are stored as individual files. This enables parallel reading and writing on a cluster.

HDF5 is a great format that provides a wealth of conveniences.
It's inefficiency for parallel writing, however, limit its applicability for handling of very large n-dimensional data.

N5 uses the native filesystem of the target platform and JSON files to specify basic and custom meta-data as attributes. It aims at preserving the convenience of HDF5 where possible but doesn't try too hard to be a full replacement. Please do not take this project too seriously, we will see where it will get us and report back when more data is available.

https://github.com/saalfeldlab/n5[+https://github.com/saalfeldlab/n5+]

oo_hdf5_fortran
^^^^^^^^^^^^^^^

Straightforward single-file/module access to HDF5. Uses Fortran 2008 submodule for clean, templatable structure. This thin object-oriented modern Fortran library abstracts away the messy parts of HDF5 so that you can read/write various types/ranks of data with a single command.

https://github.com/scivision/oo_hdf5_fortran[+https://github.com/scivision/oo_hdf5_fortran+]

z5
^^

Lightweight C++ and Python wrapper for zarr and n5 file format.

https://github.com/constantinpape/z5[+https://github.com/constantinpape/z5+]

HDR
~~~

High-dynamic-range imaging (HDRI) is a high dynamic range (HDR) technique used in imaging and photography to reproduce a greater dynamic range of luminosity than is possible with standard digital imaging or photographic techniques. The aim is to present a similar range of luminance to that experienced through the human visual system. The human eye, through adaptation of the iris and other methods, adjusts constantly to adapt to a broad range of luminance present in the environment. The brain continuously interprets this information so that a viewer can see in a wide range of light conditions.

HDR images can represent a greater range of luminance levels than can be achieved using more traditional methods, such as many real-world scenes containing very bright, direct sunlight to extreme shade, or very faint nebulae. This is often achieved by capturing and then combining several different, narrower range, exposures of the same subject matter. Non-HDR cameras take photographs with a limited exposure range, referred to as LDR, resulting in the loss of detail in highlights or shadows. 

https://en.wikipedia.org/wiki/High-dynamic-range_imaging[+https://en.wikipedia.org/wiki/High-dynamic-range_imaging+]

https://www.fxguide.com/featured/art_of_hdr/[+https://www.fxguide.com/featured/art_of_hdr/+]

exrtools
^^^^^^^^

exrtools is a set of simple command-line utilities for manipulating with high dynamic range images in OpenEXR format. OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications.

exrtools was developed to help experiment with batch processing of HDR images for tone mapping. Each application is small and reasonably self-contained such that the source code may be of most value to others.

exrtools currently only works with RGBA OpenEXR files. As well, the code assumes that the EXR files and PNG files all use sRGB primaries and gamma function.

http://scanline.ca/exrtools/[+http://scanline.ca/exrtools/+]

Luminance HDR
^^^^^^^^^^^^^

Luminance HDR, formerly Qtpfsgui, is graphics software used for the creation and manipulation of high-dynamic-range images. Released under the terms of the GPL, it is available for Linux, Windows and Mac OS X (Intel only). Luminance HDR supports several High Dynamic Range (HDR) as well as Low Dynamic Range (LDR) file formats. 

Prerequisite of HDR photography are several narrow-range digital images with different exposures. Luminance HDR combines these images and calculates a high-contrast image. In order to view this image on a regular computer monitor, Luminance HDR can convert it into a displayable LDR image format using a variety of methods, such as tone mapping. Currently eleven different tone mapping operators (algorithms) are available, each one with its tunable parameters.

Different image processing techniques can be applied to the generated HDR images, such as resizing, cropping, rotating and a number of projective transformations.

The software also provides batch processing funcionality for creating HDR images and for tone mapping them in a non-interactive way. A module for copying EXIF data among sets of images is also provided.

For users who prefers the command line, a non-GUI, non-graphical interface is also available on all supported platforms. 

https://sourceforge.net/projects/qtpfsgui/[+https://sourceforge.net/projects/qtpfsgui/+]

OpenEXR
^^^^^^^

OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light & Magic for use in computer imaging applications.

OpenEXR's features include:

* Higher dynamic range and color precision than existing 8- and 10-bit image file formats.
* Support for 16-bit floating-point, 32-bit floating-point, and 32-bit integer pixels. The 16-bit floating-point format, called "half", is compatible with the half data type in NVIDIA's Cg graphics language and is supported natively on their new GeForce FX and Quadro FX 3D graphics solutions.
* Multiple image compression algorithms, both lossless and lossy. Some of the included codecs can achieve 2:1 lossless compression ratios on images with film grain. The lossy codecs have been tuned for visual quality and decoding performance.
* Extensibility. New compression codecs and image types can easily be added by extending the Cxx classes included in the OpenEXR software distribution. New image attributes (strings, vectors, integers, etc.) can be added to OpenEXR image headers without affecting backward compatibility with existing OpenEXR applications.
* Deep Data. Pixels can store a variable length list of samples. The main rationale behind deep-images is to store multiple values at different depths for each pixel. Support for hard surface and volumetric representation requirements for deep compositing workflows.
* Multi-part image files. Files can contain a number of separate, but related, images in one file. Access to any part is independent of the others; in particular, no access of data need take place for unrequested parts.

ILM has released OpenEXR as free software. The OpenEXR software distribution includes:

* IlmImf, a library that reads and writes OpenEXR images.
* IlmImfUtil, a convenience library to simplify development of OpenEXR utilities.
* Half, a Cxx class for manipulating half values as if they were a built-in Cxx data type.
* Imath, a math library with support for matrices, 2d- and 3d-transformations, solvers for linear/quadratic/cubic equations, and more.
* exrdisplay, a sample application for viewing OpenEXR images on a display at various exposure settings.

The OpenEXR software distribution is now licensed under the modified BSD license.

http://www.openexr.com/[+http://www.openexr.com/+]

pfstools
^^^^^^^^

pfstools package is a set of command line programs for reading, writing and manipulating high-dynamic range (HDR) images and video frames. It includes also Qt and OpenGL HDR image viewers. pfstools can be integrated with GNU Octave or matlab, so that it can serve as a toolbox for reading and writing HDR images.

All programs in the package exchange image data using unix pipes and a simple generic HDR image format - pfs. pfs in not just another format for storing HDR images (and there are already quite a few of them), but is rather an attempt to integrate the existing HDR image formats by providing a simple interface for exchanging data between applications.

http://pfstools.sourceforge.net/[+http://pfstools.sourceforge.net/+]

Heads
~~~~~

Heads is an open source custom firmware and OS configuration for laptops and servers that aims to provide slightly better physical security and protection for data on the system. Unlike Tails, which aims to be a stateless OS that leaves no trace on the computer of its presence, Heads is intended for the case where you need to store data and state on the computer. It is influenced by several years of firmware vulnerability research (Thunderstrike and Thunderstrike 2) as well as many other researchers’ work.

Heads is not just another Linux distribution – it combines physical hardening of specific hardware platforms and flash security features with custom coreboot firmware and a Linux boot loader in ROM. This moves the root of trust into the write-protected region of the SPI flash and prevents further software modifications to the bootup code (and on platforms that support it, Bootguard can protect against many hardware attacks as well). Controlling the first instruction the CPU executes allows Heads to measure every step of the boot firmware and configuration into the TPM, which makes it possible to attest to the user or a remote system that the machine has not been tampered with. While modern Intel CPUs require binary blobs to boot, these non-Free components are included in the measurements and are at least guaranteed to be unchanging. Once the system is in a known good state, the TPM is used as a hardware key storage to decrypt the drive.

Additionally, the hypervisor, kernel and initrd images are signed by keys controlled by the user, and the OS uses a signed, immutable root filesystem so that any software exploits that attempt to gain persistence will be detected. While all of these firmware and software changes don’t secure the system against every possible attack vector, they address several classes of attacks against the boot process and physical hardware that have been neglected in traditional installations, hopefully raising the difficulty beyond what most attackers are willing to spend.

http://osresearch.net/[+http://osresearch.net/+]

https://github.com/osresearch/heads[+https://github.com/osresearch/heads+]

https://en.wikipedia.org/wiki/Tails_(operating_system)[+https://en.wikipedia.org/wiki/Tails_(operating_system)+]

Heh
~~~

Heh is a functional programming language with a built-in support for infinite arrays. The language is strict, but the infinite structures are lazy. The language includes three built-in higher-order constructs: imap, reduce and filter which make it possible to construct complex array operations that can be often seen in APL or SaC.

One of the main distinctive features of the language is that we use ordinals to index arrays as well as to maintain shapes of the arrays. This gives a rise to a number of important static properties that one can observe in Heh.

Heh makes it possible to write generic programs that do not make distinction between arrays and streams.  One can write truly infinity-polymorphic programs.

There is a backend that compiles programs with non-recursive finite imaps can into SaC.

https://github.com/ashinkarov/heh[+https://github.com/ashinkarov/heh+]

https://arxiv.org/abs/1710.03832[+https://arxiv.org/abs/1710.03832+]

https://github.com/ashinkarov/heh/blob/master/compile_sac.ml[+https://github.com/ashinkarov/heh/blob/master/compile_sac.ml+]

http://ashinkarov.github.io/publications/rosetta-stone.pdf[+http://ashinkarov.github.io/publications/rosetta-stone.pdf+]

http://ashinkarov.github.io/publications/array-comp.pdf[+http://ashinkarov.github.io/publications/array-comp.pdf+]

Herbgrind
~~~~~~~~~

Herbgrind is a dynamic, binary, program analysis to find the root cause of floating point error in large programs. Herbgrind aims to help programmers weed out dubious floating point code from their programs, and have more confidence in their numerical code.

Herbgrind grew out of an effort to explore the usage of floating point tools like Herbie, Salsa, Rosa, and FPTaylor, on large numerical software. Herbgrind is built on Valgrind, and draws inspiration from FpDebug.

http://herbgrind.ucsd.edu/[+http://herbgrind.ucsd.edu/+]

Herbie
~~~~~~

Herbie synthesizes floating-point programs from real-number programs, automatically handling simple numerical instabilities. Herbie can detect inaccurate floating point expressions and gives you more-accurate replacements.

https://github.com/uwplse/herbie[+https://github.com/uwplse/herbie+]

https://herbie.uwplse.org/[+https://herbie.uwplse.org/+]

HipMCL
~~~~~~

HipMCL is a high-performance parallel algorithm for large-scale network clustering. HipMCL parallelizes popular Markov Cluster (MCL) algorithm that has been shown to be one of the most successful and widely used algorithms for network clustering. It is based on random walks and was initially designed to detect families in protein-protein interaction networks. Despite MCL’s efficiency and multi-threading support, scalability remains a bottleneck as it fails to process networks of several hundred million nodes and billion edges in an affordable running time. HipMCL overcomes all of these challenges by developing massively-parallel algorithms for all components of MCL. HipMCL can be 1000 times faster than the original MCL without any information loss. It can easily cluster a network of ~75 million nodes with ~68 billion edges in ~2.4 hours using ~2000 nodes of Cori supercomputer at NERSC. HipMCL is developed in Cxx language and uses standard OpenMP and MPI libraries for shared- and distributed-memory parallelization.

https://bitbucket.org/azadcse/hipmcl/wiki/Home[+https://bitbucket.org/azadcse/hipmcl/wiki/Home+]

https://bitbucket.org/azadcse/hipmcl[+https://bitbucket.org/azadcse/hipmcl+]

https://people.eecs.berkeley.edu/\~aydin/CombBLAS/html/index.html[+https://people.eecs.berkeley.edu/~aydin/CombBLAS/html/index.html+]

hIPPYlib
~~~~~~~~

hIPPYlib implements state-of-the-art scalable adjoint-based algorithms for PDE-based deterministic and Bayesian inverse problems. It builds on FEniCS for the discretization of the PDE and on PETSc for scalable and efficient linear algebra operations and solvers.

The features include:

* Friendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form
* Automatic generation of efficient code for the discretization of weak forms using FEniCS
* Symbolic differentiation of weak forms to generate derivatives and adjoint information
* Globalized Inexact Newton-CG method to solve the inverse problem
* Low rank representation of the posterior covariace using randomized algorithms

https://github.com/hippylib/hippylib[+https://github.com/hippylib/hippylib+]

https://hippylib.github.io/[+https://hippylib.github.io/+]

http://joss.theoj.org/papers/053e0d08a5e9755e7b78898cff6f6208[+http://joss.theoj.org/papers/053e0d08a5e9755e7b78898cff6f6208+]

http://g2s3.com/labs/[+http://g2s3.com/labs/+]

HoloViews
~~~~~~~~~

HoloViews is an open-source Python library designed to make data analysis and visualization seamless and simple. With HoloViews, you can usually express what you want to do in very few lines of code, letting you focus on what you are trying to explore and convey, not on the process of plotting.

HoloViews focuses on bundling your data together with the appropriate metadata to support both analysis and visualization, making your raw data and its visualization equally accessible at all times. This process can be unfamiliar to those used to traditional data-processing and plotting tools, and this getting-started guide is meant to demonstrate how it all works at a high level. 

With HoloViews, instead of building a plot using direct calls to a plotting library, you first describe your data with a small amount of crucial semantic information required to make it visualizable, then you specify additional metadata as needed to determine more detailed aspects of your visualization. This approach provides immediate, automatic visualization that can be effortlessly requested at any time as your data evolves, rendered automatically by one of the supported plotting libraries (such as Bokeh or Matplotlib). 

http://holoviews.org/[+http://holoviews.org/+]

https://github.com/ioam/holoviews[+https://github.com/ioam/holoviews+]

HOMME
~~~~~

The Community Atmosphere Model (CAM) component (Neale et al., 2010) within CESM supports a spectral element based dynamical core option, from HOMME, the High-Order Method Modeling Environment (Dennis et al., 2005, 2011). A key motivation for the integration of HOMME into CAM was to improve CAM’s parallel scalability to enable efficient global simulations on over 100,000 processors (Taylor et al., 2008).

HOMME uses a cubed sphere grid as shown in Figure 1, which allows for a more spatially uniform discretization than the latitude-longitude grids used in CAM’s finite volume dynamical core. The MPI parallel decomposition in HOMME is across spectral elements. In the standard central processing unit (CPU) version of HOMME, computations within each element are handled serially. The excellent scalability of HOMME on the largest computing platforms was a key factor driving the choice of HOMME for initial porting work to GPU-based architectures. The element-wise decomposition in HOMME and minimal communication across elements has enabled scalability to be demonstrated to over 100,000 processors at a horizontal resolution of approximately 1/8th degree.

https://github.com/E3SM-Project/HOMMEXX[+https://github.com/E3SM-Project/HOMMEXX+]

https://wiki.ucar.edu/display/homme/The+HOMME+CMake+build+and+testing+system[+https://wiki.ucar.edu/display/homme/The+HOMME+CMake+build+and+testing+system+]

HopsML
~~~~~~

HopsML is a Python-first framework for building machine learning pipelines. HopsML is enabled by unique support for project-specific conda environments in Hopsworks. As every project in Hopsworks has its own conda environment, replicated at all data processing hosts in the cluster, Data Scientists can simply ‘pip install’ Python libraries in Hopsworks and immediately use them in their PySpark/TensorFlow/PyTorch applications, enabling interactive application development. This contrasts with an immutable infrastructure approach, where Data Scientists need to write Dockerfiles and write YML files describing cluster specifications just to install a Python library. The other unique aspect of HopsML is the use of HopsFS (a distributed filesystem) to coordinate the different steps in a pipeline. HopsFS integrates seamlessly with Estimator APIs in TensorFlow/Keras, enabling the transparent management and aggregation of logs, checkpoints, TensorBoard events, and models across many Executors in a cluster. HopsML extends these Estimator artifacts with versioned notebooks and Python environments, enabling a view of experiments that have been run and now can be easily reproduced.

A machine learning (ML) pipeline is a series of processing steps that:

* optionally ingests (raw) input data from external sources,
* wrangles the input data in an ETL job (data cleaning/validation, feature extraction, etc) to generate clean training data,
* trains a model (using GPUs) with the clean training data,
* validates and optimizes the model,
* deploys the model to production,
* monitors model performance in production.

HopsML pipelines are written as a different programs for each stage in the pipeline, and the pipeline itself is written as a Airflow DAGs (directed acyclic graph). Typically all programs in the pipeline are written in Python, although Scala/Java ca be used at the ETL stage, in particular when dealing with large volumes of input data.

For ML pipelines processing small amounts of data, developers can write a Keras/TensorFlow/PyTorch application to perform both ETL and training in a single program, although developers should be careful that the ETL stage is not so CPU intensive that GPUs cannot be fully utilized when training. For example, in an image processing pipeline, if the same Keras/TensorFlow/PyTorch application is used to both decode/scale/rotate images as well as train a deep neural network (DNN), the application will probably be CPU-bound or I/O bound, and GPUs will be underutilized.

For ML pipelines processing large amounts of data, developers can write a seperate Spark or PySpark application to perform ETL and generate training data. When that application has completed, Airflow will then schedule a PySpark application with Keras/TensorFlow/PyTorch to train the DNN, on possibly many GPUs. The training data will be read from a distributed filesystem (HopsFS), and all logs, TensorBoard events, checkpoints, and the model will be written to the same distributed filesystem. When training has completed, Airflow can schedule a simple Python/Bash job to optimize the trained model (e.g., quantize model weights, remove batch norm layers, shrink models for mobile devices), using either Nvidia’s TensorRT library or TensorFlow’s transform_graph utility. The optimized model (a .pb (protocol buffers) file in TensorFlow) can then be deployed directly from HopsFS to a model serving server (TensorFlow serving Server on Kubernetes) using a REST call on Hopsworks. Finally, Airflow can start a Spark Streaming job to monitor the deployed model by consuing logs for the deployed model from Kafka.

https://github.com/logicalclocks/hops-util-py[+https://github.com/logicalclocks/hops-util-py+]

https://hops.readthedocs.io/en/latest/hopsml/hopsML.html[+https://hops.readthedocs.io/en/latest/hopsml/hopsML.html+]

https://fosdem.org/2019/schedule/event/feature_store/[+https://fosdem.org/2019/schedule/event/feature_store/+]

HPAT
~~~~

High Performance Analytics Toolkit (HPAT) is a big data analytics and machine learning framework that provides Python’s ease of use but is extremely fast.

HPAT scales analytics programs in python to cluster/cloud environments automatically, requiring only minimal code changes.

HPAT compiles a subset of Python to efficient native parallel code (with MPI). This is in contrast to other frameworks such as Apache Spark which are master-executor libraries. Hence, HPAT is typically 100x or more faster. HPAT is built on top of Numba and LLVM compilers.

The features include:

* automatic parallelization of programs based on the map-reduce pattern;
* many data-parallel Numpy operators that HPAT can optimize and parallelize
* explicit parallel loops for operations that cannot be written in terms of data-parallel operators
* support for HDF5 and Parquet formats
* support for many Panda operators

https://intellabs.github.io/hpat-doc/dev/index.html[+https://intellabs.github.io/hpat-doc/dev/index.html+]

https://github.com/IntelLabs/hpat[+https://github.com/IntelLabs/hpat+]

https://github.com/IntelLabs/HPAT.jl[+https://github.com/IntelLabs/HPAT.jl+]

H-Store
~~~~~~~

H-Store is an experimental main-memory, parallel database management system that is optimized for on-line transaction processing (OLTP) applications. It is a highly distributed, row-store-based relational database that runs on a cluster on shared-nothing, main memory executor nodes.

https://github.com/apavlo/h-store[+https://github.com/apavlo/h-store+]

http://hstore.cs.brown.edu/[+http://hstore.cs.brown.edu/+]

hvPlot
~~~~~~

A high-level plotting API for the PyData ecosystem built on HoloViews.

The PyData ecosystem has a number of core Python data containers that allow users to work with a wide array of datatypes, including:

* http://pandas.pydata.org/[Pandas]: DataFrame, Series (columnar/tabular data)
* http://xarray.pydata.org/en/stable/[XArray]: Dataset, DataArray (multidimensional arrays)
* https://github.com/dask/dask[Dask]: DataFrame, Series, Array (distributed/out of core arrays and columnar data)
* https://github.com/mrocklin/streamz[Streamz]: DataFrame(s), Series(s) (streaming columnar data)
* https://github.com/ContinuumIO/intake[Intake]: DataSource (data catalogues)
* http://geopandas.org/[GeoPandas]: GeoDataFrame (geometry data)
* https://networkx.github.io/[NetworkX]: Graph (network graphs)

Several of these libraries have the concept of a high-level plotting API that lets a user generate common plot types very easily. The native plotting APIs are generally built on Matplotlib, which provides a solid foundation, but means that users miss out the benefits of modern, interactive plotting libraries for the web like Bokeh and HoloViews.

hvPlot provides a high-level plotting API built on HoloViews that provides a general and consistent API for plotting data in all the abovementioned formats. hvPlot can integrate neatly with the individual libraries if an extension mechanism for the native plot APIs is offered, or it can be used as a standalone component.

https://github.com/pyviz/hvplot[+https://github.com/pyviz/hvplot+]

Hy
~~

Hy (alternately, Hylang) is a programming language, a dialect of the language Lisp designed to interact with the language Python by translating expressions into Python's abstract syntax tree (AST). Hy was introduced at Python Conference (PyCon) 2013 by Paul Tagliamonte.[1]

Similar to Kawa's and Clojure's mapping of s-expressions onto the Java virtual machine (JVM),[2] Hy is meant to operate as a transparent Lisp front end to Python's abstract syntax.[3] Lisp allows operating on code as data (metaprogramming). Thus, Hy can be used to write domain-specific languages.[4] Hy also allows Python libraries, including the standard library, to be imported and accessed alongside Hy code with a compiling[note 1] step converting the data structure of both into Python's AST.

https://github.com/hylang/hy[+https://github.com/hylang/hy+]

https://en.wikipedia.org/wiki/Hy[+https://en.wikipedia.org/wiki/Hy+]

http://docs.hylang.org/en/stable/[+http://docs.hylang.org/en/stable/+]

Hybrid-Fortran
~~~~~~~~~~~~~~

Hybrid Fortran is ..

* a directive based extension for the Fortran language.
* a way for you to keep writing your Fortran code like you're used to - only now with GPGPU support.
* a preprocessor for your code - its input are Fortran files (with Hybrid Fortran extensions), its output is CUDA Fortran or OpenMP Fortran code (or whatever else you'd like to have as a backend).
* a build system that handles building two separate versions (CPU / GPU) of your codebase automatically, including all the preprocessing.
* a test system that handles verification of your outputs automatically after setup.
* a framework for you to build your own parallel code implementations (OpenCL, ARM, FPGA, Hamster Wheel.. as long as it has some parallel Fortran support you're good) while keeping the same source files.

https://github.com/muellermichel/Hybrid-Fortran[+https://github.com/muellermichel/Hybrid-Fortran+]

HyperBox
~~~~~~~~

Hyperbox is a free virtualized infrastructure manager across your different hypervisors released under GPLv3.

It aims to provide a free alternative to commercial products like VMware vCenter/ESXi and Citrix XenCenter/XenServer.
It is primarly used with VirtualBox, its favorite hypervisor backend, but is designed with a modular architecture in mind and could use any hypervisor providing a Java API.

https://kamax.io/hbox/about/[+https://kamax.io/hbox/about/+]

HyperLearn
~~~~~~~~~~

HyperLearn is written completely in PyTorch, NoGil Numba, Numpy, Pandas, Scipy & LAPACK, and mirrors (mostly) Scikit Learn. HyperLearn also has statistical inference measures embedded, and can be called just like Scikit Learn's syntax.

https://github.com/danielhanchen/hyperlearn[+https://github.com/danielhanchen/hyperlearn+]

https://hyperlearn.readthedocs.io/en/latest/index.html[+https://hyperlearn.readthedocs.io/en/latest/index.html+]

HyperSpy
~~~~~~~~

HyperSpy is an open source Python library which provides tools to facilitate the interactive data analysis of multi-dimensional datasets that can be described as multi-dimensional arrays of a given signal (e.g. a 2D array of spectra a.k.a spectrum image).

HyperSpy aims at making it easy and natural to apply analytical procedures that operate on an individual signal to multi-dimensional arrays, as well as providing easy access to analytical tools that exploit the multi-dimensionality of the dataset.

Its modular structure makes it easy to add features to analyze different kinds of signals.
The features include:

* Two families of named and scaled axes: signal and navigation.
* Visualization tools for multi-dimensional spectra and images.
* Easy access multi-dimensional curve fitting and blind source separation.
* Built on top of NumPy, SciPy, matplotlib and scikit-learn.
* Modular design for easy extensibility.

The development has been motivated by the data analysis needs of the electron microscopy community but it is proving useful in many other fields.

https://hyperspy.org/[+https://hyperspy.org/+]

hypre
~~~~~

Livermore’s HYPRE library of linear solvers makes possible larger, more detailed simulations by solving problems faster than traditional methods at large scales. It offers a comprehensive suite of scalable solvers for large-scale scientific simulation, featuring parallel multigrid methods for both structured and unstructured grid problems. The HYPRE library is highly portable and supports a number of languages.

The HYPRE team was one of the first to develop algebraic multigrid algorithms and software for extreme-scale parallel supercomputers. The team maintains an active role in the multigrid research community and is recognized for its leadership in both algorithm and software development.

The hypre library is a parallel sparse linear solver and preconditioner package featuring algebraic multigrid methods. It provides several interfaces for building linear systems that arise from structured, semi-structured, finite difference, finite element, and general matrix problems. These system interfaces improve usability but also enable multigrid solvers that take advantage of the additional information. The hypre library is also supported through the FASTMath packages PETSc and Trilinos.

https://github.com/LLNL/hypre[+https://github.com/LLNL/hypre+]

https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods[+https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods+]

https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software[+https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software+]

#IIII

Ibis
~~~~

Ibis is a toolbox to bridge the gap between local Python environments (like pandas and scikit-learn) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.).
The features include:

* A pandas-like domain specific language (DSL) designed specifically for analytics, aka Ibis expressions, that enable composable, reusable analytics on structured data. If you can express something with a SQL SELECT query, you can write it with Ibis.
* Integrated user interfaces to HDFS and other storage systems.
* An extensible translator-compiler system that targets multiple SQL systems

Ibis offers some level of support for:

* Apache Impala
* Apache Kudu
* PostgreSQL
* SQLite
* Google BigQuery
* Yandex Clickhouse
* Direct execution of ibis expressions against Pandas objects

https://docs.ibis-project.org/[+https://docs.ibis-project.org/+]

https://github.com/ibis-project/ibis[+https://github.com/ibis-project/ibis+]

ICAR
~~~~

ICAR is a simplified atmospheric model designed primarily for climate downscaling, atmospheric sensitivity tests, and hopefully educational uses. At this early stage, the model is still undergoing rapid development, and users are encouraged to get updates frequently.

To run the model 3D time-varying atmospheric data are required, though an ideal test case can be generated for simple simulations as well. See "Running the Model" below. There are some sample python scripts to help make input forcing files, but the WRF pre-processing system can also be used. Low-resolution WRF output files can be used directly, various reanalysis and GCM output files can be used with minimal pre-processing (just get all the variables in the same netcdf file.) In addition, a high-resolution netCDF topography file is required. This will define the grid that ICAR will run on. Finally and ICAR options file is used to specify various parameters for the model. A sample options file is provided in the run/ directory.

https://github.com/NCAR/icar[+https://github.com/NCAR/icar+]

https://icar.readthedocs.io/en/develop/[+https://icar.readthedocs.io/en/develop/+]

IceT
~~~~

The Image Composition Engine for Tiles (IceT) is a high-performance sort-last parallel rendering library. In addition to providing accelerated rendering for a standard display, IceT provides the unique ability to generate images for tiled displays. The overall resolution of the display may be several times larger than any viewport that may be rendered by a single machine.

IceT is currently available for use in large-scale high-performance visualization and graphics applications. It is used in multiple production products like ParaView and VisIt. You can track the development of IceT with the Open Hub project.

https://icet.sandia.gov/[+https://icet.sandia.gov/+]

https://gitlab.kitware.com/icet/icet[+https://gitlab.kitware.com/icet/icet+]

https://github.com/kmorel/IceT[+https://github.com/kmorel/IceT+]

IcoAtmosBenchmark
~~~~~~~~~~~~~~~~~

Global atmospheric model with the icosahedral grid system in one of the new generation global climate/weather model. The grid-point calculations, which can be less computational costs than the spherical harmonics transformation, are used in the model. On the other hand, patterns of the data access in differential operators are more complicated than the traditional limited-area atmospheric model with the Cartesian grid system. There are different implementations of the dynamical core on the icosahedral grid: direct/indirect memory access, staggered/co-located data distribution, and so on. The objective of this kernel suite is to provide the variety of computational pattern of the icosahedral atmospheric models. The kernels are useful for evaluating the performance of new machines and new programming models.

The IcoAtmosBenchmark v1 provides various samples of the source code for the DSLs. The icosahedral grid system is unstructured grid coordinate, and there are a lot of challenging issues about data decomposition, data layout, loop structure, cache blocking, threading, offloading the accelerators, and so on. By applying DSLs or frameworks to the kernels, the developers can try detailed, practical evaluation of their software.

The participating models are the German ICON, the French DYNAMICO, and the Japanese NICAM.

https://aimes-project.github.io/IcoAtmosBenchmark_v1/[+https://aimes-project.github.io/IcoAtmosBenchmark_v1/+]

https://wr.informatik.uni-hamburg.de/research/projects/aimes/start[+https://wr.informatik.uni-hamburg.de/research/projects/aimes/start+]

FUCK

ICON
~~~~

The ICON modelling framework is a joint project between the German Weather Service and the Max Planck Institute for Meteorology for developing a unified next-generation global numerical weather prediction and climate modelling system. The ICON model has been introduced into DWD's operational forecast system in January 2015.

https://code.mpimet.mpg.de/projects/iconpublic[+https://code.mpimet.mpg.de/projects/iconpublic+]

https://www.mpimet.mpg.de/en/science/models/icon-esm/[+https://www.mpimet.mpg.de/en/science/models/icon-esm/+]

https://mpimet.mpg.de/en/communication/news/focus-on-overview/icon-earth-system-model/[+https://mpimet.mpg.de/en/communication/news/focus-on-overview/icon-earth-system-model/+]

https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.2378[+https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.2378+]

https://linkinghub.elsevier.com/retrieve/pii/S0021999117301961[+https://linkinghub.elsevier.com/retrieve/pii/S0021999117301961+]

https://www.geosci-model-dev.net/9/2755/2016/[+https://www.geosci-model-dev.net/9/2755/2016/+]

IDL
~~~

Blah.

EBF
^^^

EBF, which stands for Efficient and Easy to use Binary Format, is a binary file format for reading and writing binary data easily. Reading writing routines are currently available in C,Cxx,Fortran,Java, Python, IDL, MATLAB. A program called ebftkpy which has a set of utility functions to work with the .ebf files , e.g., viewing the contents and getting a summary, is also provided.

The EBF specification is designed to be concise and easy to understand to make it easier for others to write their own code if needed. It is also designed to simplify the programming of input output routines in different programming languages. In a nutshell an EBF file is a collection of data objects. Each data object is specified by a unique name and a single file can have multiple data objects. Each data object is preceded by a meta-data or header which describes the binary data associated with it. Among other things, this header allows the files to be portable across systems with different endianess. In EBF binary data is always written in native endian format (either little or big), and if needed byte swapping is done while reading. This makes writing data fast and eliminates the need for unnecessary swapping when reading and writing is done on the same architecture. EBF uses the row major format to specify the multidimensional arrays, which is similar to C/Cxx. Note, Fortran, IDL and Matlab use column major format.

* Store multiple data items in one file, each having a unique tag name + tagnames follow the convention of unix style pathname e.g. /x or /mydata/x + this allows hierarchical storage of data
* Constant time lookup of items due to use of an inbuilt hash table + one can store as many data items as one wants without any slow down
* Automatic type and endian conversion
* Support for mutiple programming languages + data can easily read in C, Cxx, Fortran, Java, IDL and Matlab + facilitates easy distribution of data
* Structures supported in Python and IDL + Nested structures are also supported

https://pythonhosted.org/ebfpy/intro.html[+https://pythonhosted.org/ebfpy/intro.html+]

http://ebfformat.sourceforge.net/build/idl.html[+http://ebfformat.sourceforge.net/build/idl.html+]

GDL
^^^

GDL is a free/libre/open source incremental compiler compatible with IDL (Interactive Data Language) and to some extent with PV-WAVE. Together with its library routines it serves as a tool for data analysis and visualization in such disciplines as astronomy, geosciences and medical imaging.

GDL is a domain-specific programming language and a data analysis environment. As a language, it is dynamically-typed, array-oriented, vectorised and has object-oriented programming capabilities. GDL library routines handle numerical calculations, data visualisation, signal/image processing, interaction with host OS and data input/output. GDL supports several data formats such as netCDF, HDF4, HDF5, GRIB, PNG, TIFF, DICOM, etc. Graphical output is handled by X11, PostScript, SVG or z-buffer terminals, the last one allowing output graphics (plots) to be saved in a variety of raster graphics formats. GDL features integrated debugging facilities. The built-in widget functionality enables development of GUI-based software. GDL has also a Python bridge (Python code can be called from GDL; GDL can be compiled as a Python module). Development and maintenance of GDL is carried out targeting Linux, BSD, OSX and Windows (MinGW, Cygwin).

https://github.com/gnudatalanguage/gdl[+https://github.com/gnudatalanguage/gdl+]

https://www.scivision.co/gdl-gnudatalanguage-reading-idl-sav-files/[+https://www.scivision.co/gdl-gnudatalanguage-reading-idl-sav-files/+]

CMSVLIB
^^^^^^^

This library allows interactive users and programmers to read, write and interrogate IDL SAVE files.

http://cow.physics.wisc.edu/\~craigm/idl/cmsave.html[+http://cow.physics.wisc.edu/~craigm/idl/cmsave.html+]

i2py
^^^^

i2py provides tools for converting programs and scripts written in Research
System Inc.'s IDL programming language to Python.  It is not an IDL-compatible
front end for the Python interpreter, nor does it make any attempt to replicate
the functionality of the IDL standard library.  Rather, its only purpose is to
perform source-to-source conversion of legacy IDL code to Python.  It supports
procedural IDL, with some support for object-oriented code, as long as it is
not too complex. More sophistication may be added in the future.

https://github.com/zimmerst/i2py[+https://github.com/zimmerst/i2py+]

http://www.johnny-lin.com/cdat_tips/tips_array/idl2num.html[+http://www.johnny-lin.com/cdat_tips/tips_array/idl2num.html+]

IDLSave
^^^^^^^

IDLSave has now been merged in to Scipy and is available from Scipy 0.9.0 onwards. To use the version in scipy.io, import the read function with from scipy.io.idl import readsav and then use readsav as you would use idlsave.read.

IDLSave is a pure python module to import variables from IDL 'save' files (e.g. .sav) into python, and does not require IDL to work. It has a very simple command-line interface, and converts all IDL variables to python types. Arrays are converted to Numpy arrays, and Structures are converted to Numpy record arrays.

https://github.com/astrofrog/idlsave[+https://github.com/astrofrog/idlsave+]

https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.readsav.html[+https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.readsav.html+]

Pike
^^^^

Pike provides the ability to automatically convert IDL source code into an execution Python package, while preserving the structure, comments and operational logic of the original implementation.

https://acceleratingvector.com/pike/[+https://acceleratingvector.com/pike/+]

ImageIO
~~~~~~~

Imageio is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, video, volumetric data, and scientific formats. It is cross-platform, runs on Python 2.7 and 3.4+, and is easy to install. 

Imageio has a relatively simple core that provides a common interface to different file formats. This core takes care of reading from different sources (like http), and exposes a simple API for the plugins to access the raw data. All file formats are implemented in plugins. Additional plugins can easily be registered. 

Some plugins rely on external libraries (e.g. ffmpeg). Imageio provides a way to download these with one function call, and prompts the user to do so when needed. The download is cached in your appdata directory, this keeps imageio light and scalable. 

Imageio was based out of the frustration that many libraries that needed to read or write image data produced their own functionality for IO. PIL did not meet the needs very well, and libraries like scikit-image need to be able to deal with scientific formats. There was a need for a good image io library, which is an easy dependency, easy to maintain, and scalable to exotic file formats. 

https://github.com/imageio/imageiop[+https://github.com/imageio/imageio+]

https://imageio.readthedocs.io/en/latest/formats.html[+https://imageio.readthedocs.io/en/latest/formats.html+]

https://imageio.readthedocs.io/en/latest/index.html[+https://imageio.readthedocs.io/en/latest/index.html+]

ImageJ
~~~~~~

ImageJ is an open source image processing program designed for scientific multidimensional images.
ImageJ is highly extensible, with thousands of plugins and scripts for performing a wide variety of tasks, and a large user community.

The ImageJ software stack is composed of the following core libraries:

* SciJava Common - The SciJava application container and plugin framework.
* ImgLib2 - The N-dimensional image data model.
* ImageJ Common - Metadata-rich image data structures and SciJava extensions.
* ImageJ Ops - The framework for reusable image processing operations.
* SCIFIO - The framework for N-dimensional image I/O.

These libraries form the basis of ImageJ-based software. 

Extensibility is ImageJ's greatest strength. ImageJ provides many different types of plugins, and it is possible to extend the system with your own new types of plugins.
The SciJava Common (SJC) library provides a plugin framework with strong typing, and makes extensive use of plugins itself, to allow core functionality to be customized easily. SJC has an powerful plugin discovery mechanism that finds all plugins available on the Java classpath, without knowing in advance what they are or where they are located.

https://imagej.net/Welcome[+https://imagej.net/Welcome+]

https://imagej.net/Plugins[+https://imagej.net/Plugins+]

https://imagej.net/Scripting[+https://imagej.net/Scripting+]

Fiji
^^^^

Fiji is an image processing package—a "batteries-included" distribution of ImageJ, bundling a lot of plugins which facilitate scientific image analysis. 

Fiji is easy to install and has an automatic update function, bundles a lot of plugins and offers comprehensive documentation. 
 Like ImageJ itself, Fiji is an open source project hosted in Git version control repositories, with access to the source code of all internals, libraries and plugins, easing the development and scripting of plugins. 
The Fiji project is driven by a strong desire to improve the tools available for life sciences to process and analyze data.

https://fiji.sc/[+https://fiji.sc/+]

ScientiFig
^^^^^^^^^^

ScientiFig is a free tool to help you create, format or reformat scientific figures.

https://grr.gred-clermont.fr/labmirouse/software/[+https://grr.gred-clermont.fr/labmirouse/software/+]

ImaGen
~~~~~~

The ImaGen package provides comprehensive support for creating resolution-independent one- and two-dimensional pattern distributions. ImaGen consists of a large library of primarily two-dimensional spatial patterns, including mathematical functions, geometric primitives, and images read from files, along with many ways to combine or select from any other patterns. These patterns can be used in any Python program that needs configurable patterns or streams of patterns. Basically, as long as the code can accept a Python callable and will call it each time it needs a new pattern, users can then specify any pattern possible in ImaGen's simple declarative pattern language, and the downstream code need not worry about any of the details about how the pattern is specified or generated. This approach gives users full flexibility about which patterns they wish to use, while relieving the downstream code from having to implement anything about patterns.

http://imagen.pyviz.org/[+http://imagen.pyviz.org/+]

https://github.com/ioam/imagen[+https://github.com/ioam/imagen+]

ImageVis3D
~~~~~~~~~~

ImageVis3D is a new volume rendering program developed by the NIH/NIGMS Center for Integrative Biomedical Computing (CIBC). The main design goals of ImageVis3D are: simplicity, scalability, and interactivity. Simplicity is achieved with a new user interface that gives an unprecedented level of flexibility. Scalability and interactivity mean that users can interactively explore terabyte-sized data sets on hardware ranging from mobile devices to high-end graphics workstations. Finally, the open source nature as well as the strict component-by-component design allow developers not only to extend ImageVis3D itself but also reuse parts of it, such as the rendering core. This rendering core for instance is planned to replace the volume rendering subsystems in many applications at the SCI Institute and with our collaborators.

https://github.com/SCIInstitute/ImageVis3D[+https://github.com/SCIInstitute/ImageVis3D+]

http://www.sci.utah.edu/software/imagevis3d.html[+http://www.sci.utah.edu/software/imagevis3d.html+]

ImplicitCAD
~~~~~~~~~~~

ImplicitCAD is a programmatic CAD program, implemented in haskell. Unlike traditional CAD programs, programmatic CAD programs use text descriptions of objects, as in programming. Concepts like variables, control structures and abstraction are used, just as in programming. This provides a number of advantages:

* Objects can be abstracted and reused
* Repetitive tasks can be automated
* Objects can be designed parametrically
* The usual tools for software development (like version control) can be used

The traditional example of programmatic CAD is OpenSCAD.

Generally, objects in programmatic CAD are built with Constructive Solid Geometry or CSG. Unions, intersections and differences of simpler shapes slowly build the object. ImplicitCAD supports all this and much more! For example, it provides rounded unions so that one can have smooth interfaces between objects.

It also directly provides some GCode generation, and has a parser for OpenSCAD to make it easier for people to transition/use.

http://www.implicitcad.org/[+http://www.implicitcad.org/+]

https://github.com/colah/ImplicitCAD[+https://github.com/colah/ImplicitCAD+]

inspyred
~~~~~~~~

inspyred is a free, open source framework for creating biologically-inspired computational intelligence algorithms in Python, including evolutionary computation, swarm intelligence, and immunocomputing. Additionally, inspyred provides easy-to-use canonical versions of many bio-inspired algorithms for users who do not need much customization.

Biologically-inspired computation encompasses a broad range of algorithms including evolutionary computation, swarm intelligence, and neural networks. These concepts are sometimes grouped together under a similar umbrella term – “computational intelligence” – which is a subfield of artificial intelligence. The common theme among all such algorithms is a decentralized, bottom-up approach which often leads to emergent properties or behaviors.

The inspyred library grew out of insights from Ken de Jong’s book “Evolutionary Computation: A Unified Approach.” The goal of the library is to separate problem-specific computation from algorithm-specific computation. Any bio-inspired algorithm has at least two aspects that are entirely problem-specific: what solutions to the problem look like and how such solutions are evaluated. These components will certainly change from problem to problem. For instance, a problem dealing with optimizing the volume of a box might represent solutions as a three-element list of real values for the length, width, and height, respectively. In contrast, a problem dealing with optimizing a set of rules for escaping a maze might represent solutions as a list of pair of elements, where each pair contains the two-dimensional neighborhood and the action to take in such a case.

On the other hand, there are algorithm-specific components that may make no (or only modest) assumptions about the type of solutions upon which they operate. These components include the mechanism by which parents are selected, the way offspring are generated, and the way individuals are replaced in succeeding generations. For example, the ever-popular tournament selection scheme makes no assumptions whatsoever about the type of solutions it is selecting. The n-point crossover operator, on the other hand, does make an assumption that the solutions will be linear lists that can be “sliced up,” but it makes no assumptions about the contents of such lists. They could be lists of numbers, strings, other lists, or something even more exotic.

https://github.com/aarongarrett/inspyred[+https://github.com/aarongarrett/inspyred+]

https://pypi.org/project/inspyred/[+https://pypi.org/project/inspyred/+]

http://aarongarrett.github.io/inspyred/[+http://aarongarrett.github.io/inspyred/+]

Intake
~~~~~~

Intake is a lightweight package for finding, investigating, loading and disseminating data. It will appeal to different groups for some of the reasons below, but is useful for all and acts as a common platform that everyone can use to smooth the progression of data from developers and providers to users.

A data user can:

* Intake loads the data for a range of formats and types (see Plugin Directory) into containers you already use, like Pandas dataframes, Python lists, NumPy arrays, and more
* Intake loads, and gets out of your way
* GUI, search and introspect data-sets in Catalogs: quickly find what you need to do your work
* Install data-sets and automatically get requirements
* Leverage cloud resources and distributed computing.

A data provider can:

* Simple spec to define data sources
* Single point-of truth, no more copy&paste
* Distribute data using packages, shared files or a server
* Update definitions in-place
* Parametrise user options
* Make use of additional functionality like filename parsing and caching.

Intake solves a related set of problems:

* Python API standards for loading data (such as DB-API 2.0) are optimized for transactional databases and query results that are processed one row at a time.
* Libraries that do load data in bulk tend to each have their own API for doing so, which adds friction when switching data formats.
* Loading data into a distributed data structure (like those found in Dask and Spark) often require writing a separate loader.
* Abstractions often focus on just one data model (tabular, n-dimensional array, or semi-structured), when many projects need to work with multiple kinds of data.

Intake has the explicit goal of not defining a computational expression system. Intake plugins load the data into containers (e.g., arrays or data-frames) that provide their data processing features. As a result, it is very easy to make a new Intake plugin with a relatively small amount of Python.

The plugins available for Intake include:

* intake-astro Table and array loading of FITS astronomical data
* intake-accumulo Apache Accumulo clustered data storage
* intake-avro: Apache Avro data serialization format
* intake-cmip: load CMIP (Coupled Model Intercomparison Project) data
* intake-dynamodb link to Amazon DynamoDB
* intake-elasticsearch: Elasticsearch search and analytics engine
* intake-geopandas: load ESRI Shape Files with geopandas
* intake-hbase: Apache HBase database
* intake-iris load netCDF and GRIB files with IRIS
* intake-mongo: MongoDB noSQL query
* intake-netflow: Netflow packet format
* intake-odbc: ODBC database
* intake-parquet: Apache Parquet file format
* intake-pcap: PCAP network packet format
* intake-postgres: PostgreSQL database
* intake-s3-manifests
* intake-solr: Apache Solr search platform
* intake-spark: data processed by Apache Spark
* intake-sql: Generic SQL queries via SQLAlchemy
* intake-splunk: Splunk machine data query
* intake-xarray: load netCDF, Zarr and other multi-dimensional data

https://github.com/ContinuumIO/intake[+https://github.com/ContinuumIO/intake+]

https://intake.readthedocs.io/en/latest/[+https://intake.readthedocs.io/en/latest/+]

internetarchive
~~~~~~~~~~~~~~~

internetarchive is a command-line and Python interface to archive.org.
The ia command-line tool is installed with internetarchive, or available as a binary. ia allows you to interact with various archive.org services from the command-line.

https://archive.org/services/docs/api/index.html[+https://archive.org/services/docs/api/index.html+]

Inviwo
~~~~~~

Inviwo is a software framework for the rapid prototyping of visualizations. It is written in Cxx, exploits modern graphics hardware, and is available under the BSD license, which permits free use in any setup - also commercially.

Inviwo provides a rich visual interface for the easy creation of custom visualizations. These visualizations can be saved, modified, and reused on other data.

Besides being realized in Cxx, Inviwo exposes a Python 3.7 API, supports Python development, and has Python scripting integrated for batch processing.

Inviwo can read and visualize many data types, such as HD5, DICOM, RAW and TIFF stacks. So you can visualize data from many simulations and imaging modalities.

https://inviwo.org/[+https://inviwo.org/+]

https://github.com/inviwo/inviwo[+https://github.com/inviwo/inviwo+]

Iodide
~~~~~~

Iodide is a modern, literate, and interactive programming environment that uses the strengths of the browser to let scientists work flexibly and collaboratively with minimal friction. With Iodide you can tell the story of your findings exactly how you want, leveraging the power of HTML+CSS to display your results in whatever way communicates them most effectively -- but still keeping the live, editable code only one click away. Because Iodide runs in the browser you already have, you can extend and modify the code without having to install any software, enabling you to collaborate frictionlessly.

And thanks to WebAssembly, working in the browser doesn't mean that you're restricted to working in just Javascript. Via the Pyodide project you can already do data science work in the browser using Python and the core of the Python science stack (Numpy, Pandas, and Matplotlib). And that's just the start. We envision a future workflow that allows you to do your data munging in Python, fit a quick model in R or JAGS, solve some differential equations in Julia, and then display your results with a live interactive d3+Javascript visualization... and all that within within a single, portable, sharable, and hackable file.

Our focus is on delivering frictionless, human-centered tools to scientists. You can read more about our core principles below; if our vision resonates with you, please consider contributing to the project!

https://github.com/iodide-project/iodide[+https://github.com/iodide-project/iodide+]

https://extremely-alpha.iodide.io/[+https://extremely-alpha.iodide.io/+]

https://iodide-project.github.io/docs/[+https://iodide-project.github.io/docs/+]

pyodide
^^^^^^^

The Python scientific stack, compiled to WebAssembly.  It provides transparent conversion of objects between Javascript and Python. When inside a browser, this means Python has full access to the Web APIs.

https://github.com/iodide-project/pyodide[+https://github.com/iodide-project/pyodide+]

IoT
~~~

Blah.

Blynk
^^^^^

Blynk is an Internet of Things Platform aimed to simplify building mobile and web applications for the Internet of Things. Easily connect 400+ hardware models like Arduino, ESP8266, ESP32, Raspberry Pi and similar MCUs and drag-n-drop IOT mobile apps for iOS and Android in 5 minutes.

https://github.com/blynkkk/blynk-server[+https://github.com/blynkkk/blynk-server+]

https://www.blynk.cc/[+https://www.blynk.cc/+]

ThingsBoard
^^^^^^^^^^^

ThingsBoard is an open-source IoT platform for data collection, processing, visualization, and device management.

It enables device connectivity via industry standard IoT protocols - MQTT, CoAP and HTTP and supports both cloud and on-premises deployments. ThingsBoard combines scalability, fault-tolerance and performance so you will never lose your data.

Provision, monitor and control your IoT entities in secure way using rich server-side APIs. Define relations between your devices, assets, customers or any other entities.

Collect and store telemetry data in scalable and fault-tolerant way. Visualize your data with built-in or custom widgets and flexible dashboards. Share dashboards with your customers.

Define data processing rule chains. Transform and normalize your device data. Raise alarms on incoming telemetry events, attribute updates, device inactivity and user actions.

Construct your ThingsBoard cluster and get maximum scalability and fault-tolerance with new microservices architecture. ThingsBoard also supports both cloud and on-premises deployments.

https://thingsboard.io/[+https://thingsboard.io/+]

https://github.com/thingsboard/thingsboard[+https://github.com/thingsboard/thingsboard+]

Iris
~~~~

Iris implements a data model based on the CF conventions giving you a powerful, format-agnostic interface for working with your data. It excels when working with multi-dimensional Earth Science data, where tabular representations become unwieldy and inefficient. 

CF Standard names, units, and coordinate metadata are built into Iris, giving you a rich and expressive interface for maintaining an accurate representation of your data. Its treatment of data and associated metadata as first-class objects includes:

* a visualisation interface based on matplotlib and cartopy,
* unit conversion,
* subsetting and extraction,
* merge and concatenate,
* aggregations and reductions (including min, max, mean and weighted averages),
* interpolation and regridding (including nearest-neighbor, linear and area-weighted), and
* operator overloads (+, -, *, /, etc.).

A number of file formats are recognised by Iris, including CF-compliant NetCDF, GRIB, and PP, and it has a plugin architecture to allow other formats to be added seamlessly.

Building upon NumPy and dask, Iris scales from efficient single-machine workflows right through to multi-core clusters and HPC. Interoperability with packages from the wider scientific Python ecosystem comes from Iris' use of standard NumPy/dask arrays as its underlying data storage. 

https://scitools.org.uk/iris/docs/latest/[+https://scitools.org.uk/iris/docs/latest/+]

https://github.com/SciTools/iris[+https://github.com/SciTools/iris+]

https://anaconda.org/conda-forge/iris[+https://anaconda.org/conda-forge/iris+]

iris-grib
^^^^^^^^^

The library iris-grib provides functionality for converting between weather and climate datasets that are stored as GRIB files and Iris cubes. GRIB files can be loaded as Iris cubes using iris-grib so that you can use Iris for analysing and visualising the contents of the GRIB files. Iris cubes can be saved to GRIB files using iris-grib.

The contents of iris-grib represent the former grib loading and saving capabilities of Iris itself. These capabilities have been separated into a discrete library so that Iris becomes less monolithic as a library.

https://iris-grib.readthedocs.io/en/stable/[+https://iris-grib.readthedocs.io/en/stable/+]

https://github.com/SciTools/iris-grib[+https://github.com/SciTools/iris-grib+]

Isca
~~~~

Isca is a framework for the idealized modelling of the global circulation of planetary atmospheres at varying levels of complexity and realism. The framework is an outgrowth of models from GFDL designed for Earth's atmosphere, but it may readily be extended into other planetary regimes. Various forcing and radiation options are available. At the simple end of the spectrum a Held-Suarez case is available. An idealized grey radiation scheme, a grey scheme with moisture feedback, a two-band scheme and a multi-band scheme are also available, all with simple moist effects and astronomically-based solar forcing. At the complex end of the spectrum the framework provides a direct connection to comprehensive atmospheric general circulation models.

For Earth modelling, options include an aqua-planet and configurable (idealized or realistic) continents with idealized or realistic topography. Continents may be defined by changing albedo, heat capacity and evaporative parameters, and/or by using a simple bucket hydrology model. Oceanic Q-fluxes may be added to reproduce specified sea-surface temperatures, with any continents or on an aquaplanet. Planetary atmospheres may be configured by changing planetary size, solar forcing, atmospheric mass, radiative, and other parameters.

The underlying model is written in Fortran and may largely be configured with Python scripts, with internal coding changes required for non-standard cases. Python scripts are also used to run the model on different architectures, to archive the output, and for diagnostics, graphics, and post-processing. All of these features are publicly available on a Git-based repository.

https://github.com/ExeClim/Isca[+https://github.com/ExeClim/Isca+]

ispc
~~~~

ispc is a compiler for a variant of the C programming language, with extensions for "single program, multiple data" (SPMD) programming. Under the SPMD model, the programmer writes a program that generally appears to be a regular serial program, though the execution model is actually that a number of program instances execute in parallel on the hardware. (See the ispc documentation for more details and examples that illustrate this concept.)

ispc compiles a C-based SPMD programming language to run on the SIMD units of CPUs and the Intel Xeon Phi™ architecture; it frequently provides a 3x or more speedup on CPUs with 4-wide vector SSE units and 5x-6x on CPUs with 8-wide AVX vector units, without any of the difficulty of writing intrinsics code. Parallelization across multiple cores is also supported by ispc, making it possible to write programs that achieve performance improvement that scales by both number of cores and vector unit size.

There are a few key principles in the design of ispc:

* To build a small set of extensions to the C language that would deliver excellent performance to performance-oriented programmers who want to run SPMD programs on the CPU.
* To provide a thin abstraction layer between the programmer and the hardware—in particular, to have an execution and data model where the programmer can cleanly reason about the mapping of their source program to compiled assembly language and the underlying hardware.
* To make it possible to harness the computational power of SIMD vector units without the extremely low-programmer-productivity activity of directly writing intrinsics.
* To explore opportunities from close coupling between C/Cxx application code and SPMD ispc code running on the same processor—to have lightweight function calls between the two languages and to share data directly via pointers without copying or reformatting.

ispc is an open source compiler with a BSD license. It uses the remarkable LLVM Compiler Infrastructure for back-end code generation and optimization and is hosted on github. It supports Windows, Mac, and Linux, with both x86 and x86-64 targets. It currently supports the SSE2, SSE4, AVX1, AVX2, AVX512, and Xeon Phi "Knight's Corner" instruction sets. 

https://ispc.github.io/[+https://ispc.github.io/+]

ISSM
~~~~

The Ice Sheet System Model (ISSM) is the result of a collaboration between the Jet Propulsion Laboratory and University of California at Irvine. Its purpose is to tackle the challenge of modeling the evolution of the polar ice caps in Greenland and Antarctica.

Large scale ice flow models are necessary that can accurately model the evolution of Greenland and Antarctica in a warming climate. In order to achieve this goal, and improve projections of future sea level rise, ISSM relies on state of the art technologies. These include:

* Finite Element Modeling, which allows for the use of unstructured meshes to reach high resolutions in areas where ice flow dynamics is critical.
* Higher-order ice dynamics: instead of relying on the Shallow Ice Approximation (SIA), ISSM includes a suite of model of increasing complexity, including Full-Stokes.
* Parallel technologies, using state of the art clusters such as the NASA Advanced Supercomputing Pleiades cluster. This allows ISSM to run bigger models, with a faster turn around.
* Anisotropic mesh refinement, which allows ISSM to zoom in on areas of interest, while saving computational resources by using coarse meshes where ice flow is stagnant.
* Data assimilation using inverse methods to infer unknown parameters from observations, using either variational data assimilation or using automatic differentiation.
* Sensitivity analysis tools, based on the Dakota toolkit from Sandia National Laboratories. This suite of tools allows ISSM to constrain projections of future sea level rise, and to assess the reliability of such projections.

https://issm.jpl.nasa.gov/[+https://issm.jpl.nasa.gov/+]

https://www.geosci-model-dev.net/12/215/2019/[+https://www.geosci-model-dev.net/12/215/2019/+]

istSOS
~~~~~~

istSOS is sensor data management tool that allows collection, maintenance and publishing of monitoring observations using the Open Geospatial Consortium (OGC) Sensor Observation Service (SOS) standard.

istSOS strictly implements the SOS 1.0.0 standard, and has passed the OGC CITE (Compliance + Interopability Testing + Evaluation) units tests.

The features include:

* Publish sensor data in accordance with the Sensor Observation Service (SOS) standard.
* Administer sensors and data with an intuitive web-based interface.
* Use a complete RestFul API to access istSOS functionality from external clients to create interactive charts, display sensor on a dinamic map or just write some maintainance scripts.
* Get notified through mail, twitter or other social media when sensor data meets specific conditions.
* User authentication and authorization with different access levels (administrator, network manager, data manager and visitor).
* Create Virtual Procedures that looks like normal stations but whose data results from on-the-fly elaboration of other sensor data.
* Associate quality indexes to each observation thanks to embedded validation tests.
* Online editing of data with a seamless interface and advanced observation calculator.

http://istsos.org/[+http://istsos.org/+]

https://github.com/istSOS[+https://github.com/istSOS+]

#JJJJ

JAS-mine
~~~~~~~~

JAS-mine is a Java platform that aims at providing a unique simulation tool for discrete-event simulations, including agent-based and microsimulation models.
With the aim to develop large-scale, data-driven models, the main architectural choice of JAS-mine is to use whenever possible standard, open-source tools already available in the software development community.

The main value added of the platform are: (1) integration of I/O communication tools, in the form of embedded RDBMS (relational database management systems) tools and automatic CSV table creation, (2) advanced multi-run tools to facilitate design of experiments (DOE), (3) sophisticated regression libraries that allow a complete separation of regression specifications from the code, and permit uncertainty analysis of the model outcome by bootstrapping the estimated coefficients across different simulation runs.

JAS-mine allows to separate data representation and management, which is automatically taken care of by the simulation engine, from the implementation of processes and behavioral algorithms, which should be the primary concern of the modeler. This results in quicker, more robust and more transparent model building.

The features include:

* A discrete-event simulation engine, allowing for both discrete-time and continuous-time simulation modeling
* A Model-Collector-Observer structure
* Interactive (GUI based) batch and multi-run execution modes, the latter allowing for detailed design of experiments (DOE)
* A library implementing a number of different matching methods, to match different lists of agents
* A library implementing a number of different alignment methods (including binary and multiple choice alignment), to force the microsimulation outcomes meeting some exogenous aggregate targets
* A Regression library implementing a number of common econometric models, from continuous response linear regression models to binomial and multinomial logit and probit models, which includes automatic bootstrapping of the coefficients for uncertainty analysis of the model oucomes
* A statistical package based on the cern.jet libraries
* Embedded H2 database
* Export to .CSV files as a faster alternative to database persistence
* MS Excel I/O communication tools
* Automatic GUI creation for parameters by using Java annotation
* Automatic output database creation
* Automatic agents’ sampling and recording
* Powerful probes for real-time statistical analysis and data collection
* A rich graphical library for real-time plotting of simulation outcomes
* Eclipse plugin, which allows to create a JAS-mine project in just a few clicks, with template classes organised in the JAS-mine standard package and folder structure

http://www.jas-mine.net/[+http://www.jas-mine.net/+]

https://github.com/jasmineRepo[+https://github.com/jasmineRepo+]

JAX
~~~

JAX is https://github.com/HIPS/autograd[Autograd] and https://www.tensorflow.org/xla[XLA], brought together for high-performance machine learning research.

With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via grad as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.

What’s new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, jit. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python.

Dig a little deeper, and you'll see that JAX is really an extensible system for composable function transformations. Both grad and jit are instances of such transformations. Another is vmap for automatic vectorization, with more to come.

https://github.com/google/jax[+https://github.com/google/jax+]

https://github.com/hips/autograd[+https://github.com/hips/autograd+]

JEM
~~~

Blah.

https://www.jem.gov/Modeling[+https://www.jem.gov/Modeling+]

EverView
^^^^^^^^

Over the past few years, modelers in South Florida have started using Network Common Data Form (NetCDF) as the standard data container format for storing hydrologic and ecologic modeling inputs and outputs. With its origins in the meteorological discipline, NetCDF was created by the Unidata Program Center at the University Corporation for Atmospheric Research, in conjunction with the National Aeronautics and Space Administration and other organizations. NetCDF is a portable, scalable, self-describing, binary file format optimized for storing array-based scientific data. Despite attributes which make NetCDF desirable to the modeling community, many natural resource managers have few desktop software packages which can consume NetCDF and unlock the valuable data contained within. The U.S. Geological Survey and the Joint Ecosystem Modeling group, an ecological modeling community of practice, are working to address this need with the EverVIEW Data Viewer. Available for several operating systems, this desktop software currently supports graphical displays of NetCDF data as spatial overlays on a three-dimensional globe and views of grid-cell values in tabular form. An included Open Geospatial Consortium compliant, Web-mapping service client and charting interface allows the user to view Web-available spatial data as additional map overlays and provides simple charting visualizations of NetCDF grid values.

Along with the EverVIEW Data Viewer, JEM has produced a set of data analytics and visualization extensions. These extensions offer enhanced capabilities to users, which were not available in earlier versions of the software. 

https://www.jem.gov/Modeling/GetView/EverView[+https://www.jem.gov/Modeling/GetView/EverView+]

https://www.jem.gov/Modeling/GetView/Extensions[+https://www.jem.gov/Modeling/GetView/Extensions+]

https://pubs.usgs.gov/fs/2010/3046/[+https://pubs.usgs.gov/fs/2010/3046/+]

NetCDF to CSV Converter
^^^^^^^^^^^^^^^^^^^^^^^

JEM NetCDF to CSV Converter exports time series data from a data variable in a CERP NetCDF-compliant file to a comma-separated values (CSV) file. The CSV file contains time stamp, location, and data value information for each cell in the grid, and has the option to filter out cells with missing data values from the export.

https://www.jem.gov/Modeling/GetView/CSVConverter[+https://www.jem.gov/Modeling/GetView/CSVConverter+]

NetCDF Grid Converter
^^^^^^^^^^^^^^^^^^^^^

n support of the Comprehensive Everglades Restoration Plan (CERP) NetCDF Standards, the JEM Data Toolbox has been expanded to include the NetCDF Grid Converter. The JEM NetCDF Grid Converter is an application used to convert NetCDF data variables between different metadata conventions, based on either orthogonal or unstructured grids. Currently, the converter supports the CERP Orthogonal NetCDF Standard, the CERP Unstructured Grid NetCDF Standard, and the South Florida Water Management District (SFWMD) Regional Simulation Model (RSM).

https://www.jem.gov/Modeling/GetView/GridConverter[+https://www.jem.gov/Modeling/GetView/GridConverter+]

Slice and Dice
^^^^^^^^^^^^^^

The NetCDF Slice and Dice tool is the first tool from the JEM Data Toolbox to be released. This tool allows the user to “subset” a netCDF data file both temporally and geographically.

https://www.jem.gov/Modeling/GetView/SliceAndDice[+https://www.jem.gov/Modeling/GetView/SliceAndDice+]

https://pubs.usgs.gov/fs/2010/3035/[+https://pubs.usgs.gov/fs/2010/3035/+]

JIDT
~~~~

JIDT provides a stand-alone, open-source code Java implementation (also usable in Matlab, Octave, Python, R, Julia and Clojure) of information-theoretic measures of distributed computation in complex systems: i.e. information storage, transfer and modification.

JIDT includes implementations:

* principally for the measures transfer entropy, mutual information, and their conditional variants, as well as active information storage, entropy, etc;
* for both discrete and continuous-valued data;
* using various types of estimators (e.g. Kraskov-Stögbauer-Grassberger estimators, box-kernel estimation, linear-Gaussian), as described in full at ImplementedMeasures.

JIDT is distributed under the GNU GPL v3 license (or later).

http://jlizier.github.io/jidt/[+http://jlizier.github.io/jidt/+]

JIGSAW
~~~~~~

JIGSAW(GEO) is a set of algorithms designed to generate unstructured grids for geoscientific modelling. Applications include: large-scale atmospheric simulation and numerical weather prediction, global and coastal ocean-modelling, and ice-sheet dynamics.

JIGSAW(GEO) can be used to produce high-quality 'generalised' Delaunay / Voronoi tessellations for unstructured finite-volume / element type models. Grids can be generated in local two-dimensional domains, and over general spheroidal surfaces. Mesh resolution can be adapted to follow complex user-defined metrics, including: topographic contours, discrete solution profiles or coastal features. These features enable the generation of complex, multi-resolution climate process models, with simulation fidelity enhanced in regions of interest.

JIGSAW(GEO) is a stand-alone mesh generator written in cxx, based on the general-purpose
meshing package https://github.com/dengwirda/jigsaw[JIGSAW]. This toolbox provides a MATLAB / OCTAVE based scripting interface, including file I/O, mesh visualisation and post-processing facilities. The underlying JIGSAW library is a collection of unstructured triangle- and tetrahedron-based meshing algorithms, designed to produce high quality Delaunay-based grids for computational simulation. JIGSAW includes both Delaunay-refinement based algorithms for the construction of new meshes, as well as optimisation driven methods for the improvement of existing grids.

JIGSAW(GEO) is typically able to produce the very high-quality staggered unstructured grids required by contemporary unstructued general circulation models (i.e. MPAS, https://github.com/csiro-coasts/EMS/tree/master/model/hd-us[COMPAS], https://fesom.de/[FESOM],
https://code.mpimet.mpg.de/projects/iconpublic[ICON], etc), generating highly optimised, multi-resolution meshes that are locally-orthogonal, mutually-centroidal and self-centred.

JIGSAW(GEO) has been compiled and tested on various 64-bit Linux , Windows and Mac based platforms.

https://github.com/dengwirda/jigsaw-geo-matlab[+https://github.com/dengwirda/jigsaw-geo-matlab+]

job schedulers
~~~~~~~~~~~~~~

A job scheduler is a computer application for controlling unattended background program execution of jobs.[1] This is commonly called batch scheduling, as execution of non-interactive jobs is often called batch processing, though traditional job and batch are distinguished and contrasted; see that page for details. Other synonyms include batch system, distributed resource management system (DRMS), distributed resource manager (DRM), and, commonly today, workload automation (WLA). The data structure of jobs to run is known as the job queue.

Modern job schedulers typically provide a graphical user interface and a single point of control for definition and monitoring of background executions in a distributed network of computers. Increasingly, job schedulers are required to orchestrate the integration of real-time business activities with traditional background IT processing across different operating system platforms and business application environments. 

http://xed.ch/project/xeduler/[+http://xed.ch/project/xeduler/+]

https://en.wikipedia.org/wiki/Job_scheduler[+https://en.wikipedia.org/wiki/Job_scheduler+]

GNUbatch
^^^^^^^^

GNUbatch is a job scheduler. It executes jobs at specified dates and times, according to dependencies, or interlocks defined by the user.

It provides Batch scheduling with "job control variables" which may be atomically tested and set to provide arbitrarily complicated job chaining and interlocking. Interfaces are provided for shell, "curses", GTK, API for C and Cxx (has been adapted for Java), web browser, and Motif. 

https://www.gnu.org/software/gnubatch/[+https://www.gnu.org/software/gnubatch/+]

GNU parallel
^^^^^^^^^^^^

GNU parallel is a shell tool for executing jobs in parallel using one or more computers. A job can be a single command or a small script that has to be run for each of the lines in the input. The typical input is a list of files, a list of hosts, a list of users, a list of URLs, or a list of tables. A job can also be a command that reads from a pipe. GNU parallel can then split the input and pipe it into commands in parallel.

If you use xargs and tee today you will find GNU parallel very easy to use as GNU parallel is written to have the same options as xargs. If you write loops in shell, you will find GNU parallel may be able to replace most of the loops and make them run faster by running several jobs in parallel.

GNU parallel makes sure output from the commands is the same output as you would get had you run the commands sequentially. This makes it possible to use output from GNU parallel as input for other programs.

For each line of input GNU parallel will execute command with the line as arguments. If no command is given, the line of input is executed. Several lines will be run in parallel. GNU parallel can often be used as a substitute for xargs or cat | bash. 

https://www.gnu.org/software/parallel/[+https://www.gnu.org/software/parallel/+]

Slurm
^^^^^

The Slurm Workload Manager (formerly known as Simple Linux Utility for Resource Management or SLURM), or Slurm, is a free and open-source job scheduler for Linux and Unix-like kernels, used by many of the world's supercomputers and computer clusters.

It provides three key functions:

* allocating exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work,
* providing a framework for starting, executing, and monitoring work (typically a parallel job such as MPI) on a set of allocated nodes, and
* arbitrating contention for resources by managing a queue of pending jobs.

Slurm is the workload manager on about 60% of the TOP500 supercomputers.

Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for its operation and is relatively self-contained. As a cluster workload manager, Slurm has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work. Optional plugins can be used for accounting, advanced reservation, gang scheduling (time sharing for parallel jobs), backfill scheduling, topology optimized resource selection, resource limits by user or bank account, and sophisticated multifactor job prioritization algorithms. 

Slurm has a centralized manager, slurmctld, to monitor resources and work. There may also be a backup manager to assume those responsibilities in the event of failure. Each compute server (node) has a slurmd daemon, which can be compared to a remote shell: it waits for work, executes that work, returns status, and waits for more work. The slurmd daemons provide fault-tolerant hierarchical communications. There is an optional slurmdbd (Slurm DataBase Daemon) which can be used to record accounting information for multiple Slurm-managed clusters in a single database. User tools include srun to initiate jobs, scancel to terminate queued or running jobs, sinfo to report system status, squeue to report the status of jobs, and sacct to get information about jobs and job steps that are running or have completed. The smap and sview commands graphically reports system and job status including network topology. There is an administrative tool scontrol available to monitor and/or modify configuration and state information on the cluster. The administrative tool used to manage the database is sacctmgr. It can be used to identify the clusters, valid users, valid bank accounts, etc.

https://slurm.schedmd.com/[+https://slurm.schedmd.com/+]

https://en.wikipedia.org/wiki/Slurm_Workload_Manager[+https://en.wikipedia.org/wiki/Slurm_Workload_Manager+]

Son of Grid Engine
^^^^^^^^^^^^^^^^^^

The Son of Grid Engine is a community project to continue Sun's old gridengine free software project
after Oracle shut down the site and stopped contributing code. (Univa now own the copyright — see below.) It will maintain copies of as much as possible/useful from the old site. 

The idea is to encourage sharing, in the spirit of the original project, informed by long experience of free software projects and scientific computing support. Please contribute, and share code or ideas for improvement, especially any ideas for encouraging contribution. 
This effort precedes Univa taking over gridengine maintenance and subsequently apparently making it entirely proprietary, rather than the originally-promised ‘open core’. What's here was originally based on ​Univa's free code and was intended to be fed into that. 

Currently most information you find for the gridengine v6.2u5 release will apply to this effort, but the non-free documentation that used to be available from Oracle has been expurgated and no-one has the time/interest to replace it.

https://arc.liv.ac.uk/trac/SGE[+https://arc.liv.ac.uk/trac/SGE+]

https://arc.liv.ac.uk/SGE/[+https://arc.liv.ac.uk/SGE/+]

Torque
^^^^^^

TORQUE (Terascale Open-source Resource and Queue manger) is an open source project based on the original PBS* resource manager developed by NASA, LLNL, and MRJ. It possesses a large number of enhancements contributed by organizations such as OSC, NCSA, TeraGrid, the U.S Dept of Energy, USC, and many, many others. It continues to incorporate significant advancements in the areas of scalability, fault-tolerance, usability, functionality, and security development from the community and vendor supporters.

https://github.com/adaptivecomputing/torque[+https://github.com/adaptivecomputing/torque+]

http://www.adaptivecomputing.com/support/documentation-index/[+http://www.adaptivecomputing.com/support/documentation-index/+]

xeduler
^^^^^^^

Xeduler simplistically (FIFO) schedules and dispatches jobs to multiple resources. Because this is all it does, it is possible for an administrator to deploy and completely understand the scheduling system with very little effort. Also, by focusing on such a limited scope, a user can preserve his choice of tools for other things or choose not to do anything else.

There are many things that Xeduler does not do. Xeduler does not do intelligent, NP hard bin packing kind of scheduling; besides being very hard to do, it is very hard to make it broadly applicable. Xeduler does not provide health monitoring services for resources. Why should it? Maybe you don’t need that. Maybe your resources are very exotic and you want a custom monitoring system. Whatever your monitoring requirements are, Xeduler makes it easy to integrate them. Xeduler does not do load balancing. Again, requirements for that could be so varied that extreme complexity would be necessary to cover all possibilities. Xeduler is designed so that it would be quite simple to incorporate a load balancing system. Xeduler does not provide redundant systems to remotely execute jobs somewhere else. For that, use SSH or RSH or OpenMosix or whatever you like. Those facilities exist, work fine, and don’t need replacing.

http://xed.ch/project/xeduler/[+http://xed.ch/project/xeduler/+]

JOSM
~~~~

JOSM is an extensible editor for OpenStreetMap (OSM) for Java 8+.

It supports loading GPX tracks, background imagery and OSM data from local sources as well as from online sources and allows to edit the OSM data (nodes, ways, and relations) and their metadata tags. 

https://josm.openstreetmap.de/[+https://josm.openstreetmap.de/+]

JPype
~~~~~

JPype is an effort to allow python programs full access to java class libraries. This is achieved not through re-implementing Python, as Jython/JPython has done, but rather through interfacing at the native level in both virtual machines. Eventually, it should be possible to replace Java with python in many, though not all, situations. JSP, Servlets, RMI servers and IDE plugins are good candidates.

https://jpype.readthedocs.io/en/latest/[+https://jpype.readthedocs.io/en/latest/+]

https://github.com/jpype-project/jpype[+https://github.com/jpype-project/jpype+]

JSON
~~~~

Blah.

ceda-di
^^^^^^^

The ceda-di project is a suite of Python scripts and tools to extract JSON metadata from various scientific data formats, including ENVI BIL/BSQ, NetCDF, GeoTIFF and HDF.

The Python backend is designed to be run on a system with a large number of CPU cores. It extracts metadata from scientific data files and outputs it as platform-independent JSON documents.

This JSON metadata can then be stored in a NoSQL data store such as ElasticSearch. This repository contains some example applications of the toolkit, including an Ansible playbook to set up and configure an ElasticSearch cluster. This repo also contains a sample web interface that allows for real-time faceted search (including full-text, temporal, and geospatial facets) and live display of data files on a map.

https://github.com/cedadev/ceda-di[+https://github.com/cedadev/ceda-di+]

jsonlines
^^^^^^^^^

jsonlines is a Python library to simplify working with jsonlines and ndjson data.

This data format is straight-forward: it is simply one valid JSON value per line, encoded using UTF-8. While code to consume and create such data is not that complex, it quickly becomes non-trivial enough to warrant a dedicated library when adding data validation, error handling, support for both binary and text streams, and so on. This small library implements all that (and more!) so that applications using this format do not have to reinvent the wheel.

https://github.com/wbolster/jsonlines[+https://github.com/wbolster/jsonlines+]

https://jsonlines.readthedocs.io/en/latest/[+https://jsonlines.readthedocs.io/en/latest/+]

CoverageJSON
^^^^^^^^^^^^

A format for publishing geotemporal data to the web.

https://covjson.org/[+https://covjson.org/+]

Create CovJSON files from common scientific data formats using Python.

https://github.com/Reading-eScience-Centre/pycovjson[+https://github.com/Reading-eScience-Centre/pycovjson+]

JSON Lines
^^^^^^^^^^

JSON Lines is a convenient format for storing structured data that may be processed one record at a time. It works well with unix-style text processing tools and shell pipelines. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes.

http://jsonlines.org/[+http://jsonlines.org/+]

https://github.com/jsonlines[+https://github.com/jsonlines+]

NDJSON
^^^^^^

NDJSON is a convenient format for storing or streaming structured data that may be processed one record at a time. It works well with unix-style text processing tools and shell pipelines. It's a great format for log files. It's also a flexible format for passing messages between cooperating processes.

http://ndjson.org/[+http://ndjson.org/+]

https://github.com/ndjson[+https://github.com/ndjson+]

simdjson
^^^^^^^^

We present the first standard-compliant JSON parser to process gigabytes of data per second on a single core, using commodity processors. We can use a quarter or fewer instructions than a state-of-the-art reference parser like RapidJSON. Unlike other validating parsers, our software (simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD) instructions.

https://arxiv.org/abs/1902.08318[+https://arxiv.org/abs/1902.08318+]

https://github.com/lemire/simdjson[+https://github.com/lemire/simdjson+]

UltraJSON
^^^^^^^^^

UltraJSON is an ultra fast JSON encoder and decoder written in pure C with bindings for Python 2.5+ and 3.

https://github.com/esnme/ultrajson[+https://github.com/esnme/ultrajson+]

Jug
~~~

Jug allows you to write code that is broken up into tasks and run different tasks on different processors.

It uses the filesystem to communicate between processes and works correctly over NFS, so you can coordinate processes on different machines.

Jug is a pure Python implementation and should work on any platform.  Python 2.6/2.7 and Python 3.3+ are supported.

https://github.com/luispedro/jug[+https://github.com/luispedro/jug+]

https://jug.readthedocs.io/en/latest/[+https://jug.readthedocs.io/en/latest/+]

Julia
~~~~~

Julia is a high-level general-purpose dynamic programming language that is designed to address the needs of high-performance numerical analysis and computational science, without the need of separate compilation to be fast. It is also useful for low-level systems programming, as a specification language, with work being done on client and server web use.

Distinctive aspects of Julia's design include a type system with parametric polymorphism and types in a fully dynamic programming language and multiple dispatch as its core programming paradigm. It allows concurrent, parallel and distributed computing, and direct calling of C and Fortran libraries without glue code.

Julia is garbage-collected, uses eager evaluation, and includes efficient libraries for floating-point calculations, linear algebra, random number generation, and regular expression matching. Many libraries are available.

https://julialang.org/[+https://julialang.org/+]

https://juliaobserver.com/[+https://juliaobserver.com/+]

https://en.wikipedia.org/wiki/Julia_(programming_language)[+https://en.wikipedia.org/wiki/Julia_(programming_language)+]

https://benlauwens.github.io/ThinkJulia.jl/latest/book.html[+https://benlauwens.github.io/ThinkJulia.jl/latest/book.html+]

ExpressionTemplates.jl
^^^^^^^^^^^^^^^^^^^^^^

This package provides Julia with deferred evaluation of array expressions using a Cxx-style Expression Template framework. The purpose of the package is to greatly accelerate the evaluation of vectorized expressions by eliminating allocation of intermediate calculations. For example, for scalar a, b, c, d, and length-N vectors A, B, C, D, the expression R = a*A + b*B + c*C + d*D requires the construction of seven temporary arrays. A more optimal way to compute this is to allocate an output array and compute the result index-by-index. Expression templates exploit the language's type system to do precisely this, thus avoiding the overhead of intermediate array allocations.

As far as users are concerned, this package provides one thing: the @et macro. This macro can be put at the beginning of a line or expression to trigger the usage of expression templates. The macro then parses the expression it is given, wraps arrays in appropriate types (no copying done), lets julia's multiple dispatch system build a complex type based on the operations called for in the expression, and finally constructs an array object index-by-index from this complex type. The result is that the two expressions below are exactly equal, but the one using @et will have avoided any unnecessary array allocations.

https://github.com/tjolsen/ExpressionTemplates.jl[+https://github.com/tjolsen/ExpressionTemplates.jl+]

JuliaStats
~~~~~~~~~~

Blah.

Clustering.jl
^^^^^^^^^^^^^

A Julia package that provides a set of algorithms for data clustering.
The clustering algorithms are:

* K-means
* K-medoids
* Affinity Propagation
* Density-based spatial clustering of applications with noise (DBSCAN)
* Markov Clustering Algorithm (MCL)
* Fuzzy C-Means Clustering
* Hierarchical Clustering
** Single Linkage
** Average Linkage
** Complete Linkage
** Ward's Linkage

https://github.com/JuliaStats/Clustering.jl[+https://github.com/JuliaStats/Clustering.jl+]

Distances.jl
^^^^^^^^^^^^

A Julia package for evaluating distances(metrics) between vectors.
This package also provides optimized functions to compute column-wise and pairwise distances, which are often substantially faster than a straightforward loop implementation.
The supported distances are:

* Euclidean distance
* Squared Euclidean distance
* Cityblock distance
* Total variation distance
* Jaccard distance
* Rogers-Tanimoto distance
* Chebyshev distance
* Minkowski distance
* Hamming distance
* Cosine distance
* Correlation distance
* Chi-square distance
* Kullback-Leibler divergence
* Generalized Kullback-Leibler divergence
* Rényi divergence
* Jensen-Shannon divergence
* Mahalanobis distance
* Squared Mahalanobis distance
* Bhattacharyya distance
* Hellinger distance
* Haversine distance
* Mean absolute deviation
* Mean squared deviation
* Root mean squared deviation
* Normalized root mean squared deviation
* Bray-Curtis dissimilarity
* Bregman divergence

https://github.com/JuliaStats/Distances.jl[+https://github.com/JuliaStats/Distances.jl+]

Distributions.jl
^^^^^^^^^^^^^^^^

A Julia package for probability distributions and associated functions. Particularly, Distributions implements:

* Moments (e.g mean, variance, skewness, and kurtosis), entropy, and other properties
* Probability density/mass functions (pdf) and their logarithm (logpdf)
* Moment generating functions and characteristic functions
* Sampling from population or from a distribution
* Maximum likelihood estimation

https://github.com/JuliaStats/Distributions.jl[+https://github.com/JuliaStats/Distributions.jl+]

GLM.jl
^^^^^^

Generalized linear models in Julia.
Typical distributions for use with GLM and their canonical link functions are:

* Bernoulli (LogitLink)
* Binomial (LogitLink)
* Gamma (InverseLink)
* InverseGaussian (InverseSquareLink)
* NegativeBinomial (LogLink)
* Normal (IdentityLink)
* Poisson (LogLink)

The methods available to apply to fitted models are:

* coef: extract the estimates of the coefficients in the model
* deviance: measure of the model fit, weighted residual sum of squares for lm's
* dof_residual: degrees of freedom for residuals, when meaningful
* glm: fit a generalized linear model (an alias for fit(GeneralizedLinearModel, ...))
* lm: fit a linear model (an alias for fit(LinearModel, ...))
* stderror: standard errors of the coefficients
* vcov: estimated variance-covariance matrix of the coefficient estimates
* predict : obtain predicted values of the dependent variable from the fitted model

https://github.com/JuliaStats/GLM.jl[+https://github.com/JuliaStats/GLM.jl+]

http://juliastats.github.io/GLM.jl/dev/manual/[+http://juliastats.github.io/GLM.jl/dev/manual/+]

Lasso.jl
^^^^^^^^

A Julia package of Lasso/Elastic Net linear and generalized linear models.

https://github.com/JuliaStats/Lasso.jl[+https://github.com/JuliaStats/Lasso.jl+]

https://lassojl.readthedocs.io/en/latest/[+https://lassojl.readthedocs.io/en/latest/+]

MultivariateStats.jl
^^^^^^^^^^^^^^^^^^^^

A Julia package for multivariate statistics and data analysis (e.g. dimensionality reduction).
The functionality includes:

* Linear Least Square Regression
* Ridge Regression
* Data Whitening
* Principal Components Analysis (PCA)
* Canonical Correlation Analysis (CCA)
* Classical Multidimensional Scaling (MDS)
* Linear Discriminant Analysis (LDA)
* Multiclass LDA
* Independent Component Analysis (ICA), FastICA
* Probabilistic PCA
* Factor Analysis
* Kernel PCA

https://github.com/JuliaStats/MultivariateStats.jl[+https://github.com/JuliaStats/MultivariateStats.jl+]

StatsFuns.jl
^^^^^^^^^^^^

This package provides a collection of mathematical constants and numerical functions for statistical computing.
Dozens and dozens of such things.

https://github.com/JuliaStats/StatsFuns.jl[+https://github.com/JuliaStats/StatsFuns.jl+]

StatsModels.jl
^^^^^^^^^^^^^^

Basic functionality for specifying, fitting, and evaluating statistical models in Julia.
This package provides common abstractions and utilities for specifying, fitting, and evaluating statistical models. The goal is to provide an API for package developers implementing different kinds of statistical models (see the GLM package for example), and utilities that are generally useful for both users and developers when dealing with statistical models and tabular data.  This includes:

* Formula notation for transforming tabular data into numerical arrays for modeling.
* Mechanisms for extending the @formula notation in external modeling packages.
* Contrast coding for categorical data
* Types and API for fitting and working with statistical models, extending StatsBase.jl's API to tabular data.

https://juliastats.github.io/StatsModels.jl/latest/[+https://juliastats.github.io/StatsModels.jl/latest/+]

https://github.com/JuliaStats/StatsModels.jl[+https://github.com/JuliaStats/StatsModels.jl+]

TimeSeries.jl
^^^^^^^^^^^^^

TimeSeries aims to provide a lightweight framework for working with time series data in Julia.

https://github.com/JuliaStats/TimeSeries.jl[+https://github.com/JuliaStats/TimeSeries.jl+]

http://juliastats.github.io/TimeSeries.jl/latest/[+http://juliastats.github.io/TimeSeries.jl/latest/+]

JuMP.jl
~~~~~~~

JuMP is a domain-specific modeling language for mathematical optimization embedded in Julia. It currently supports a number of open-source and commercial solvers (Artelys Knitro, BARON, Bonmin, Cbc, Clp, Couenne, CPLEX, ECOS, FICO Xpress, GLPK, Gurobi, Ipopt, MOSEK, NLopt, SCS) for a variety of problem classes, including linear programming, (mixed) integer programming, second-order conic programming, semidefinite programming, and nonlinear programming.

JuMP makes it easy to specify and solve optimization problems without expert knowledge, yet at the same time allows experts to implement advanced algorithmic techniques such as exploiting efficient hot-starts in linear programming or using callbacks to interact with branch-and-bound solvers. JuMP is also fast - benchmarking has shown that it can create problems at similar speeds to special-purpose commercial tools such as AMPL while maintaining the expressiveness of a generic high-level programming language. JuMP can be easily embedded in complex work flows including simulations and web servers.

Our documentation includes an installation guide, quick-start guide, and reference manual. The juliaopt-notebooks repository contains a small but growing collection of contributed examples.

https://github.com/JuliaOpt/JuMP.jl[+https://github.com/JuliaOpt/JuMP.jl+]

http://www.juliaopt.org/JuMP.jl/0.18/[+http://www.juliaopt.org/JuMP.jl/0.18/+]

Jupyter
~~~~~~~

The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.

The features include:

* The Notebook has support for over 40 programming languages, including Python, R, Julia, and Scala.
* Notebooks can be shared with others using email, Dropbox, GitHub and the Jupyter Notebook Viewer
* Your code can produce rich, interactive output: HTML, images, videos, LaTeX, and custom MIME types
* Leverage big data tools, such as Apache Spark, from Python, R and Scala. Explore that same data with pandas, scikit-learn, ggplot2, TensorFlow

https://jupyter.org/[+https://jupyter.org/+]

https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html[+https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html+]

https://github.com/mwouts/awesome-jupyter[+https://github.com/mwouts/awesome-jupyter+]

DRILSDOWN
^^^^^^^^^

DRILSDOWN brings the GUI-driven 3D geospatial data visualization power of Unidata’s IDV into the world of Jupyter notebooks. It also envisions a higher-level object, a “Case Study”, comprising a notebook.ipynb, an IDV bundle.zidv, images, and anything else. Finally, we have a repository structure (in Geode Systems’ RAMADDA system) for these Case Study objects, with services.

https://unidata.github.io/drilsdown/index.html[+https://unidata.github.io/drilsdown/index.html+]

Geonotebook
^^^^^^^^^^^

GeoNotebook is an application that provides client/server environment with interactive visualization and analysis capabilities using Jupyter, GeoJS and other open source tools. 

GeoNotebook extends the Jupyter Notebook interface by adding a large Open Street Map style map to the right of the traditional notebook cells.

We provide a python API from with-in the notebook cells for visualizing raster and vector data on the map. We also provide basic tools for selecting regions of the visualized data and making those regions available as data subsets in the notebook.

GeoNotebook extends Jupyter’s web server to provide a fully integrated default tile server based on KTile (a fork of TileStache), Mapnik, and GDAL. This tile server allows data to be rendered server-side and delivered as down-sampled images for visualization while full resolution access to the data is still available via the notebook cells.

https://github.com/OpenGeoscience/geonotebook[+https://github.com/OpenGeoscience/geonotebook+]

https://geonotebook.readthedocs.io/en/latest/[+https://geonotebook.readthedocs.io/en/latest/+]

iPyPublish
^^^^^^^^^^

A program for creating and editing publication ready scientific reports and presentations, from one or more Jupyter Notebooks.

In essence, the dream is to have the ultimate hybrid of Jupyter Notebook, WYSIWYG editor (e.g. MS Word) and document preparation system (e.g. TexMaker), being able to:

* Dynamically (and reproducibly) explore data, run code and output the results
* Dynamically edit and visualise the basic components of the document (text, math, figures, tables, references, citations, etc).
* Have precise control over what elements are output to the final document and how they are layed out and typeset.
* Also be able to output the same source document to different layouts and formats (pdf, html,presentation slides, etc).

https://github.com/chrisjsewell/ipypublish/[+https://github.com/chrisjsewell/ipypublish/+]

JupyterLab
^^^^^^^^^^

JupyterLab is the next-generation web-based user interface for Project Jupyter.

JupyterLab enables you to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner. You can arrange multiple documents and activities side by side in the work area using tabs and splitters. Documents and activities integrate with each other, enabling new workflows for interactive computing, for example:

* Code Consoles provide transient scratchpads for running code interactively, with full support for rich output. A code console can be linked to a notebook kernel as a computation log from the notebook, for example.
* Kernel-backed documents enable code in any text file (Markdown, Python, R, LaTeX, etc.) to be run interactively in any Jupyter kernel.
* Notebook cell outputs can be mirrored into their own tab, side by side with the notebook, enabling simple dashboards with interactive controls backed by a kernel.
* Multiple views of documents with different editors or viewers enable live editing of documents reflected in other viewers. For example, it is easy to have live preview of Markdown, Delimiter-separated Values, or Vega/Vega-Lite documents.

JupyterLab also offers a unified model for viewing and handling data formats. JupyterLab understands many file formats (images, CSV, JSON, Markdown, PDF, Vega, Vega-Lite, etc.) and can also display rich kernel output in these formats.

JupyterLab extensions can customize or enhance any part of JupyterLab, including new themes, file editors, and custom components.

JupyterLab is served from the same server and uses the same notebook document format as the classic Jupyter Notebook.

https://jupyterlab.readthedocs.io/en/stable/[+https://jupyterlab.readthedocs.io/en/stable/+]

jupyterlab-latex
^^^^^^^^^^^^^^^^

An extension for JupyterLab which allows for live-editing of LaTeX documents.

https://github.com/jupyterlab/jupyterlab-latex[+https://github.com/jupyterlab/jupyterlab-latex+]

jupyterlab_thredds
^^^^^^^^^^^^^^^^^^

JupyterLab dataset browser for THREDDS catalog
Can inject iris/xarray/leaflet code cells into a Python notebook of a selected dataset to further process/visualize the dataset.

https://github.com/eWaterCycle/jupyterlab_thredds[+https://github.com/eWaterCycle/jupyterlab_thredds+]

Jupytext
^^^^^^^^

Jupytext can save Jupyter notebooks as

* Markdown and R Markdown documents,
* Julia, Python, R, Bash, Scheme, Cxx and q/kdb+ scripts.

There are multiple ways to use jupytext:

* Directly from Jupyter Notebook or JupyterLab. Jupytext provides a contents manager that allows Jupyter to save your notebook to your favorite format (.py, .R, .jl, .md, .Rmd...) in addition to (or in place of) the traditional .ipynb file. The text representation can be edited in your favorite editor. When you're done, refresh the notebook in Jupyter: inputs cells are loaded from the text file, while output cells are reloaded from the .ipynb file if present. Refreshing preserves kernel variables, so you can resume your work in the notebook and run the modified cells without having to rerun the notebook in full.
* On the command line. jupytext converts Jupyter notebooks to their text representation, and back. The command line tool can act on noteboks in many ways. It can synchronize multiple representations of a notebook, pipe a notebook into a reformatting tool like black, etc... It can also work as a pre-commit hook if you wish to automatically update the text representation when you commit the .ipynb file.
* in Vim: edit your Jupyter notebooks, represented as a Markdown document, or a Python script, with jupytext.vim.

https://github.com/mwouts/jupytext[+https://github.com/mwouts/jupytext+]

Scripted Forms
^^^^^^^^^^^^^^

Making GUIs easy for everyone on your team.

The primary benefit is that front ends for Python code become easily accessible to everyone on your team. Easy to use, easy to update, easy to extend, and easy to understand.

* Quickly create live-update GUIs for Python packages using Markdown and a few custom HTML elements.
* Just write in markdown + variables / UI types
* Based on Jupyter

https://github.com/SimonBiggs/scriptedforms[+https://github.com/SimonBiggs/scriptedforms+]

SoS
^^^

Script of Scripts (SoS) is a computational environment for the development and execution of scripts in multiple languages for daily computational research. It can be used to develop scripts to analyze data interactively in a Jupyter environment, and, with minimal effort, convert the scripts to a workflow that analyzes a large amount of data in batch mode.

Script of Scripts (SoS) is a Jupyter-based polyglot notebook that allows the use of multiple Jupyter kernels in one notebook, and a workflow engine for the execution of workflows in both process- and outcome-oriented styles. It is designed for data scientists and bioinformatics who routinely work with scripts in different languages such as R, Python, Perl, and bash.

The SoS components are:

* SoS Polyglot Notebook is a Jupyter Notebook with a SoS kernel.
* SoS Notebook serves as a super kernel to all other Jupyter kernels and allows the use of multiple kernels in one Jupyter notebook.
* SoS Workflow System is a Python based workflow system that is designed to be readable, sharable, and suitable for daily data analysis.
* SoS Workflow System can be used from command line or use SoS Notebook as its IDE.

https://github.com/vatlab/SoS[+https://github.com/vatlab/SoS+]

https://vatlab.github.io/sos-docs/[+https://vatlab.github.io/sos-docs/+]

vpython-jupyter
^^^^^^^^^^^^^^^

This package enables one to run VPython in a browser, using the GlowScript VPython API, documented in the Help at https://glowscript.org. If the code is in a cell in a Jupyter notebook, the 3D scene appears in the Jupyter notebook. If the code is launched outside a notebook (e.g. from the command line), a browser window will open displaying the scene.

VPython makes it unusually easy to create navigable real-time 3D animations. The one-line program "sphere()" produces a 3D sphere with appropriate lighting and with the camera positioned so that the scene fills the view. It also activates mouse interactions to zoom and rotate the camera view.

https://github.com/BruceSherwood/vpython-jupyter[+https://github.com/BruceSherwood/vpython-jupyter+]

https://vpython.org/[+https://vpython.org/+]

Jupyterhub
~~~~~~~~~~

JupyterHub, a multi-user Hub, spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. JupyterHub can be used to serve notebooks to a class of students, a corporate data science group, or a scientific research group.
The features include:

* Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more.

* Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub).

* Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users.

* Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware.

Three subsystems make up JupyterHub:

* a multi-user Hub (tornado process)
* a configurable http proxy (node-http-proxy)
* multiple single-user Jupyter notebook servers (Python/IPython/tornado)

JupyterHub performs the following functions:

* The Hub launches a proxy
* The proxy forwards all requests to the Hub by default
* The Hub handles user login and spawns single-user servers on demand
* The Hub configures the proxy to forward URL prefixes to the single-user notebook servers

For convenient administration of the Hub, its users, and services, JupyterHub also provides a REST API.

https://jupyterhub.readthedocs.io/en/stable/[+https://jupyterhub.readthedocs.io/en/stable/+]

https://jupyter.org/hub[+https://jupyter.org/hub+]

https://jupyterhub.readthedocs.io/en/stable/[+https://jupyterhub.readthedocs.io/en/stable/+]

The Littlest Jupyterhub
^^^^^^^^^^^^^^^^^^^^^^^

A simple JupyterHub distribution for a small (0-100) number of users on a single server.
The Littlest JupyterHub (TLJH) is an opinionated and pre-configured distribution to deploy a JupyterHub on a single machine (in the cloud or on your own hardware). It is designed to be a more lightweight and maintainable solution for use-cases where size, scalability, and cost-savings are not a huge concern.

https://tljh.jupyter.org/en/latest/[+https://tljh.jupyter.org/en/latest/+]

sshspawner
^^^^^^^^^^

The sshspawner enables JupyterHub to spawn single-user notebook servers on remote hosts over SSH.
The features include:

* Supports SSH key-based authentication
* Pool of remote hosts for spawning notebook servers
* Extensible custom load-balacing for remote host pool
* Remote-side scripting to return IP and port

https://github.com/NERSC/sshspawner[+https://github.com/NERSC/sshspawner+]

#KKKK

Kahler
~~~~~~

Kahler, a Python library that implements discrete exterior calculus on
arbitrary Hermitian manifolds. Borrowing techniques and ideas first
implemented in PyDEC, Kahler provides a uniquely general framework for
computation using discrete exterior calculus. Manifolds can have arbitrary
dimension, topology, bilinear Hermitian metrics, and embedding dimension.
Kahler comes equipped with tools for generating triangular meshes in arbitrary
dimensions with arbitrary topology. Kahler can also generate discrete sharp
operators and implement de Rham maps. Computationally intensive tasks are
automatically parallelized over the number of cores detected. The program
itself is written in Cython--a superset of the Python language that is
translated to C and compiled for extra speed. Kahler is applied to several
example problems: normal modes of a vibrating membrane, electromagnetic
resonance in a cavity, the quantum harmonic oscillator, and the Dirac-Kahler
equation. Convergence is demonstrated on random meshes.

http://arxiv.org/abs/1405.7879[+http://arxiv.org/abs/1405.7879+]

https://github.com/aeftimia/kahler[+https://github.com/aeftimia/kahler+]

KEALib
~~~~~~

KEALib provides an implementation of the GDAL data model. The format supports raster attribute tables, image pyramids, meta-data and in-built statistics while also handling very large files and compression throughout.

Based on the HDF5 standard, it also provides a base from which other formats can be derived and is a good choice for long term data archiving. An independent software library (libkea) provides complete access to the KEA image format and a GDAL driver allowing KEA images to be used from any GDAL supported software. 

http://kealib.org/[+http://kealib.org/+]

https://bitbucket.org/chchrsc/kealib[+https://bitbucket.org/chchrsc/kealib+]

KeplerMapper
~~~~~~~~~~~~

This is a library implementing the Mapper algorithm in Python. KeplerMapper can be used for visualization of high-dimensional data and 3D point cloud data. KeplerMapper can make use of Scikit-Learn API compatible cluster and scaling algorithms. 

https://github.com/MLWave/kepler-mapper[+https://github.com/MLWave/kepler-mapper+]

https://kepler-mapper.scikit-tda.org/[+https://kepler-mapper.scikit-tda.org/+]

http://www.ayasdi.com/wp-content/uploads/2015/02/Topological_Methods_for_the_Analysis_of_High_Dimensional_Data_Sets_and_3D_Object_Recognition.pdf[+http://www.ayasdi.com/wp-content/uploads/2015/02/Topological_Methods_for_the_Analysis_of_High_Dimensional_Data_Sets_and_3D_Object_Recognition.pdf+]

KGen
~~~~

KGen is a Python tool that extracts partial codes out of a large Fortran application and converts them into a standalone/verifiable/executable kernel.

A kernel is a small software that represents a certain characteristic of a larger application. It can be compiled and run generally without using external library on a single computing node. Due to its simple usage, it can greatly improve productivity of various software engineering tasks such as performance optimization, debugging, porting, verification, and so on.

In addition, a kernel could be an efficient vehcle for enhancing communication between collaborators possibly from various disciplines. For example, a kernel that contains a compiler bug is useful not only for reporting the bug but also for producing and fixing the bug by compiler engineer.

While a kernel is useful for many software engineering tasks, it is generally hard to create one. Mere copying and pasting an interesting block of code generally does not produce compilable software. In manual kernel extraction, it is common to scan through all source files to find required statements such as variable declaration and importing other modules. Furthermore, preparing state data for driving the execution of a generated kernel is generally harder task. For example, if a structured variable contains a pointer variable of another structured variable, user should manually copy those variables, aka, deep copying.

Fortunately, most of kernel extraction task from large Fortran application can be automated through static analysis, which is a core function of KGen.

KGen is simple to use. First user specifies a block of code in a target Fortran software with Linux commands for clean/build/run the software. Using these information, KGen scans through all the necessary source files and marks statements that are required to make the marked block to be a standalone/executable/verifiable kernel. Current version of KGen extensively uses F2PY, an excellant Fortran parser written in Python, to generate Abstract Syntax Trees. KGen traverses the ASTs instead of actually reading text source files for collecting analysis information and eventually selects a minimal set of statements for kernel generation.

KGen also produces input data to drive kernel execution and output data to verify its corrrectness. The data generation supports Fortran derived types that can contain another derived type variables. When KGen-generated kernel is executed, the details of verification result will be automaticaly shown on screen with timing information for performance measurement.

https://ncar.github.io/kgendocs/[+https://ncar.github.io/kgendocs/+]

https://github.com/NCAR/KGen[+https://github.com/NCAR/KGen+]

KML
~~~

Keyhole Markup Language (KML) is an XML notation for expressing geographic annotation and visualization within Internet-based, two-dimensional maps and three-dimensional Earth browsers. 

The KML file specifies a set of features (place marks, images, polygons, 3D models, textual descriptions, etc.) that can be displayed on maps in geospatial software implementing the KML encoding. Each place always has a longitude and a latitude. Other data can make the view more specific, such as tilt, heading, altitude, which together define a "camera view" along with a timestamp or timespan. KML shares some of the same structural grammar as GML. Some KML information cannot be viewed in Google Maps or Mobile.[4]

KML files are very often distributed in KMZ files, which are zipped KML files with a .kmz extension. These must be legacy (ZIP 2.0) compression compatible (i.e. stored or deflate method), otherwise the .kmz file might not uncompress in all geobrowsers.[5] The contents of a KMZ file are a single root KML document (notionally "doc.kml") and optionally any overlays, images, icons, and COLLADA 3D models referenced in the KML including network-linked KML files. The root KML document by convention is a file named "doc.kml" at the root directory level, which is the file loaded upon opening. By convention the root KML document is at root level and referenced files are in subdirectories (e.g. images for overlay images).

https://developers.google.com/kml/documentation/[+https://developers.google.com/kml/documentation/+]

http://www.opengeospatial.org/standards/kml/[+http://www.opengeospatial.org/standards/kml/+]

https://en.wikipedia.org/wiki/Keyhole_Markup_Language[+https://en.wikipedia.org/wiki/Keyhole_Markup_Language+]

fastkml
^^^^^^^

Fastkml is a library to read, write and manipulate KML files. It aims to keep it simple and fast (using lxml if available). Fast refers to the time you spend to write and read KML files as well as the time you spend to get aquainted to the library or to create KML objects. It aims to provide all of the functionality that KML clients such as OpenLayers, Google Maps, and Google Earth provides.

Geometries are handled as pygeoif or, if installed, shapely objects.

https://github.com/cleder/fastkml[+https://github.com/cleder/fastkml+]

https://fastkml.readthedocs.io/en/latest/[+https://fastkml.readthedocs.io/en/latest/+]

https://pypi.org/project/pygeoif/[+https://pypi.org/project/pygeoif/+]

https://lxml.de/[+https://lxml.de/+]

KNIME
~~~~~

KNIME Analytics Platform is the open source software for creating data science applications and services. Intuitive, open, and continuously integrating new developments, KNIME makes understanding data and designing data science workflows and reusable components accessible to everyone.

KNIME Server is the enterprise software for team based collaboration, automation, management, and deployment of data science workflows, data, and guided analytics. Non experts are given access to data science via KNIME WebPortal or can use REST APIs to integrate workflows as analytic services to applications and IoT systems.

Open source extensions for KNIME Analytics Platform are developed and maintained by KNIME and provide additional functionalities such as access to and processing of complex data types, as well as the addition of advanced machine learning algorithms.

Open source integrations for KNIME Analytics Platform (also developed and maintained by KNIME), provide seamless access to large open source projects such as Keras for deep learning, H2O for high performance machine learning, Apache Spark for big data processing, Python and R for scripting, and more.

Community Extensions are user contributed capabilities from industry specific applications to sophisticated, scientific software integrations, all created by our global community of active users.

https://www.knime.com/[+https://www.knime.com/+]

https://github.com/knime/knime-core[+https://github.com/knime/knime-core+]

https://forum.knime.com/[+https://forum.knime.com/+]

Kodiak
~~~~~~

Kodiak is a Cxx library that implements a generic branch and bound algorithm for rigorous numerical approximations. Particular instances of the branch and bound algorithm allow the user to refine and isolate solutions to systems of nonlinear equations and inequalities, global optimization problems, and bifurcation sets for systems of ODEs. Kodiak utilizes interval arithmetic (via the filibxx library) and Bernstein enclosure (for polynomials and rational functions) as self-validating enclosure methods. Symbolic operations support procedures such as automatic partial differentiation.

https://github.com/nasa/Kodiak[+https://github.com/nasa/Kodiak+]

https://shemesh.larc.nasa.gov/fm/Kodiak/[+https://shemesh.larc.nasa.gov/fm/Kodiak/+]

Kokkos
~~~~~~

Kokkos Core implements a programming model in Cxx for writing performance portable
applications targeting all major HPC platforms. For that purpose it provides
abstractions for both parallel execution of code and data management.
Kokkos is designed to target complex node architectures with N-level memory
hierarchies and multiple types of execution resources. It currently can use
OpenMP, Pthreads and CUDA as backend programming models.

Kokkos Core is part of the Kokkos Cxx Performance Portability Programming EcoSystem,
which also provides math kernels (https://github.com/kokkos/kokkos-kernels), as well as 
profiling and debugging tools (https://github.com/kokkos/kokkos-tools). 

https://github.com/kokkos/kokkos[+https://github.com/kokkos/kokkos+]

https://github.com/kokkos/kokkos-tutorials[+https://github.com/kokkos/kokkos-tutorials+]

Koolmogorov
~~~~~~~~~~~

Koolmogorov is a machine learning, hierarchical clustering, and visualization library in Python based on CompLearn. Koolmogorov uses approaches, techniques, and data structures from:

* Algorithmic Information Theory
* Kolmogorov Complexity (https://www.jstor.org/stable/25049284)
* Normalized Information Distance (https://arxiv.org/abs/0809.2553)
* Normalized Compression Distance (https://arxiv.org/abs/cs/0312044)
* Normalized Web Distance (https://arxiv.org/abs/0905.4039)
* Normalized Count-Min Sketch Distance (Unpublished research)
* Normalized Compression Neighbors (Unpublished research)
* Topological Data Analysis (http://brown.edu/Research/kalisnik/mapperPBG.pdf)
* Count-Min Sketch (http://dl.acm.org/citation.cfm?id=1073718)
* A New Quartet Tree Heuristic for Hierarchical Clustering (https://arxiv.org/abs/cs/0606048)
* Force-Directed Graphs (http://www.cs.cmu.edu/afs/cs/academic/class/15462-s13/www/lec_slides/Jakobsen.pdf)
* K-means Clustering based on Voronoi iteration (http://www.kanungo.com/pubs/tpami02-kmeans.pdf)

Koolmogorov is licensed under MIT.

https://github.com/MLWave/koolmogorov[+https://github.com/MLWave/koolmogorov+]

Kramdown
~~~~~~~~

kramdown (sic, not Kramdown or KramDown, just kramdown) is a free MIT-licensed Ruby library for parsing and converting a superset of Markdown. It is completely written in Ruby, supports standard Markdown (with some minor modifications) and various extensions that have been made popular by the PHP Markdown Extra package and Maruku.

kramdown is first and foremost a library for converting text written in a superset of Markdown to HTML.  It can also convert to LateX and PDF.

https://kramdown.gettalong.org/[+https://kramdown.gettalong.org/+]

Kratos
~~~~~~

KRATOS Multiphysics ("Kratos") is a framework for building parallel, multi-disciplinary simulation software, aiming at modularity, extensibility, and high performance, under a BSD license. Kratos is written in Cxx, and counts with an extensive Python interface.

Kratos is a framework for building multi-disciplinary finite element programs. It provides several tools for easy implementation of finite element applications and a common platform providing effortless interaction between them. Kratos has an innovative variable base interface designed to be used at different levels of abstraction and implemented to be very clear and extendible. It also provides an efficient yet flexible data structure which can be used to store any type of data in a type-safe manner. The Python scripting language is used to define the main procedure of Kratos which significantly improves the flexibility of the framework in time of use.

The kernel and application approach is used to reduce the possible conflicts arising between developers of different fields. Also layers are designed to reflect the working space of different people, considering their programming knowledge.

Kratos is Parallelized for Shared Memory Machines (SMMs) and Distributed Memory Machines (DMMs). In the same way it provides tools for its applications to adapt easily their algorithms to these architectures. Its scalability has been verified up to thousands of cores.

The capabilities include:

* CSD: 2-D & 3-D linear and nonlinear static and dynamic analysis. Various constitutive models (elasticity, plasticity, damage, fracture...). Elements library (beams, trusses, solids, shells, membranes, etc.)

* CFD: 2-D & 3-D Navier-Stokes equations. Incompressible or compressible flows. Transient analysis. Laminar or turbulent flows. Lagrangian, Eulerian and ALE formulation. Elemental and edge-based implementations. Various methods for treating free-surface flows. Various stabilization techniques (OSS, ASGS, FIC, SUPG, ...).

* Thermal problems: 2-D & 3-D conduction, convection and radiation. Transientsolutions. Coupling to fluids and structures. Various stabilization techniques.

* FSI: Partitioned and monolithic approaches. Strong and weak couplings. Dynamic underrelaxation techniques (Aitken), interface Laplacian techniques. Embedded technologies.

* Particle Methods: PFEM 2-D and 3-D Solution of Free Surfaces Flows, FSI and Excavation problems. Discrete Element Method (DEM). Mesh genereation units. Tetrahedra/triangular and spherical meshers. Optimized neighbour search algorithms (bins, kd-trees, octress, ...)

* Linear Solvers library: Multiple direct and iterative linear solvers (CG, SuperLU, SkyLine, BiCG, ...) 

http://www.cimne.com/kratos/[+http://www.cimne.com/kratos/+]

https://github.com/KratosMultiphysics/Kratos[+https://github.com/KratosMultiphysics/Kratos+]

Kubernetes
~~~~~~~~~~

An open-source container-orchestration system for automating deployment, scaling and management of containerized applications.[4] It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation. It aims to provide a "platform for automating deployment, scaling, and operations of application containers across clusters of hosts".[3] It works with a range of container tools, including Docker, since its first release.

https://en.wikipedia.org/wiki/Kubernetes[+https://en.wikipedia.org/wiki/Kubernetes+]

https://kubernetes.io/[+https://kubernetes.io/+]

https://github.com/kubernetes/kubernetes[+https://github.com/kubernetes/kubernetes+]

Helm
^^^^

Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application.
Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.

Use Helm to:

* Find and use popular software packaged as Helm charts to run in Kubernetes
* Share your own applications as Helm charts
* Create reproducible builds of your Kubernetes applications
* Intelligently manage your Kubernetes manifest files
* Manage releases of Helm packages

Helm is a tool that streamlines installing and managing Kubernetes applications. Think of it like apt/yum/homebrew for Kubernetes.

* Helm has two parts: a client (helm) and a server (tiller)
* Tiller runs inside of your Kubernetes cluster, and manages releases (installations) of your charts.
* Helm runs on your laptop, CI/CD, or wherever you want it to run.
* Charts are Helm packages that contain at least two things:
** A description of the package (Chart.yaml)
** One or more templates, which contain Kubernetes manifest files
* Charts can be stored on disk, or fetched from remote chart repositories (like Debian or RedHat packages)

https://github.com/helm/helm[+https://github.com/helm/helm+]

https://helm.sh/[+https://helm.sh/+]


#LLLL

Laghos
~~~~~~

Laghos (LAGrangian High-Order Solver) is a miniapp that solves the time-dependent Euler equations of compressible gas dynamics in a moving Lagrangian frame using unstructured high-order finite element spatial discretization and explicit high-order time-stepping.

Laghos captures the basic structure of many compressible shock hydrocodes, including the BLAST code at Lawrence Livermore National Laboratory. The miniapp is built on top of a general discretization library, MFEM, thus separating the pointwise physics from finite element and meshing concerns.

The Laghos miniapp is part of the CEED software suite, a collection of software benchmarks, miniapps, libraries and APIs for efficient exascale discretizations based on high-order finite element and spectral element methods.

https://github.com/CEED/Laghos[+https://github.com/CEED/Laghos+]

https://ceed.exascaleproject.org/miniapps/[+https://ceed.exascaleproject.org/miniapps/+]

LaGriT
~~~~~~

LaGriT (Los Alamos Grid Toolbox) is a library of user callable tools that provide mesh generation, mesh optimization and dynamic mesh maintenance in two and three dimensions. LaGriT is used for a variety of geology and geophysics modeling applications including porous flow and transport model construction, finite element modeling of stress/strain in crustal fault systems, seismology, discrete fracture networks, asteroids and hydrothermal systems.

The general capabilities of LaGriT can also be used outside of earth science applications and applied to nearly any system that requires a grid/mesh and initial and boundary conditions, setting of material properties and other model setup functions. It can also be use as a tool to pre- and post-process and analyze vertex and mesh based data. 

The features include:

* Representation of 2- and 3-dimensional complex geometries
* Unstructured triangle/tetrahedral and structured or unstructured quadrilateral/hexahedral meshing
* Model set-up including assigning material properties, boundary conditions, and initial conditions
* Adaptive mesh refinement, smoothing, and optimization
* 2D and 3D Delaunay triangulation conforming to complex geometry
* Output for solver packages including specialized format for
https://fehm.lanl.gov/[FEHM],
https://github.com/amanzi/ats[Amanzi/ATS],
https://www.pflotran.org/[PFLOTRAN], and https://tough.lbl.gov/[TOUGH]
* Interactive command line, batch input file, or embedded in Fortran/C interfaces (no GUI)

https://lagrit.lanl.gov/[+https://lagrit.lanl.gov/+]

La Malinette
~~~~~~~~~~~~

Malinette is an all-in-one solution, a rapid prototyping tool to show and make simple interaction and multimedia projects. It is a software and a hardware set. The software is based now on Pure Data Vanilla, previous versions was based on Pure Data Extended. It is also a very convenient suitcase and wood boxes with sensors, actuators, Arduino, Teensy and electronic components to test ideas and projects very quickly.

* malinette-soft: the standalone way to use the Malinette, a single folder containing Pure Data, externals, malinette and a startup script;
* malinette-ide: a framework and an interface to learn programming and rapid prototyping;
* malinette-abs: a set of abstractions to use Pure Data easily;

http://malinette.info/en/[+http://malinette.info/en/+]

https://framagit.org/malinette/malinette-ide[+https://framagit.org/malinette/malinette-ide+]

LAPACK
~~~~~~

LAPACK (Linear Algebra Package) is a standard software library for numerical linear algebra. It provides routines for solving systems of linear equations and linear least squares, eigenvalue problems, and singular value decomposition. It also includes routines to implement the associated matrix factorizations such as LU, QR, Cholesky and Schur decomposition. LAPACK was originally written in FORTRAN 77, but moved to Fortran 90 in version 3.2 (2008). The routines handle both real and complex matrices in both single and double precision.

LAPACK was designed as the successor to the linear equations and linear least-squares routines of LINPACK and the eigenvalue routines of EISPACK. LINPACK, written in the 1970s and 1980s, was designed to run on the then-modern vector computers with shared memory. LAPACK, in contrast, was designed to effectively exploit the caches on modern cache-based architectures, and thus can run orders of magnitude faster than LINPACK on such machines, given a well-tuned BLAS implementation. LAPACK has also been extended to run on distributed memory systems in later packages such as ScaLAPACK and PLAPACK.

http://www.netlib.org/lapack/[+http://www.netlib.org/lapack/+]

https://en.wikipedia.org/wiki/LAPACK[+https://en.wikipedia.org/wiki/LAPACK+]

https://github.com/Reference-ScaLAPACK/scalapack[+https://github.com/Reference-ScaLAPACK/scalapack+]

BLACS
^^^^^

The BLACS (Basic Linear Algebra Communication Subprograms) project is an ongoing investigation whose purpose is to create a linear algebra oriented message passing interface that may be implemented efficiently and uniformly across a large range of distributed memory platforms.

The length of time required to implement efficient distributed memory algorithms makes it impractical to rewrite programs for every new parallel machine. The BLACS exist in order to make linear algebra applications both easier to program and more portable. It is for this reason that the BLACS are used as the communication layer of ScaLAPACK.

https://www.netlib.org/blacs/[+https://www.netlib.org/blacs/+]

LAPACK95
^^^^^^^^

LAPACK95 is a Fortran 95 interface to the Fortran 77 LAPACK library. It improves upon the original user-interface to the LAPACK package, taking advantage of the considerable simplifications which Fortran 95 allows. The design of LAPACK95 exploits assumed-shape arrays, optional arguments, and generic interfaces. The Fortran 95 interface has been implemented by writing Fortran 95 ``wrappers'' to call existing routines from the LAPACK package. This interface can persist unchanged even if the underlying Fortran 77 LAPACK code is rewritten to take advantage of the new features of Fortran 95.

http://www.netlib.org/lapack95/[+http://www.netlib.org/lapack95/+]

https://github.com/scivision/LAPACK95[+https://github.com/scivision/LAPACK95+]

LapackDoc
^^^^^^^^^

The intension of this project is a robust, maintainable, standalone and simple-to-use documentation tool for LAPACK. However, some of the core ingredients like the Fortran 77 lexer and parser f77crash might also be valuable for other documentation tools (e.g. doxygen). Technically f77crash is based on Netlib's f2c.

f77crash is short for “Fortran 77 cross referencing and syntax highlighting”. The tool lexes and parses a Fortran source code. An in particular it extracts from the source code the locations of types, variables, statements, constants and so on.
The output of f77crash can be used to create syntax highlighted and cross referenced listings. The f2html tool is a simple Perl script for this purpose.

The deparch tool can also be used to extract call and caller graphs. The output is in a format that can be used as input for dot (from Graphviz).

http://apfel.mathematik.uni-ulm.de/\~lehn/LapackDoc/index.html[+http://apfel.mathematik.uni-ulm.de/~lehn/LapackDoc/index.html+]

PLAPACK
^^^^^^^

The Parallel Linear Algebra Package (PLAPACK) is a maturing fourth generation linear algebra infrastructure which uses a application-centric view of vector and matrix distribution, Physically Based Matrix Distribution. It also uses an "MPI-like" programming interface that hides distribution and indexing details in opaque objects, provides a natural layering in the library, and provides a straight-forward application interface.

Extensions of LAPACK to distributed memory architectures like the Cray T3E, IBM SP-2, and Intel Paragon are explored by the ScaLAPACK project.  While this package does manage to provide a subset of the functionality of LAPACK, it is our belief that it has also clearly demonstrated that mimicking the coding styles that were effective on sequential and shared memory computers does not create maintainable and flexible code for distributed memory architectures. The PLAPACK infrastructure attempts to show that by adopting an object based coding style, already popularized by the Message-Passing Infrastructure (MPI), the coding of parallel linear algebra algorithms is simplified compared to the more traditional sequential coding approaches.

http://www.cs.utexas.edu/users/plapack/[+http://www.cs.utexas.edu/users/plapack/+]

https://www.researchgate.net/publication/2590430_PLAPACK_Parallel_Linear_Algebra_Libraries_Design_Overview[+https://www.researchgate.net/publication/2590430_PLAPACK_Parallel_Linear_Algebra_Libraries_Design_Overview+]

https://mitpress.mit.edu/books/using-plapack[+https://mitpress.mit.edu/books/using-plapack+]

ReLAPACK
^^^^^^^^

ReLAPACK offers a collection of recursive algorithms for many of LAPACK's compute kernels. Since it preserves LAPACK's established interfaces, ReLAPACK integrates effortlessly into existing application codes. ReLAPACK's routines not only outperform the reference LAPACK but also improve upon the performance of tuned implementations, such as OpenBLAS and MKL.

https://github.com/HPAC/ReLAPACK[+https://github.com/HPAC/ReLAPACK+]

ScaLAPACK
^^^^^^^^^

ScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines. ScaLAPACK solves dense and banded linear systems, least squares problems, eigenvalue problems, and singular value problems. The key ideas incorporated into ScaLAPACK include the use of

* a block cyclic data distribution for dense matrices and a block data distribution for banded matrices, parametrizable at runtime;
* block-partitioned algorithms to ensure high levels of data reuse;
* well-designed low-level modular components that simplify the task of parallelizing the high level routines by making their source code the same as in the sequential case.

ScaLAPACK is a continuation of the LAPACK project, which designed and produced analogous software for workstations, vector supercomputers, and shared-memory parallel computers. Both libraries contain routines for solving systems of linear equations, least squares problems, and eigenvalue problems. The goals of both projects are efficiency (to run as fast as possible), scalability  (as the problem size and number of processors grow), reliability  (including error bounds), portability  (across all important parallel machines), flexibility (so users can construct new routines from well-designed parts), and ease of use (by making the interface to LAPACK and ScaLAPACK look as similar as possible). Many of these goals, particularly portability, are aided by developing and promoting standards , especially for low-level communication and computation routines. We have been successful in attaining these goals, limiting most machine dependencies to two standard libraries called the BLAS, or Basic Linear Algebra Subprograms, and BLACS, or Basic Linear Algebra Communication Subprograms. LAPACK will run on any machine where the BLAS are available, and ScaLAPACK will run on any machine where both the BLAS and the BLACS are available.

http://www.netlib.org/scalapack/[+http://www.netlib.org/scalapack/+]

http://netlib.org/scalapack/slug/index.html[+http://netlib.org/scalapack/slug/index.html+]

https://github.com/scivision/scalapack[+https://github.com/scivision/scalapack+]

https://www.researchgate.net/publication/220840168_ScaLAPACK_Tutorial[+https://www.researchgate.net/publication/220840168_ScaLAPACK_Tutorial+]

https://developer.arm.com/products/software-development-tools/hpc/resources/porting-and-tuning/building-scalapack-with-arm-compiler[+https://developer.arm.com/products/software-development-tools/hpc/resources/porting-and-tuning/building-scalapack-with-arm-compiler+]

https://wiki.numascale.com/tips/building-openmp-based-scalapack[+https://wiki.numascale.com/tips/building-openmp-based-scalapack+]

SCALAPACKFX
^^^^^^^^^^^

The open source library SCALAPACKFX is an effort to provide modern Fortran (Fortran 2003) wrappers around routines of the SCALAPACK, PBLAS and BLACS libraries to make their use as simple as possible. 

A few essential routines (especially those related to diagonalization) are already covered. If not, you are cordially invited to extend SCALAPACKFX and to share it in order to let others profit from your work. SCALAPACKFX is licensed under the simplified BSD license.

https://github.com/dftbplus/scalapackfx[+https://github.com/dftbplus/scalapackfx+]

scalapy
^^^^^^^

scalapy is a wrapping of Scalapack such that it can be called by Python in a friendly manner.
Operations are performed on DistributedMatrix objects which can be easily created whilst hiding all the nasty details of block cyclic distribution.
scalapy supports both Python 2 and 3 (2.7, 3.2 or later).

The package depends upon two python packages numpy and mpi4py. It is written largely in pure Python, but some parts require Cython and f2py. It also requires an MPI distribution (OpenMPI and IntelMPI supported out the box), and a Scalapack installation (both Intel MKL and NETLIB are currently supported).

https://github.com/jrs65/scalapy[+https://github.com/jrs65/scalapy+]

Lasagne
~~~~~~~

Lasagne is a lightweight library to build and train neural networks in Theano. Its main features are:

* Supports feed-forward networks such as Convolutional Neural Networks (CNNs), recurrent networks including Long Short-Term Memory (LSTM), and any combination thereof
* Allows architectures of multiple inputs and multiple outputs, including auxiliary classifiers
* Many optimization methods including Nesterov momentum, RMSprop and ADAM
* Freely definable cost function and no need to derive gradients due to Theano's symbolic differentiation
* Transparent support of CPUs and GPUs due to Theano's expression compiler

https://github.com/Lasagne/Lasagne[+https://github.com/Lasagne/Lasagne+]

https://lasagne.readthedocs.io/en/latest/[+https://lasagne.readthedocs.io/en/latest/+]

http://deeplearning.net/software/theano/[+http://deeplearning.net/software/theano/+]

LASSPTools
~~~~~~~~~~

LASSPTools is a collection of Unix utilities for numerical analysis and graphics. It was written around 1990, before Linux, before the World Wide Web. Considering that, many of the tools remain remarkably useful.

The operating system of most computers is designed around their primary use: text processing. We provide the same system utilities for number crunching and dynamical displays of results that are available for sorting, searching, and manipulating text files. The files are available without charge via the World Wide Web. 

 The LASSP Software toolkit is a set of more than 35 programs for assisting researchers in physics and engineering. The tools are available on Linux (recompiled April 2002), and have been compiled on a variety of other Unix platforms. LASSPTools was developed at Cornell University as part of the Cornell-IBM Joint Study on Computing for Scientific Research.

The tools perform various functions and roughly can be grouped into the following categories:

* Graphics and Animation: Display matrices, lattice dynamics simulations, polygons and polygonalized surfaces, circles and spheres: watch them change with time.
* Analog Input Devices Input numbers with slider bars, input rotation matrices to turn your figures in three dimensions...
* Numerical Analysis Fourier transforms, matrix eigenvalues, inversions, rotations - all as Unix filters.
* Data filtering sort, prune, histogram, ...
* Miscellaneous Utilities (some of which were written outside of our project). 

The programs typically are used together in pipelines to generate, manipulate and display data. For example, raw data may be run through a numerical transformation, a section selected via a filtering program and the results sent to a program to animate the results graphically.

http://www.lassp.cornell.edu/LASSPTools/LASSPTools.html[+http://www.lassp.cornell.edu/LASSPTools/LASSPTools.html+]

LavaVu
~~~~~~

A scientific visualisation tool with a python interface for fast and flexible visual analysis.
The acronym stands for: lightweight, automatable visualisation and analysis viewing utility, but "lava" is also a reference to its primary application as a viewer for geophysical simulations. It was also chosen to be unique enough to find the repository with google.

The project started in the gLucifer1 framework for visualising geodynamics simulations. The OpenGL visualisation module was separated from the simulation and sampling libraries and became a more general purpose visualisation tool. gLucifer continues as a set of sampling tools for Underworld simulations as part of the Underworld2 code. LavaVu provides the rendering library for creating 2d and 3d visualisations to view this sampled data, inline within interactive IPython notebooks and offline through saved visualisation databases and images/movies.

As a standalone tool it is a scriptable 3D visualisation tool capable of producing publication quality high res images and video output from time varying data sets along with HTML5 3D visualisations in WebGL. Rendering features include correctly and efficiently rendering large numbers of opaque and transparent points and surfaces and volume rendering by GPU ray-marching. There are also features for drawing vector fields and tracers (streamlines).

Control is via python and a set of simple verbose scripting commands along with mouse/keyboard interaction. GUI components can be generated for use from a web browser via the python "control" module and a built in web server.

A native data format called GLDB is used to store and visualisations in a compact single file, using SQLite for storage and fast loading. A small number of other data formats are supported for import (OBJ surfaces, TIFF stacks etc). Further data import formats are supported with python scripts, with the numpy interface allowing rapid loading and manipulation of data.

https://github.com/OKaluza/LavaVu[+https://github.com/OKaluza/LavaVu+]

Legion
~~~~~~

Legion is a parallel programming model for distributed, heterogeneous machines.

Legion is a programming model and runtime system designed for decoupling the specification of parallel algorithms from their mapping onto distributed heterogeneous architectures. Since running on the target class of machines requires distributing not just computation but data as well, Legion presents the abstraction of logical regions for describing the structure of program data in a machine independent way. Programmers specify the partitioning of logical regions into subregions, which provides a mechanism for communicating both the independence and locality of program data to the programming system. Since the programming system has knowledge of both the structure of tasks and data within the program, it can aid the programmer in host of problems that are commonly the burden of the programmer.

The Legion programming model is designed to abstract computations in a way that makes them portable across many different potential architectures. The challenge then is to make it easy to map the abstracted computation of the program onto actual architectures. 

To facilitate this process Legion introduces a novel runtime 'mapping' interface. One of the NON-goals of the Legion project was to design a programming system that was magically capable of making intelligent mapping decisions. Instead the mapping interface provides a declarative mechanism for the programmer to communicate mapping decisions to the runtime system without having to actually write any code to perform the mapping (e.g. actually writing the code to perform a copy or synchronization). Furthermore, by making the mapping interface dynamic, it allows the programmer to make mapping decisions based on information that may only be available at runtime.

https://legion.stanford.edu/[+https://legion.stanford.edu/+]

https://github.com/StanfordLegion/legion[+https://github.com/StanfordLegion/legion+]

Lepton
~~~~~~

Source code is very hard to maintain when the documentation is missing. Recognizing this fact, D. Knuth proposed the literate programming paradigm, i.e. that source code and documentation should be written at the same time, inside the same file, and in a format designed for human understanding.

'Lepton'' adds documents such as data analysis reports and scientific papers to this vision. To enable reproducible research, ''Lepton'' makes it easy to include scripts or complete programs, compilation and execution instructions, as well as execution results in the same file. Offloading execution to ''Lepton'' makes the analysis operator-independent and easy to reproduce. In the spirit of literate programming, the plain text file format used by ''Lepton'' is intended to be human-understandable as opposed to machine-readable, and simple enough to be usable without the software.

''Lepton'' consists in a standalone executable that processes plain text files written in a documentation format such as HTML or LaTeX with optional blocks that can contain files to be written to disk, source code or executable instructions. It is distributed as a ''Lepton'' file containing the full source code, manual and a tutorial. The package contains an extracted copy of the source code that can be compiled without ''Lepton''. For more information, please read the PDF manual.

https://github.com/slithiaote/lepton[+https://github.com/slithiaote/lepton+]

LESGO
~~~~~

LESGO solves the filtered Navier-Stokes equations in the high-Reynolds number limit on a Cartesian mesh. Originally designed to simulate flow in the atmospheric boundary layer, LESGO has been extended and used to simulate flow over tree canopies, wall-mounted cubes, and wind turbine arrays, among other things. At its core is the LES flow solver. Built on top of the solver are modules that provide additional functionality such as immersed boundary methods, wind farm modeling, and so on.

The features include:

* a flow solver that uses a pseudospectral discretization in the longitudinal and spanwise directions, a second-order centered finite differencing in the wall-normal direction, and an explicit Adams-Bashforth method in time
* wall-normal boundary condition options including various wall models which prescribe the surface stresses (for LES), no-slip velocity (for DNS), and stress-free
* five subgrid scale models: the Smagorinsky, dynamic, scale-dependent, Lagrangian scale similarity, and Lagrangian scale-dependent models
* a level set immersed boundary method for representing solid objects in the flow domain
* wind turbine modeling features including an actuator disk  model and an actuator line/sector model
* a concurrent inflow method where the inflow condition is generated from a concurrent precursor simulation instead of the
usual standard periodic boundary condition

https://lesgo.me.jhu.edu/[+https://lesgo.me.jhu.edu/+]

https://github.com/lesgo-jhu/lesgo[+https://github.com/lesgo-jhu/lesgo+]

LFC
~~~

LFC is a software project that merges the ease of use of LAPACK with parallel processing capabilities of ScaLAPACK. The latter one's software dependences are reduced to an MPI implementation. It is a self-contained package with built-in knowledge of how to run linear algebra software on a cluster. BLAS may be supplied by the user for best performance but a decent replacement is provided.

LFC is for people who are familiar with LAPACK but need performance of ScaLAPACK and convenience of automated tuning of parallel computing resources. 

LFC software supports a serial, single processor user interface, but delivers the computing power achievable by an expert user working on the same problem who optimally utilizes the resources of a cluster. The basic premise is to design numerical library software that addresses both computational time and space complexity issues on the user' s behalf and in a manner as transparent to the user as possible. The software intends to allow users to either link against an archived library of executable routines or benefit from the convenience of prebuilt executable programs without the hassle of resolving linker dependencies. The user is assumed to call one of the LFC routines from a serial environment while working on a single processor of the cluster. The software executes the application. If it is possible to finish executing the problem faster by mapping the problem into a parallel environment, then this is the thread of execution taken. Otherwise, the application is executed locally with the best choice of a serial algorithm.

http://icl.cs.utk.edu/lfc/[+http://icl.cs.utk.edu/lfc/+]

libarchive
~~~~~~~~~~

A multi-format and compression library.  The features include:

* Support for a variety of archive and compression formats.
* Robust automatic format detection, including archive/compression combinations such as tar.gz.
* Zero-copy internal architecture for high performance.
* Streaming architecture eliminates all limits on size of archive, limits on entry sizes depend on particular formats.
* Carefully factored code to minimize bloat when programs are statically linked.
* Growing test suite to verify correctness of new ports.
* Works on most POSIX-like systems (including FreeBSD, Linux, Solaris, etc.) 

The bsdtar and bsdcpio command-line utilities are feature- and performance-competitive with other tar and cpio implementations:

* Reads a variety of formats, including tar, pax, cpio, zip, xar, lha, ar, cab, mtree, rar, and ISO images.
* Writes tar, pax, cpio, zip, xar, ar, ISO, mtree, and shar archives.
* Automatically handles archives compressed with gzip, bzip2, lzip, xz, lzma, or compress.
* Unique format conversion feature.

http://libarchive.org/[+http://libarchive.org/+]

https://github.com/libarchive/libarchive[+https://github.com/libarchive/libarchive+]

LibBi
~~~~~

LibBi is used for state-space modelling and Bayesian inference on high-performance computer hardware, including multi-core CPUs, many-core GPUs (graphics processing units) and distributed-memory clusters.

The staple methods of LibBi are based on sequential Monte Carlo (SMC), also known as particle filtering. These methods include particle Markov chain Monte Carlo (PMCMC) and SMC2. Other methods include the extended Kalman filter and some parameter optimisation routines.

LibBi consists of a Cxx template library, as well as a parser and compiler, written in Perl, for its own modelling language.

http://libbi.org/[+http://libbi.org/+]

Libcamera
~~~~~~~~~

Libcamera aims to ease embedded camera application development, improving both on V4L2 and libv4l. The core piece is a libcamera framework, written in Cxx, that exposes kernel driver APIs to userspace. On top of the framework are optional language bindings for languages such as C. 

The next layer up is a libcamera application layer that translates to existing camera APIs, including V4L2, Gstreamer, and the Android Camera Framework, which Pinchart said would not contain the usual vendor specific Android HAL code. As for V4L2, “we will attempt to maintain compatibility as a best effort, but we won’t implement every feature,” said Pinchart. There will also be a native libcamera app format, as well as plans to support Chrome OS. 

Libcamera keeps the kernel level hidden from the upper layers. The framework is built around the concept of a camera device, “which is what you would expect from a camera as an end user,” said Pinchart. “We will want to implement each camera’s capabilities, and we’ll also have a concept of profiles, which is a higher view of features. For example, you could choose a video or point-and-shoot profile.”

Libcamera will support multiple video streams from a single camera. “In videoconferencing, for example, you might want a different resolution and stream than what you encode over the network,” said Pinchart. “You may want to display the live stream on the screen and, at the same time, capture stills or record video, perhaps at different resolutions.”

http://libcamera.org/[+http://libcamera.org/+]

https://git.linuxtv.org/libcamera.git/[+https://git.linuxtv.org/libcamera.git/+]

https://fosdem.org/2019/schedule/event/cameras/[+https://fosdem.org/2019/schedule/event/cameras/+]

https://www.linux.com/blog/2018/12/libcamera-aims-make-embedded-cameras-easier[+https://www.linux.com/blog/2018/12/libcamera-aims-make-embedded-cameras-easier+]

libCEED
~~~~~~~

libCEED is a high-order API library, that provides a common algebraic low-level operator description, allowing a wide variety of applications to take advantage of the efficient operator evaluation algorithms in the different CEED packages (from a single source).

One of the challenges with high-order methods is that a global sparse matrix is no longer a good representation of a high-order linear operator, both with respect to the FLOPs needed for its evaluation, as well as the memory transfer needed for a matvec. Thus, high-order methods require a new "format" that still represents a linear (or more generally non-linear) operator, but not through a sparse matrix.

The goal of libCEED is to propose such a format, as well as supporting implementations and data structures, that enable efficient operator evaluation on a variety of computational device types (CPUs, GPUs, etc.). This new operator description is based on algebraically factored form, which is easy to incorporate in a wide variety of applications, without significant refactoring of their own discretization infrastructure.

https://ceed.exascaleproject.org/libceed/[+https://ceed.exascaleproject.org/libceed/+]

https://github.com/CEED/libCEED[+https://github.com/CEED/libCEED+]

Libextractor
~~~~~~~~~~~~

GNU Libextractor is a library used to extract meta data from files. The goal is to provide developers of file-sharing networks, browsers or WWW-indexing bots with a universal library to obtain simple keywords and meta data to match against queries and to show to users instead of only relying on filenames. libextractor contains the shell command extract that, similar to the well-known file command, can extract meta data from a file and print the results to stdout.

Currently, libextractor supports the following formats: HTML, MAN, PS, DVI, OLE2 (DOC, XLS, PPT), OpenOffice (sxw), StarOffice (sdw), FLAC, MP3 (ID3v1 and ID3v2), OGG, WAV, S3M (Scream Tracker 3), XM (eXtended Module), IT (Impulse Tracker), NSF(E) (NES music), SID (C64 music), EXIV2, JPEG, GIF, PNG, TIFF, DEB, RPM, TAR(.GZ), LZH, LHA, RAR, ZIP, CAB, 7-ZIP, AR, MTREE, PAX, CPIO, ISO9660, SHAR, RAW, XAR FLV, REAL, RIFF (AVI), MPEG, QT and ASF. Also, various additional MIME types are detected.

GNU libextractor uses helper-libraries (plugins) to perform the actual extraction. As a result, GNU libextractor can be extended simply by installing additional plugins. Writing robust parsers can be difficult. GNU libextractor protects the main applications from haning or crashing plugins by executing all plugins out-of-process.

https://www.gnu.org/software/libextractor/[+https://www.gnu.org/software/libextractor/+]

libMesh
~~~~~~~

The libMesh library provides a framework for the numerical simulation of partial differential equations using arbitrary unstructured discretizations on serial and parallel platforms. A major goal of the library is to provide support for adaptive mesh refinement (AMR) computations in parallel while allowing a research scientist to focus on the physics they are modeling.

libMesh currently supports 1D, 2D, and 3D steady and transient simulations on a variety of popular geometric and finite element types. The library makes use of high-quality, existing software whenever possible. PETSc or the Trilinos Project are used for the solution of linear systems on both serial and parallel platforms, and LASPack is included with the library to provide linear solver support on serial machines. An optional interface to SLEPc is also provided for solving both standard and generalized eigenvalue problems. 

http://libmesh.github.io/[+http://libmesh.github.io/+]

https://github.com/libMesh/libmesh[+https://github.com/libMesh/libmesh+]

libMeshb
~~~~~~~~

A library to handle the *.meshb file format.
he Gamma Mesh Format (GMF) and the associated library libMeshb provide programers of simulation and meshing software with an easy way to store their meshes and physical solutions.  The features include:

* more than 80 kinds of data types, like vertex, polyhedron, normal vector or vector solution field.
* a convenient way to move data between those files, via keyword tags, and the user's own structures.
* Transparent handling of ASCII & binary files.
* Transparent handling of little & big endian files.
* Optional ultra fast asynchronous and low level transfers.
* Can call user's own pre and post processing routines in a separate thread while accessing a file.

The libMeshb library is written in ANSI C.
It is made of a single C file and a header file to be compiled and linked alongside the calling program.
It may be used in C, Cxx, F77 and F90 programs (Fortran 77 and 90 APIs are provided).

https://github.com/LoicMarechal/libMeshb[+https://github.com/LoicMarechal/libMeshb+]

https://ugawg.github.io/[+https://ugawg.github.io/+]

https://github.com/UGAWG[+https://github.com/UGAWG+]

https://github.com/nasa/refine[+https://github.com/nasa/refine+]

libnova
~~~~~~~
 
A double precision celestial mechanics, astrometry and astrodynamics library.

http://libnova.sourceforge.net/[+http://libnova.sourceforge.net/+]

libNT
~~~~~

Researchers are increasingly incorporating numeric high-order data, i.e., numeric tensors, within their practice. Just like the matrix/vector (MV) paradigm, the development of multi-purpose, but high-performance, sparse data structures and algorithms for arithmetic calculations, e.g., those found in Einstein-like notation, is crucial for the continued adoption of tensors. We use the example of high-order differential operators to illustrate this need. As sparse tensor arithmetic is an emerging research topic, with challenges distinct from the MV paradigm, many aspects require further articulation. We focus on three core facets. First, aligning with prominent voices in the field, we emphasise the importance of data structures able to accommodate the operational complexity of tensor arithmetic. However, we describe a linearised coordinate (LCO) data structure that provides faster and more memory-efficient sorting performance. Second, flexible data structures, like the LCO, rely heavily on sorts and permutations. We introduce an innovative permutation algorithm, based on radix sort, that is tailored to rearrange already-sorted sparse data, producing significant performance gains. Third, we introduce a novel poly-algorithm for sparse tensor products, where hyper-sparsity is a possibility. Different manifestations of hyper-sparsity demand their own approach, which our poly-algorithm is the first to provide. These developments are incorporated within our LibNT and NTToolbox software libraries. Benchmarks, frequently drawn from the high-order differential operators example, demonstrate the practical impact of our routines, with speed-ups of 40% or higher compared to alternative high-performance implementations.

https://github.com/extragoya/LibNT[+https://github.com/extragoya/LibNT+]

https://arxiv.org/abs/1802.02619[+https://arxiv.org/abs/1802.02619+]

libParanumal
~~~~~~~~~~~~

An experimental set of finite element flow solvers for heterogeneous (GPU/CPU) systems. The initial development of libParanumal was performed by the Parallel Numerical Algorithms Group at Virginia Tech.

libParanumal is funded in part by the US Department of Energy as part of the activities of the Center for Efficient Exscale Discretizations.

Why is it called libParanumal ?: the high-order finite-element implementations in libParanumal are spectrally accurate and rely heavily on ghost elements for MPI communications.

https://github.com/paranumal/libparanumal[+https://github.com/paranumal/libparanumal+]

libpod
~~~~~~

Libpod provides a library for applications looking to use the Container Pod concept, popularized by Kubernetes. libpod also contains the podman tool, for managing Pods, Containers, and Container Images.

https://github.com/containers/libpod[+https://github.com/containers/libpod+]

https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons/[+https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons/+]

https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/[+https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/+]

https://developers.redhat.com/blog/2018/08/29/intro-to-podman/[+https://developers.redhat.com/blog/2018/08/29/intro-to-podman/+]

https://github.com/containers/buildah[+https://github.com/containers/buildah+]

Libreboot
~~~~~~~~~

Libreboot (formerly known as GNU Libreboot[4]) is a free-software project aimed at replacing the proprietary BIOS firmware found in most computers with an open-source, lightweight system designed to perform only the minimum number of tasks necessary to load and run a modern 32-bit or 64-bit operating system. 

Libreboot is established as a distribution of coreboot without proprietary binary blobs.[5][6] Libreboot is not a straight fork of coreboot; instead, it is a parallel effort that works closely with and re-bases every so often on the latest coreboot as the upstream supplier, with patches merged upstream whenever possible. In addition to removing proprietary software, libreboot also attempts to make coreboot easy to use by automating the build and installation processes.[7][8]

The Libreboot project made possible the required modifications for completely open source variants of some ThinkPad, Chromebook, and MacBook laptops as well as desktop and server and workstation motherboards.[9][10] According to its own documentation, it can work with any Linux distribution that uses kernel mode setting (KMS) for the graphics, while Windows is not supported and its use is discouraged by Libreboot.

https://en.wikipedia.org/wiki/Libreboot[+https://en.wikipedia.org/wiki/Libreboot+]

https://libreboot.org/[+https://libreboot.org/+]

libremesh
~~~~~~~~~

LibreMesh is a modular framework for creating OpenWrt/LEDE-based firmwares for wireless mesh nodes. Several communities around the world use LibreMesh as the foundation of their local mesh firmwares.

The LibreMesh project includes the development of several tools. The firmware (the main piece) allow simple deployment of auto-configurable, yet versatile, multi-radio mesh networks.

https://libremesh.org/[+https://libremesh.org/+]

https://libremesh.org/docs/en_quick_starting_guide.html[+https://libremesh.org/docs/en_quick_starting_guide.html+]

LibROM
~~~~~~

LibRom is a library designed to facilitate Proper Orthogonal Decomposition (POD) based Reduced Order Modeling (ROM). In POD ROM one attempts to represent the solution of a full order model (FOM) by a set of basis vectors in a reduced (hopefully greatly reduced) dimension sub-space. libROM generates basis vectors from FOM solutions, by providing guidance for selecting appropriate solution snapshots, and computing the associated singular value decomposition (SVD) incrementally.

LibROM addresses a fundamental issue in POD ROM, namely how the solution snapshots should be optimally selected. One of the key features of libROM is an adaptive sampling algorithm that controls out of sub-space error with a minimum number of snapshots. This produces accurate ROMs of minimum dimension which may be generated and evaluated in correspondingly less time.

The library implements 3 different variations of the fundamental basis generation algorithm. All 3 variations are parallel. In the fundamental algorithm an SVD of the solution snapshots is performed. The column vectors of that SVD form the basis vectors. The first 2 variations are implementations of the incremental SVD algorithms described by Matthew Brand (Incremental singular value decomposition of uncertain data with missing values in Computer VisionECCV 2002, Springer, 2002, pp. 707-720).

These algorithms are parallel and scalable. Details of the algorithms and their parallel performance are included in the library documentation. The third algorithm is a fairly literal interpretation of the fundamental algorithm. It requires that an SVD be performed on the globalized snapshots and is therefore clearly not scalable. It is included in the library primarily for completeness.

It has been our experience that inserting libROM into an existing code to generate basis vectors is fairly easy and non-invasive. The use of the basis vectors to form and evaluate the ROM are highly problem dependent and therefore not addressable by an external library such as libROM.

https://computation.llnl.gov/projects/librom-pod-based-reduced-order-modeling[+https://computation.llnl.gov/projects/librom-pod-based-reduced-order-modeling+]

https://github.com/llnl/libROM[+https://github.com/llnl/libROM+]

librsb
~~~~~~

librsb is a library for sparse matrix computations featuring the Recursive Sparse Blocks (RSB) matrix format. This format allows cache efficient and multi-threaded (that is, shared memory parallel) operations on large sparse matrices. The most common operations necessary to iterative solvers are available, e.g.: matrix-vector multiplication, triangular solution, rows/columns scaling, diagonal extraction / setting, blocks extraction, norm computation, formats conversion. The RSB format is especially well suited for symmetric and transposed multiplication variants. Most numerical kernels code is auto generated, and the supported numerical types can be chosen by the user at build time. librsb can also be built serially (without OpenMP parallelism), if required.
librsb also implements the Sparse BLAS standard.

The RSB format is hierarchical. Its basic idea is a recursive subdivision of a matrix in the two dimensions. The subdivision algorithm attempts to partition the matrix until the individual submatrices occupy approximately the amount of memory contained in the CPU caches. The number of cores/thread available also plays a role. 
The memory layout of the individual sparse submatrices follows a Z (or Morton) ordering. Each submatrix is stored either in coordinate or CSR (Compressed Sparse Rows) format, and the numerical index type size is chosen in order to minimize memory consumption.
This layout enables efficient and multithreaded algorithms implementing the Sparse BLAS operations. 

http://librsb.sourceforge.net/[+http://librsb.sourceforge.net/+]

https://home.mpcdf.mpg.de/~mima/librsb-1.2.0-rc4/doc/html/index.html[+https://home.mpcdf.mpg.de/~mima/librsb-1.2.0-rc4/doc/html/index.html+]

https://github.com/dcjones/RecursiveSparseBlocks.jl[+https://github.com/dcjones/RecursiveSparseBlocks.jl+]

https://github.com/michelemartone/pyrsb[+https://github.com/michelemartone/pyrsb+]

http://www.nongnu.org/fbi-improved/[+http://www.nongnu.org/fbi-improved/+]

LIBSVM
~~~~~~

LIBSVM is an integrated software for support vector classification, (C-SVC, nu-SVC), regression (epsilon-SVR, nu-SVR) and distribution estimation (one-class SVM). It supports multi-class classification. 
Our goal is to help users from other fields to easily use SVM as a tool. LIBSVM provides a simple interface where users can easily link it with their own programs. Main features of LIBSVM include:

* Different SVM formulations
* Efficient multi-class classification
* Cross validation for model selection
* Probability estimates
* Various kernels (including precomputed kernel matrix)
* Weighted SVM for unbalanced data
* Both C++ and Java sources
* GUI demonstrating SVM classification and regression
* Python, R, MATLAB, Perl, Ruby, Weka, Common LISP, CLISP, Haskell, OCaml, LabVIEW, and PHP interfaces. C# .NET code and CUDA extension is available.
* It's also included in some data mining environments: RapidMiner, PCP, and LIONsolver.
* Automatic model selection which can generate contour of cross validation accuracy.

https://www.csie.ntu.edu.tw/\~cjlin/libsvm/[+https://www.csie.ntu.edu.tw/~cjlin/libsvm/+]

https://github.com/cjlin1/libsvm[+https://github.com/cjlin1/libsvm+]

*LISVM: A Library for SVM* - https://www.csie.ntu.edu.tw/\~cjlin/papers/libsvm.pdf[+https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf+]

*Practical Guide to SVM* - https://www.csie.ntu.edu.tw/\~cjlin/papers/guide/guide.pdf[+https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf+]

LIBXSMM
~~~~~~~

LIBXSMM is a library for specialized dense and sparse matrix operations as well as for deep learning primitives such as small convolutions targeting Intel Architecture. Small matrix multiplication kernels (SMMs) are generated for Intel SSE, Intel AVX, Intel AVX2, IMCI (KNCni) for Intel Xeon Phi coprocessors (KNC), and Intel AVX‑512 as found in the Intel Xeon Phi processor family (KNL, KNM) and Intel Xeon processors (SKX). Highly optimized code for small convolutions is targeting Intel AVX2 and Intel AVX‑512, whereas other targets can automatically leverage specialized SMMs to perform convolutions.

The library supports statically generated code at configuration time (SMMs), uses optimized code paths based on compiler-generated code as well as Intrinsic functions, but mainly utilizes Just‑In‑Time (JIT) code specialization for compiler-independent performance (matrix multiplications, matrix transpose/copy, sparse functionality, and small convolutions). LIBXSMM is suitable for "build once and deploy everywhere" i.e., no special target flags are needed to exploit the available performance.

https://github.com/hfp/libxsmm[+https://github.com/hfp/libxsmm+]

https://libxsmm.readthedocs.io/en/latest/[+https://libxsmm.readthedocs.io/en/latest/+]

LiDAR
~~~~~

Lidar (also called LIDAR, LiDAR, and LADAR) is a surveying method that measures distance to a target by illuminating the target with pulsed laser light and measuring the reflected pulses with a sensor. Differences in laser return times and wavelengths can then be used to make digital 3-D representations of the target. The name lidar, now used as an acronym of light detection and ranging (sometimes light imaging, detection, and ranging), was originally a portmanteau of light and radar. Lidar sometimes is called 3D laser scanning, a special combination of a 3D scanning and laser scanning. It has terrestrial, airborne, and mobile applications.

Lidar is commonly used to make high-resolution maps, with applications in geodesy, geomatics, archaeology, geography, geology, geomorphology, seismology, forestry, atmospheric physics, laser guidance, airborne laser swath mapping (ALSM), and laser altimetry. The technology is also used in control and navigation for some autonomous cars.

https://en.wikipedia.org/wiki/Lidar[+https://en.wikipedia.org/wiki/Lidar+]

CloudCompare
^^^^^^^^^^^^

CloudCompare is a 3D point cloud (and triangular mesh) processing software. It was originally designed to perform comparison between two 3D points clouds (such as the ones obtained with a laser scanner) or between a point cloud and a triangular mesh. It relies on an octree structure that is highly optimized for this particular use-case. It was also meant to deal with huge point clouds (typically more than 10 millions points, and up to 120 millions with 2 Gb of memory).

CloudCompare provides a set of basic tools for manually editing and rendering 3D points clouds and triangular meshes. It also offers various advanced processing algorithms, among which methods for performing:

* projections (axis-based, cylinder or a cone unrolling, ...)
* registration (ICP, ...)
* distance computation (cloud-cloud or cloud-mesh nearest neighbor distance, ...)
* statistics computation (spatial Chi-squared test, ...)
* segmentation (connected components labeling, front propagation based, ...)
* geometric features estimation (density, curvature, roughness, geological plane orientation, ...)

CloudCompare can handle unlimited scalar fields per point cloud on which various dedicated algorithms can be applied (smoothing, gradient evaluation, statistics, etc.). A dynamic color rendering system helps the user to visualize per-point scalar fields in an efficient way. Therefore, CloudCompare can also be used to visualize N-D data.

The user can interactively segment 3D entities (with a 2D polyline drawn on screen), interactively rotate/translate one or more entities relatively to the others, interactively pick single points or couples of points (to get the corresponding segment length) or triplets of points (to get the corresponding angle and plane normal). The latest version also supports the creation of 2D labels attached to points or rectangular areas annotations. 

https://github.com/cloudcompare/cloudcompare[+https://github.com/cloudcompare/cloudcompare+]

http://www.cloudcompare.org/[+http://www.cloudcompare.org/+]

GRASS
^^^^^

Point cloud data, as a type of representation of 3D surfaces, are usually produced by airborne or on-ground laser scanning, also known as Light Detection and Ranging (LiDAR). The data are often provided as sets of very dense (x, y, z) points or in a more complex, public file binary format called LAS that may include multiple returns as well as intensities. GRASS GIS supports basic and advanced lidar data processing and analysis. 
Import modules are:

* r.in.xyz - Create a raster map from an assemblage of many coordinates using univariate statistics. (example)

* r.in.lidar - (GRASS 7 only; GRASS must be compiled with libLAS support) Create a raster map from a binary LAS format LiDAR file (*.las) using univariate statistics and filtering. r.in.lidar is based on r.in.xyz. In addition to the options of r.in.xyz, r.in.lidar provides some basic lidar point filter options.

* v.in.ascii - Import data from an ASCII file to GRASS vector format.

* v.in.lidar - (GRASS 7 only; GRASS must be compiled with libLAS support). Creates a vector points file from a binary LAS format LiDAR file (*.las or *.laz). r.in.lidar also can create a new location based on the LAS file, and can filter the input points by return and subregion.

Analysis modules are:

* v.outlier - Removes outliers from vector point data.

* v.lidar.edgedetection - Uses interpolation and edge detection to create a new vector points file of LiDAR data so that the resulting attribute table is reclassified with CAT=1 for points associated with the ground surface (i.e., terrain) and useful for interpolating a raster terrain (DEM) map, CAT=2 for points pertaining to edges of human-contructed objects, and CAT=3 for other points that could pertain to vegetation or other features.

* v.lidar.growing - Building contour determination and region growing algorithm for determining the building inside.

* v.lidar.correction - Correction of the v.lidar.growing output. It is the last of the three algorithms for LIDAR filtering.

Surface generation modules are:

* v.surf.rst - Spatial approximation and topographic analysis using regularized spline with tension.

* v.surf.idw - Surface interpolation from vector point data by Inverse Distance Squared Weighting.

* v.surf.bspline - Surface interpolation from vector point data by bicubic or bilineal interpolation with Tykhonov regularization.

* r.fillnulls - Fills no-data areas in raster maps using v.surf.rst splines interpolation.

* r.surf.nnbathy - Natural Neighbor interpolation using the 'nn' addon.

https://grasswiki.osgeo.org/wiki/LIDAR[+https://grasswiki.osgeo.org/wiki/LIDAR+]

https://grasswiki.osgeo.org/wiki/Processing_lidar_and_UAV_point_clouds_in_GRASS_GIS_(workshop_at_FOSS4G_Boston_2017)[+https://grasswiki.osgeo.org/wiki/Processing_lidar_and_UAV_point_clouds_in_GRASS_GIS_(workshop_at_FOSS4G_Boston_2017)+]

HELIOS
^^^^^^

Heidelberg LiDAR Operations Simulator (HELIOS) is a highly flexible laser scanning simulation framework.
It is implemented as a Java library and split up into a core component and multiple extension modules. Extensible Markup Language (XML) is used to define scanner, platform and scene models and to configure the behaviour of modules. Modules are developed and implemented for (1) loading of simulation assets and configuration (i.e. 3D scene models, scanner definitions, survey descriptions etc.), (2) playback of XML survey descriptions, (3) TLS survey planning (i.e. automatic computation of recommended scanning positions) and (4) interactive real-time 3D visualization of simulated surveys. 


https://www.geog.uni-heidelberg.de/gis/helios.html[+https://www.geog.uni-heidelberg.de/gis/helios.html+]

https://github.com/GIScience/helios[+https://github.com/GIScience/helios+]

LASzip
^^^^^^

LASzip - a free open source product of rapidlasso GmbH - quickly turns bulky LAS files into compact LAZ files without information loss. Terabytes of LAZ data are now available for free download from various agencies making LASzip, winner of the 2012 Geospatial World Forum Technology Innovation Award in LiDAR Processing and runner-up for innovative product at INTERGEO 2012, the de-facto standard for LiDAR compression.

LASzip compression can be many times smaller and many times faster than generic compressors like bz2, gzip, and rar because it knows what the different bytes in a LAS file represent. Another advantage of LASzip is that it allows you to treat compressed LAZ files just like standard LAS files. You can load them directly from compressed form into your application without needing to decompress them onto disk first. The availability of the LASzip DLL and two APIs, libLAS and LASlib, with LASzip capability makes it easy to add native LAZ support to your own software package.

https://laszip.org/[+https://laszip.org/+]

https://github.com/LASzip/LASzip[+https://github.com/LASzip/LASzip+]

lidar2dems
^^^^^^^^^^

The lidar2dems project is a collection open-source (FreeBSD license) command line utilities for supporting the easy creation of Digital Elevation Models (DEMs) from LiDAR data. lidar2dems uses the PDAL library (and associated dependencies) for doing the actual point processing and gridding of point clouds into raster data.

A Digital Elevation Model (DEM) is the generic name for any raster data containing elevation data. There are 3 types of DEMs that lidar2dems can generate:

* Digital Terrain Model (DTM) - This is the calculated elevation using only points classified as ground.
* Digtial Surface Model (DSM) - This is the calculated elevation using the highest non-ground points. In forested areas this corresponds to the absolute elevation of the top of the canopy, but it could also be the roofs of buildings or other structures.
* Canopy Height Model (CHM) - This is the difference between the DSM and DTM, which is presumed to be a forested area, thus ‘Canopy Height’.

Currently only LAS LiDAR files are supported. However, lidar2dems can handle any number of input point clouds for a given region of interest, be they tiles, swaths, or any arbitrary footprint. A region of interest is defined by a shapefile, called a ‘site’ file, which must also be in the same reference system as the LAS file(s). Points can be filters with a variety of options, and resulting DEMs can be gap-filled.

http://applied-geosolutions.github.io/lidar2dems/[+http://applied-geosolutions.github.io/lidar2dems/+]

https://github.com/Applied-GeoSolutions/lidar2dems[+https://github.com/Applied-GeoSolutions/lidar2dems+]

OPALS
^^^^^

OPALS stands for Orientation and Processing of Airborne Laser Scanning data. It is a modular program system consisting of small components (modules) grouped together thematically in packages. The aim of OPALS is to provide a complete processing chain for processing airborne laser scanning data (waveform decomposition, quality control, georeferencing, structure line extraction, point cloud classification, DTM generation) and several fields of application like forestry, hydrography, city modelling and power lines).

The OPALS program system is mainly designed for automatic processing. Thus, a sophisticated graphical user interface and interactive editing steps are omitted deliberately. This may seem disadvantageous at first glance. However, OPALS is split into small, well-defined modules that may be combined freely, resulting in flexible, custom processing chains. For instance, the derivation of a hypsometric map of a single ALS flight strip is achieved using three different modules: import of strip point data, DTM grid interpolation and derivation of the colour coded rastermap, facilitating the re-usability of the respective components in different application environments. To further ease this combination, OPALS modules can be accessed in three different ways: (i) from command prompts / Unix or Linux shells as executables, (ii) from Python shells using platform-independent Python code, and (iii) from custom Cxx programs by dynamic linkage (DLL). The latter allows experienced users direct embedding of OPALS components in their own Cxx programming environment. By contrast, the former two options (stand-alone executable and Python API) allow combining OPALS modules in either Unix/Batch or Python scripts. Scripting is a powerful instrument, as it enables the construction of complex, custom processing chains by freely combining OPALS modules.

As pointed out before, efficient data management is regarded to be of crucial importance. Thus, the OPALS Data Manager (ODM) was developed, featuring high-performance spatial queries of point and line data even for large project areas. The ODM acts as a spatial cache combining the simplicity and efficiency of file based processing and the flexibility and expandability of database systems. An independent import module is provided to read data in arbitrary data formats or even to extract data from spatial databases, and to build up the ODM data structure. Subsequent application modules take an ODM file as input and have access to the coordinate and echo attribute information via an internal ODM library. Analogously, an export module is available for converting data stored in the ODM back to a series of supported file formats and databases. Due to the omission of interactivity, high-performance interfaces to external program systems like DTM, GIS, editing or visualisation programs are provided. In this regard, OPALS makes intensive use of open source solutions like the geodata abstraction library GDAL for accessing grid data and the OGR simple feature library (subset of GDAL) for interchange of point and line related data. This provides conformance of OPALS to the specifications of the Open Geospatial Consortium (OGC) on the one hand and facilitates data exchange with software systems like GRASS on the other hand which use the same technology.

https://www.geo.tuwien.ac.at/opals/html/index.html[+https://www.geo.tuwien.ac.at/opals/html/index.html+]

PPTK
^^^^

The Point Processing Toolkit (pptk) is a Python package for visualizing and processing 2-d/3-d point clouds.

At present, pptk consists of the following features.

* A 3-d point cloud viewer that
** accepts any 3-column numpy array as input,
** renders tens of millions of points interactively using an octree-based level of detail mechanism,
** supports point selection for inspecting and annotating point data.
* A fully parallelized point k-d tree that supports k-nearest neighbor queries and r-near range queries (both build and queries have been parallelized).
* A normal estimation routine based on principal component analysis of point cloud neighborhoods.

https://github.com/heremaps/pptk[+https://github.com/heremaps/pptk+]

https://heremaps.github.io/pptk/[+https://heremaps.github.io/pptk/+]

Whitebox
^^^^^^^^

The whitebox Python package is built on WhiteboxTools, an advanced geospatial data analysis platform.
WhiteboxTools can be used to perform common geographical information systems (GIS) analysis operations, such as cost-distance analysis, distance buffering, and raster reclassification. Remote sensing and image processing tasks include image enhancement (e.g. panchromatic sharpening, contrast adjustments), image mosaicing, numerous filtering operations, simple classification (k-means), and common image transformations. WhiteboxTools also contains advanced tooling for spatial hydrological analysis (e.g. flow-accumulation, watershed delineation, stream network analysis, sink removal), terrain analysis (e.g. common terrain indices such as slope, curvatures, wetness index, hillshading; hypsometric analysis; multi-scale topographic position analysis), and LiDAR data processing. LiDAR point clouds can be interrogated (LidarInfo, LidarHistogram), segmented, tiled and joined, analyized for outliers, interpolated to rasters (DEMs, intensity images), and ground-points can be classified or filtered. WhiteboxTools is not a cartographic or spatial data visualization package; instead it is meant to serve as an analytical backend for other data visualization software, mainly GIS.

https://github.com/giswqs/whitebox[+https://github.com/giswqs/whitebox+]

LIGGGHTS
~~~~~~~~

LIGGGHTS® is an Open Source Discrete Element Method Particle Simulation Software. It can be used for the simulation of particulate materials, and aims to for applications it to industrial problems LIGGGHTS® is currently used by a variety of research institutions world-wide. A number of Blue Chip companies in the fields of chemical, consumer goods, pharmaceutical, agricultural engineering, food production, steel production, mining, plastics production use LIGGGHTS® for improvement of production processes.

LIGGGHTS®-PUBLIC features:

* Import and handling of complex geometries: STL walls and VTK tet volume meshes
* Moving mesh feature with a varierty of motion schemes and a model for conveyor belts
* Force and wear analysis on meshes as well as stress-controlled walls
* A variety of particle-particle contact implementations, including models for tangential history, non-spericity and cohesion
* Interface to easily extend contact implementations
* Heat conduction between particles
* Particle insertion based on pre-defined volumes, meshes and particle streams from faces as well as particle growth and shrinkage
* Flexible definition of particle distributions
* Smoothed Particle Hydrodynamics (SPH) fluid models
* Output to the widely used, open source VTK data format
* A strong eco-system of fellow simulation engines for co-simulation, efficienty and tightly coupled via MPI: CFDEM®coupling for CFD-DEM simulations and Lagrange-Euler coupling in general and the simulation engine ParScale for the modelling of intra-particle transport processes

https://www.cfdem.com/liggghtsr-open-source-discrete-element-method-particle-simulation-code[+https://www.cfdem.com/liggghtsr-open-source-discrete-element-method-particle-simulation-code+]

ParScale
^^^^^^^^

ParScale stands for “Particle Scale Models”, and is a publicly available library to simulate intra-particle transport phenomena, e.g., heat or mass transfer, as well as homogeneous and heterogeneous reactions in porous particles. For example, the rate of oxidation of porous metal particles suspended by a hot gas stream can be predicted by ParScale.

ParScale is available via https://github.com/CFDEMproject, and is designed as an add-on to the soft-sphere particle simulation tool LIGGGHTS.

https://github.com/CFDEMproject/ParScale-PUBLIC[+https://github.com/CFDEMproject/ParScale-PUBLIC+]

https://www.tugraz.at/institute/ippt/downloads-software/[+https://www.tugraz.at/institute/ippt/downloads-software/+]

linnea
~~~~~~

Linnea is an experimental tool for the automatic generation of optimized code for linear algebra problems. It is developed at the High-Performance and Automatic Computing group at RWTH Aachen University.

Linnea is a prototype of a compiler/program synthesis tool that automates the translation of the mathematical description of a linear algebra problem to an efficient sequence of calls to BLAS and LAPACK kernels. The main idea of Linnea is to construct a search graph that represents a large number of programs, taking into account knowledge about linear algebra, numerical linear algebra and high-performance computing. The algebraic nature of the domain is used to reduce the size of the search graph, without reducing the size of the search space that is explored.

The input to Linnea are linear algebra expressions. As operands, matrices, vectors and scalars are supported. Operands can be annotated with properties, such as 'lower triangular' or 'symmetric'. Supported operations are addition, multiplication, transposition and inversion. At the moment, Linnea generates Julia code (see https://julialang.org), using BLAS and LAPACK wrappers whenever possible.

https://github.com/HPAC/linnea[+https://github.com/HPAC/linnea+]

linuxbrew
~~~~~~~~~

The Homebrew package manager may be used on Linux and Windows 10, using Windows Subsystem for Linux (WSL). Homebrew is referred to as Linuxbrew when running on Linux or Windows. It can be installed in your home directory, in which case it does not use sudo. Linuxbrew does not use any libraries provided by your host system, except glibc and gcc if they are new enough. Linuxbrew can install its own current versions of glibc and gcc for older distribution of Linux.

The features include:

* Can install software to your home directory and so does not require sudo
* Install software not packaged by your host distribution
* Install up-to-date versions of software when your host distribution is old
* Use the same package manager to manage your macOS, Linux, and Windows systems

http://linuxbrew.sh/[+http://linuxbrew.sh/+]

https://formulae.brew.sh/[+https://formulae.brew.sh/+]

Livy
~~~~

Livy is an open source REST interface for interacting with Apache Spark from anywhere. It supports executing snippets of code or programs in a Spark context that runs locally or in Apache Hadoop YARN.
The features include:

* Interactive Scala, Python and R shells
* Batch submissions in Scala, Java, Python
* Multiple users can share the same server (impersonation support)
* Can be used for submitting jobs from anywhere with REST
* Does not require any code change to your programs

https://github.com/cloudera/livy[+https://github.com/cloudera/livy+]

https://www.jowanza.com/blog/creating-a-spark-server-for-every-job-with-livy[+https://www.jowanza.com/blog/creating-a-spark-server-for-every-job-with-livy+]

LMDZ
~~~~

LMDZ is a general circulation model (or global climate model) developped since the 70s at the "Laboratoire de Météorologie Dynamique", which includes various variants for the Earth and other planets (Mars, Titan, Venus, Exoplanets). The 'Z' in LMDZ stands for "zoom" (and the 'LMD' is for  'Laboratoire de Météorologie Dynamique").

The Earth version of LMDZ is the atmospheric component of the IPSL "Integrated Climate Model", the development of which is coordinated by the "pôle modélisation", and is thus part of the significant international effort to study the evolution and future of the Earth's climate.

Concerning the other planets, dedicated versions have been developed  in strong connection with space exploration of the solar system activities, and more recently applied to the case of extra-solar planets.

LMDZ is first and foremost a research tool. A constant concern in the development of LMDZ is to keep it light and easily adaptable.

There is also a constant effort done regarding the evaluation of the capabilities of the model. It is possible with LMDZ to simulate satellite observations (RTTOV, ISCCP, CALIPSO, ...) and the model can be used in semi-operational mode: with zoomed versions guided in real time or not, transport of poluttants and retro-transport, etc.

http://lmdz.lmd.jussieu.fr/[+http://lmdz.lmd.jussieu.fr/+]

http://lmdz.lmd.jussieu.fr/utilisateurs/distribution-du-modele/versions-intermediaires[+http://lmdz.lmd.jussieu.fr/utilisateurs/distribution-du-modele/versions-intermediaires+]

http://lmdz.lmd.jussieu.fr/utilisateurs/guides/lmdz-pas-a-pas/installation[+http://lmdz.lmd.jussieu.fr/utilisateurs/guides/lmdz-pas-a-pas/installation+]

http://forge.ipsl.jussieu.fr/igcmg/wiki/LMDZ4OR_v2[+http://forge.ipsl.jussieu.fr/igcmg/wiki/LMDZ4OR_v2+]

lmod
~~~~

Lmod is program to manage the user environment under Unix: (Linux, Mac OS X, ...). It is a new implementation of environment modules.

Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users’ environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.

Environment Modules provide a convenient way to dynamically change the users’ environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable.

A modulefile contains the necessary information to allow a user to run a particular application or provide access to a particular library. All of this can be done dynamically without logging out and back in. Modulefiles for applications modify the user’s path to make access easy. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.

Packages can be loaded and unloaded cleanly through the module system. All the popular shells are supported: bash, ksh, csh, tcsh, zsh. Also available for perl and python.

It is also very easy to switch between different versions of a package or remove it.

https://lmod.readthedocs.io/en/latest/[+https://lmod.readthedocs.io/en/latest/+]

https://github.com/TACC/Lmod[+https://github.com/TACC/Lmod+]

https://mkmod.readthedocs.io/en/latest/[+https://mkmod.readthedocs.io/en/latest/+]

Lua
~~~

Lua (/ˈluːə/ LOO-ə; from Portuguese: lua [ˈlu.(w)ɐ] meaning moon)[a] is a lightweight, multi-paradigm programming language designed primarily for embedded use in applications.[2] Lua is cross-platform, since the interpreter is written in ANSI C,[3] and has a relatively simple C API.[4]

Lua was originally designed in 1993 as a language for extending software applications to meet the increasing demand for customization at the time. It provided the basic facilities of most procedural programming languages, but more complicated or domain-specific features were not included; rather, it included mechanisms for extending the language, allowing programmers to implement such features. As Lua was intended to be a general embeddable extension language, the designers of Lua focused on improving its speed, portability, extensibility, and ease-of-use in development. 

Lua is commonly described as a "multi-paradigm" language, providing a small set of general features that can be extended to fit different problem types. Lua does not contain explicit support for inheritance, but allows it to be implemented with metatables. Similarly, Lua allows programmers to implement namespaces, classes, and other related features using its single table implementation; first-class functions allow the employment of many techniques from functional programming; and full lexical scoping allows fine-grained information hiding to enforce the principle of least privilege.

In general, Lua strives to provide simple, flexible meta-features that can be extended as needed, rather than supply a feature-set specific to one programming paradigm. As a result, the base language is light—the full reference interpreter is only about 247 kB compiled[3]—and easily adaptable to a broad range of applications.

Lua is a dynamically typed language intended for use as an extension or scripting language and is compact enough to fit on a variety of host platforms. It supports only a small number of atomic data structures such as boolean values, numbers (double-precision floating point and 64-bit integers by default), and strings. Typical data structures such as arrays, sets, lists, and records can be represented using Lua's single native data structure, the table, which is essentially a heterogeneous associative array.

Lua implements a small set of advanced features such as first-class functions, garbage collection, closures, proper tail calls, coercion (automatic conversion between string and number values at run time), coroutines (cooperative multitasking) and dynamic module loading. 

https://en.wikipedia.org/wiki/Lua_(programming_language)[+https://en.wikipedia.org/wiki/Lua_(programming_language)+]

https://www.lua.org/[+https://www.lua.org/+]

http://luaforge.net/projects/[+http://luaforge.net/projects/+]

http://lua-users.org/wiki/[+http://lua-users.org/wiki/+]

http://lua-users.org/wiki/LuaAddons[+http://lua-users.org/wiki/LuaAddons+]

LuaJIT
^^^^^^

LuaJIT is a Just-In-Time Compiler (JIT) for the Lua programming language. Lua is a powerful, dynamic and light-weight programming language. It may be embedded or used as a general-purpose, stand-alone language. 

LuaJIT has been successfully used as a scripting middleware in games, appliances, network and graphics apps, numerical simulations, trading platforms and many other specialty applications. It scales from embedded devices, smartphones, desktops up to server farms. It combines high flexibility with high performance and an unmatched low memory footprint. 

http://luajit.org/luajit.html[+http://luajit.org/luajit.html+]

https://github.com/LuaJIT/LuaJIT[+https://github.com/LuaJIT/LuaJIT+]

Luigi
~~~~~

Luigi is a Python (2.7, 3.6, 3.7 tested) package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more.

The purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures will happen. These tasks can be anything, but are typically long running things like Hadoop jobs, dumping data to/from databases, running machine learning algorithms, or anything else.

There are other software packages that focus on lower level aspects of data processing, like Hive, Pig, or Cascading. Luigi is not a framework to replace these. Instead it helps you stitch many tasks together, where each task can be a Hive query, a Hadoop job in Java, a Spark job in Scala or Python, a Python snippet, dumping a table from a database, or anything else. It's easy to build up long-running pipelines that comprise thousands of tasks and take days or weeks to complete. Luigi takes care of a lot of the workflow management so that you can focus on the tasks themselves and their dependencies.

You can build pretty much any task you want, but Luigi also comes with a toolbox of several common task templates that you use. It includes support for running Python mapreduce jobs in Hadoop, as well as Hive, and Pig, jobs. It also comes with file system abstractions for HDFS, and local files that ensures all file system operations are atomic. This is important because it means your data pipeline will not crash in a state containing partial data.

Conceptually, Luigi is similar to GNU Make where you have certain tasks and these tasks in turn may have dependencies on other tasks. There are also some similarities to Oozie and Azkaban. One major difference is that Luigi is not just built specifically for Hadoop, and it's easy to extend it with other kinds of tasks.

Everything in Luigi is in Python. Instead of XML configuration or similar external data files, the dependency graph is specified within Python. This makes it easy to build up complex dependency graphs of tasks, where the dependencies can involve date algebra or recursive references to other versions of the same task. However, the workflow can trigger things not in Python.

https://github.com/spotify/luigi[+https://github.com/spotify/luigi+]

Lush
~~~~

Lush is an object-oriented programming language designed for researchers, experimenters, and engineers interested in large-scale numerical and graphic applications. Lush is designed to be used in situations where one would want to combine the flexibility of a high-level, weakly-typed interpreted language, with the efficiency of a strongly-typed, natively-compiled language, and with the easy integration of code written in C, Cxx, or other languages.

Lush is Free Software (under the GPL license) and runs on GNU/Linux, Solaris, Irix, and Windows under Cygwin.

Lush can be used advantageously for projects where one would otherwise use a combination of an interpreted language like Matlab, Python, Perl, S+, or even (gasp!) BASIC, and a compiled language like C. Lush brings the best of both worlds by wrapping three languages into one: (1) a weakly-typed, garbage-collected, dynamically scoped, interpreted language with a simple Lisp-like syntax, (2) a strongly-typed, lexically-scoped compiled language that uses the same Lisp-like syntax, and (3) the C language, which can be freely mixed with Lush code within a single program, even within a single function. It sounds complicated, but it is not. In fact, Lush is designed to be very simple to learn and easy to use.

If you do research and development in signal processing, image processing, machine learning, computer vision, bio-informatics, data mining, statistics, simulation, optimization, or artificial intelligence, and feel limited by Matlab and other existing tools, Lush is for you. If you want a simple environment to experiment with graphics, video, and sounds, Lush is for you. 

The features include:

* A very clean, simple, and easy to learn Lisp-like syntax.
* A compiler that produces very efficient C code and relies on the C compiler to produce efficient native code (no inefficient bytecode or virtual machine).
* An easy way to interface C functions and libraries, and a powerful dynamic linker/loader for object files or libraries (.o, .a and .so files) written in other compiled languages.
* The ability to freely mix Lisp and C in a single function.
* A powerful set of vector/matrix/tensor operations.
* A huge library of over 10,000 numerical routines, including full interfaces to GSL, LAPACK, and BLAS.
* A library of image and signal processing routines.
* An extensive set of graphic routines, including an object-oriented GUI toolkit, an interface to OpenGL/GLU/GLUT, and the OpenInventor scene rendering engine.
* An interface to the Simple Directmedia Layer (SDL) multimedia library, including a sprite class with pixel-accurate collision detection (perfect for 2D games).
* Sound and video grabbing (using ALSA and Video4Linux).
* Several libraries for machine learning, neural net, statistical estimation, Hidden Markov Models (gblearn2, Torch, HTK, SVM).
* libraries for computer vision (OpenCV, Intel's open source Vision Library), and 3D scene rendering (OpenInventor).
* bindings to the JavaVM API and to the Python C API. 

This combination of flexibility, efficiency, and extensive libraries with over 14,000 functions and classes makes Lush an ideal platform for research and development in signal processing, image processing, machine learning, computer vision, bio-informatics, data mining, statistics, and artificial intelligence. Its speed and extensive libraries allow such things as real-time sound, image, and video processing. Most users use Lush as a research tool, but many use it as a general purpose script language, or as a general language for application development. Some have been known to use Lush to develop 2D and 3D games. A few have even used Lush to develop commercial software for embedded processors. 

http://lush.sourceforge.net/[+http://lush.sourceforge.net/+]

lxml
~~~~

The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. It is unique in that it combines the speed and XML feature completeness of these libraries with the simplicity of a native Python API, mostly compatible but superior to the well-known ElementTree API.

https://lxml.de/[+https://lxml.de/+]

https://github.com/lxml/lxml[+https://github.com/lxml/lxml+]

LyX
~~~

LyX (pronounced [ˈlɪks]) is an open source document processor based on the LaTeX typesetting system. Unlike most word processors, which follow the WYSIWYG ("what you see is what you get") paradigm, LyX has a WYSIWYM ("what you see is what you mean") approach, where what shows up on the screen is only an approximation of what will show up on the page.

Since LyX largely functions as a front-end to the LaTeX typesetting system, it has the power and flexibility of LaTeX, and can handle documents including books, notes, theses, to academic papers, letters, etc. Knowledge of the LaTeX markup language is not necessary for basic usage, although a variety of specialized formatting is only possible by adding LaTeX directives directly into the page.

LyX is popular among technical authors and scientists for its advanced mathematical modes, though it is increasingly used by non-mathematically-oriented scholars as well[1][2] for its bibliographic database integration[3] and ability to manage multiple files.[3] LyX has also become popular among self-publishers.

The math and science features include:

* Mathematical formula editor which is easily best of breed. Fully harnesses the power of LaTeX.
* Equations can be entered via point-and-click interface or via keyboard with LaTeX commands (optionally via auto-completion). Formulas are immediately visually rendered on screen.
* Copy/paste to and from LaTeX source code.
* Equation arrays, equation numbering, theorems, matrices, algorithms, and much more.
* Support of customizable math macros
* Basic support for various Computer Algebra Systems (CAS) - Maple, Maxima, Octave, Mathematica 

The structured document creation features include:

* Advanced features for labels, references, index and bibliography (including advanced BibTeX support)
* Standard word processor operations, like cut/paste, multiple open documents, undo/redo, spellchecking (uses aspell or hunspell in the background), and thesaurus and revision tracking
* Different textclasses allow you to type letters, articles, books, movie scripts, LinuxDoc, slides, presentations. Also included are some textclasses for scientific societies, such as AMS, APS, IEEE, or specific journals like Astronomy and Astrophysics.
* Dedicated modules let you dynamically enhance the functionality of the textclass by adding features that are needed for specific tasks (such as endnotes, linguistic glosses, Noweb/Sweave or LilyPond support)
* Numbered section headings, table of contents (with hypertext functionality), lists of figures/tables
* The outliner mode allows for easy navigating in your document as well as for moving or nesting complete chapters and sections
* Character styles provide access to fully semantic markup 

And much bloody more.

https://www.lyx.org/[+https://www.lyx.org/+]

https://en.wikipedia.org/wiki/LyX[+https://en.wikipedia.org/wiki/LyX+]

https://wiki.lyx.org/[+https://wiki.lyx.org/+]

eLyXer
^^^^^^

eLyXer (pronounced elixir) is a LyX to HTML converter. While there are a ton of such projects all over the web, eLyXer has a clear focus on flexibility and elegant output. 
LyX is a wonderful text editor which produces beautiful PDF files. Internally it exports documents to LaTeX, and from there to PDF. Sadly there is not an equivalent “export to HTML” option… Until now! With eLyXer you can convert your master’s thesis, learned article, fascinating novel or love letter to a web page that you can then share, publish on the web or import into other text editors. 

http://pinchito.es/elyxer/[+http://pinchito.es/elyxer/+]

http://savannah.nongnu.org/projects/elyxer[+http://savannah.nongnu.org/projects/elyxer+]

#MMMM

Macrostat
~~~~~~~~~

Macrostrat is a platform for the aggregation and distribution of geological data relevant to the spatial and temporal distribution of sedimentary, igneous, and metamorphic rocks as well as data extracted from them. It is linked to the GeoDeepDive digital library and machine reading system, and it aims to become a community resource for the addition, editing, and distribution of new stratigraphic, lithological, environmental, and economic data. Interactive applications built upon Macrostrat are designed for educational and research purposes.

https://macrostrat.org/[+https://macrostrat.org/+]

https://macrostrat.org/sift/#/[+https://macrostrat.org/sift/#/+]

https://rockd.org/[+https://rockd.org/+]

https://macrostrat.org/map/#/z=1.5/x=16/y=23/bedrock/lines/[+https://macrostrat.org/map/#/z=1.5/x=16/y=23/bedrock/lines/+]

https://macrostrat.org/classic/[+https://macrostrat.org/classic/+]

http://fc.umn.edu/[+http://fc.umn.edu/+]

https://github.com/UW-Macrostrat/node-api-template[+https://github.com/UW-Macrostrat/node-api-template+]

MADNESS
~~~~~~~

MADNESS provides a high-level environment for the solution of integral and differential equations in many dimensions using adaptive, fast methods with guaranteed precision based on multi-resolution analysis and novel separated representations. There are three main components to MADNESS. At the lowest level is a new petascale parallel programming environment that increases programmer productivity and code performance/scalability while maintaining backward compatibility with current programming tools such as MPI and Global Arrays. The numerical capabilities built upon the parallel tools provide a high-level environment for composing and solving numerical problems in many (1-6+) dimensions. Finally, built upon the numerical tools are new applications with initial focus upon chemistry, atomic and molecular physics, material science, and nuclear structure.

https://github.com/m-a-d-n-e-s-s/madness[+https://github.com/m-a-d-n-e-s-s/madness+]

https://github.com/scibuilder/SciBuilder[+https://github.com/scibuilder/SciBuilder+]

TiledArray
^^^^^^^^^^

TiledArray is a scalable, block-sparse tensor framework for rapid composition of high-performance tensor arithmetic, appearing for example in many-body quantum mechanics. It allows users to compose tensor expressions of arbitrary complexity in native C++ code that closely resembles the standard mathematical notation. The framework is designed to scale from a single multicore computer to a massive distributed-memory multiprocessor.

TiledArray is built on top of MADNESS parallel runtime (MADWorld), part of MADNESS numerical calculus framework.

https://github.com/ValeevGroup/tiledarray[+https://github.com/ValeevGroup/tiledarray+]

MADS
~~~~

Model Analysis and Decision Support (MADS) is an open-source high-performance computational framework for
data- & model-based analyses in
Julia and C.

The capabilities include:

* Sensitivity Analysis
* Parameter Estimation
* Model Inversion and Calibration
* Uncertainty Quantification
* Model Selection and Model Averaging
* Model Reduction and Surrogate Modeling
* Machine Learning and Blind Source Separation
* Decision Analysis and Support
* can be internally or externally coupled with any existing model simulator
* includes built-in analytical solutions for groundwater flow and contaminant transport
* includes built-in test functions
* includes verification and example problems
* performs automatic bookkeeping of model results for efficient restarts and reruns
* has been successfully applied to perform analyses related to environmental management
* available in C and Julia versions

https://mads.lanl.gov/[+https://mads.lanl.gov/+]

https://github.com/zemjulia/FEHM.jl[+https://github.com/zemjulia/FEHM.jl+]

Magni
~~~~~

Magni is a Python package which provides functionality for increasing the speed of image acquisition using Atomic Force Microscopy (AFM). The image acquisition algorithms of Magni are based on the Compressed Sensing (CS) signal acquisition paradigm and include both sensing and reconstruction. The sensing part of the acquisition generates sensed data from regular images possibly acquired using AFM. This is done by AFM hardware simulation. The reconstruction part of the acquisition reconstructs images from sensed data. This is done by CS reconstruction using well-known CS reconstruction algorithms modified for the purpose. The Python implementation of the above functionality uses the standard library, a number of third-party libraries, and additional utility functionality designed and implemented specifically for Magni. The functionality provided by Magni can thus be divided into five groups:

* Atomic Force Microscopy: AFM specific functionality including AFM image acquisition, AFM hardware simulation, and AFM data file handling.
* Compressed Sensing: General CS functionality including signal reconstruction and phase transition determination.
* Imaging: General imaging functionality including measurement matrix and dictionary construction in addition to visualisation and evaluation.
* Reproducibility: Tools that may aid in increasing the reproducibility of results obtained using Magni.
* Utilities: General Python utilities including multiprocessing, tracing, and validation.

https://github.com/SIP-AAU/Magni[+https://github.com/SIP-AAU/Magni+]

Makeflow
~~~~~~~~

Makeflow is a workflow engine for large scale distributed computing. It accepts a specification of a large amount of work to be performed, and runs it on remote machines in parallel where possible. In addition, Makeflow is fault-tolerant, so you can use it to coordinate very large tasks that may run for days or weeks in the face of failures. Makeflow is designed to be similar to Make, so if you can write a Makefile, then you can write a Makeflow.

Makeflow makes it easy to move a large amount of work from one facility to another. After writing a workflow, you can test it out on your local laptop, then run it at your university computing center, move it over to a national computing facility like XSEDE, and then again to a commercial cloud system. Using the (bundled) Work Queue system, you can even run across multiple systems simultaneously. No matter where you run your tasks, the workflow language stays the same.

Makeflow is used in production to support large scale problems in science and engineering. Researchers in fields such as bioinformatics, biometrics, geography, and high energy physics all use Makeflow to compose workflows from existing applications.
Makeflow can send your jobs to a wide variety of services, such as batch systems (HTCondor, SGE, PBS, Torque), cluster managers (Mesos and Kubernetes), cloud services (Amazon EC2 or Lambda) and container environments like Docker and Singularity.

http://ccl.cse.nd.edu/software/makeflow/[+http://ccl.cse.nd.edu/software/makeflow/+]

MAME
~~~~

MAME is a multi-purpose emulation framework.

MAME's purpose is to preserve decades of software history. As electronic technology continues to rush forward, MAME prevents this important "vintage" software from being lost and forgotten. This is achieved by documenting the hardware and how it functions. The source code to MAME serves as this documentation. The fact that the software is usable serves primarily to validate the accuracy of the documentation (how else can you prove that you have recreated the hardware faithfully?). Over time, MAME (originally stood for Multiple Arcade Machine Emulator) absorbed the sister-project MESS (Multi Emulator Super System), so MAME now documents a wide variety of (mostly vintage) computers, video game consoles and calculators, in addition to the arcade video games that were its initial focus.

The MAME core coordinates the emulation of several elements at the same time. These elements replicate the behavior of the hardware present in the original arcade machines. MAME can emulate many different central processing units (CPUs) and associated hardware. These elements are virtualized so MAME acts as a software layer between the original program of the game, and the platform MAME runs on. MAME supports arbitrary screen resolutions, refresh rates and display configurations. Multiple emulated monitors, as required by for example Darius, are supported as well.

Individual arcade systems are specified by drivers which take the form of C preprocessor macros. These drivers specify the individual components to be emulated and how they communicate with each other. While MAME was originally written in C, the need for object oriented programming caused the development team to begin to compile all code as Cxx for MAME 0.136, taking advantage of additional features of that language in the process.

Although a great majority of the CPU emulation cores are interpretive, MAME also supports dynamic recompilation through an intermediate language called the Universal Machine Language (UML) to increase the emulation speed. Back-end targets supported are x86 and x64. A C backend is also available to further aid verification of the correctness. CPUs emulated in this manner are SH-2, MIPS R3000 and PowerPC. 

The original program code, graphics and sound data need to be present so that the game can be emulated. In most arcade machines, the data is stored in read-only memory chips (ROMs), although other devices such as cassette tapes, floppy disks, hard disks, laserdiscs, and compact discs are also used. The contents of most of these devices can be copied to computer files, in a process called "dumping". The resulting files are often generically called ROM images or ROMs regardless of the kind of storage they came from. A game usually consists of multiple ROM and PAL images; these are collectively stored inside a single ZIP file, constituting a ROM set. In addition to the "parent" ROM set (usually chosen as the most recent "World" version of the game), games may have "clone" ROM sets with different program code, different language text intended for different markets etc. For example, Street Fighter II Turbo is considered a variant of Street Fighter II Champion Edition. System boards like the Neo Geo that have ROMs shared between multiple games require the ROMs to be stored in "BIOS" ROM sets and named appropriately.

Hard disks, compact discs and laserdiscs are stored in a MAME-specific format called CHD (Compressed Hunks of Data). Some arcade machines use analog hardware, such as laserdiscs, to store and play back audio/video data such as soundtracks and cinematics. This data must be captured and encoded into digital files that can be read by MAME. MAME does not support the use of external analog devices, which (along with identical speaker and speaker enclosures) would be required for a 100% faithful reproduction of the arcade experience. A number of games use sound chips that have not yet been emulated successfully. These games require sound samples in WAV file format for sound emulation. MAME additionally supports artwork files in PNG format for bezel and overlay graphics. 

https://www.mamedev.org/index.php[+https://www.mamedev.org/index.php+]

https://github.com/mamedev/mame[+https://github.com/mamedev/mame+]

https://en.wikipedia.org/wiki/MAME[+https://en.wikipedia.org/wiki/MAME+]

MAOOAM
~~~~~~

A reduced-order quasi-geostrophic coupled ocean–atmosphere model that allows for an arbitrary number of atmospheric and oceanic modes to be retained in the spectral decomposition. The modularity of this new model allows one to easily modify the model physics. Using this new model, coined the "Modular Arbitrary-Order Ocean-Atmosphere Model" (MAOOAM), we analyse the dependence of the model dynamics on the truncation level of the spectral expansion, and unveil spurious behaviour that may exist at low resolution by a comparison with the higher-resolution configurations. In particular, we assess the robustness of the coupled low-frequency variability when the number of modes is increased. An "optimal" configuration is proposed for which the ocean resolution is sufficiently high, while the total number of modes is small enough to allow for a tractable and extensive analysis of the dynamics.

The atmospheric component of the model is based on the papers of Charney and Straus (1980), Reinhold and Pierrehumbert (1982) and Cehelsky and Tung (1987), all published in the Journal of Atmospheric Sciences. The ocean component is based on the papers of Pierini (2012), Barsugli and Battisti (1998). The coupling between the two components includes wind forcings, radiative and heat exchanges.

https://github.com/Climdyn/MAOOAM[+https://github.com/Climdyn/MAOOAM+]

https://www.geosci-model-dev.net/9/2793/2016/[+https://www.geosci-model-dev.net/9/2793/2016/+]

MapD
~~~~

MapD Core is an in-memory, column store, SQL relational database that was designed from the ground up to run on GPUs.  

The foundation of the platform is OmniSci Core, an open-source, GPU-accelerated database. OmniSci Core harnesses GPU processing power and returns SQL query results in milliseconds, even on tables with billions of rows. OmniSci Core delivers high performance with rapid query compilation, query vectorization, and advanced memory management. 

With native SQL support, OmniSci Core returns query results hundreds of times faster than CPU-only analytical database platforms. Use your existing SQL knowledge to query data. You can use the standalone SQL engine with the command line, or the SQL editor that is part of the OmniSci Immerse visual analytics interface. Your SQL query results can output to OmniSci Immerse or to third-party software such as Birst, Power BI, Qlik, or Tableau.

OmniSci Core can store and query data using native Open Geospatial Consortium (OGC) types, including POINT, LINESTRING, POLYGON, and MULTIPOLYGON. With geo type support, you can query geo data at scale using special geospatial functions. Using the power of GPU processing, you can quickly and interactively calculate distances between two points and intersections between objects. 

OmniSci Core is open source and encourages contribution and innovation from a global community of users. It is available on Github under the Apache 2.0 license, along with components like a Python interface (pymapd) and JavaScript infrastructure (mapd-connector, mapd-charting), making OmniSci the leader in open-source analytics.

https://github.com/omnisci/mapd-core[+https://github.com/omnisci/mapd-core+]

https://www.omnisci.com/docs/latest/index.html[+https://www.omnisci.com/docs/latest/index.html+]

mapelia
~~~~~~~

This software was created to help with the development of 3D models of planets, moons and so on, used in the non-profit project A Touch of The Universe on educational astronomy.

There are several programs related to images of maps and 3D files:

* mapelia - convert maps into 3D figures with reliefs.
* guapelia - optional GUI to use mapelia.
* pintelia - convert maps into colored 3D figures.
* poligoniza - form faces (polygons) from 3D points.
* stl-split - split a 3D globe into the north and south hemispheres.

The images are jpg or png files that contain maps (that is, gridded datasets where the value of each pixel is the elevation) in any of the following projections: equirectangular, Mercator, central cylindrical, Mollweide or sinusoidal.

The output of the programs are 3D files (of polygons like ply or stl, or points in space like asc), that can be visualized and manipulated with programs like MeshLab or Blender.

https://github.com/jordibc/mapelia[+https://github.com/jordibc/mapelia+]

http://joss.theoj.org/papers/c13440773d37cc5abfd05016708eb351[+http://joss.theoj.org/papers/c13440773d37cc5abfd05016708eb351+]

MapKnitter
~~~~~~~~~~

MapKnitter (MapKnitter.org) is a free and open source tool for combining and positioning images (often from MapMill.org) in geographic space into a composite image map. Known as “orthorectification” or “georectification” to geographers, this step covers the process of figuring out where images can be placed on an existing map, and how they can be combined, or “stitched” together. You are likely to have many images of overlapping or identical areas, which is why MapMill or some type of sorting is used to determine which source images to use from the original set.   

MapKnitter can make maps from any image source, but it particularly lends itself to making maps with balloons and kites. The manual process of making maps with MapKnitter differs greatly from automated aerial imaging systems. In those systems the imaging is of a higher precision and processed with spatial and telemetry data collected along with the imagery, typically at higher altitudes and with consistent image overlap in the flight path sequence.

With MapKnitter the cartographer dynamically places each image and selects which images to include in the mosaic. Although the approaches are similar in that they use some type of additional information (usually pre-existing imagery of a lower resolution) as a reference, and that they are bound to specific cartographic elements such as map scale and map projection. 

https://github.com/publiclab/mapknitter[+https://github.com/publiclab/mapknitter+]

https://mapknitter.org/[+https://mapknitter.org/+]

https://publiclab.org/wiki/mapknitter[+https://publiclab.org/wiki/mapknitter+]

https://mapmill.org/[+https://mapmill.org/+]

Mapnik
~~~~~~

Mapnik is an open source toolkit for developing mapping applications. At the core is a Cxx shared library providing algorithms and patterns for spatial data access and visualization.

Mapnik is basically a collection of geographic objects like maps, layers, datasources, features, and geometries. The library doesn't rely on any OS specific "windowing systems" and it can be deployed to any server environment. It is intended to play fair in a multi-threaded environment and is aimed primarily, but not exclusively, at web-based development.

Mapnik has bindings for Node.js, Python, and Cxx.

https://github.com/mapnik/mapnik[+https://github.com/mapnik/mapnik+]

https://mapnik.org/[+https://mapnik.org/+]

MapSlicer
~~~~~~~~~

MapSlicer is a desktop application for the creation of map tiles for rapid raster map publishing. Geodata is transformed to tiles compatible with Google Maps and Earth - ready for publishing via direct upload to any webserver or a cloud storage (such as Amazon S3).

No extensive configuration on the server side is necessary, any simple file hosting is fine. Dynamic interaction such as panning and zooming, overlay of markers and vector data is provided by powerful browser functionality.

The application directly generates a ready to use simple viewer based on OpenLayers and Google Maps API and can be easily customized.

The features include:

* Well known Javascript APIs supported: OpenLayers, Google Maps API
* No extra server software installation necessary
* Hosting almost everywhere: cloud storage such as Amazon S3 or any cheap “unlimited” webhosting with FTP access
* Easy mashup with commercial layers (Google, Bing, Yahoo) or OpenStreetMap
* Tiles follow OSGEO TMS (Tile Map Service Specification)
* Ability to process raster data in various formats: TIFF/GeoTIFF, MrSID, ECW, JPEG2000, Erdas HFA, NOAA BSB, JPEG and more…

https://github.com/geopython/mapslicer[+https://github.com/geopython/mapslicer+]

https://wiki.osgeo.org/wiki/MapSlicer[+https://wiki.osgeo.org/wiki/MapSlicer+]

Marble
~~~~~~

Marble is an open source KDE Education Project program similar to NASA World Wind or Google Earth. Besides choosing any number of maps to view on your globe (including OpenStreetMaps), you are encouraged to include a KDE Marble widget in your application.

Explore the neighborhood with Marble’s rich set of city and street level maps. Search for addresses and places of interest. Marble takes care of querying various search backends and presents their results in a unified view. Calculate pedestrian, bike and motorcar routes with ease - online and offline, with an arbitrary number of via points.

Start exploring the world. View clouds and sun shadow, follow satellites and space stations and display their orbits, all updated in real-time. Travel back in time and learn about historic views of our planet using maps from past centuries. Earth is not enough? Marble also offers maps of the moon and other planets.

https://marble.kde.org/[+https://marble.kde.org/+]

MASA
~~~~

MASA (Manufactured Analytical Solution Abstraction) is a library written in Cxx (with C, python and Fortran90 interfaces) which provides a suite of manufactured solutions for the software verification of partial differential equation solvers in multiple dimensions.   Example formulations include:

* Heat Equation
* Laplace's Equation
* Euler Equations (with and without thermal equilibrium chemistry)
* Navier-Stokes Equations

An important tool that has emerged over the past decade to assist in the code verification process is the Method of Manufactured Solutions (MMS). MMS, instead of relying upon the availability of an exact solution to the governing equations, specifies a solution. This artificial solution is then substituted into the equations. Naturally, there will be a residual term since the chosen function is unlikely to be an exact solution to the equations. This residual can then be added to the code as a source term; the MMS test then uses the code to solve the modified equations and checks that the chosen function is recovered. Although previous work has focused mainly on partial differential equations (PDE’s), this idea applies to a broad range of systems in mathematical physics including nonlinear equations, systems of algebraic equations, and ordinary differential equations.

MASA began as a centralized repository for the MMS generated across the Center for Predictive Engineering and Computational Science (PECOS) at the Institute for Computational Engineering and Sciences (ICES) at the University of Texas for use with verification. Given that there appears to be no openly available, application-independent software package that provides generated MMS source terms, solutions, etc., it was decided to centralize the Center’s MMS efforts into one library to enhance reusability and consistency across the various software packages. The library is written in Cxx (with C, Fortran90 and python interfaces) and provides a suite of manufactured solutions for the software verification of partial differential equation solvers in multiple dimensions.

https://github.com/manufactured-solutions/MASA[+https://github.com/manufactured-solutions/MASA+]

https://github.com/manufactured-solutions/presentations[+https://github.com/manufactured-solutions/presentations+]

https://arxiv.org/abs/1902.07608[+https://arxiv.org/abs/1902.07608+]

MathGL
~~~~~~

MathGL is ...

* a library for making high-quality scientific graphics under Linux and Windows;
* a library for the fast data plotting and data processing of large data arrays;
* a library for working in window and console modes and for easy embedding into other programs;
* a library with large and growing set of graphics. 

Now MathGL has more than 35000 lines of code, more than 55 general types of graphics for 1d, 2d and 3d data arrays, including special ones for chemical and statistical graphics. It can export graphics to raster and vector (EPS or SVG) formats. It has Qt, FLTK, OpenGL interfaces and can be used even from console programs. It has functions for data processing and script MGL language for simplification of data plotting. Also it has several types of transparency and smoothed lightning, vector fonts and TeX-like symbol parsing, arbitrary curvilinear coordinate system and many over useful things. It can be used from code written on Cxx/C/Fortran/Python/Octave and many other languages. Finally it is platform independent and free (under GPL v.2.0 license). 

http://mathgl.sourceforge.net/doc_en/Main.html[+http://mathgl.sourceforge.net/doc_en/Main.html+]

http://groups.google.com/group/mathgl[+http://groups.google.com/group/mathgl+]

http://downloads.sourceforge.net/mathgl/mathgl-2.4.2.1.eng.pdf[+http://downloads.sourceforge.net/mathgl/mathgl-2.4.2.1.eng.pdf+]

MATK
~~~~

The Model Analysis ToolKit (MATK) facilitates model analysis within the Python computational environment. MATK expects a model defined as a Python function that accepts a dictionary of parameter values as the first argument and returns model results as a dictionary, array, integer, or float. Many model analyses are provided by MATK. These model analyses can be easily modified and/or extended within the Python scripting languange. New model analyses can easily be hooked up to a MATK model as well.

http://dharp.github.io/matk/[+http://dharp.github.io/matk/+]

https://github.com/dharp/matk[+https://github.com/dharp/matk+]

matplotlib
~~~~~~~~~~

Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.

Matplotlib tries to make easy things easy and hard things possible. You can generate plots, histograms, power spectra, bar charts, errorcharts, scatterplots, etc., with just a few lines of code.

For simple plotting the pyplot module provides a MATLAB-like interface, particularly when combined with IPython. For the power user, you have full control of line styles, font properties, axes properties, etc, via an object oriented interface or via a set of functions familiar to MATLAB users.

https://matplotlib.org/[+https://matplotlib.org/+]

https://github.com/matplotlib/matplotlib[+https://github.com/matplotlib/matplotlib+]

animatplot
^^^^^^^^^^

animatplot is a library for producing interactive animated plots in python built on top of matplotlib.

https://animatplot.readthedocs.io/en/latest/index.html[+https://animatplot.readthedocs.io/en/latest/index.html+]

https://github.com/t-makaro/animatplot[+https://github.com/t-makaro/animatplot+]

MATSim
~~~~~~

MATSim is an open-source framework for implementing large-scale agent-based transport simulations.

MATSim provides a toolbox to run and implement large-scale agent-based transport simulations. The toolbox consists of several modules which can be combined or used stand-alone. Modules can be replaced by own implementations to test single aspects of your own work. Currently, MATSim offers a toolbox for demand-modeling, agent-based mobility-simulation (traffic flow simulation), re-planning, a controller to iteratively run simulations as well as methods to analyze the output generated by the modules.

https://github.com/matsim-org/matsim[+https://github.com/matsim-org/matsim+]

https://www.matsim.org/[+https://www.matsim.org/+]

Maxima
~~~~~~

Maxima (/ˈmæksɪmə/) is a computer algebra system (CAS) based on a 1982 version of Macsyma. It is written in Common Lisp and runs on all POSIX platforms such as macOS, Unix, BSD, and Linux.
It  includes a complete programming language with ALGOL-like syntax but Lisp-like semantics. It is written in Common Lisp and can be accessed programmatically and extended, as the underlying Lisp can be called from Maxima. It uses gnuplot for drawing. 

Maxima is a full-featured CAS that specializes in symbolic operations, but it also offers numerical capabilities[1] such as arbitrary-precision arithmetic: integers and rational numbers that can grow to sizes limited only by machine memory, and floating-point numbers whose precision can be set arbitrarily large ("bfloats"). 
For calculations using floating point and arrays heavily, Maxima offers the possibility of generating code in other programming languages (notably Fortran), which may execute more efficiently.
Maxima is a general-purpose system, and special-case calculations such as factorization of large numbers, manipulation of extremely large polynomials, etc. are sometimes better done in specialized systems. 

Maxima is included as part of https://en.wikipedia.org/wiki/SageMath[SageMath], which would be the easiest way to spin up on it.

http://maxima.sourceforge.net/[+http://maxima.sourceforge.net/+]

http://maxima.sourceforge.net/docs/manual/maxima.html[+http://maxima.sourceforge.net/docs/manual/maxima.html+]

http://web.csulb.edu/\~woollett/[+http://web.csulb.edu/~woollett/+]

https://en.wikipedia.org/wiki/Maxima_(software)[+https://en.wikipedia.org/wiki/Maxima_(software)+]

Clifford
^^^^^^^^

Clifford is a lightweight package for performing Geometric Algebra calculations. It is based on an elementary symbolical construction of the Clifford algebra. 

http://dprodanov.github.io/clifford/[+http://dprodanov.github.io/clifford/+]

https://github.com/dprodanov/clifford[+https://github.com/dprodanov/clifford+]

Maxima.jl
^^^^^^^^^

Maxima.jl is a Julia package for performing symbolic computations using Maxima. Maxima is computer algebra software that provides a free and open source alternative to proprietary software such as Mathematica, Maple and others.

https://github.com/nsmith5/Maxima.jl[+https://github.com/nsmith5/Maxima.jl+]

maxima-jupyter
^^^^^^^^^^^^^^

A Maxima kernel for Jupyter, based on CL-Jupyter (Common Lisp kernel).

https://github.com/robert-dodier/maxima-jupyter[+https://github.com/robert-dodier/maxima-jupyter+]

wxMaxima
^^^^^^^^

wxMaxima is a document based interface for the computer algebra system Maxima. wxMaxima provides menus and dialogs for many common maxima commands, autocompletion, inline plots and simple animations.

http://wxmaxima-developers.github.io/wxmaxima/[+http://wxmaxima-developers.github.io/wxmaxima/+]

https://github.com/wxMaxima-developers/wxmaxima[+https://github.com/wxMaxima-developers/wxmaxima+]

mba
~~~

This library provides the adaptive MBA algorithm from [1] implemented in Cxx11. This is a fast algorithm for scattered N-dimensional data interpolation and approximation. Python bindings are also provided.

https://github.com/ddemidov/mba[+https://github.com/ddemidov/mba+]

MCLEAN
~~~~~~

MCLEAN (Multilevel CLustering Exploration As Network) proposes a visual analytics clustering methodology for guiding the user in the exploration and detection of clusters. We thereby combine a graphical representation of the clustered dataset as a network with the community finding algorithms into one coherent framework. Our approach entails displaying the results of the heuristics to users, providing a setting from which to start the exploration and data analysis. This is considered multilevel because allows the user to have an overview and detailed of the merging of elements into clusters.

The MCLEAN workflow is split into four stages:

* Graph transformation:As hierarchical clustering, the only required input in MCLEAN is the distance matrix. This step transforms the distance matrix into a node-link network.
* Node aggregation:The resulting networks in [2] are simplifications of the networks obtained in [1] (Graph transformation). The process of simplification is founded on the use of aggregated nodes (meta-nodes) that represent a subgraph that is at a lower level of abstraction.
* Community detection:Community detection: MCLEAN aims at guiding the identification of substructures through community detection algorithm. Communities detected in a connected component are shown in different color.
* Barcode-tree: A visual representation of clusters arrangement. The individual compounds are arranged along the vertical axis of a plot.  At any given threshold, the number of connected components is the number of lines that intersect the vertical line through a threshold. Meta-nodes are formed in the join points that are aggregations of individual data elements or existing meta-nodes at a smaller threshold.

https://bitbucket.org/vda-lab/mclean[+https://bitbucket.org/vda-lab/mclean+]

https://peerj.com/articles/cs-145/[+https://peerj.com/articles/cs-145/+]

MDP
~~~

The Modular toolkit for Data Processing (MDP) package is a library of widely used data processing algorithms, and the possibility to combine them together to form pipelines for building more complex data processing software.
MDP has been designed to be used as-is and as a framework for scientific data processing development.

From the user’s perspective, MDP consists of a collection of units, which process data. For example, these include algorithms for supervised and unsupervised learning, principal and independent components analysis and classification.

These units can be chained into data processing flows, to create pipelines as well as more complex feed-forward network architectures. Given a set of input data, MDP takes care of training and executing all nodes in the network in the correct order and passing intermediate data between the nodes. This allows the user to specify complex algorithms as a series of simpler data processing steps.

The number of available algorithms is steadily increasing and includes signal processing methods (Principal Component Analysis, Independent Component Analysis, Slow Feature Analysis), manifold learning methods ([Hessian] Locally Linear Embedding), several classifiers, probabilistic methods (Factor Analysis, RBM), data pre-processing methods, and many others.

Particular care has been taken to make computations efficient in terms of speed and memory. To reduce the memory footprint, it is possible to perform learning using batches of data. For large data-sets, it is also possible to specify that MDP should use single precision floating point numbers rather than double precision ones. Finally, calculations can be parallelised using the parallel subpackage, which offers a parallel implementation of the basic nodes and flows.

http://mdp-toolkit.sourceforge.net/[+http://mdp-toolkit.sourceforge.net/+]

https://github.com/mdp-toolkit/mdp-toolkit[+https://github.com/mdp-toolkit/mdp-toolkit+]

MDSPlus
~~~~~~~

MDSplus is a set of software tools for data acquisition and storage and a methodology for management of complex scientific data.

MDSplus allows all data from an experiment or simulation code to be stored into a single, self-descriptive, hierarchical structure. The system was designed to enable users to easily construct complete and coherent data sets.

The MDSplus programming interface contains only a few basic commands, simplifyng data access even into complex structures. Using the client/server model, data at remote sites can be read or written without file transfers. MDSplus includes x-windows and java tools for viewing data or for modifying or viewing the underlying structures. 

MDSplus was designed to give researchers the ability to produce complete, coherent, self-descriptive data sets and to provide tools for efficient access to that data in a distributed computing environment. The result was a system with sufficient flexibility and extensibility that ALL information associated with an experiment (or simulation) can be stored in the same structure and accessed through the same set of simple calls. By unifying setup, calibration, geometry and so forth with task descriptions, scheduling, status and all raw and processed data, MDSplus makes it easy to share data and tools across applications, facilitating collaborative research and reducing duplication of efforts. 

With MDSplus, it becomes practical to remove data entirely from codes and move them into the data structures. This approach allows highly flexible applications to be created whose operation can be modified easily without re-writing code. For example, a routine can create a menu of signals for display or processing by interacting with the data. New signals appear automatically in the application when they are added to the data structure - no new programming is required. Similarly, the names of routines for analysis and the parameters to control their operation can be specified as MDSplus data. Steps used during interactive processing can be saved for future reference or used as a script to "replay" the analysis. Perhaps the most important data driven application supported by MDSplus is data acquisition. Composite structures called DEVICES are defined which contain all the setup parameters, task descriptions, scheduling information and raw data which are associated with each physical data acquisition module on an experiment. Tools for scheduling and dispatching the data acquisition and analysis tasks are also part of MDSplus.

http://www.mdsplus.org/[+http://www.mdsplus.org/+]

MediaInfo
~~~~~~~~~

MediaInfo is a convenient unified display of the most relevant technical and tag data for video and audio files. 

The data display includes:

* Container: format, profile, commercial name of the format, duration, overall bit rate, writing application and library, title, author, director, album, track number, date, duration...
* Video: format, codec id, aspect, frame rate, bit rate, color space, chroma subsampling, bit depth, scan type, scan order...
* Audio: format, codec id, sample rate, channels, bit depth, language, bit rate...
* Text: format, codec id, language of subtitle...
* Chapters: count of chapters, list of chapters...

The analyticals include:

*  Container: MPEG-4, QuickTime, Matroska, AVI, MPEG-PS (including unprotected DVD), MPEG-TS (including unprotected Blu-ray), MXF, GXF, LXF, WMV, FLV, Real...
*  Tags: Id3v1, Id3v2, Vorbis comments, APE tags...
*  Video: MPEG-1/2 Video, H.263, MPEG-4 Visual (including DivX, XviD), H.264/AVC, H.265/HEVC, FFV1...
*  Audio: MPEG Audio (including MP3), AC3, DTS, AAC, Dolby E, AES3, FLAC...
*  Subtitles: CEA-608, CEA-708, DTVCC, SCTE-20, SCTE-128, ATSC/53, CDP, DVB Subtitle, Teletext, SRT, SSA, ASS, SAMI...

MediaInfo is available as a CLI, a GUI and a library.

https://mediaarea.net/en/MediaInfo[+https://mediaarea.net/en/MediaInfo+]

MEDINA
~~~~~~

A  source-to-source parser that enables support for Graphics Processing Units (GPU) by the Kinetic Pre-Processor (KPP) general purpose open-source software tool.
The numerical global atmosphere-chemistry model EMAC
 is a modular application used for climate modeling simulations.
EMAC uses the chemical kinetic module MECCA [8], utilizing the Kinetic Pre-Processor (KPP) general purpose open-source software tool to calculate the concentrations and the interactions between different chemical species in the atmosphere.

The MECCA sub-model uses KPP to numerically solve ordinary differential equations (ODE) describing atmospheric chemical kinetics. KPP takes as input chemical reactions written in a domain-specific language and produces C and FORTRAN compatible code. The output ODE solver allows for the temporal integration of the full kinetic system. KPP utilizes the sparsity of the Jacobian matrices to increase the efficiency of the solver.

MEDINA is a source-to-source parser that transforms the output of KPP from FORTRAN to GPU accelerated code by generating a CUDA compatible solver.
The goal is to significantly improve the performance of numerical chemical kinetics (in terms of time-to-solution and problem complexity) in climate simulation models using GPU accelerators.

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.158/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.158/+]

https://github.com/CyICastorc/medina[+https://github.com/CyICastorc/medina+]

KPP
^^^

The KPP kinetic preprocessor is a software tool that assists the computer simulation of chemical kinetic systems. The concentrations of a chemical system evolve in time according to the differential law of mass action kinetics. A numerical simulation requires an implementation of the differential laws and a numerical integration in time.

 KPP translates a specification of the chemical mechanism into Fortran77, Fortran90, C, or Matlab simulation code that implements the concentration time derivative function, its Jacobian, and it Hessian, together with a suitable numerical integration scheme. Sparsity in Jacobian/Hessian is carefully exploited in order to obtain computational efficiency.

 KPP incorporates a library with several widely used atmospheric chemistry mechanisms; the users can add their own chemical mechanisms to the library. KPP also includes a comprehensive suite of stiff numerical integrators. The KPP development environment is designed in a modular fashion and allows for rapid prototyping of new chemical kinetic schemes as well as new numerical integration methods. 

http://people.cs.vt.edu/asandu/Software/Kpp/[+http://people.cs.vt.edu/asandu/Software/Kpp/+]

https://github.com/samposm/kpp-helsinki[+https://github.com/samposm/kpp-helsinki+]

MEPP
~~~~

Based on CGAL, Qt, libQGLViewer, OpenGL, Boost, Assimp, FFmpeg and CMake, MEPP aims at building a framework around novel mesh processing techniques. Oriented towards modularity, it targets developers as well as GUI users.

It federates the developments on mesh processing of the LIRIS research team M2DISCO.

MEPP works on Windows, Linux and Mac OS X.
It supports dynamic meshes. 

MEPP is a platform development environment based on the class "Polyhedron" of CGAL, for processing and visualization of mesh and mesh sequences.
It allows the loading of multiple meshes or mesh sequences, processing and visualization.
It is intended for engineers, researchers but also to students with a quick start, facilitated by the proposed architecture. 

https://projet.liris.cnrs.fr/mepp/[+https://projet.liris.cnrs.fr/mepp/+]

https://github.com/MEPP-team/MEPP[+https://github.com/MEPP-team/MEPP+]

https://github.com/MEPP-team/MEPP2[+https://github.com/MEPP-team/MEPP2+]

Mercantile
~~~~~~~~~~

Mercantile is a module of utilities for working with XYZ style spherical mercator tiles (as in Google Maps, OSM, Mapbox, etc.) and includes a set of command line programs built on these utilities.
The commands are:

* `mercantile shapes` - generates GeoJSON from tiles and mercantile tiles does the reverse operation
* `mercantile parent` and `mercantile children` - traverse the hierarchy of Web Mercator tiles.
* `mercantile quadkey` - converts to/from quadkey representations of tiles
* `shapes` - writes Mercator tile shapes to several forms of GeoJSON
* `bounding-tile` - write the input’s bounding tile, the smallest mercator tile of any resolution that completely contains the input
* `tiles` - write descriptions of tiles intersecting with a geographic point, bounding box, or GeoJSON object

https://mercantile.readthedocs.io/en/latest/[+https://mercantile.readthedocs.io/en/latest/+]

https://github.com/mapbox/mercantile[+https://github.com/mapbox/mercantile+]

MeshLab
~~~~~~~

An open source system for processing and editing 3D triangular meshes.
It provides a set of tools for editing, cleaning, healing, inspecting, rendering, texturing and converting meshes. It offers features for processing raw data produced by 3D digitization tools/devices and for preparing models for 3D printing.

The 3D data alignment phase (also known as registration) is a fundamental step in the pipeline for processing 3D scanned data. MeshLab provides a powerful tool for moving the different meshes into a common reference system, able to manage large set of range-maps.

The visualization features of MeshLab (including Decorators and Shaders) can help in graphically present the peculiar characteristics of a 3D model. It is possible to control the camera perspective/orthographic view parameters, and use predefined canonical views.

The process of transforming independent acquisitions, or point clouds, into a single-surface triangulated mesh can be fulfilled with different algorithmic approaches. MeshLab provides several solutions to reconstruct the shape of an object, ranging from volumetric (Marching Cube) to implicit surfaces (Screened Poisson).

http://www.meshlab.net/[+http://www.meshlab.net/+]

http://www.meshlabjs.net/[+http://www.meshlabjs.net/+]

Mesh Tensorflow
~~~~~~~~~~~~~~~

Mesh TensorFlow (mtf) is a language for distributed deep learning, capable of specifying a broad class of distributed tensor computations. The purpose of Mesh TensorFlow is to formalize and implement distribution strategies for your computation graph over your hardware/processors. For example: "Split the batch over rows of processors and split the units in the hidden layer across columns of processors." Mesh TensorFlow is implemented as a layer over TensorFlow.

The ideas behind Mesh TensorFlow are:

* A "Mesh" is an n-dimensional array of processors, connected by a network.
* Each tensor is distributed (split and/or replicated) across all processors in a mesh.
* The "layout" of a tensor on a mesh is an injective partial map from the dimensions of the tensor to the dimensions of the mesh, specifying which dimensions of the tensor are split across which dimensions of the mesh. An empty layout means that the tensor is fully replicated across all processors.
* Tensor dimensions and mesh dimensions are named. The layouts of all tensors follow from a set of user-defined layout rules which specify which tensor-dimensions are split across which mesh-dimensions. This ensures that the corresponding dimensions in different tensors are split in the same manner.
* Layouts do not affect results - only performance.
* The implementation of an operation involves parallel computation on all processors in the mesh, and sometimes also collective communication. A processor usually just manipulates the slices of the input tensors already resident on that processor, and produces the slice of the output that goes on that processor.

https://github.com/tensorflow/mesh[+https://github.com/tensorflow/mesh+]

https://arxiv.org/abs/1811.02084[+https://arxiv.org/abs/1811.02084+]

Mesh Toolkit
~~~~~~~~~~~~

MSTK is a mesh framework that allows users to represent, manipulate and query unstructured 3D arbitrary topology meshes in a general manner without the need to code their own data structures. MSTK is a flexible framework in that it allows a variety of underlying representations for the mesh while maintaining a common interface. It allows users to choose from different mesh representations either at initialization (implemented) or during the program execution (not yet implemented) so that the optimal data structures are used for the particular algorithm. The interaction of users and applications with MSTK is through a functional interface that acts as though the mesh always contains vertices, edges, faces and regions and maintains connectivity between all these entities.

MSTK allows for the simultaneous existence of an arbitrary number of meshes. However, any entity in MSTK can belong to only one mesh at a time. MSTK also allows applications to attach application or field data to entities. This data may be integers, reals and pointers.
It supports distributed meshes for parallel computing. In the future it will allow for parallel mesh modification.
It is not a mesh generator but the infrastructure it provides can be used to develop sophisticated mesh generators and other mesh based applications.

https://github.com/MeshToolkit/MSTK[+https://github.com/MeshToolkit/MSTK+]

Jali
^^^^

Jali is a parallel unstructured mesh infrastructure library designed for use by multi-physics simulations. It supports 2D and3. arbitrary polyhedral meshes distributed over hundreds to thousands of nodes. Jali can read and write Exodus II meshes along with fields and sets on the mesh and support for other formats is partially implemented or is in the plans. Jali is built upon MSTK,
an open source general purpose unstructured mesh infrastructure library.
Jali supports distributed as well as on-node parallelism. Support of on-node parallelism is through direct use of the mesh calls in multi-threaded constructs or through use of "tiles" which are submeshes or sub-partitions of a partition destined for a compute node.

https://github.com/lanl/jali[+https://github.com/lanl/jali+]

Meson
~~~~~

Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible.

The main design point of Meson is that every moment a developer spends writing or debugging build definitions is a second wasted. So is every second spent waiting for the build system to actually start compiling code.

The features include:

* multiplatform support for Linux, macOS, Windows, GCC, Clang, Visual Studio and others
* supported languages include C, Cxx, D, Fortran, Java, Rust
* build definitions in a very readable and user friendly non-Turing complete DSL
* cross compilation for many operating systems as well as bare metal
* optimized for extremely fast full and incremental builds without sacrificing correctness
* built-in multiplatform dependency provider that works together with distro packages

http://mesonbuild.com/[+http://mesonbuild.com/+]

https://github.com/mesonbuild/meson[+https://github.com/mesonbuild/meson+]

http://mesonbuild.com/Users.html[+http://mesonbuild.com/Users.html+]

MessagePack
~~~~~~~~~~~

MessagePack is a computer data interchange format. It is a binary form for representing simple data structures like arrays and associative arrays. MessagePack aims to be as compact and simple as possible. The official implementation is available in a variety of languages.

Data structures processed by MessagePack loosely correspond to those used in JSON format.
MessagePack is more compact than JSON, but imposes limitations on array and integer sizes. On the other hand, it allows binary data and non UTF-8 encoded strings. In JSON, map keys have to be strings, but in MessagePack there is no such limitation and any type can be a map key, including types like maps and arrays, and, like YAML, numbers. 

https://msgpack.org/index.html[+https://msgpack.org/index.html+]

https://github.com/msgpack/msgpack[+https://github.com/msgpack/msgpack+]

https://github.com/msgpack/msgpack-python[+https://github.com/msgpack/msgpack-python+]

MESSy
~~~~~

The Modular Earth Submodel System (MESSy) is a software providing a framework for a standardized, bottom-up implementation of Earth System Models (or parts of those) with flexible complexity. "Bottom-up" means, the MESSy software provides an infrastructure with generalized interfaces for the standardized control and interconnection (=coupling) of "low-level ESM components" (dynamic cores, physical parameterizations, chemistry packages, diagnostics etc.), which are called submodels. MESSy comprises currently about 60 submodels (i.e., coded MESSy conform):

* infrastructure (= the framework) submodels
* diagnostic submodels
* atmospheric chemistry related submodels
* model physics related submodels

MESSy is a multi-institutional project.

The main design concept of MESSy is the strict separation of process description (=process and diagnostic submodels) from model infrastructure (e.g., memory management, input/output, flow control, ...).

Within MESSy, the operator splitting is formalized as the fundamental concept. Model codes are organized in 4 conceptual software layers, object oriented approaches are utilized as far as possible in the used programming language standard, and as far as feasible with respect to the computational performance.

The MESSy approach offers several benefits: A scalable model development is achieved and several different implementations of processes and diagnostics can coexist in the same model code. Nonetheless, the overall complexity remains controllable in a transparent and user friendly way. A high flexibility is achieved through the modularity, providing a research tool for a large community serving a wide variety of scientific needs. 

https://www.messy-interface.org/[+https://www.messy-interface.org/+]

https://www.geosci-model-dev.net/11/4987/2018/[+https://www.geosci-model-dev.net/11/4987/2018/+]

MET
~~~

MET is designed to be a highly-configurable, state-of-the-art suite of verification tools. It was developed using output from the Weather Research and Forecasting (WRF) modeling system but may be applied to the output of other modeling systems as well.

MET provides a variety of verification techniques, including:

* Standard verification scores comparing gridded model data to point-based observations
* Standard verification scores comparing gridded model data to gridded observations
* Spatial verification methods comparing gridded model data to gridded observations using neighborhood, object-based, and intensity-scale decomposition approaches
* Ensemble and probabilistic verification methods comparing gridded model data to point-based or gridded observations
* Aggregating the output of these verification methods through time and space

https://dtcenter.org/met/users/[+https://dtcenter.org/met/users/+]

MetaLift
~~~~~~~~

MetaLift is a framework for building compilers for domain-specific languages (DSLs). If you are a developer and you want to use a new DSL for your application, you would need to rewrite your code manually, which is often tedious and error-prone. Rather than doing that, you can use MetaLift to generate a compiler that translates from your source language to your favorite DSL.

MetaLift is a compiler generator. Unlike traditional syntax-driven compilers, which consists of rules that recognize patterns in the input code and translate them into the target language, MetaLift uses verified lifting to search for possible candidate programs in the target language that the given input can be translated to. This frees you from the need to devise, check, and maintain those pesky syntax-driven rules!

To make the search efficient, rather than searching programs that are expressible in the concrete syntax of the target DSL, MetaLift searches over the space of programs expressed in a high-level specification language instead. The specification language has a functional language-like syntax (think Haskell) and represents the semantics of the target DSL.

http://metalift.uwplse.org/[+http://metalift.uwplse.org/+]

MetaPhysicL
~~~~~~~~~~~

MetaPhysicL is a set of template classes and metaprogramming tools
useful for the generation of physics codes which are optimized at
compile-time.

This includes many classes which are designed to be compatible with
generic codes written for plain numeric data types like float and
double, but which may provide more featureful output:
* DualNumber, for automatic differentiation. 
* NumberArray, for vectorization.
* SparseNumberArray and SparseNumberStruct, for homogeneous or
  heterogeneous sparse vectors whose structure can be inferred at
  compile time.
* EquationSet, for the automatic assembly of physics equations into
  evaluation codes.

https://github.com/roystgnr/MetaPhysicL[+https://github.com/roystgnr/MetaPhysicL+]

https://www.freshports.org/science/metaphysicl/[+https://www.freshports.org/science/metaphysicl/+]

METIS
~~~~~

METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes developed in our lab.

METIS's key features are the following: 

* Experiments on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation show that METIS produces partitions that are consistently better than those produced by other widely used algorithms. 
* Experiments on a wide range of graphs has shown that METIS is one to two orders of magnitude faster than other widely used partitioning algorithms. Graphs with several millions of vertices can be partitioned in 256 parts in a few seconds on current generation workstations and PCs. 
* The fill-reducing orderings produced by METIS are significantly better than those produced by other widely used algorithms including multiple minimum degree. For many classes of problems arising in scientific computations and linear programming, METIS is able to reduce the storage and computational requirements of sparse matrix factorization, by up to an order of magnitude.

http://glaros.dtc.umn.edu/gkhome/metis/metis/overview[+http://glaros.dtc.umn.edu/gkhome/metis/metis/overview+]

hMETIS
^^^^^^

hMETIS is a set of programs for partitioning hypergraphs such as those corresponding to VLSI circuits. The algorithms implemented by hMETIS are based on the multilevel hypergraph partitioning schemes developed in our lab.

On a wide range of hypergraphs arising in the VLSI domain hMETIS produces bisections that cut 10% to 300% fewer hyperedges than those cut by other popular algorithms such as PARABOLI, PROP, and CLIP-PROP, especially for circuits with over 100,000 cells, and circuits with non-unit cell area.

A single run of hMETIS is faster than a single run of simpler schemes such as FM, KL, or CLIP. Furthermore, because of its very good average cut characteristics, it produces high quality partitionings in significantly fewer runs. It can bisect circuits with over 100,000 vertices in a couple of minutes on Pentium-class workstations. 

http://glaros.dtc.umn.edu/gkhome/metis/hmetis/overview[+http://glaros.dtc.umn.edu/gkhome/metis/hmetis/overview+]

ParMETIS
^^^^^^^^

ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes developed in our lab. 

http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview[+http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview+]

MFEM
~~~~

MFEM is a modular parallel Cxx library for finite element methods. Its goal is
to enable the research and development of scalable finite element discretization
and solver algorithms through general finite element abstractions, accurate and
flexible visualization, and tight integration with the hypre library.

Conceptually, MFEM can be viewed as a finite element toolbox that provides the
building blocks for developing finite element algorithms in a manner similar to
that of MATLAB for linear algebra methods. In particular, MFEM provides support
for arbitrary high-order H1-conforming, discontinuous (L2), H(div)-conforming,
H(curl)-conforming and NURBS finite element spaces in 2D and 3D, as well as many
bilinear, linear and nonlinear forms defined on them. It enables the quick
prototyping of various finite element discretizations, including Galerkin
methods, mixed finite elements, Discontinuous Galerkin (DG), isogeometric
analysis, hybridization and Discontinuous Petrov-Galerkin (DPG) approaches.

MFEM includes classes for dealing with a wide range of mesh types: triangular,
quadrilateral, tetrahedral and hexahedral, as well as surface and topologically
periodical meshes. It has general support for mesh refinement, including local
conforming and non-conforming (AMR) adaptive refinement. Arbitrary element
transformations, allowing for high-order mesh elements with curved boundaries,
are also supported.

MFEM is commonly used as a "finite element to linear algebra translator", since
it can take a problem described in terms of finite element-type objects, and
produce the corresponding linear algebra vectors and sparse matrices. In order
to facilitate this, MFEM uses compressed sparse row (CSR) sparse matrix storage
and includes simple smoothers and Krylov solvers, such as PCG, MINRES and GMRES,
as well as support for sequential sparse direct solvers from the SuiteSparse
library. Nonlinear solvers (the Newton method), eigensolvers (LOBPCG), and
several explicit and implicit Runge-Kutta time integrators are also available.

MFEM supports MPI-based parallelism throughout the library, and can readily be
used as a scalable unstructured finite element problem generator. MFEM-based
applications require minimal changes to transition from a serial to a
high-performing parallel version of the code, where they can take advantage of
the integrated scalable linear solvers from the hypre library. Comprehensive
support for other external packages, e.g. PETSc and SUNDIALS is also included,
giving access to many additional linear and nonlinear solvers, preconditioners,
time integrators, etc.

https://github.com/CEED/MFEM[+https://github.com/CEED/MFEM+]

https://mfem.org/[+https://mfem.org/+]

MicroPython
~~~~~~~~~~~

MicroPython is a lean and efficient implementation of the Python 3 programming language that includes a small subset of the Python standard library and is optimised to run on microcontrollers and in constrained environments.

The MicroPython pyboard is a compact electronic circuit board that runs MicroPython on the bare metal, giving you a low-level Python operating system that can be used to control all kinds of electronic projects.

MicroPython is packed full of advanced features such as an interactive prompt, arbitrary precision integers, closures, list comprehension, generators, exception handling and more. Yet it is compact enough to fit and run within just 256k of code space and 16k of RAM.

MicroPython aims to be as compatible with normal Python as possible to allow you to transfer code with ease from the desktop to a microcontroller or embedded system. 

https://micropython.org/[+https://micropython.org/+]

https://github.com/adafruit/circuitpython[+https://github.com/adafruit/circuitpython+]

https://hackaday.io/search?term=micropython[+https://hackaday.io/search?term=micropython+]

MilkCheck
~~~~~~~~~

MilkCheck is a Python-based distributed, highly parallel and flexible service manager. It runs commands across various servers, based on dependencies between them, and offers a compact execution summary. It aims to manage service starting and checking on very large number of servers, like in HPC world. It can run tens of thousands of commands across thousand servers in very short time.

https://github.com/cea-hpc/milkcheck[+https://github.com/cea-hpc/milkcheck+]

Miller
~~~~~~

Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON. You get to work with your data using named fields, without needing to count positional column indices.

This is something the Unix toolkit always could have done, and arguably always should have done. It operates on key-value-pair data while the familiar Unix tools operate on integer-indexed fields: if the natural data structure for the latter is the array, then Miller’s natural data structure is the insertion-ordered hash map. This encompasses a variety of data formats, including but not limited to the familiar CSV, TSV, and JSON. (Miller can handle positionally-indexed data as a special case.) 

The features include:

* Miller is multi-purpose: it’s useful for data cleaning, data reduction, statistical reporting, devops, system administration, log-file processing, format conversion, and database-query post-processing.
* You can use Miller to snarf and munge log-file data, including selecting out relevant substreams, then produce CSV format and load that into all-in-memory/data-frame utilities for further statistical and/or graphical processing.
* Miller complements data-analysis tools such as R, pandas, etc.: you can use Miller to clean and prepare your data. While you can do basic statistics entirely in Miller, its streaming-data feature and single-pass algorithms enable you to reduce very large data sets.
* Miller complements SQL databases: you can slice, dice, and reformat data on the client side on its way into or out of a database. (Examples here and here). You can also reap some of the benefits of databases for quick, setup-free one-off tasks when you just need to query some data in disk files in a hurry.
* Miller also goes beyond the classic Unix tools by stepping fully into our modern, no-SQL world: its essential record-heterogeneity property allows Miller to operate on data where records with different schema (field names) are interleaved.
* Miller is streaming: most operations need only a single record in memory at a time, rather than ingesting all input before producing any output. For those operations which require deeper retention (sort, tac, stats1), Miller retains only as much data as needed. This means that whenever functionally possible, you can operate on files which are larger than your system’s available RAM, and you can use Miller in tail -f contexts.
* Miller is pipe-friendly and interoperates with the Unix toolkit
* Miller’s I/O formats include tabular pretty-printing, positionally indexed (Unix-toolkit style), CSV, JSON, and others
* Miller does conversion between formats
* Miller’s processing is format-aware: e.g. CSV sort and tac keep header lines first
* Miller has high-throughput performance on par with the Unix toolkit
* Not unlike jq (for JSON), Miller is written in portable, modern C, with zero runtime dependencies. You can download or compile a single binary, scp it to a faraway machine, and expect it to work. 

http://johnkerl.org/miller/doc/[+http://johnkerl.org/miller/doc/+]

https://github.com/johnkerl/miller[+https://github.com/johnkerl/miller+]

Mininet
~~~~~~~

Mininet provides a virtual test bed and development environment for software-defined networks (SDN). Mininet enables SDN development on any laptop or PC, and SDN designs can move seamlessly between Mininet (allowing inexpensive and streamlined development), and the real hardware running at line rate in live deployments.

Mininet emulates a complete network of hosts, links, and switches on a single machine.
Mininet is useful for interactive development, testing, and demos, especially those using OpenFlow and SDN. OpenFlow-based network controllers prototyped in Mininet can usually be transferred to hardware with minimal changes for full line-rate execution.

Mininet creates virtual networks using process-based virtualization and network namespaces - features that are available in recent Linux kernels. In Mininet, hosts are emulated as bash processes running in a network namespace, so any code that would normally run on a Linux server (like a web server or client program) should run just fine within a Mininet "Host". The Mininet "Host" will have its own private network interface and can only see its own processes. Switches in Mininet are software-based switches like Open vSwitch or the OpenFlow reference switch. Links are virtual ethernet pairs, which live in the Linux kernel and connect our emulated switches to emulated hosts (processes).

https://github.com/mininet/mininet[+https://github.com/mininet/mininet+]

https://www.opennetworking.org/mininet/[+https://www.opennetworking.org/mininet/+]

MITcgm
~~~~~~

The MITgcm (MIT General Circulation Model) is a numerical model designed for study of the atmosphere, ocean, and climate. Its non-hydrostatic formulation enables it to simulate fluid phenomena over a wide range of scales; its adjoint capability enables it to be applied to parameter and state estimation problems. By employing fluid isomorphisms, one hydrodynamical kernel can be used to simulate flow in both the atmosphere and ocean.

http://mitgcm.org/[+http://mitgcm.org/+]

https://github.com/MITgcm/MITgcm[+https://github.com/MITgcm/MITgcm+]

MIXXX
~~~~~

Mixxx integrates the tools DJs need to perform creative live mixes with digital music files. Whether you are a new DJ with just a laptop or an experienced turntablist, Mixxx can support your style and techniques of mixing.
The features include:

* BPM and musical key detection help you find the perfect next track from your library. Use master sync to match the tempo and beats of four songs for seamless mixing.
* Built-in mappings for DJ controller hardware gives you hands-on control of Mixxx's features. Use the programmable mapping system to customize your workflow and add support for any MIDI or HID device. 
* Add your unique spin to tracks with sound effects. Get creative by chaining multiple effects together and twisting all their knobs. 
* Use turntables with timecode vinyl records to control playback and scratch your digital music files as if they were pressed on vinyl. Mix music vinyls into your set by toggling vinyl passthrough mode. 
* Compatible with music files in lossless FLAC, WAV, and AIFF formats as well as lossy MP3, M4A/AAC, Ogg Vorbis, and Opus formats. 
* Choose between multiple equalizers and isolators with adjustable shelves. Crossfader curve control lets you deliver quick cuts or long, smooth crossfades. 
* Change the tempo of songs without changing their pitch with keylock, or change the pitch without breaking sync so your songs play in harmony. Temporarily nudge the tempo faster or slower for manual beatmatching, with smooth changes in pitch just like vinyl. 
* Engage master sync on your decks and they'll stay locked in time even if you change speed. Construct remixes on the fly with multiple tracks and loops without losing control.
* Set hotcues to mark places in tracks. Remix and mash up tracks with accurate, rapid-fire hotcue triggering. 

https://mixxx.org/[+https://mixxx.org/+]

http://www.nongnu.org/crossfade/[+http://www.nongnu.org/crossfade/+]

Crossfade
^^^^^^^^^

A cross-platform digital DJ system for USB flash and portable hard drives. Crossfade GNU/Linux allows you to use a USB drive with your music collection to DJ on any modern PC (with an x86 or x86_64 CPU), including Apple Macs, using the DJ program Mixxx customized however you like. USB drives with Crossfade GNU/Linux installed on them show up in Windows, Mac OS X, and GNU/Linux as normal USB drives that music or any other data can be copied onto. Unlike ordinary USB drives, they can also be used to boot Crossfade GNU/Linux. After rebooting, the PC will be back to how it was before.

Crossfade GNU/Linux is setup with a realtime Linux kernel for optimal performance. It includes the Xfce graphical desktop environment, Midori web browser, and Clementine music player. It has a number of other programs for live musical performance including the Hydrogen drum machine, SooperLooper and Giada loopers, Guitarix electric guitar amplifier, Rakarrak guitar effects board, Ardour digital audio workstation, Audacity wave editor, and many LV2 and LADSPA audio effects plugins. Additionally, Crossfade GNU/Linux includes utilities that make it useful as a computer rescue system, such as the GParted partition manager, GNU GRUB bootloader, TestDisk data recovery program, FSArchiver filesystem backup program, and MATE Disk Usage Analyzer. Crossfade GNU/Linux is a Fedora® Remix containing software from sources other than Fedora, namely RPMFusion and PlanetCCRMA, as well as scripts and configuration specific to Crossfade GNU/Linux.

http://www.nongnu.org/crossfade/[+http://www.nongnu.org/crossfade/+]

http://savannah.nongnu.org/projects/crossfade[+http://savannah.nongnu.org/projects/crossfade+]

mkmod
~~~~~

Mkmod automatically creates and installs a basic modulefile for an installed package, given the package name, version and directory location set by the user in the NAME, VER, and TOPDIR environment variables. Other environment variables can be used to customize the environment and operation of the modulefile for complex environments.

The mkmod utility was designed to allow users, who install their own package, to automatically create and install a basic modulefile for their installed package. Requirements are: Tcl/Lua Environment Module System (EMS). If mkmod is not available on the system, installation in user-space is simple, requiring only a make execution.

A basic modulefile is automatically generated for an installed package from appropriate values set in the NAME, VER and TOPDIR environment variables. Mkmod has advanced features, accessible by setting other environment variables-- to accommodate the more sophisticated needs of some package environments.

Mkmod installs the generated modulefile in a modulefiles directory. It includes the module use $HOME/modulefiles command in appropriate startup files, directing EMS to be aware of the mkmod-installed modulefiles.

Other capabilities are available for developers who want to provide a modulefile for automatic installation, or at least provide help and whatis components of a modulefile for users and site managers to include in their modulefile installation.

https://mkmod.readthedocs.io/en/latest/[+https://mkmod.readthedocs.io/en/latest/+]

https://github.com/TACC/mkmod[+https://github.com/TACC/mkmod+]

MLeap
~~~~~

MLeap is a common serialization format and execution engine for machine learning pipelines. It supports Spark, Scikit-learn and Tensorflow for training pipelines and exporting them to an MLeap Bundle. Serialized pipelines (bundles) can be deserialized back into Spark for batch-mode scoring or the MLeap runtime to power realtime API services.

Many companies that use Spark and Scikit-learn have a difficult time deploying their research ML/data pipelines models to production API services. Even using Tensorflow can be difficult to set these services up if a company does not wish to use Python in their API stack or does not use Google ML Cloud. MLeap provides simple interfaces to execute entire ML pipelines, from feature transformers to classifiers, regressions, clustering algorithms, and neural networks.

Mixing and matching ML technologies becomes a simple task. Instead of requiring an entire team of developers to make research pipelines production ready, simply export to an MLeap Bundle and run your pipeline wherever it is needed.

In addition to providing a useful execution engine, MLeap Bundles provide a common serialization format for a large set of ML feature extractors and algorithms that are able to be exported and imported across Spark, Scikit-learn, Tensorflow and MLeap. This means you can easily convert pipelines between these technologies depending on where you need to execute a pipeline.

http://mleap-docs.combust.ml/[+http://mleap-docs.combust.ml/+]

MLib
~~~~

MLlib is Apache Spark's scalable machine learning library. 
MLlib fits into Spark's APIs and interoperates with NumPy in Python (as of Spark 0.9) and R libraries (as of Spark 1.5). You can use any Hadoop data source (e.g. HDFS, HBase, or local files), making it easy to plug into Hadoop workflows. 

Spark excels at iterative computation, enabling MLlib to run fast. At the same time, we care about algorithmic performance: MLlib contains high-quality algorithms that leverage iteration, and can yield better results than the one-pass approximations sometimes used on MapReduce. 

https://spark.apache.org/mllib/[+https://spark.apache.org/mllib/+]

MMG
~~~

Mmg is an open source software for simplicial remeshing. It provides 3 applications and 4 libraries:

* the mmg2d application and the libmmg2d library: adaptation and optimization of a two-dimensional triangulation and generation of a triangulation from a set of points or from given boundary edges

* the mmgs application and the libmmgs library: adaptation and optimization of a surface triangulation and isovalue discretization

* the mmg3d application and the libmmg3d library: adaptation and optimization of a tetrahedral mesh and implicit domain meshing

* the libmmg library gathering the libmmg2d, libmmgs and libmmg3d libraries

http://www.mmgtools.org/[+http://www.mmgtools.org/+]

https://github.com/MmgTools/mmg[+https://github.com/MmgTools/mmg+]

MOAB
~~~~

The Mesh-Oriented datABase (MOAB) is a component for representing and evaluating mesh data. MOAB can store structured and unstructured mesh, consisting of elements in the finite element “zoo” plus polygons and polyhedra. The functional interface to MOAB is simple yet powerful, allowing the representation of many types of metadata commonly found on the mesh. MOAB is optimized for efficiency in space and time, based on access to mesh in chunks rather than through individual entities, while also versatile enough to support individual entity access. The MOAB library can naturally represent finite element and other types of mesh data.  Various types of meta-data are often used in conjunction with a mesh. Examples include boundary condition groupings, material types, and provenance information for the mesh. Because the data model used in MOAB is so abstract, conventions are useful for describing how meta-data is stored into that data model.

MOAB implements the ITAPS iMesh interface; iMesh is a common interface to mesh data implemented by several different packages, including MOAB. Various tools like smoothing, adaptive mesh refinement, and parallel mesh communication are implemented on top of iMesh. Because the data models used by MOAB and iMesh, the ITAPS mesh interface, are so similar, the conventions described here apply almost unmodified to iMesh as well as to MOAB.

The meshes represented in MOAB originate in a variety of forms, including mesh read from files of various formats (e.g. CUBIT “.cub” file, VTK, etc.) as well as mesh written into MOAB directly by
various software libraries (e.g. MeshKit). Although there is no standard for naming or storing meta-data with a mesh, there is a great deal of commonality in the types of meta-data typically found with mesh-data. This document describes conventions that have been established for commonly encountered meta-data. Various mesh readers implemented in MOAB attempt to read meta-data from a file and write it into the MOAB data model using these conventions. Although there is no requirement to store a given type of meta-data in the form described here, a number of services have been written to handle meta-data using these conventions, no matter the source of the meta-data being processed.

Several specific tools are often used in concert with MOAB and bear special mention here. The CUBIT toolkit generates finite element meshes, and saves them to a native save file (referred to as a “.cub” file) which MOAB is able to read. Reading CUBIT meshes into MOAB through the .cub file format is preferred over other formats, since most other mesh formats written by CUBIT do not save most meta-data. The MeshKit library also generates mesh using CGM and MOAB, and uses the same conventions for storing meshes into MOAB. Finally, MOAB includes a CGM reader which can read a geometric model into a faceted representation in MOAB. 

MOAB supports common parallel mesh operations like parallel import and export (to/from a single HDF5-based file), parallel ghost exchange, communication of field data, and general sending and receiving of mesh and metadata between processors. Parallel read has been demonstrated on up to 16K processors.

https://press3.mcs.anl.gov/sigma/moab-library/[+https://press3.mcs.anl.gov/sigma/moab-library/+]

modelbase
~~~~~~~~~

Kinetic models allow simulating the dynamics of the complex biochemistry of cells. Therefore, they have the power to explain which processes are responsible for observed emergent properties and they facilitate predictions on how the system behaves under various scenarios, such as changed environmental conditions or modification of molecular components. 
The process of building a kinetic model can be divided into a number of mandatory steps such as i) establishing the biological network structure (the stoichiometry), ii) defining the kinetic rate expressions, iii) formulation of the differential equations, iv) parametrisation, v) validation and, finally, vi) application.

modelbase supports researchers in every step of model development and application with its simple design aimed at being minimally restrictive.
It provides an environment for relatively easy implementation of models that were published without source code, in a general-purpose and reusable format. Moreover, modelbase supports the export of a structural (stoichiometric) model into Systems Biology Markup Language (SBML) for further structural analysis with the appropriate software.

https://gitlab.com/ebenhoeh/modelbase[+https://gitlab.com/ebenhoeh/modelbase+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.236/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.236/+]

Modin
~~~~~

Modin uses Ray to provide an effortless way to speed up your pandas notebooks, scripts, and libraries. Unlike other distributed DataFrame libraries, Modin provides seamless integration and compatibility with existing pandas code. Even using the DataFrame constructor is identical.

To use Modin, you do not need to know how many cores your system has and you do not need to specify how to distribute the data. In fact, you can continue using your previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Once you’ve changed your import statement, you’re ready to use Modin just like you would pandas.

The modin.pandas DataFrame is an extremely light-weight parallel DataFrame. Modin transparently distributes the data and computation so that all you need to do is continue using the pandas API as you were before installing Modin. Unlike other parallel DataFrame systems, Modin is an extremely light-weight, robust DataFrame. Because it is so light-weight, Modin provides speed-ups of up to 4x on a laptop with 4 physical cores.

We have focused heavily on bridging the solutions between DataFrames for small data (e.g. pandas) and large data. Often data scientists require different tools for doing the same thing on different sizes of data. The DataFrame solutions that exist for 1KB do not scale to 1TB+, and the overheads of the solutions for 1TB+ are too costly for datasets in the 1KB range. With Modin, because of its light-weight, robust, and scalable nature, you get a fast DataFrame at 1KB and 1TB+.

https://github.com/modin-project/modin[+https://github.com/modin-project/modin+]

https://github.com/ray-project/ray/[+https://github.com/ray-project/ray/+]

https://rise.cs.berkeley.edu/projects/modin/[+https://rise.cs.berkeley.edu/projects/modin/+]

Modules
~~~~~~~

The Modules package is a tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles.

Each modulefile contains the information needed to configure the shell for an application. Once the Modules package is initialized, the environment can be modified on a per-module basis using the module command which interprets modulefiles. Typically modulefiles instruct the module command to alter or set shell environment variables such as PATH, MANPATH, etc. modulefiles may be shared by many users on a system and users may have their own collection to supplement or replace the shared modulefiles.

Modules can be loaded and unloaded dynamically and atomically, in an clean fashion. All popular shells are supported, including bash, ksh, zsh, sh, csh, tcsh, fish, as well as some scripting languages such as tcl, perl, python, ruby, cmake and r.

Modules are useful in managing different versions of applications. Modules can also be bundled into metamodules that will load an entire suite of different applications.

https://github.com/cea-hpc/modules[+https://github.com/cea-hpc/modules+]

MOM6
~~~~

MOM6 is the latest generation of the Modular Ocean Model which is a numerical model code for simulating the ocean general circulation. MOM6 represents a major algorithmic departure from the previous generations of MOM (up to and including MOM5). Most notably, it uses the Arbitrary-Lagrangian-Eulerian (ALE) algorithm in the vertical direction to allow the use of any vertical coordinate system including, geo-potential coordinates (z or z*), isopycnal coordinates, terrain-following coordinates and hybrid-/user-defined coordinates. It is also based on the horizontal C-grid stencil, rather than the B-grid used by earlier MOM versions.

https://github.com/NOAA-GFDL/MOM6[+https://github.com/NOAA-GFDL/MOM6+]

https://mom6.readthedocs.io/en/dev-gfdl/[+https://mom6.readthedocs.io/en/dev-gfdl/+]

https://github.com/NOAA-GFDL/MOM6-examples/wiki[+https://github.com/NOAA-GFDL/MOM6-examples/wiki+]

MonetDB
~~~~~~~

MonetDB is built on the canonical representation of database relations as columns, a.k.a. arrays. They are sizeable entities -up to hundreds of megabytes- swapped into memory by the operating system and compressed on disk upon need.

MonetDB excels in  applications where the database hot-set - the part actually touched  - can be largely held in main-memory or where a few columns of a broad relational table are sufficient to handle individual requests. Further exploitation of cache-conscious algorithms proved the validity of these design decisions.

MonetDB is designed for multi-core parallel execution on desktops to reduce response time for complex query processing. Several techniques for distributed processing are explored, but as many has found out, there is no silver bullet to improve parallel processing performance. For simple data-parallel problems a map-reduce scheme suffice, but for more complex cases careful database design and (partial) replication is called for.

When your database grows into millions of records spread over many tables and business intelligence/ science becomes the prevalent application domain, a column-store database management system is called for. Unlike traditional row-stores, such as MySQL and PostgreSQL, a column-store provides a modern and scalable solution without calling for substantial hardware investments.

MonetDB  pioneered column-store solutions for high-performance data warehouses for business intelligence and eScience since 1993. It achieves its goal by innovations at all layers of a DBMS, e.g. a storage model based on vertical fragmentation, a modern CPU-tuned query execution architecture, automatic and adaptive indices, run-time query optimization, and a modular software architecture. It is based on the SQL 2003 standard with full support for foreign keys, joins, views, triggers, and stored procedures. It is fully ACID compliant and supports a rich spectrum of programming interfaces (JDBC, ODBC, PHP, Python, RoR, C/Cxx, Perl).

MonetDB is distributed both as a source tarball, packages for installation, and binary installers on a variety of platforms. The latest release has been tested on Linux (Fedora, RedHat Enterprise Linux, Debian, Ubuntu), Gentoo, Mac OS, SUN Solaris, Open Solaris, Windows XP, Windows Sever 2003, Windows Vista. A regular release schedule ensures the latest functional improvements to reach the community quickly.

MonetDB is the focus of database research pushing the technology envelop in many areas. Its three-level software stack, comprised of SQL front-end, tactical-optimizers, and columnar abstract-machine kernel, provide a flexible environment to customize it many different ways. A rich collection of linked-in libraries provide functionality for temporal data types, math routine, strings, and URLs.

https://www.monetdb.org/Home[+https://www.monetdb.org/Home+]

https://github.com/uwsampa/MonetDB[+https://github.com/uwsampa/MonetDB+]

MORSE
~~~~~

MORSE is an generic simulator for academic robotics. It focuses on realistic 3D simulation of small to large environments, indoor or outdoor, with one to tenths of autonomous robots.

MORSE can be entirely controlled from the command-line. Simulation scenes are generated from simple Python scripts.

MORSE comes with a set of standard sensors (cameras, laser scanner, GPS, odometry,...), actuators (speed controllers, high-level waypoints controllers, generic joint controllers) and robotic bases (quadrotors, ATRV, Pioneer3DX, generic 4 wheel vehicle, PR2,...). New ones can easily be added.

MORSE rendering is based on the Blender Game Engine. The OpenGL-based Game Engine supports shaders, provides advanced lightning options, supports multi-texturing, and use the state-of-the-art Bullet library for physics simulation.

https://www.openrobots.org/wiki/morse[+https://www.openrobots.org/wiki/morse+]

http://ompl.kavrakilab.org/integration.html[+http://ompl.kavrakilab.org/integration.html+]

MOSAICmodeling
~~~~~~~~~~~~~~

MOSAICmodeling is a free, web-based modeling, simulation, and optimization environment. Based on a LaTeX-style entry method for algebraic and differential equations, equation systems can be built and subsequently used for simulation and optimization.

Based on a steadily growing library of existing models of chemical engineering applications large-scale flowsheets, optimization problems, etc. can be built. MOSAICmodeling provides an automatic code generation for numerous simulation and optimization environments, such as AMPL, Aspen Custom Modeler, GAMS, gPROMS, MATLAB, Modelica, and for solvers interfaced via C++, FORTRAN, Python, etc.

http://mosaic-modeling.de/[+http://mosaic-modeling.de/+]

https://peerj.com/articles/cs-161/[+https://peerj.com/articles/cs-161/+]

MOT
~~~

The Multi-threaded Optimization Toolbox (MOT) is a library for parallel optimization and sampling using the OpenCL compute platform. Using OpenCL allows parallel processing using all CPU cores or using the GPU (Graphics card). MOT implements OpenCL parallelized versions of the Powell, Nelder-Mead Simplex and Levenberg-Marquardt non-linear optimization algorithms alongside various flavors of Markov Chain Monte Carlo (MCMC) sampling.

https://mot.readthedocs.io/en/latest_release/[+https://mot.readthedocs.io/en/latest_release/+]

https://github.com/cbclab/MOT[+https://github.com/cbclab/MOT+]

MPAS
~~~~

The Model for Prediction Across Scales (MPAS) is a collaborative project for developing atmosphere, ocean, and other earth-system simulation components for use in climate, regional climate, and weather studies. The primary development partners are the climate modeling group at Los Alamos National Laboratory (COSIM) and the National Center for Atmospheric Research. Both primary partners are responsible for the MPAS framework, operators, and tools common to the applications; LANL has primary responsibility for the ocean model, and NCAR has primary responsibility for the atmospheric model.

The MPAS framework facilitates the rapid development and prototyping of models by providing infrastructure typically required by model developers, including high-level data types, communication routines, and I/O routines. By using MPAS, developers can leverage pre-existing code and focus more on development of their model.

https://github.com/MPAS-Dev/MPAS-Model[+https://github.com/MPAS-Dev/MPAS-Model+]

http://mpas-dev.github.io/atmosphere/atmosphere_download.html[+http://mpas-dev.github.io/atmosphere/atmosphere_download.html+]

http://mpas-dev.github.io/land_ice/download.html[+http://mpas-dev.github.io/land_ice/download.html+]

http://mpas-dev.github.io/ocean/releases.html[+http://mpas-dev.github.io/ocean/releases.html+]

http://mpas-dev.github.io/sea_ice/releases.html[+http://mpas-dev.github.io/sea_ice/releases.html+]

MPI
~~~

Blah.

Alchemist
^^^^^^^^^

Alchemist is an interface between Apache Spark applications and MPI-based libraries for accelerating large-scale linear algebra and machine learning computations.

Performing communication-intense linear algebra computations in Spark can incur large overheads. Alchemist bypasses these overheads by sending the data from the Spark application to existing or custom MPI-based libraries, and then transmitting the results back to the application. This leads to significantly fewer overheads and to computations that are efficient and scalable.

https://github.com/alexgittens/alchemist[+https://github.com/alexgittens/alchemist+]

https://rise.cs.berkeley.edu/projects/alchemist/[+https://rise.cs.berkeley.edu/projects/alchemist/+]

BDMPI
^^^^^

BDMPI is a message passing library and associated runtime system for developing out-of-core distributed computing applications for problems whose aggregate memory requirements exceed the amount of memory that is available on the underlying computing cluster. BDMPI is based on the Message Passing Interface (MPI) and provides a subset of MPI's API along with some extensions that are designed for BDMPI's memory and execution model.

A BDMPI-based application is a standard memory-scalable parallel MPI program that was developed assuming that the underlying system has enough computational nodes to allow for the in-memory execution of the computations. This program is then executed using a sufficiently large number of processes so that the per-process memory fits within the physical memory available on the underlying computational node(s). BDMPI maps one or more of these processes to the computational nodes by relying on the OS's virtual memory management to accommodate the aggregate amount of memory required by them. BDMPI prevents memory thrashing by coordinating the execution of these processes using node-level co-operative multi-tasking that limits the number of processes that can be running at any given time. This ensures that the currently running process(es) can establish and retain memory residency and thus achieve efficient execution. BDMPI exploits the natural blocking points that exist in MPI programs to transparently schedule the co-operative execution of the different processes. In addition, BDMPI's implementation of MPI's communication operations is done so that to maximize the time over which a process can execute between successive blocking points. This allows it to amortize the cost of loading data from disk over the maximal amount of computations that can be performed.

Since BDMPI is based on the standard MPI library, it also provides a framework that allows the automated out-of-core execution of existing MPI applications. BDMPI is implemented in such a way so that to be a drop-in replacement of existing MPI implementations and allow existing codes that utilize the subset of MPI functions implemented by BDMPI to compile unchanged.

http://glaros.dtc.umn.edu/gkhome/bdmpi/overview[+http://glaros.dtc.umn.edu/gkhome/bdmpi/overview+]

http://glaros.dtc.umn.edu/gkhome/software[+http://glaros.dtc.umn.edu/gkhome/software+]

Cram
^^^^

Cram runs many small MPI jobs inside one large MPI job.

https://github.com/LLNL/cram[+https://github.com/LLNL/cram+]

KMR
^^^

KMR is a set of high-performance map-reduce operations in the MPI (Message Passing Interface) environment. It makes programming for data-processing much easier by hiding low-level details of message passing. Its main targets are large-scale supercomputers with thousands of compute nodes. KMR provides utilities other than map-reduce operations to address issues such as accessing very large file-systems, on platforms K and Fujitsu FX10.

KMR is designed to work in-memory and exploit large amount of memory available on supercomputers, whereas most map-reduce implementations are designed to work with external (disk-based) operations. So, data exchanges in KMR occur as message passing instead of remote file operations. The KMR routines work in bulk-synchronous and the most part of the code is sequential, but the code inside the mapper and reducer are multi-threaded. 

https://mt.r-ccs.riken.jp/kmr/[+https://mt.r-ccs.riken.jp/kmr/+]

https://github.com/RIKEN-RCCS/kmr[+https://github.com/RIKEN-RCCS/kmr+]

libcircle
^^^^^^^^^

An API to provide an efficient distributed queue on a cluster. Libcircle is currently used in production to quickly traverse and perform operations on a file tree which contains several hundred-million file nodes.
This installs and runs on top of MPI.

http://hpc.github.io/libcircle/[+http://hpc.github.io/libcircle/+]

https://github.com/hpc/libcircle[+https://github.com/hpc/libcircle+]

MPI-AMRVAC
^^^^^^^^^^

MPI-AMRVAC is a parallel adaptive mesh refinement framework aimed at solving (primarily hyperbolic) partial differential equations by a number of different numerical schemes. The emphasis is on (near) conservation laws and on shock-dominated problems in particular. A number of physics modules are included; the hydrodynamics and the magnetohydrodynamics module are most frequently used. Users can add their own physics module or modify existing ones. The framework supports 1D to 3D simulations, in a number of different geometries (Cartesian, cylindrical, spherical).

MPI-AMRVAC is written in Fortran 90 and uses MPI for parallelization. The VACPP preprocessor is used to extend Fortran with dimensional independent notation, but users are not required to learn the VACPP syntax.

The philosophy behind MPI-AMRVAC is to use a single versatile code with options and switches for various problems. The advantage of such a general approach is easier maintenance, the compatibility of different parts, and the automatic extension of new features to existing applications. MPI-AMRVAC is not a fool-proof black-box design. A user needs to write subroutines for initial conditions, and for source terms or special boundary conditions when needed.

http://amrvac.org/[+http://amrvac.org/+]

https://github.com/amrvac/amrvac[+https://github.com/amrvac/amrvac+]

mpiFileUtils
^^^^^^^^^^^^

mpiFileUtils provides both a library called libmfu and a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files. High-performance computing users often generate large datasets with parallel applications that run with many processes (millions in some cases). However those users are then stuck with single-process tools like cp and rm to manage their datasets. This suite provides MPI-based tools to handle typical jobs like copy, remove, and compare for such datasets, providing speedups of up to 50x. It also provides a library that simplifies the creation of new tools or can be used in applications.

The tools in mpiFileUtils are actually MPI applications. They must be launched as MPI applications, e.g., within a compute allocation on a cluster using mpirun. The tools do not currently checkpoint, so one must be careful that an invocation of the tool has sufficient time to complete before it is killed. 
The utilities are:

* dbcast - Broadcast files to compute nodes.
* dchmod - Change owner, group, and permissions on files.
* dcmp - Compare files.
* dcp - Copy files.
* ddup - Find duplicate files.
* dfilemaker - Generate random files.
* dreln - Relink symlinks.
* drm - Remove files.
* dstripe - Restripe files.
* dsync - Synchronize files
* dwalk - List files
* dbz2 - Compress a file with bz2.
* dfind - Search for files in parallel.
* dgrep - Run grep on files in parallel.
* dparallel - Perform commands in parallel. experimental/dparallel.1
* dsh - List and remove files with interactive commands.
* dtar - Create file tape archives

https://mpifileutils.readthedocs.io/en/latest/[+https://mpifileutils.readthedocs.io/en/latest/+]

https://github.com/hpc/mpifileutils[+https://github.com/hpc/mpifileutils+]

TAMPI
^^^^^

The Task-Aware MPI or TAMPI library extends the functionality of standard MPI libraries by providing new mechanisms for improving the interoperability between parallel task-based programming models, such as OpenMP or OmpSs-2, and both blocking and non-blocking MPI operations.

By following the MPI Standard, programmers must pay close attention to avoid deadlocks that may occur in hybrid applications (e.g., MPI+OpenMP) where MPI calls take place inside tasks. This is given by the out-of-order execution of tasks that consequently alter the execution order of the enclosed MPI calls. The TAMPI library ensures a deadlock-free execution of such hybrid applications by implementing a cooperation mechanism between the MPI library and the parallel task-based runtime system.

TAMPI provides two main mechanisms: the blocking mode and the non-blocking mode. The blocking mode targets the efficient and safe execution of blocking MPI operations (e.g., MPI_Recv) from inside tasks, while the non-blocking mode focuses on the efficient execution of non-blocking or immediate MPI operations (e.g., MPI_Irecv), also from inside tasks.

https://github.com/bsc-pm/tampi[+https://github.com/bsc-pm/tampi+]

mpnum
~~~~~

mpnum is a flexible, user-friendly, and expandable toolbox for the matrix product state/tensor train tensor format. It is available under the BSD license at mpnum on Github. mpnum provides:

* support for well-known matrix product representations, such as:
** matrix product states (MPS), also known as tensor trains (TT)
** matrix product operators (MPO)
** local purification matrix product states (PMPS)
** arbitrary matrix product arrays (MPA)
* arithmetic operations: addition, multiplication, contraction etc.
* compression, canonical forms, etc. (see compress(), canonicalize())
* finding extremal eigenvalues and eigenvectors of MPOs (see eig())

https://mpnum.readthedocs.io/en/latest/intro.html[+https://mpnum.readthedocs.io/en/latest/intro.html+]

https://github.com/dseuss/mpnum[+https://github.com/dseuss/mpnum+]

mps-youtube
~~~~~~~~~~~
A terminal-based Youtube player and downloader whose capabilities include:

* Search and play audio/video from YouTube
* Search tracks of albums by album title
* Search and import YouTube playlists
* Create and save local playlists
* Download audio/video
* Convert to mp3 & other formats (requires ffmpeg or avconv)
* View video comments
* Works with Python 3.x
* Works with Windows, Linux and Mac OS X
* Requires mplayer or mpv

https://github.com/mps-youtube/mps-youtube[+https://github.com/mps-youtube/mps-youtube+]

MRF
~~~

MRF is a raster format implemented as a GDAL. The MRF driver itself is maintained within GDAL since GDAL version 2.1. This repository contains MRF documentation and MRF related utilities.

MRF is used by the OnEarth web server software. OnEarth consists of image formatting and serving modules which facilitate the deployment of a web service capable of efficiently serving standards-based requests for georeferenced raster imagery at multiple spatial resolutions including, but not limited to, full spatial resolution. The Meta Raster Format and OnEarth software were originally developed at the Jet Propulsion Laboratory (JPL) to serve Landsat imagery and global daily composites of MODIS imagery. Since then, it has been deployed and repurposed in other installations, including at the Physical Oceanography Distributed Active Archive Center (PO.DAAC) in support of the State of the Oceans (SOTO) visualization tool, the Lunar Mapping and Modeling Project (now MoonTrek), and Worldview.

https://github.com/nasa-gibs/mrf[+https://github.com/nasa-gibs/mrf+]

https://github.com/nasa-gibs/onearth[+https://github.com/nasa-gibs/onearth+]

https://github.com/nasa-gibs/mrf/blob/master/doc/MUG.md[+https://github.com/nasa-gibs/mrf/blob/master/doc/MUG.md+]

MUMPS
~~~~~

The MUltifrontal Massively Parallel sparse direct Solver (MUMPS) is a parallel sparse direct solver.
The features include:

* Solution of large linear systems with
** symmetric positive definite matrices
** general symmetric matrices
** general unsymmetric matrices
* Real or complex arithmetic (single or double precision)
* Parallel factorization and solve phases
(uniprocessor version also available)
* Out of core numerical phases
* Iterative refinement and backward error analysis
* Various matrix input formats
(assembled, distributed, elemental format)
* Partial factorization and Schur complement matrix (centralized or 2D block-cyclic) with reduced/condensed right-hand side
* Interfaces to MUMPS: Fortran, C, Matlab and Scilab
* Several reorderings interfaced: AMD, QAMD, AMF, PORD, METIS, PARMETIS, SCOTCH, PT-SCOTCH
* Symmetric indefinite matrices: preprocesssing and 2-by-2 pivots
* Parallel analysis and matrix scaling
* Computation of the determinant (with an option to discard factors)
* Forward elimination during factorization

http://mumps.enseeiht.fr/[+http://mumps.enseeiht.fr/+]

https://github.com/scivision/mumps[+https://github.com/scivision/mumps+]

qr_mumps
^^^^^^^^

qr_mumps is a software package for the solution of sparse, linear systems on multicore computers. It implements a direct solution method based on the QR factorization of the input matrix. Therefore, it is suited to solving sparse least-squares problems and to computing the minimum-norm solution of sparse, underdetermined problems.

qr_mumps is a parallel, multithreaded software based on the StarPU runtime engine (older versions based on OpenMP are still available for download) and currently runs on multicore or, more generally, shared memory multiprocessor computers. qr_mumps does not run on distributed memory (e.g. clusters) parallel computers or GPUs. Parallelism is achieved through a decomposition of the workload into fine-grained computational tasks which basically correspond to the execution of a BLAS or LAPACK operation on blocks. It is strongly recommended to use sequential BLAS and LAPACK libraries and let qr_mumps have full control of the parallelism.

http://buttari.perso.enseeiht.fr/qr_mumps/[+http://buttari.perso.enseeiht.fr/qr_mumps/+]

MUST
~~~~

MUST detects usage errors of the Message Passing Interface (MPI) and reports them to the user. As MPI calls are complex and usage errors common, this functionality is extremely helpful for application developers that want to develop correct MPI applications. This includes errors that already manifest – segmentation faults or incorrect results – as well as many errors that are not visible to the application developer or do not manifest on a certain system or MPI implementation.

To detect errors, MUST intercepts the MPI calls that are issued by the target application and evaluates their arguments. The two main usage scenarios for MUST arise during application development and when porting an existing application to a new system. When a developer adds new MPI communication calls, MUST can detect newly introduced errors, especially also some that may not manifest in an application crash. Further, before porting an application to a new system, MUST can detect violations to the MPI standard that might manifest on the target system. MUST reports errors in a log file that can be investigated once the execution of the target executable finishes. 

https://doc.itc.rwth-aachen.de/display/CCP/Project+MUST[+https://doc.itc.rwth-aachen.de/display/CCP/Project+MUST+]

https://www.cresta-project.eu/cresta-co-design/software/[+https://www.cresta-project.eu/cresta-co-design/software/+]

Mutil
~~~~~

Copies between local file systems are a daily activity. Files are constantly being moved to locations accessible by systems with different functions and/or storage limits, being backed up and restored, or being moved due to upgraded and/or replaced hardware. Hence, maximizing the performance of copies as well as checksums that ensure the integrity of copies is desirable to minimize the turnaround time of user and administrator activities. Modern parallel file systems provide very high performance for such operations using a variety of techniques such as striping files across multiple disks to increase aggregate I/O bandwidth and spreading disks across multiple servers to increase aggregate interconnect bandwidth.

To achieve peak performance from such systems, it is typically necessary to utilize multiple concurrent readers/writers from multiple systems to overcome various single-system limitations such as number of processors and network bandwidth. The standard cp and md5sum tools of GNU coreutils found on every modern Unix/Linux system, however, utilize a single execution thread on a single CPU core of a single system, hence cannot take full advantage of the increased performance of clustered file systems.

Mutil provides mcp and msum, which are drop-in replacements for cp and md5sum that utilize multiple types of parallelism to achieve maximum copy and checksum performance on clustered file systems. Multi-threading is used to ensure that nodes are kept as busy as possible. Read/write parallelism allows individual operations of a single copy to be overlapped using asynchronous I/O. Multi-node cooperation allows different nodes to take part in the same copy/checksum. Split file processing allows multiple threads to operate concurrently on the same file. Finally, hash trees allow inherently serial checksums to be performed in parallel.

https://github.com/pkolano/mutil[+https://github.com/pkolano/mutil+]

MVE
~~~

The Multi-View Environment is an effort to ease the work with multi-view datasets and to support the development of algorithms based on multiple views, such as Multi-View Stereo or Photometric Stereo.

MVE provides an API for management of scenes. A scene is composed of a set of views and each view describes an image viewport with associated camera parameters, images of various data type, and generic (binary) data. MVE also implements an incremental Structure from Motion system, a Multi-View Stereo algorithm based on [GSC+07], and a user interface called UMVE.

MVE is written in Cxx and consists of efficient, cross-platform and easy-to-use libraries. The code runs on Linux, MacOS X, Windows and others. MVE has minimal dependencies on external libraries: It depends on libpng, libjpeg and libtiff. A front-end Qt-based application called UMVE — the Ultimate MVE — is built on top of these libraries, for inspection of the datasets.

The MVE libraries support the following features:

* A view container to aggregate per-view attributes (such as camera parameters), images of arbitrary size and type, and arbitrary data embeddings.
* Scene management which aggregates multiple views.
* An incremental Structure-from-Motion system, plus support to import from external applications such as Noah Snavely's Bundler, VisualSfM and OpenMVG, using the makescene application.
* A Multi-View Stereo library implementing [GSC+07].
* Image data type with loaders, writers and processing tools. Supported file types are PNG, JPG, TIFF, PFM and PPM.
* Mesh data type with loaders, writers and processing tools. Supported file types are OFF, PLY and PBRT.
* Various cross-platform routines, such as argument parser, timer, string processing, tokenizer, INI-file parser, endian conversion, threads and mutex, smart pointer, file system abstraction, atomic operations, sleep, ...
* Mathematical data types (e.g. matrix, vector, quaternion) and routines (e.g. geometric predicates).
* A simple OpenGL abstraction for rapid OpenGL 3+ visualization.

The UMVE application supports the following features:

* Inspection of the scene and the 3D structure.
* Inspection and management of the views and the image embeddings.
* Loading and rendering of triangle meshes.
* Triangulation of depth maps.

Rudimentary Python bindings are available.

https://github.com/simonfuhrmann/mve[+https://github.com/simonfuhrmann/mve+]

https://github.com/simonfuhrmann/mve/wiki[+https://github.com/simonfuhrmann/mve/wiki+]

http://www.cs.cornell.edu/\~snavely/bundler/[+http://www.cs.cornell.edu/~snavely/bundler/+]

MXE
~~~

MXE (M cross environment) is a Makefile that compiles a cross compiler and cross compiles many free libraries such as SDL and Qt. Thus, it provides a nice cross compiling environment for various target platforms, which:

* is designed to run on any Unix system

* builds many free libraries in addition to the cross compiler 

* can also build just a subset of the packages, and automatically builds their dependencies

* downloads all needed packages and verifies them by their checksums 

* is able to update the version numbers of all packages automatically 

* directly uses source packages, thus ensuring the whole build mechanism is transparent

* allows inter-package and intra-package parallel builds whenever possible 

* integrates well with autotools, cmake, qmake, and hand-written makefiles

* has been in continuous development since 2007 and is used by several projects

* has pre-compiled binaries that can be used in Continuous Integration systems

https://mxe.cc/[+https://mxe.cc/+]

https://github.com/mxe/mxe[+https://github.com/mxe/mxe+]

https://thebeezspeaks.blogspot.com/2009/04/cross-compilers-new-wave.html[+https://thebeezspeaks.blogspot.com/2009/04/cross-compilers-new-wave.html+]

MXNet
~~~~~

Apache MXNet is a modern open-source deep learning software framework, used to train, and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple programming languages (including Cxx, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl, and Wolfram Language.)

The MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by public cloud providers including Amazon Web Services (AWS) and Microsoft Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS. Currently, MXNet is supported by Intel, Dato, Baidu, Microsoft, Wolfram Research, and research institutions such as Carnegie Mellon, MIT, the University of Washington, and the Hong Kong University of Science and Technology.

Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility. It allows you to mix symbolic and imperative programming to maximize efficiency and productivity. At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer on top of that makes symbolic execution fast and memory efficient. MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.

MXNet is also more than a deep learning project. It is also a collection of blue prints and guidelines for building deep learning systems, and interesting insights of DL systems for hackers.

https://mxnet.apache.org/[+https://mxnet.apache.org/+]

https://github.com/apache/incubator-mxnet[+https://github.com/apache/incubator-mxnet+]

https://github.com/zackchase/mxnet-the-straight-dope[+https://github.com/zackchase/mxnet-the-straight-dope+]

https://en.wikipedia.org/wiki/Apache_MXNet[+https://en.wikipedia.org/wiki/Apache_MXNet+]

#NNNN

nabla
~~~~~

In this context, the numerical-analysis specific language ∇ improves applied mathematicians productivity throughput and enables new algorithmic developments for the construction of hierarchical and composable high-performance scientific applications.

The introduction of the hierarchical logical time within the high-performance computing scientific community represents an innovation that addresses the major exascale challenges. This new dimension to parallelism is explicitly expressed to go beyond the classical single-program-multiple-data or bulk-synchronous-parallel programming models. Control and data concurrencies are combined consistently to achieve statically analyzable transformations and efficient code generation. Shifting the complexity to ∇'s compiler offers an ease of programming and a more intuitive approach, while reaching the ability to target new hardware and leading to performance portability.

The ∇ domain-specific language (DSL) provides a productive development way for exascale HPC technologies, flexible enough to be competitive in terms of performances. This software is a computer program whose purpose is to translate specific numerical-analysis sources and generate optimized code for different targets and architectures.

http://nabla-lang.org/[+http://nabla-lang.org/+]

https://hal.inria.fr/hal-01910139/file/sle18-nablab.pdf[+https://hal.inria.fr/hal-01910139/file/sle18-nablab.pdf+]

NabLlab
~~~~~~~

Advanced and mature language workbenches have been proposed in the past decades to develop Domain-Specific Languages (DSL) and rich associated environments. They all come in various flavors, mostly depending on the underlying technological space (e.g., grammarware or modelware). However, when the time comes to start a new DSL project, it often comes with the choice of a unique technological space which later bounds the possible expected features. In this tool paper, we introduce NabLab, a full-fledged industrial environment for scientific computing and High Performance Computing (HPC), involving several metamod-els and grammars. Beyond the description of an industrial experience of the development and use of tool-supported DSLs, we report in this paper our lessons learned, and demonstrate the benefits from usefully combining metamodels and grammars in an integrated environment. 

https://github.com/cea-hpc/NabLab[+https://github.com/cea-hpc/NabLab+]

nansat
~~~~~~

Nansat is a Python toolbox for analysing and processing 2-dimensional geospatial data, such as satellite imagery, output from numerical models, and gridded in-situ data. It is created with strong focus on facilitating research, and development of algorithms and autonomous processing systems.

Nansat extends the widely used Geospatial Abstraction Data Library (GDAL) by adding scientific meaning to the datasets through metadata, and by adding common functionality for data analysis and handling (e.g., exporting to various data formats). Nansat uses metadata vocabularies that follow international metadata standards, in particular the Climate and Forecast (CF) conventions, and the NASA Directory Interchange Format (DIF) and Global Change Master Directory (GCMD) keywords.

Functionality that is commonly needed in scientific work, such as seamless access to local or remote geospatial data in various file formats, collocation of datasets from different sources and geometries, and visualization, is also built into Nansat.

https://github.com/nansencenter/nansat[+https://github.com/nansencenter/nansat+]

https://nansat.readthedocs.io/en/latest/[+https://nansat.readthedocs.io/en/latest/+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.120/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.120/+]

https://github.com/nansencenter/nansatmap[+https://github.com/nansencenter/nansatmap+]

https://github.com/nansencenter/nansat-lectures[+https://github.com/nansencenter/nansat-lectures+]

Natural Earth
~~~~~~~~~~~~~

Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.

Natural Earth solves a problem: finding suitable data for making small-scale maps. In a time when the web is awash in geospatial data, cartographers are forced to waste time sifting through confusing tangles of poorly attributed data to make clean, legible maps. Because your time is valuable, Natural Earth data comes ready-to-use.

The carefully generalized linework maintains consistent, recognizable geographic shapes at 1:10m, 1:50m, and 1:110m scales. Natural Earth was built from the ground up so you will find that all data layers align precisely with one another. For example, where rivers and country borders are one and the same, the lines are coincident.

Natural Earth, however, is more than just a collection of pretty lines. The data attributes are equally important for mapmaking. Most data contain embedded feature names, which are ranked by relative importance. Other attributes facilitate faster map production, such as width attributes assigned to river segments for creating tapers.

http://www.naturalearthdata.com/[+http://www.naturalearthdata.com/+]

Nautilus
~~~~~~~~

Nautilus is the first example of an AeroKernel that is available for public use and development. AeroKernels are extermely lightweight OS kernels that are intended to support the Hybrid Runtime (HRT) model, in which a parallel runtime enjoys full access to the entire machine. An AeroKernel like Nautilus provides a minimal set of functionality to the runtime. Nautilus currently runs on modern x64 HPC-class hardware and on the Intel Xeon Phi accelerator. Some features of Nautilus include the following:

* Support for large-scale, many-core architectures
* NUMA-aware memory subsystem
* Multiboot2 compliant
* Minimal ACPI and SFI support
* Fast, lightweight synchronization and threading facilities
* Configurable to run in x64, Xeon Phi (KNC/KNL), and HVM/HRT environments
* An extremely light-weight event subsystem called Nemo

Nautilus is part of the Interweaving project, the V3VEE project, and the Hobbes project. 

http://cs.iit.edu/\~khale/nautilus/[+http://cs.iit.edu/~khale/nautilus/+]

https://github.com/HExSA-Lab/nautilus[+https://github.com/HExSA-Lab/nautilus+]

NCL
~~~

The NCAR Command Language (NCL), a product of the Computational & Information Systems Laboratory at the National Center for Atmospheric Research (NCAR) and sponsored by the National Science Foundation, is a free interpreted language designed specifically for scientific data processing and visualization.

NCL has robust file input and output. It can read and write netCDF-3, netCDF-4 classic, netCDF-4, HDF4, binary, and ASCII data. It can read HDF-EOS2, HDF-EOS5, GRIB1, GRIB2, and OGR files (shapefiles, MapInfo, GMT, Tiger). It can be built as an OPeNDAP client. 

NCL visualizations are world class and highly customizable.

Both NCL and NCAR Graphics are released as one package in source code or pre-compiled binary format. See the download page for more information. NCAR Graphics still has its own home page. 

The software comes with useful command line tools:

* ncl_filedump - prints the contents of supported files (netCDF, HDF4/5, GRIB1, GRIB2, HDF-EOS2/5, and shapefile, and CCM History Tape)
* ncl_convert2nc - converts one or more GRIB1, GRIB2, HDF4/5, HDF-EOS2/5, or shapefile files to netCDF formatted files.
* WRAPIT - allows you to wrap Fortran 77/90 routines and call them from ncl. 

NCL can be run in interactive mode, where each line is interpreted as it is entered at your workstation, or it can be run in batch mode as an interpreter of complete scripts. You can also use command line options to set options or variables on the NCL command line. 

NCL comes with many useful built-in functions and procedures for processing and manipulating data. There are over 600 functions and procedures that include routines for:

* use specifically with climate and model data
* computing empirical orthogonal functions, Fourier coefficients, singular value decomposition, averages, standard deviations, sin, cosine, log, min, max, etc.
* retrieving and converting date information
* drawing primitives (lines, filled areas, and markers), wind barbs, weather map symbols, isosurfaces, and other graphical objects
* robust file handling
* 1-dimensional, 2-dimensional, and 3-dimensional interpolation, approximation, and regridding
* facilitating computer analysis of scalar and vector global geophysical quantities (most are based on the package known as Spherepack)
* retrieving environment variables and executing system commands 

NCL supports calling C and Fortran external routines, which makes NCL infinitely configurable.

http://www.ncl.ucar.edu/[+http://www.ncl.ucar.edu/+]

https://github.com/NCAR/ncl[+https://github.com/NCAR/ncl+]

NCO
~~~

The netCDF Operators (NCO) comprise a dozen standalone, command-line programs that take netCDF, HDF, and/or DAP files as input, then operate (e.g., derive new data, compute statistics, print, hyperslab, manipulate metadata) and output the results to screen or files in text, binary, or netCDF formats. NCO aids analysis of gridded scientific data. The shell-command style of NCO allows users to manipulate and analyze files interactively, or with expressive scripts that avoid some overhead of higher-level programming environments.

Traditional geoscience data analysis requires users to work with numerous flat (data in one level or namespace) files. In that paradigm instruments or models produce, and then repositories archive and distribute, and then researchers request and analyze, collections of flat files. NCO works well with that paradigm, yet it also embodies the necessary algorithms to transition geoscience data analysis from relying solely on traditional (or “flat”) datasets to allowing newer hierarchical (or “nested”) datasets.

The next logical step is to support and enable combining all datastreams that meet user-specified criteria into a single or small number of files that hold all the science-relevant data organized in hierarchical structures. NCO (and no other software to our knowledge) can do this now. We call the resulting data storage, distribution, and analysis paradigm Group-Oriented Data Analysis and Distribution (GODAD). GODAD lets the scientific question organize the data, not the ad hoc granularity of all relevant datasets.

The operators in NCO are:

* ncap2 - netCDF Arithmetic Processor
* ncatted - netCDF ATTribute EDitor
* ncbo - netCDF Binary Operator
* ncclimo - netCDF CLIMatOlogy Generator
* nces - netCDF Ensemble Statistics
* ncecat - netCDF Ensemble conCATenator
* ncflint - netCDF FiLe INTerpolator
* ncks - netCDF Kitchen Sink
* ncpdq - netCDF Permute Dimensions Quickly, Pack Data Quietly
* ncra - netCDF Record Averager
* ncrcat - netCDF Record conCATenator
* ncremap - netCDF REMAPer
* ncrename - netCDF RENAMEer
* ncwa - netCDF Weighted Averager

https://github.com/nco/nco/[+https://github.com/nco/nco/+]

http://nco.sourceforge.net/nco.html[+http://nco.sourceforge.net/nco.html+]

pynco
^^^^^

This package contains the module python nco, which implements a python style access to the NetCDF Operators (NCO). NCO is a command line tool for processing netCDF data. Its main focus is climate data, but it can by used for other purposes too.

https://pynco.readthedocs.io/en/latest/[+https://pynco.readthedocs.io/en/latest/+]

ncurses
~~~~~~~

ncurses (new curses) is a programming library providing an application programming interface (API) that allows the programmer to write text-based user interfaces in a terminal-independent manner. It is a toolkit for developing "GUI-like" application software that runs under a terminal emulator. It also optimizes screen changes, in order to reduce the latency experienced when using remote shells. 

The first curses library was developed at the University of California at Berkeley, for a BSD operating system, around 1980 to support Rogue, a text-based adventure game. It originally used the termcap library, which was used in other programs, such as the vi editor.
The success of the BSD curses library prompted Bell Labs to release an enhanced curses library in their System V Release 2 Unix systems. This library was more powerful and instead of using termcap, it used terminfo.
Ncurses can use either terminfo (with extensible data) or termcap.

The GNU ncurses distribution includes:

* *captoinfo*, a termcap conversion tool
* *clear*, utility for clearing the screen
* *infocmp*, the terminfo decompiler
* *tabs*, set tabs on a terminal
* *tic*, the terminfo compiler
* *toe*, list (table of) terminfo entries
* *tput*, utility for retrieving terminal capabilities in shell scripts
* *tset*, to initialize the terminal

https://en.wikipedia.org/wiki/Ncurses[+https://en.wikipedia.org/wiki/Ncurses+]

https://www.gnu.org/software/ncurses/ncurses.html[+https://www.gnu.org/software/ncurses/ncurses.html+]

*HOWTO* - http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/[+http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/+]

https://invisible-island.net/ncurses/ncurses.html[+https://invisible-island.net/ncurses/ncurses.html+]

*FAQ* - https://invisible-island.net/ncurses/ncurses.faq.html[+https://invisible-island.net/ncurses/ncurses.faq.html+]

CDK
^^^

CDK is a library written in C that provides a collection of widgets for text user interfaces (TUI) development. The widgets wrap ncurses functionality to make writing full screen curses programs faster. Perl[1][2] and Python[3] bindings are also available. 

Cdk  provides  functions  to  use a large number of predefined curses
widgets.  To use the Cdk widgets  the  header  file,  cdk.h  must  be
included in the source.

https://invisible-island.net/cdk/cdk.html[+https://invisible-island.net/cdk/cdk.html+]

https://en.wikipedia.org/wiki/CDK_(programming_library)[+https://en.wikipedia.org/wiki/CDK_(programming_library)+]

pyCDK
^^^^^

A python wrapper around the Curses Development Kit. The CDK provides a set of high level widgets for curses applications.

https://sourceforge.net/projects/pycdk/[+https://sourceforge.net/projects/pycdk/+]

ndbkit
~~~~~~

NBD — Network Block Device — is a protocol for accessing Block Devices
(hard disks and disk-like things) over a Network.  nbdkit is a toolkit
for creating NBD servers.

The key features are:

* Multithreaded NBD server written in C with good performance.
* Minimal dependencies for the basic server.
* Liberal license (BSD) allows nbdkit to be linked to proprietary
libraries or included in proprietary code.
* Well-documented, simple plugin API with a stable ABI guarantee.
* Lets you export “unconventional” block devices easily.
* You can write plugins in C, [new!] Lua, Perl, Python, OCaml, Ruby or Tcl.
* Filters can be stacked in front of plugins to transform the output.

Linux Network Block Device (NBD) with the nbdkit server takes the concept of loop mounting to the next level, giving you a flexible, scriptable loop device, useful for end users, and for developers wanting to test anything involving a block device.

Loop mounts let you mount a simple file as a device. Network Block Device (NBD) with the nbdkit server takes this concept to the next level. You can mount compressed files. Turn multiple files into a partitioned device. Mount esoteric formats like VMDK. NBD can also be used for testing: You can create giant devices up to 2^63 bytes in RAM and find out how filesystems cope. Inject errors on demand into your block devices to test error detection and recovery. Add delays to make disks deliberately slow. I will also show you how to write useful block devices using 10 line shell scripts, and show some advanced live visualizations of how the kernel and filesystems use block devices.

https://github.com/libguestfs/nbdkit[+https://github.com/libguestfs/nbdkit+]

https://rwmj.wordpress.com/tag/nbdkit/[+https://rwmj.wordpress.com/tag/nbdkit/+]

https://fosdem.org/2019/schedule/event/nbdkit/[+https://fosdem.org/2019/schedule/event/nbdkit/+]

NebulOS
~~~~~~~

NebulOS is a flexible, user-friendly, Big Data analysis platform. It can be thought of as a cluster operating system that allows a user to treat a group of Linux systems (e.g., a typical data center) as a single machine. Apache Mesos and the Hadoop Distributed File System (HDFS) act as the OS kernel and file system, respectively. The component that differentiates NebulOS from other Big Data systems is its Mesos-based framework, which allows the user to...

* run pre-existing software on the cluster, without modification.
* easily write monitoring code in any language to examine the standard error and output streams, memory usage, and CPU usage of tasks launched on the system.
* write code which performs actions, based upon the behavior of the individual tasks. For instance, tasks that meet certain user-defined conditions can be terminated and automatically relaunched with modified parameters or modified input data.

Of course, the system is also able to handle node failures seamlessly and it is aware of data locality; tasks are preferentially performed on nodes that contain the greatest amount of relevant data. The user interface is Python-based, so that the user can issue commands interactively or write Python scripts to build more complex analysis routines.

http://www.nrstickley.com/projects/#nebulos[+http://www.nrstickley.com/projects/#nebulos+]

https://bitbucket.org/nebulos-project/nebulos[+https://bitbucket.org/nebulos-project/nebulos+]

NEFI
~~~~

NEFI2 is a Python tool created to extract networks from images.

Given a suitable 2D image of a network as input, NEFI2 outputs a mathematical representation of the structure of the depicted network as a weighted undirected planar graph. Representing the structure of the network as a graph enables subsequent studies of its properties using tools and concepts from graph theory.

NEFI2 builds on top of well-documented open source libraries in order to provide a reliable, transparent and extendable collection of interchangeable solutions. NEFI2 facilitates single image analysis as well as batch processing and aims to enable scientists and practitioners of various domains to freely explore, analyze and process their data in an intuitive, hands-on fashion.

Our major motivation in developing NEFI2 is to enable virtually everyone to automatically extract networks from images. NEFI2 was designed to be an extensible image processing pipeline which can be augmented with custom algorithms and algorithm configurations. For example, you can add your own faster implementation of "Adaptive Threshold" or "Guided Watershed", you can easily set default settings and even construct a complete pipeline of your custom algorithms that solve a particular image processing task. You don't need to write tons of boilerplate code, reimplement existing UI widgets and then connect them.

https://github.com/05dirnbe/nefi[+https://github.com/05dirnbe/nefi+]

http://nefi.mpi-inf.mpg.de/[+http://nefi.mpi-inf.mpg.de/+]

Nek5000
~~~~~~~

High-order methods have the potential to overcome the current limitations of standard CFD solvers. For this reason, we have been developing and improving the spectral element code NEK5000 for more than 30 years now. It features state-of-the-art, scalable algorithms that are fast and efficient on platforms ranging from laptops to the world’s fastest computers. Applications span a wide range of fields, including fluid flow, thermal convection, combustion and magnetohydrodynamics. Our user community includes hundreds of scientists and engineers in academia, laboratories and industry.

https://nek5000.mcs.anl.gov/[+https://nek5000.mcs.anl.gov/+]

https://nek5000.mcs.anl.gov/[+https://nek5000.mcs.anl.gov/+]

NekBox
^^^^^^

NekBox is a highly scalable and portable spectral element code, which is inspired by the Nek5000 code. NekBox is specialized for box geometries and intended to prototype new methods as well as to leverage FORTRAN beyond the FORTRAN 77 standard. 

https://github.com/NekBox/NekBox[+https://github.com/NekBox/NekBox+]

https://libxsmm.readthedocs.io/en/latest/[+https://libxsmm.readthedocs.io/en/latest/+]

NekCEM
^^^^^^

NekCEM is a discontinous-Galerkin, spectral-element solver for Maxwell's equations and the drift-diffusion equations written in Fortran and C. It runs efficiently in parallel on a wide variety of systems, from laptops to the supercomputers at the Argonne Leadership Computing Facility (ALCF) and the Oak Ridge Leadership Computing Facility (OLCF). Its core is based on the computational fluid dynamics code Nek5000.

https://github.com/NekCEM/NekCEM[+https://github.com/NekCEM/NekCEM+]

Nektar
~~~~~~

Nektar is a tensor product based finite element package designed to allow one to construct efficient classical low polynomial order h-type solvers (where h is the size of the finite element) as well as higher p-order piecewise polynomial order solvers. The framework currently has the following capabilities:

* Representation of one, two and three-dimensional fields as a collection of piecewise continuous or discontinuous polynomial domains.
& Segment, plane and volume domains are permissible, as well as domains representing curves and surfaces (dimensionally-embedded domains).
* Hybrid shaped elements, i.e triangles and quadrilaterals or tetrahedra, prisms and hexahedra.
* Both hierarchical and nodal expansion bases.
* Continuous or discontinuous Galerkin operators.
* Cross platform support for Linux, Mac OS X and Windows.

The framework comes with a number of solvers and also allows one to construct a variety of new solvers. 

https://www.nektar.info/[+https://www.nektar.info/+]

https://www.sciencedirect.com/science/article/pii/S0010465518300973[+https://www.sciencedirect.com/science/article/pii/S0010465518300973+]

NEMO
~~~~

NEMO for Nucleus for European Modelling of the Ocean is a state-of-the-art modelling framework of ocean related engines for oceanographic research, operational oceanography, seasonal forecast and [paleo]climate studies. 

The physical core engines are OPA (ocean), LIM (sea-ice) and TOP-PISCES (biogeochemistry) which are fully described in their reference publications. They are completed by a 2-way nesting package (AGRIF) and a versatile data assimilation interface (ASM, OBS or TAM). 


Shared under CeCILL free software license, the framework contains builtins reference configurations and idealized test cases to serve as examples and to study particular processes. A set of tools is also provided to setup your own configuration, (pre|post)process your data and interact with other models (XIOS and OASIS). 

https://www.nemo-ocean.eu/[+https://www.nemo-ocean.eu/+]

http://forge.ipsl.jussieu.fr/nemo/wiki/Users[+http://forge.ipsl.jussieu.fr/nemo/wiki/Users+]

NET
~~~

.NET Framework (pronounced as "dot net") is a software framework developed by Microsoft that runs primarily on Microsoft Windows. It includes a large class library named Framework Class Library (FCL) and provides language interoperability (each language can use code written in other languages) across several programming languages. Programs written for .NET Framework execute in a software environment (in contrast to a hardware environment) named Common Language Runtime (CLR), an application virtual machine that provides services such as security, memory management, and exception handling. As such, computer code written using .NET Framework is called "managed code". FCL and CLR together constitute the .NET Framework.

FCL provides user interface, data access, database connectivity, cryptography, web application development, numeric algorithms, and network communications. Programmers produce software by combining their source code with .NET Framework and other libraries. The framework is intended to be used by most new applications created for the Windows platform. Microsoft also produces an integrated development environment largely for .NET software called Visual Studio. 

https://opensource.com/article/17/11/net-linux[+https://opensource.com/article/17/11/net-linux+]

https://docs.microsoft.com/en-us/dotnet/core/linux-prerequisites[+https://docs.microsoft.com/en-us/dotnet/core/linux-prerequisites+]

https://fosdem.org/2019/schedule/event/open_source_microsoft/[+https://fosdem.org/2019/schedule/event/open_source_microsoft/+]

When .NET made its debut in 2002, it supported multiple languages, including C# and Visual Basic (VB). Over the years, many languages have been added. The initial release of .NET Core supports C# and F#, with VB coming soon. Thanks to .NET Core being open source, you can also install and use the .NET Framework on your Linux machine. Even better: An application created on any system can run on any other system, regardless of operating system.

This Refcard will guide you along the path to being productive using .NET on Linux, from installation to debugging. Information is available to help you find documentation and discussions related to .NET Core. An architectural overview is presented, as well as tips for using the new Command Line Interface (CLI). Building MVC web sites, RESTful services, and standalone applications are also covered. Finally, some tools and helpful settings are discussed as they relate to your development efforts.

https://dzone.com/refcardz/net-on-linux[+https://dzone.com/refcardz/net-on-linux+]

netCDF
~~~~~~

Blah.

CFGeom
^^^^^^

A Python Reference Implementation for Representing Geometries in NetCDF Following the CF Conventions

This project demonstrates how points, lines, polygons, and their multipart equivalents can be represented in NetCDF-CF. The project includes a Python reference implementation for reading and writing geometries in a netCDF file.

CFGeom is primarily concerned with reading and writing the geometry portion of a netCDF file. While supporting CF 1.8, the implementation also demonstrates use of enhanced netCDF-4 capabilities for possible inclusion in CF 2.0.

CF version 1.8 introduced a specification for representing line and polygon geometries, allowing for the shape of a watershed boundary or river line to be encoded in a netCDF file in a standard way.
The geometries described in CF 1.8 include points, lines, polygons (including polygons with holes), and their multipart equivalents. The nodes comprising lines or polygons are assumed to be connected with straight lines in the coordinate system of the data variable.

https://github.com/twhiteaker/CFGeom[+https://github.com/twhiteaker/CFGeom+]

https://twhiteaker.github.io/CFGeom/[+https://twhiteaker.github.io/CFGeom/+]

https://github.com/cf-convention/cf-conventions/blob/master/ch07.adoc#geometries[+https://github.com/cf-convention/cf-conventions/blob/master/ch07.adoc#geometries+]

cf-plot
^^^^^^^

cf-plot is a set of Python routines for making the common contour, vector and line plots that climate researchers use. cf-plot generally uses cf-python to present the data and CF attributes for plotting as two-dimensional data fields.  
You will need to download and install cf-python to use cf-plot. Other cf-plot dependencies are: Numpy, Matplotlib, NetCDF4 and Cartopy.

http://ajheaps.github.io/cf-plot/[+http://ajheaps.github.io/cf-plot/+]

cf-python
^^^^^^^^^

CF is a netCDF convention which is in wide and growing use for the storage of model-generated and observational data relating to the atmosphere, ocean and Earth system.  The capabilities of cf-python include:

* Read CF-netCDF files, CFA-netCDF files and UK Met Office fields files and PP files.
* Create CF fields.
* Write fields to CF-netCDF and CFA-netCDF files on disk.
* Aggregate collections of fields into as few multidimensional fields as possible using the CF aggregation rules.
* Create, delete and modify a field's data and metadata.
* Select and subspace fields according to their metadata.
* Perform broadcastable, metadata-aware arithmetic, comparison and trigonometric operation with fields.
* Collapse fields by statistical operations.
* Sensibly deal with date-time data.
* Allow for cyclic axes.
* Regrid fields.
* Visualize fields with cf-plot. 

All of the above use Large Amounts of Massive Arrays (LAMA) functionality, which allows multiple fields larger than the available memory to exist and be manipulated.

The package provides command line utilities for viewing CF fields (cfdump) and aggregating datasets (cfa). 

https://cfpython.bitbucket.io/[+https://cfpython.bitbucket.io/+]

https://cfpython.bitbucket.io/docs/latest/index.html[+https://cfpython.bitbucket.io/docs/latest/index.html+]

fixnc
^^^^^

fixnc helps to change meta information of the netCDF files. You can easilly add, delete and rename dimensions, variables and attributes.

https://fixnc.readthedocs.io/en/latest/[+https://fixnc.readthedocs.io/en/latest/+]

https://github.com/koldunovn/fixnc[+https://github.com/koldunovn/fixnc+]

fuse-netcdf
^^^^^^^^^^^

ESoWC project to develop a Python utility to mount NetCDF files as a file-system in user space (FUSE).

https://github.com/dvalters/fuse-netcdf[+https://github.com/dvalters/fuse-netcdf+]

ncdump-json
^^^^^^^^^^^

Modified netCDF ncdump program to output data in json format.

https://github.com/jllodra/ncdump-json[+https://github.com/jllodra/ncdump-json+]

netCDF-LD
^^^^^^^^^

netCDF-LD is an approach for constructing Linked Data descriptions using the metadata and structures found in netCDF files. Linked Data is a method of publishing structured data on the web so that it can be interlinked and become more useful through semantic queries. It uses the W3 Resource Description Framework (RDF) standard to express the information and relationships. 

netCDF-LD enhances netCDF metadata, enabling information found in netCDF files to be linked with published conventions and controlled vocabularies used to express the content.

netCDF is a data format commonly used for array and gridded data in environmental applications, particularly climatology and oceanography, and recently in the geosciences. netCDF files typically use “community conventions” within the file to encode details relevant for users of the data, e.g. Climate & Forecast (CF) Metadata Conventions and the Attribute Convention for Data Discovery (ACDD). While these conventions allow the detailed definitions to be delegated to a common place, the links to the definitions are indirect, and not generally known to users outside the communities, so the definitions cannot be immediately used for discovery, integration and understanding.

Furthermore, the existing conventions (CF, ACDD) do not cover all possible applications, nor should they try to. Other relevant vocabularies and schemas exist, are maintained by various institutions and communities, and have their own lifecycle and usage. We need a mechanism that allows different vocabularies and schemas to play nicely together, so that data authors can combine conventions without a confusing and possibly conflicting mix of namings and interpretations.

The netCDF-LD project:

* Provides a view of definitions of nearly every part of my netCDF file by following the URIs in the generated RDF triples.
* Provides tools and support for enhancing discoverability and access of netCDF data from data repositories.
* Provides views of netCDF metadata that can be linked with other netCDF metadata and related information on the Web.

https://binary-array-ld.github.io/netcdf-ld/[+https://binary-array-ld.github.io/netcdf-ld/+]

https://github.com/binary-array-ld/netcdf-ld[+https://github.com/binary-array-ld/netcdf-ld+]

https://github.com/binary-array-ld/bald[+https://github.com/binary-array-ld/bald+]

netmap
~~~~~~

netmap is a framework for high speed packet I/O. Together with its companion VALE software switch, it is implemented as a single kernel module and available for FreeBSD, Linux and now also Windows (OSX still missing, unfortunately). netmap supports access to network cards (NICs), host stack, virtual ports (the "VALE" switch), and "netmap pipes". It can easily reach line rate on 10G NICs (14.88 Mpps), over 30 Mpps on 40G NICs (limited by the NIC's hardware), over 20 Mpps on VALE ports, and over 100 Mpps on netmap pipes. There is netmap support for QEMU, libpcap (hence, all libpcap applications can use it), the bhyve hypervisor, the Click modular ruter, and a number of other applications.

netmap/VALE can be used to build extremely fast traffic generators, monitors, software switches, network middleboxes, interconnect virtual machines or processes, do performance testing of high speed networking apps without the need for expensive hardware. We have full support for libpcap so most pcap clients can use it with no modifications.

netmap, VALE and netmap pipes are implemented as a single, non intrusive kernel module. Native netmap support is available for several NICs through slightly modified drivers; for all other NICs, we provide an emulated mode on top of standard drivers. netmap/VALE are part of standard FreeBSD distributions, and available in source format for Linux too.

https://github.com/luigirizzo/netmap[+https://github.com/luigirizzo/netmap+]

http://info.iet.unipi.it/\~luigi/netmap/[+http://info.iet.unipi.it/~luigi/netmap/+]

https://github.com/NTAP/warpcore[+https://github.com/NTAP/warpcore+]

netsniff-ng
~~~~~~~~~~~

netsniff-ng is a free Linux networking toolkit, a Swiss army knife for your daily Linux network plumbing if you will.

Its gain of performance is reached by zero-copy mechanisms, so that on packet reception and transmission the kernel does not need to copy packets from kernel space to user space and vice versa.

Our toolkit can be used for network development and analysis, debugging, auditing or network reconnaissance.

The netsniff-ng toolkit consists of the following utilities: 

* *netsniff-ng* is a fast network analyzer based on packet mmap(2) mechanisms. It can record pcap files to disc, replay them and also do an offline and online analysis. Capturing, analysis or replay of raw 802.11 frames are supported as well. pcap files are also compatible with tcpdump or Wireshark traces. netsniff-ng processes those pcap traces either in scatter-gather I/O or by mmap(2) I/O.

* *trafgen* is a multi-threaded network traffic generator based on packet mmap(2) mechanisms. It has its own flexible, macro-based low-level packet configuration language. Injection of raw 802.11 frames are supported as well. trafgen has a significantly higher speed than mausezahn and comes very close to pktgen, but runs from user space. pcap traces can also be converted into a trafgen packet configuration.

* *mausezahn* is a high-level packet generator that can run on a hardware-software appliance and comes with a Cisco-like CLI. It can craft nearly every possible or impossible packet. Thus, it can be used, for example, to test network behaviour under strange circumstances (stress test, malformed packets) or to test hardware-software appliances for several kind of attacks.

* *bpfc* is a Berkeley Packet Filter (BPF) compiler that understands the original BPF language developed by McCanne and Jacobson. It accepts BPF mnemonics and converts them into kernel/netsniff-ng readable BPF ``opcodes''. It also supports undocumented Linux filter extensions. This can especially be useful for more complicated filters, that high-level filters fail to support.

* *ifpps* is a tool which periodically provides top-like networking and system statistics from the Linux kernel. It gathers statistical data directly from procfs files and does not apply any user space traffic monitoring that would falsify statistics on high packet rates. For wireless, data about link connectivity is provided as well.

* *flowtop* is a top-like connection tracking tool that can run on an end host or router. It is able to present TCP or UDP flows that have been collected by the kernel's netfilter framework. GeoIP and TCP state machine information is displayed. Also, on end hosts flowtop can show PIDs and application names that flows relate to. No user space traffic monitoring is done, thus all data is gathered by the kernel.

* *curvetun* is a lightweight, high-speed ECDH multiuser tunnel for Linux. curvetun uses the Linux TUN/TAP interface and supports {IPv4,IPv6} over {IPv4,IPv6} with UDP or TCP as carrier protocols. Packets are encrypted end-to-end by a symmetric stream cipher (Salsa20) and authenticated by a MAC (Poly1305), where keys have previously been computed with the ECDH key agreement protocol (Curve25519).

* *astraceroute* is an autonomous system (AS) trace route utility. Unlike traceroute or tcptraceroute, it not only display hops, but also their AS information they belong to as well as GeoIP information and other interesting things. On default, it uses a TCP probe packet and falls back to ICMP probes in case no ICMP answer has been received. 

http://netsniff-ng.org/[+http://netsniff-ng.org/+]

https://en.wikipedia.org/wiki/Netsniff-ng[+https://en.wikipedia.org/wiki/Netsniff-ng+]

https://github.com/netsniff-ng/netsniff-ng[+https://github.com/netsniff-ng/netsniff-ng+]

http://www.draconyx.net/articles/netsniff-ng-high-performant-packet-sniff.html[+http://www.draconyx.net/articles/netsniff-ng-high-performant-packet-sniff.html+]

NFMW
~~~~

The Forecast Model Web Map Service (NFMW) is software that runs on a web server and produces custom visualizations of atmospheric forecast data for display in a web browser. The NFMW implements the Open Geospatial Consortium (OGC) Web Map Service (WMS) specification.

The Forecast Model Web Map Service (NFMW) is able to read the output of numerical models that simulate Earth science processes and to produce visualizations of the desired output field(s) showing the geographic area, time and elevation specified by the user. Users request visualizations according to the industry-standard Web Map Service (WMS). The model outputs are in formats such as HDF, NetCDF, GRIB or raw binary. The NFMW code reads data for the desired model run time, forecast time, and field(s), subsets the data to the region of interest, interpolates the data to the specified size, generates a visualization of the data using colors, contour lines or arrows, and sends the visualization to the client. The client is typically a normal web browser. The NFMW code is a combination or Perl and Interactive Data Language (IDL).

The primary NFMW goal is to enable both expert scientists and members of the public to view the results of simulations through a normal web browser without needing to install or learn special software. A secondary goal was to demonstrate concretely the benefit of adopting open-standard interface specifications such as the OGC WMS.

https://opensource.gsfc.nasa.gov/projects/NFMW/index.php[+https://opensource.gsfc.nasa.gov/projects/NFMW/index.php+]

https://software.nasa.gov/software/GSC-15276-1[+https://software.nasa.gov/software/GSC-15276-1+]

NFS-Ganesha
~~~~~~~~~~~

Nfs-ganesha is a user-mode file server for NFS (v3, 4.0, 4.1, 4.1 pNFS, 4.2) and for 9P from the Plan9 operating system. It can support all these protocols concurrently. 

https://github.com/nfs-ganesha/nfs-ganesha[+https://github.com/nfs-ganesha/nfs-ganesha+]

https://github.com/nfs-ganesha/nfs-ganesha/wiki/Compiling[+https://github.com/nfs-ganesha/nfs-ganesha/wiki/Compiling+]

NiaPy
~~~~~~

NiaPy’s documentation

Python micro framework for building nature-inspired algorithms.

Nature-inspired algorithms are a very popular tool for solving optimization problems. Since the beginning of their era, numerous variants of nature-inspired algorithms were developed. To prove their versatility, those were tested in various domains on various applications, especially when they are hybridized, modified or adapted. However, implementation of nature-inspired algorithms is sometimes difficult, complex and tedious task. In order to break this wall, NiaPy is intended for simple and quick use, without spending a time for implementing algorithms from scratch.

The algorithms implemented include:

* the bat algorithm
* the firefly algorithm
* the differential evolution algorithm
* the flower pollination algorithm
* the grey wolf optimizer
* genetic algorithms
* the artificial bee colony algorithm
* the particle swarm optimization algorithm
* the bare  bones fireworks algorithm
* an implementation of camel traveling behavior
* the monkey king evolution algorithm versions 1, 2 and 3
* the (1+1) evolution strategy algorithm
* the (mu+1) evolution strategy algorithm
* the (mu+lambda) evolution strategy algorithm
* the sine cosine algorithm
* the glowworm and glowworm swarm optimization algorithms
* the harmony search algorithm
* the krill herd algorithm
* the fireworks, enhanced fireworks, and dynamic fireworks algorithms
* the gravitational search algorithm
* the hybrid bat algorithm
* the self-adaptive differential evolution algorithm
* the Nelder Mead of downhill simplex or amoeba algorithm
* the iterative hill climbing algorithm
* the simulated annealing algorithm
* the multiple trajectory search algorithm
* the anarchic society optimization algorithm

https://niapy.readthedocs.io/en/stable/[+https://niapy.readthedocs.io/en/stable/+]

https://arxiv.org/abs/1307.4186[+https://arxiv.org/abs/1307.4186+]

https://arxiv.org/abs/1301.0929[+https://arxiv.org/abs/1301.0929+]

https://github.com/NiaOrg/NiaPy[+https://github.com/NiaOrg/NiaPy+]

http://joss.theoj.org/papers/17ad9f9b7f05dcc6ab8a733c1aab319e[+http://joss.theoj.org/papers/17ad9f9b7f05dcc6ab8a733c1aab319e+]

NICAM
~~~~~

NICAM-DC is the dynamical core package, which is a part of NICAM full model. The development of NICAM with full physics has been co-developed mainly by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC), Atmosphere and Ocean Research Institute (AORI) at The University of Tokyo, and RIKEN / Advanced Institute for Computational Science (AICS).

http://r-ccs-climate.riken.jp/nicam-dc/[+http://r-ccs-climate.riken.jp/nicam-dc/+]

https://github.com/nicamdc-dev/nicamdc[+https://github.com/nicamdc-dev/nicamdc+]

http://nicam.jp/hiki/[+http://nicam.jp/hiki/+]

NIFTy
~~~~~

NIFTy, "Numerical Information Field Theory", is a versatile library designed to enable the development of signal inference algorithms that operate regardless of the underlying grids (spatial, spectral, temporal, …) and their resolutions. Its object-oriented framework is written in Python, although it accesses libraries written in Cxx and C for efficiency.

NIFTy offers a toolkit that abstracts discretized representations of continuous spaces, fields in these spaces, and operators acting on these fields into classes. This allows for an abstract formulation and programming of inference algorithms, including those derived within information field theory. NIFTy's interface is designed to resemble IFT formulae in the sense that the user implements algorithms in NIFTy independent of the topology of the underlying spaces and the discretization scheme. Thus, the user can develop algorithms on subsets of problems and on spaces where the detailed performance of the algorithm can be properly evaluated and then easily generalize them to other, more complex spaces and the full problem, respectively.

The set of spaces on which NIFTy operates comprises point sets, n-dimensional regular grids, spherical spaces, their harmonic counterparts, and product spaces constructed as combinations of those. NIFTy takes care of numerical subtleties like the normalization of operations on fields and the numerical representation of model components, allowing the user to focus on formulating the abstract inference procedures and process-specific model properties.

https://gitlab.mpcdf.mpg.de/ift/nifty[+https://gitlab.mpcdf.mpg.de/ift/nifty+]

http://ift.pages.mpcdf.de/nifty/[+http://ift.pages.mpcdf.de/nifty/+]

https://en.wikipedia.org/wiki/Information_field_theory[+https://en.wikipedia.org/wiki/Information_field_theory+]

https://arxiv.org/abs/1804.03350[+https://arxiv.org/abs/1804.03350+]

Ninja
~~~~~

Ninja is a small build system with a focus on speed. It differs from other build systems in two major respects: it is designed to have its input files generated by a higher-level build system, and it is designed to run builds as fast as possible.

https://ninja-build.org/[+https://ninja-build.org/+]

https://github.com/ninja-build/ninja[+https://github.com/ninja-build/ninja+]

Nix
~~~

Nix is a powerful package manager for Linux and other Unix systems that makes package management reliable and reproducible. It provides atomic upgrades and rollbacks, side-by-side installation of multiple versions of a package, multi-user package management and easy setup of build environments.

Nix’s purely functional approach ensures that installing or upgrading one package cannot break other packages. This is because it won’t overwrite dependencies with newer versions that might cause breakage elsewhere. It allows you to roll back to previous versions, and ensures that no package is in an inconsistent state during an upgrade.

Nix builds packages in isolation from each other. This ensures that they are reproducible and don’t have undeclared dependencies, so if a package works on one machine, it will also work on another.

Nix makes it trivial to set up and share build environments for your projects, regardless of what programming languages and tools you’re using. For instance, running the command “nix-shell '<nixpkgs>' -A firefox” gives you a Bash shell in which all of Firefox’s build-time dependencies are present and all necessary environment variables are set.

https://nixos.org/nix/[+https://nixos.org/nix/+]

https://nixos.org/nixpkgs/[+https://nixos.org/nixpkgs/+]

NLopt
~~~~~

NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. Its features include:

* Callable from C, Cxx, Fortran, Matlab or GNU Octave, Python, GNU Guile, Julia, GNU R, Lua, and OCaml.
* A common interface for many different algorithms—try a different algorithm just by changing one parameter.
* Support for large-scale optimization (some algorithms scalable to millions of parameters and thousands of constraints).
* Both global and local optimization algorithms.
* Algorithms using function values only (derivative-free) and also algorithms exploiting user-supplied gradients.
* Algorithms for unconstrained optimization, bound-constrained optimization, and general nonlinear inequality/equality constraints.
* Free/open-source software under the GNU LGPL (and looser licenses for some portions of NLopt).

https://nlopt.readthedocs.io/en/latest/[+https://nlopt.readthedocs.io/en/latest/+]

https://github.com/stevengj/nlopt[+https://github.com/stevengj/nlopt+]

https://github.com/JuliaOpt/NLopt.jl[+https://github.com/JuliaOpt/NLopt.jl+]

NodePy
~~~~~~

NodePy (Numerical ODEs in Python) is a Python package for designing, analyzing, and testing numerical methods for initial value ODEs. Its development was motivated by my own research in time integration methods for PDEs. I found that I was frequently repeating tasks that could be automated and integrated. Initially I developed a collection of MATLAB scripts, but this became unwieldy due to the large number of files that were necessary and the more limited capability for code reuse.

NodePy represents an object-oriented approach, in which the basic object is a numerical ODE solver. The idea is to design a laboratory for such methods in the same sense that MATLAB is a laboratory for matrices. Some distinctive design goals are:

* Plug-and-play: any method can be applied to any problem using the same syntax. Also, properties of different kinds of methods are available through the same syntax. This makes it easy to compare different methods.
* Abstract representations: Generally, the most abstract (hence powerful) representaton of an object is used whenever possible. Thus, order conditions are generated using products on rooted trees (or other recursions) rather than being hard-coded.
* Numerical representation: The most precise representation possible is used for quantities such as coefficients: rational numbers (using SymPy’s Rational class) when available, floating-point numbers otherwise. Where necessary, method properties are determined by numerical calculations, using appropriate tolerances. Thus the “order” of a method with floating-point coefficients is determined by checking whether the order conditions are satisfied to within a small value (near machine-epsilon). For efficiency reasons, coefficients are always converted to floating-point for purposes of applying the method to a problem.

In general, user-friendliness of the interface and readability of the code are prioritized over performance.

NodePy includes capabilities for applying the methods to solve systems of ODEs. This is mainly intended for testing and comparison; for realistic problems of interest in most fields, time-stepping in Python will be too slow. One way around this is to wrap Fortran or C functions representing the right-hand-side of the ODE, and we are looking into this.

https://github.com/ketch/nodepy[+https://github.com/ketch/nodepy+]

https://nodepy.readthedocs.io/en/latest/[+https://nodepy.readthedocs.io/en/latest/+]

node.py
~~~~~~~

Node.py is a Python runtime compatible with CPython 2.7 and 3.3 – 3.6. It provides a separate but superior import mechanism for modules, bringing dependency management and ease of deployment for Python applications up to par with other languages, without virtualenvs.

nppm is Node.py's package manager that allows you to install and manage standard Python packages (using Pip under the hood) as well as Node.py packages without the hazzle of virtual environments. nppm is a powerful tool for deploying Node.py applications and command-line tools. 

https://github.com/nodepy/nodepy[+https://github.com/nodepy/nodepy+]

https://github.com/nodepy/nppm[+https://github.com/nodepy/nppm+]

Nonconformist
~~~~~~~~~~~~~

Python implementation of the conformal prediction framework. 

https://github.com/donlnz/nonconformist[+https://github.com/donlnz/nonconformist+]

http://machina-sapiens.net/sweds/[+http://machina-sapiens.net/sweds/+]

http://www.alrw.net/[+http://www.alrw.net/+]

http://onlineprediction.net/[+http://onlineprediction.net/+]

http://www.alrw.net/events.html[+http://www.alrw.net/events.html+]

https://github.com/ryantibs/conformal[+https://github.com/ryantibs/conformal+]

NorESM
~~~~~~

NorESM1 is the Norwegian Earth System model used for CMIP5. The model is based on the CCSM framework.
However, NorESM has special features developed by Norwegian researchers. 

https://www.geosci-model-dev.net/special_issue20.html[+https://www.geosci-model-dev.net/special_issue20.html+]

https://wiki.met.no/noresm/start[+https://wiki.met.no/noresm/start+]

https://www.geosci-model-dev.net/12/343/2019/[+https://www.geosci-model-dev.net/12/343/2019/+]

NPEET
~~~~~

This package contains Python code implementing several entropy estimation functions for both discrete and continuous variables. Information theory provides a model-free way find structure in complex systems, but difficulties in estimating these quantities has traditionally made these techniques infeasible. This package attempts to allay these difficulties by making modern state-of-the-art entropy estimation methods accessible in a single easy-to-use python library.

The implementation is very simple. It only requires that numpy/scipy be installed. It includes estimators for entropy, mutual information, and conditional mutual information for both continuous and discrete variables. Additionally it includes a KL Divergence estimator for continuous distributions and mutual information estimator between continuous and discrete variables along with some non-parametric tests for evaluating estimator performance.

https://github.com/gregversteeg/NPEET[+https://github.com/gregversteeg/NPEET+]

https://github.com/BiuBiuBiLL/NPEET_LNC[+https://github.com/BiuBiuBiLL/NPEET_LNC+]

nteract
~~~~~~~

nteract is a desktop-based computing environment, which means that the application can take advantage of all the goodies that your operating system provides, like file search and click to open. nteract and the desktop belong together.

nteract is built on top of a rich ecosystem of packages that allow developers to write software built on top of the notebook document format and the code execution protocol. You can visit our GitHub organization to find out which packages you can start to develop with.

https://nteract.io/[+https://nteract.io/+]

https://github.com/nteract[+https://github.com/nteract+]

nuclear
~~~~~~~

A modern music player focused on streaming from free sources.
There's no need to use services that limit your freedom and seek to exploit you just to listen to your favourite artists. Nuclear empowers you to listen to what you want, where you want, and how you want, for free.

The features include:

* Searching for and playing music from youtube (including integration with playlists), bandcamp (including albums), and soundcloud
* Searching for related songs in youtube
* Downloading from youtube
* Searching for albums (powered by last.fm and musicbrainz), album view, automatic song lookup based on artist and track name (in progress, can be dodgy sometimes)
* Song queue, which can be exported as a playlist
* Loading saved playlists (stored in json files)
* Scrobbling to last.fm (along with updating the 'now playing' status)
* Newest releases with reviews - tracks and albums
* Browsing by genre

https://github.com/nukeop/nuclear[+https://github.com/nukeop/nuclear+]

https://nuclear.gumblert.tech/[+https://nuclear.gumblert.tech/+]

Numba
~~~~~

Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.

Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.

You don't need to replace the Python interpreter, run a separate compilation step, or even have a C/Cxx compiler installed. Just apply one of the Numba decorators to your Python function, and Numba does the rest. 

Numba is designed to be used with NumPy arrays and functions. Numba generates specialized code for different array data types and layouts to optimize performance. Special decorators can create universal functions that broadcast over NumPy arrays just like NumPy functions do.

Numba also works great with Jupyter notebooks for interactive computing, and with distributed execution frameworks, like Dask and Spark.

http://numba.pydata.org/[+http://numba.pydata.org/+]

https://github.com/numba/numba[+https://github.com/numba/numba+]

https://devblogs.nvidia.com/seven-things-numba/[+https://devblogs.nvidia.com/seven-things-numba/+]

Numcodecs
~~~~~~~~~

Numcodecs is a Python package providing buffer compression and transformation codecs for use in data storage and communication applications. These include:

* Compression codecs, e.g., Zlib, BZ2, LZMA and Blosc.
* Pre-compression filters, e.g., Delta, Quantize, FixedScaleOffset, PackBits, Categorize.
* Integrity checks, e.g., CRC32, Adler32.

All codecs implement the same API, allowing codecs to be organized into pipelines in a variety of ways.

https://numcodecs.readthedocs.io/en/stable/[+https://numcodecs.readthedocs.io/en/stable/+]

https://github.com/zarr-developers/numcodecs[+https://github.com/zarr-developers/numcodecs+]

NU-Minebench
~~~~~~~~~~~~

NU-MineBench is a data mining benchmark suite containing a mix of several representative data mining applications from different application domains. This benchmark is intended for use in computer architecture research, systems research, performance evaluation, and high-performance computing. The well-known applications assembled in this benchmark suite have been collected from research groups in industry and academia. The applications contain highly optimized versions of the data mining algorithms. Scalable versions of the applications are also provided. Such extensions were designed and implemented by developers at Northwestern University. Currently, the benchmark has applications with algorithms based on clustering, association rules, classification, bayesian network, pattern recognition, support vector machines and several other well known data mining methodologies. These applications are used in diverse fields like bioinformatics, network intrusion, customer relationship management, and marketing. 

List of algorithms and applications:

* Approximate Frequent Itemset Miner
* Apriori association rule mining
* Naive Bayesian Network data classifier
* BIRCH data clustering
* ECLAT association rule mining
* GeneNet, a DNA sequencing application using Bayesian network
* HOP, a density-based data clustering
* K-means and Fuzzy K-means data clustering
* Parallel ETI Mining
* PLSA (Parallel Linear Space Alignment)
* Recursive_Weak, Recursive_Weak_pp
* RSearch, a sequence database searching with RNA structure queries
* ScalParC decision-tree based data classification
* Semphy, a structure learning algorithm that is based on phylogenetic trees
* SNP (Single Nucleotide Polymorphisms) data classification
* SVM-RFE (Support Vector Machines - Recursive Feature Elimination) is a feature selection algorithm
* Utility mining, association rule-based mining algorithm

http://cucis.ece.northwestern.edu/projects/DMS/MineBench.html[+http://cucis.ece.northwestern.edu/projects/DMS/MineBench.html+]

Numpy
~~~~~

Blah.

ClPy
^^^^

ClPy is an implementation of CuPy's OpenCL backend. In other words, ClPy enables softwares written in CuPy to work also on OpenCL devices, not only on CUDA (NVIDIA) devices.

https://github.com/fixstars/clpy[+https://github.com/fixstars/clpy+]

CuPy
^^^^

CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA. CuPy consists of cupy.ndarray, the core multi-dimensional array class, and many functions on it. It supports a subset of numpy.ndarray interface.

The following is a brief overview of supported subset of NumPy interface:

* Basic indexing (indexing by ints, slices, newaxes, and Ellipsis)
* Most of Advanced indexing (except for some indexing patterns with boolean masks)
* Data types (dtypes): bool_, int8, int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64, complex64, complex128
* Most of the array creation routines (empty, ones_like, diag, etc.)
* Most of the array manipulation routines (reshape, rollaxis, concatenate, etc.)
* All operators with broadcasting
* All universal functions for elementwise operations (except those for complex numbers).
* Linear algebra functions, including product (dot, matmul, etc.) and decomposition (cholesky, svd, etc.), accelerated by cuBLAS.
* Reduction along axes (sum, max, argmax, etc.)

CuPy also includes the following features for performance:

* User-defined elementwise CUDA kernels
* User-defined reduction CUDA kernels
* Fusing CUDA kernels to optimize user-defined calculation
* Customizable memory allocator and memory pool
* cuDNN utilities

CuPy uses on-the-fly kernel synthesis: when a kernel call is required, it compiles a kernel code optimized for the shapes and dtypes of given arguments, sends it to the GPU device, and executes the kernel. The compiled code is cached. It may make things slower at the first kernel call, though this slow down will be resolved at the second execution. CuPy also caches the kernel code sent to GPU device within the process, which reduces the kernel transfer time on further calls.

https://github.com/cupy/cupy[+https://github.com/cupy/cupy+]

http://docs-cupy.chainer.org/en/stable/[+http://docs-cupy.chainer.org/en/stable/+]

https://cupy.chainer.org/[+https://cupy.chainer.org/+]

https://us.pycon.org/2018/schedule/presentation/119/[+https://us.pycon.org/2018/schedule/presentation/119/+]

https://docs.chainer.org/en/stable/[+https://docs.chainer.org/en/stable/+]

Sparse
^^^^^^

This implements sparse arrays of arbitrary dimension on top of numpy and scipy.sparse. It generalizes the scipy.sparse.coo_matrix and scipy.sparse.dok_matrix layouts, but extends beyond just rows and columns to an arbitrary number of dimensions.

Additionally, this project maintains compatibility with the numpy.ndarray interface rather than the numpy.matrix interface used in scipy.sparse

These differences make this project useful in certain situations where scipy.sparse matrices are not well suited, but it should not be considered a full replacement. It lacks layouts that are not easily generalized like CSR/CSC and depends on scipy.sparse for some computations.

Sparse arrays, or arrays that are mostly empty or filled with zeros, are common in many scientific applications. To save space we often avoid storing these arrays in traditional dense formats, and instead choose different data structures. Our choice of data structure can significantly affect our storage and computational costs when working with these arrays.

https://github.com/pydata/sparse[+https://github.com/pydata/sparse+]

https://sparse.pydata.org/en/latest/[+https://sparse.pydata.org/en/latest/+]

NVIDIA-Docker
~~~~~~~~~~~~~

NVIDIA uses containers to develop, test, benchmark, and deploy deep learning (DL) frameworks and HPC applications.  NVIDIA offers GPU accelerated containers via NVIDIA GPU Cloud (NGC) for use on DGX systems, public cloud infrastructure, and even local workstations with GPUs. NVIDIA-Docker has been the critical underlying technology for these initiatives.

The NVIDIA Container Runtime introduced here is our next-generation GPU-aware container runtime. It is compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologies.

https://devblogs.nvidia.com/gpu-containers-runtime/[+https://devblogs.nvidia.com/gpu-containers-runtime/+]

https://github.com/NVIDIA/nvidia-docker[+https://github.com/NVIDIA/nvidia-docker+]

#OOOO

OAMap
~~~~~

Data analysts are often faced with a choice between speed and flexibility. Tabular data, such as SQL tables, can be processed rapidly enough for a truly interactive analysis session, but hierarchically nested formats, such as JSON, are better at representing relationships in complex data models. In some domains (such as particle physics), we want to perform calculations on JSON-like structures at the speed of SQL.

The key to high throughput on large datasets, particularly ones with more attributes than are accessed in a single pass, is laying out the data in "columns." All values of an attribute should be contiguous on disk or memory because data are paged from one cache to the next in locally contiguous blocks. The ROOT and Parquet file formats represent JSON-like data in columns on disk, but these data are usually deserialized into objects for processing in memory. Higher performance can be achieved by maintaining the columnar structure through all stages of the calculation (see this talk and this paper).

The OAMap toolkit implements an Object Array Mapping in Python. Object Array Mappings, by analogy with Object Relational Mappings (ORMs) are one-to-one relationships between conceptual objects and physical arrays. You can write functions that appear to be operating on ordinary Python objects-- lists, tuples, class instances-- but are actually being performed on low-level, contiguous buffers (Numpy arrays). The result is fast processing of large, complex datasets with a low memory footprint.

OAMap has two primary modes: (1) pure-Python object proxies, which pretend to be Python objects but actually access array data on demand, and (2) bare-metal bytecode compiled by Numba. The pure-Python form is good for low-latency, exploratory work, while the compiled form is good for high throughput. They are seamlessly interchangeable: a Python proxy converts to the compiled form when it enters a Numba-compiled function and switches back when it leaves. You can, for instance, do a fast search in compiled code and examine the results more fully by hand.

Any columnar file format or database can be used as a data source: OAMap can get arrays of data from any dict-like object (any Python object implementing __getitem__), even from within a Numba-compiled function. Backends to ROOT, Parquet, and HDF5 are included, as well as a Python shelve alternative. Storing and accessing a complete dataset, including metadata, requires no more infrastructure than a collection of named arrays. (Data types are encoded in the names, values in the arrays.) OAMap is intended as a middleware layer above file formats and databases but below a fully integrated analysis suite.

https://github.com/diana-hep/oamap[+https://github.com/diana-hep/oamap+]

Oasis
~~~~~

Oasis is an open source finite element Navier-Stokes solver written from scratch in Python using building blocks from FEniCS. The solver is unstructured, runs with MPI and interfaces, through FEniCS, to the state-of-the-art linear algebra backend PETSc. Oasis advocates a high-level, programmable Python user interface, where the user is placed in complete control of every aspect of the solver. A version of the solver, which is using piecewise linear elements for both velocity and pressure, has been shown to more or less exactly reproduce the classical spectral turbulent channel simulations of Moser, Kim and Mansour at Re_tau=180 [Phys. Fluids, vol 11(4), p. 964]. The computational speed is dominated by the iterative solvers provided by the linear algebra backend (PETSc). That is, there is very little overhead in the high-level finite element assembly of the variational forms. Higher order accuracy is also demonstrated and new solvers may be easily added within the same framework. In addition to solving for the velocity and pressure, the user may also add any number of passive or reactive scalars to the flow field.

There are currently two solvers implemented, one for steady-state and one for transient flows. The transient solver uses the fractional step algorithm for any finite element discretization of the actual Navier Stokes equations. The steady-state solver is coupled using a mixed space for velocity and pressure. 

https://github.com/mikaem/oasis/wiki[+https://github.com/mikaem/oasis/wiki+]

https://github.com/mikaem/Oasis[+https://github.com/mikaem/Oasis+]

OBS Studio
~~~~~~~~~~

OBS is a free and open-source software suite for recording and live streaming. Written in C and Cxx, OBS provides real-time source and device capture, scene composition, encoding, recording, and broadcasting. Transmission of data is primarily done via the Real Time Messaging Protocol (RTMP) and can be sent to any RTMP supporting destination, including many presets for streaming websites such as YouTube, Twitch.tv, Instagram and Facebook.[6]

For video encoding, OBS is capable of using the x264 free software library,[7] Intel Quick Sync Video, Nvidia NVENC and the AMD Video Coding Engine to encode video streams into the H.264/MPEG-4 AVC format and the H.265/HEVC format. Audio can be encoded using either the MP3 or AAC codecs. Advanced users can choose to use any codecs and containers available in libavcodec / libavformat as well as output the stream to a custom ffmpeg URL. 

The main user interface is organized into five sections: scenes, sources, audio mixer, transitions, and controls. Scenes are groups of sources like live and recorded video, text and audio. The mixer panel lets the user mute the audio, and adjust the volume through virtual faders, and apply effects by pressing the cogwheel next to the mute button. The control panel has options for starting/stopping a stream or recording, a button to transform OBS to a more professional Studio Mode (see below), a button for opening the settings menu and a button to exit the program. The upper section has a live video preview, used to monitor and edit the current scene. The user interface can be switched to dark or light theme depending on what the user prefers.

https://github.com/obsproject/obs-studio[+https://github.com/obsproject/obs-studio+]

https://obsproject.com/[+https://obsproject.com/+]

https://opensource.com/article/19/1/basic-live-video-streaming-server[+https://opensource.com/article/19/1/basic-live-video-streaming-server+]

https://en.wikipedia.org/wiki/Open_Broadcaster_Software[+https://en.wikipedia.org/wiki/Open_Broadcaster_Software+]

OCCA
~~~~

An open source library that aims to:

* Make it easy to program different types of devices (e.g. CPU, GPU, FPGA)
* Provide a unified API for interacting with backend device APIs (e.g. OpenMP, CUDA, OpenCL)
* Use just-in-time compilation to build backend kernels
* Provide a kernel language, a minor extension to C, to abstract programming for each backend

The inability to predict lasting languages and architectures led us to develop
OCCA, a Cxx library focused on host-device interaction. Using run-time
compilation and macro expansions, the result is a novel single kernel language
that expands to multiple threading languages. Currently, OCCA supports device
kernel expansions for the xref:OpenMP[OpenMP],
xref:OpenCL[OpenCL], and xref:CUDA[CUDA] platforms. Computational
results using finite difference, spectral element and discontinuous Galerkin
methods show OCCA delivers portable high performance in different
architectures and platforms.

High-order finite-difference methods are commonly used in wave propagators for
industrial subsurface imaging algorithms. Computational aspects of the reduced
linear elastic vertical transversely isotropic propagator are considered.
Thread parallel algorithms suitable for implementing this propagator on
multi-core and many-core processing devices are introduced. Portability is
addressed through the use of the OCCA runtime programming interface. Finally,
performance results are shown for various architectures on a representative
synthetic test case.

http://arxiv.org/abs/1410.1387[+http://arxiv.org/abs/1410.1387+]

http://libocca.org/[+http://libocca.org/+]

https://github.com/libocca/occa[+https://github.com/libocca/occa+]

https://github.com/CEED/OCCA[+https://github.com/CEED/OCCA+]

Oceananigans.jl
~~~~~~~~~~~~~~~

A fast non-hydrostatic n-dimensional ocean model based on the MITgcm algorithm (Marshall et al., 1997) in Julia. The plan is to make it useful as a large eddy simulation (LES) model or as a 2D/3D super-parameterization to be embedded within a global ocean model. As an embedded model it could resolve the sub-grid scale physics and communicate their effects back to the global model or act as a source of training data for statistical learning algorithms (Campin et al., 2011).

It can be used as a general-purpose ocean model in a hydrostatic or non-hydrostatic configuration. A big aim is to have a friendly and intuitive user interface allowing users to focus on the science and not on fixing compiler errors. Thanks to high-level zero-cost abstractions in Julia we think we can make the model look and behave the same no matter the dimension or grid of the underlying simulation.

https://github.com/ali-ramadhan/Oceananigans.jl[+https://github.com/ali-ramadhan/Oceananigans.jl+]

OceanWave3D-Fortran90
~~~~~~~~~~~~~~~~~~~~~

A very efficient coastal engineering research tool used worldwide for simulation of nonlinear and dispersive free surface waves in varying batheymetries from very deep to shallow water.

https://github.com/apengsigkarup/OceanWave3D-Fortran90[+https://github.com/apengsigkarup/OceanWave3D-Fortran90+]

ocgis
~~~~~

OpenClimateGIS (OCGIS) is a Python package designed for geospatial manipulation, subsetting, computation, and translation of spatiotemporal datasets stored in local NetCDF files or files served through THREDDS data servers. OpenClimateGIS has a straightforward, request-based API that is simple to use yet complex enough to perform a variety of computational tasks. The software is built entirely from open source packages.

The GIS capabilities include:

* Subsetting (intersects and intersection) of climate datasets by bounding box, Shapely geometries, or shapefiles (city centroid, river reach, a single county or watershed, state boundaries).
* Time and level range subsetting. Also allows for arbitrary label-based slicing.
* Single or multi-dataset requests (concatenation).
* Area-weighted aggregation (spatial averaging) to selection geometries.
* Handles CF-based coordinate systems with full support for coordinate transformations (including the rotated pole coordinate system)
* Geometry wrapping and unwrapping to maintain logically consistent longitudinal domains.
* Polygon, line, and point geometric abstractions.

Data conversion capabilities include:

* Access to local NetCDF data or data hosted remotely on a THREDDS (OPeNDAP protocol) data server. Only the piece of data selected by an area-of-interest is transferred from the remote server.
* Stream climate data to multiple formats.
* Extensible converter framework to add custom formats.
* Automatic generation of request metadata.
* Push data to a familiar format to perform analysis or keep the data as NumPy arrays, perform analysis, and dump to a supported format.

Computational capabilities are:

* Extensible computational framework for arbitrary inclusion of NumPy-based calculations.
* Apply computations to entire data arrays or temporal groups.
* Computed data may be streamed to any supported formats.

https://github.com/NCPP/ocgis[+https://github.com/NCPP/ocgis+]

https://ocgis.readthedocs.io/en/latest/[+https://ocgis.readthedocs.io/en/latest/+]

oct2py
~~~~~~

Oct2Py allows you to seamlessly call M-files and Octave functions from Python. It manages the Octave session for you, sharing data behind the scenes using MAT files.  Features include:

* Supports all Octave datatypes and most Python datatypes and Numpy dtypes.
* Provides OctaveMagic for IPython, including inline plotting in notebooks.
* Supports cell arrays and structs/struct arrays with arbitrary nesting.
* Supports sparse matrices.
* Builds methods on the fly linked to Octave commands (e.g. zeros above).
* Thread-safety: each Oct2Py object uses an independent Octave session.
* Can be used as a context manager.
* Supports Unicode characters.
* Supports logging of session commands.
* Optional timeout command parameter to prevent runaway Octave sessions.

https://github.com/blink1073/oct2py[+https://github.com/blink1073/oct2py+]

OctApps
~~~~~~~

Gravitational waves are minute ripples in spacetime, first predicted by Einstein's general theory of relativity in 1916. Gravitational waves from rapidly-rotating neutron stars, whose shape deviates from perfect axisymmetry, are a potential astrophysical source of gravitational waves, but which so far have not been detected. The search for this type of signals, also known as continuous waves, presents a significant data analysis challenge, as their weak signatures are expected to be buried deep within the instrumental noise of the LIGO and Virgo detectors. The OctApps library provides various functions, written in Octave, intended to aid research scientists who perform searches for continuous gravitational waves. 

https://github.com/octapps/octapps[+https://github.com/octapps/octapps+]

https://arxiv.org/abs/1806.07442[+https://arxiv.org/abs/1806.07442+]

http://joss.theoj.org/papers/10.21105/joss.00707[+http://joss.theoj.org/papers/10.21105/joss.00707+]

Octave
~~~~~~

GNU Octave is a high-level language, primarily intended for numerical computations. It provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab. It may also be used as a batch-oriented language.

Octave has extensive tools for solving common numerical linear algebra problems, finding the roots of nonlinear equations, integrating ordinary functions, manipulating polynomials, and integrating ordinary differential and differential-algebraic equations. It is easily extensible and customizable via user-defined functions written in Octave’s own language, or using dynamically loaded modules written in Cxx, C, Fortran, or other languages.

The Octave language is an interpreted programming language. It is a structured programming language (similar to C) and supports many common C standard library functions, and also certain UNIX system calls and functions.[15] However, it does not support passing arguments by reference.[16]

Octave programs consist of a list of function calls or a script. The syntax is matrix-based and provides various functions for matrix operations. It supports various data structures and allows object-oriented programming.[17]

Its syntax is very similar to MATLAB, and careful programming of a script will allow it to run on both Octave and MATLAB.

Octave also has packages available for free. Those packages are located at Octave-Forge.

https://www.gnu.org/software/octave/[+https://www.gnu.org/software/octave/+]

https://octave.sourceforge.io/[+https://octave.sourceforge.io/+]

https://en.wikipedia.org/wiki/GNU_Octave[+https://en.wikipedia.org/wiki/GNU_Octave+]

ODEPACK
~~~~~~~

ODEPACK is a collection of Fortran solvers for the initial value
problem for ordinary differential equation systems.  It consists of nine
solvers, namely a basic solver called LSODE and eight variants of it --
LSODES, LSODA, LSODAR, LSODPK, LSODKR, LSODI, LSOIBT, and LSODIS.
The collection is suitable for both stiff and nonstiff systems.  It
includes solvers for systems given in explicit form, dy/dt = f(t,y),
and also solvers for systems given in linearly implicit form, 
A(t,y) dy/dt = g(t,y).  Two of the solvers use general sparse matrix
solvers for the linear systems that arise.  Two others use iterative
(preconditioned Krylov) methods instead of direct methods for these
linear systems.  The most recent addition is LSODIS, which solves
implicit problems with general sparse treatment of all matrices involved.

The ODEPACK solvers are written in standard Fortran 77, with a few
exceptions, and with minimal machine dependencies.  There are separate
double and single precision versions of ODEPACK.  The actual solver
names are those given above with a prefix of D- or S- for the double
or single precision version, respectively, i.e. DLSODE/SLSODE, etc.
Each solver consists of a main driver subroutine having the same name
as the solver and some number of subordinate routines.  For each
solver, there is also a demonstration program, which solves one or two
simple problems in a somewhat self-checking manner.

https://computation.llnl.gov/casc/odepack/[+https://computation.llnl.gov/casc/odepack/+]

https://people.sc.fsu.edu/\~jburkardt/f77_src/odepack/odepack.html[+https://people.sc.fsu.edu/~jburkardt/f77_src/odepack/odepack.html+]

https://www.netlib.org/odepack/[+https://www.netlib.org/odepack/+]

odespy
~~~~~~

Odespy (ODE Software in Python) offers a unified interface to a large collection of software for solving systems of ordinary differential equations (ODEs). There is also some support for Differential Algebraic Equations (DAEs).

Odespy features the following collection of numerical methods and implementations:

* Pure Python implementations of classical explicit schemes such as the Forward Euler method (also called Euler); Runge-Kutta methods of 2nd, 3rd, and 4th order; Heun's method; Adams-Bashforth methods of 2nd, 3rd, and 4th order; Adams-Bashforth-Moulton methods of 2nd and 3rd order.
* Pure Python implementations of classical implicit schemes such as Backward Euler; 2-step backward scheme; the theta rule; the Midpoint (or Trapezoidal) method.
* Pure Python implementations of adaptive explicit Runge-Kutta methods of type Runge-Kutta-Fehlberg of order (4,5), Dormand-Prince of order (4,5), Cash-Karp of order (4,5), Bogacki-Shampine of order (2,3).
* Wrappers for all FORTRAN solvers in ODEPACK.
* Wrappers for the wrappers of FORTRAN solvers in scipy: vode and zvode (adaptive Adams or BDF from vode.f); dopri5 (adaptive Dormand-Prince method of order (4,5)); dop853 (adaptive Dormand-Prince method of order 8(5,3)); odeint (adaptive Adams or BDF, basically the same as vode, but in the implementation lsoda from ODEPACK).
* Wrapper for the Runge-Kutta-Chebyshev formulas of order 2 as offered by the well-known FORTRAN code rkc.f.
* Wrapper for the Runge-Kutta-Fehlberg method of order (4,5) as provided by the well-known FORTRAN code rkf45.f.
* Wrapper for the Radau5 method as provided by the well-known FORTRAN code radau5.f. There have been some unidentified problems with running this solver (segmentation fault).
* Wrapper for some solvers in the odelab.

The ODE problem can always be specified in Python, but for wrappers of FORTRAN codes one can also implement the problem in FORTRAN and avoid callback to Python.

https://github.com/hplgit/odespy[+https://github.com/hplgit/odespy+]

https://github.com/olivierverdier/odelab[+https://github.com/olivierverdier/odelab+]

Odo
~~~

Odo takes two arguments, a source and a target for a data transfer.
It efficiently migrates data from the source to the target through a network of conversions.

Odo migrates data using network of small data conversion functions between type pairs.
Each node is a container type (like pandas.DataFrame or sqlalchemy.Table) and each directed edge is a function that transforms or appends one container into or onto another. We annotate these functions/edges with relative costs.

This network approach allows odo to select the shortest path between any two types (thank you networkx). For performance reasons these functions often leverage non-Pythonic systems like NumPy arrays or native CSV->SQL loading functions. Odo is not dependent on only Python iterators.

This network approach is also robust. When libraries go missing or runtime errors occur odo can work around these holes and find new paths.

https://github.com/blaze/odo[+https://github.com/blaze/odo+]

https://odo.readthedocs.io/en/latest/[+https://odo.readthedocs.io/en/latest/+]

Omegalib
~~~~~~~~

A framework for virtual reality and cluster-driven display systems.

http://uic-evl.github.io/omegalib/[+http://uic-evl.github.io/omegalib/+]

https://github.com/uic-evl/omegalib[+https://github.com/uic-evl/omegalib+]

Omni Compiler
~~~~~~~~~~~~~

Omni compiler is a collection of programs and libraries that allow users to build code transformation compilers. Omni Compiler is to translate C and Fortran programs with XcalableMP and OpenACC directives into parallel code suitable for compiling with a native compiler linked with the Omni Compiler runtime library. Moreover, Omni Compiler supports XcalableACC programming model for accelerated cluster systems. The Omni compiler project is proceeded by Programming Environment Research Team of RIKEN Center for Computational Science and HPCS Lab. of University of Tsukuba, Japan.

Omni compiler consists of following components. 

* XcalableMP is a directive-based language extension of C and Fortran for distributed memory systems. XcalableMP allows users to develop a parallel application and to tune its performance with minimal and simple notation. 

* OpenACC is a directive-based programming interface for accelerators such as GPGPU. OpenACC allows users to express the offloading of data and computations to accelerators to simplify the porting process for legacy CPU-based applications.

* XcalableACC is a hybrid model of XcalableMP and OpenACC. XcalableACC defines directives that enable programmers to mix XMP and OpenACC directives to develop applications on accelerated cluster systems.

* XcodeML is an intermediate code written in XML for C and Fortran languages.

http://omni-compiler.org/[+http://omni-compiler.org/+]

https://github.com/omni-compiler/omni-compiler[+https://github.com/omni-compiler/omni-compiler+]

OnEarth
~~~~~~~

OnEarth is a software package consisting of image formatting and serving modules which facilitate the deployment of a web service capable of efficiently serving standards-based requests for georeferenced raster imagery at multiple spatial resolutions including, but not limited to, full spatial resolution. The software was originally developed at the Jet Propulsion Laboratory (JPL) to serve global daily composites of MODIS imagery. Since then, it has been deployed and repurposed in other installations, including at the Physical Oceanography Distributed Active Archive Center (PO.DAAC) in support of the State of the Oceans (SOTO) visualization tool, the Lunar Mapping and Modeling Project (LMMP), and Worldview.

The source code contains the mod_onearth Apache module (formerly Tiled WMS/KML server), a Meta Raster Format (MRF) imagery generator, a legend generator, and server configuration tools.

https://github.com/nasa-gibs/onearth[+https://github.com/nasa-gibs/onearth+]

https://earthdata.nasa.gov/about/science-system-description/eosdis-components/global-imagery-browse-services-gibs[+https://earthdata.nasa.gov/about/science-system-description/eosdis-components/global-imagery-browse-services-gibs+]

OnEarth-Boxes is a tool that can build a virtual machine with demo imagery and pre-configured endpoints for demos, development, and getting started with MRF and OnEarth. Multiple VM image formats are supported.

https://github.com/nasa-gibs/onearth-boxes[+https://github.com/nasa-gibs/onearth-boxes+]

ontop
~~~~~

Ontop is a framework for ontology based data access (OBDA). It supports SPARQL over virtual RDF graphs defined through mappings to RDBMS.
The features include:

* SPARQL 1.0 Support
* Intuitive/powerful mapping language
* Support for free and commercial DBMS
* Compatible with DBMS federation tools
* Providers for Sesame and OWLAPI
* Integrated with Protégé
* SPARQL end-point
* the SPARQL engine supports OWL 2 QL and RDFS
* implements the latest generation query rewriting techniques to translate your SPARQL into SQL

http://ontop.inf.unibz.it/[+http://ontop.inf.unibz.it/+]

https://github.com/ontop/ontop[+https://github.com/ontop/ontop+]

oocgcm
~~~~~~

This project provides tools for processing and analysing output of general circulation models and gridded satellite data in the field of Earth system science.

Our aim is to simplify the analysis of very large datasets of model output (~1-100Tb) like those produced by basin-to-global scale sub-mesoscale permitting ocean models and ensemble simulations of eddying ocean models by leveraging the potential of xarray and dask python packages.

The main ambition of this project is to provide simple tools for performing out-of-core computations with model output and gridded data, namely processing data that is too large to fit into memory at one time.

The project is so far mostly targeting NEMO ocean model and gridded ocean satellite data (AVISO, SST, ocean color...) but our aim is to build a framework that can be used for a variety of models based on the Arakawa C-grid. The framework can in principle also be used for atmospheric general circulation models.

We are trying to develop a framework flexible enough in order not to impose too strictly a specific workflow to the end user.

oocgcm is a pure Python package and we try to keep the list of dependencies as small as possible in order to simplify the deployment on a number of platforms.

oocgcm is not intended to provide advanced visualization functionalities for gridded geographical data as several powerful tools already exist in the python ecosystem (see in particular cartopy and basemap).

We rather focus on building a framework that simplifies the design and production of advanced statistical and dynamical analyses of large datasets of model output and gridded data.

https://oocgcm.readthedocs.io/en/latest/[+https://oocgcm.readthedocs.io/en/latest/+]

https://github.com/lesommer/oocgcm[+https://github.com/lesommer/oocgcm+]

OpenACC
~~~~~~~

OpenACC (for open accelerators) is a programming standard for parallel computing developed by Cray, CAPS, Nvidia and PGI. The standard is designed to simplify parallel programming of heterogeneous CPU/GPU systems.

As in OpenMP, the programmer can annotate C, Cxx and Fortran source code to identify the areas that should be accelerated using compiler directives and additional functions.[2] Like OpenMP 4.0 and newer, OpenACC can target both the CPU and GPU architectures and launch computational code on them. 

https://en.wikipedia.org/wiki/OpenACC[+https://en.wikipedia.org/wiki/OpenACC+]

https://www.openacc.org/[+https://www.openacc.org/+]

OpenACC on GCC
^^^^^^^^^^^^^^

This page contains information on GCC's implementation of the OpenACC specification and related functionality. OpenACC is intended for programming accelerator devices such as GPUs, including code offloading to these devices. Currently devices using Nvidia PTX (nvptx) are supported, and AMD GCN device support is in development.

The GCC 7, and 8 release series are currently supported by the GCC community, and implement most of the OpenACC 2.0a specification. The upcoming GCC 9 implements most of the OpenACC 2.5 specification, and contains performance optimizations. The OpenACC development branch (see below) implements most of the OpenACC 2.6 specification, and contains further improvements. 

https://gcc.gnu.org/wiki/OpenACC[+https://gcc.gnu.org/wiki/OpenACC+]

https://fosdem.org/2019/schedule/event/openacc/[+https://fosdem.org/2019/schedule/event/openacc/+]

Open Build Service
~~~~~~~~~~~~~~~~~~

The Open Build Service (OBS) is a generic system to build and distribute binary packages from sources in an automatic, consistent and reproducible way. You can release packages as well as updates, add-ons, appliances and entire distributions for a wide range of operating systems and hardware architectures. More information can be found on openbuildservice.org.

The OBS consists of a backend and a frontend. The backend implements all the core functionality (i.e. building packages). The frontend provides a web application and XML API for interacting with the backend. Additionally there is a command line client (osc) for the API.

https://openbuildservice.org/[+https://openbuildservice.org/+]

https://github.com/openSUSE/open-build-service[+https://github.com/openSUSE/open-build-service+]

https://github.com/openSUSE/osc[+https://github.com/openSUSE/osc+]

OpenCL
~~~~~~

Blah.

clBLAS
^^^^^^

This library provides an implementation of the Basic Linear Algebra Subprograms levels 1, 2 and 3, using OpenCL and optimized for AMD GPU hardware. It provides BLAS-1 functions SWAP, SCAL, COPY, AXPY, DOT, DOTU, DOTC, ROTG, ROTMG, ROT, ROTM, iAMAX, ASUM and NRM2, BLAS-2 functions GEMV, SYMV, TRMV, TRSV, HEMV, SYR, SYR2, HER, HER2, GER, GERU, GERC, TPMV, SPMV, HPMV, TPSV, SPR, SPR2, HPR, HPR2, GBMV, TBMV, SBMV, HBMV and TBSV and BLAS-3 functions GEMM, SYMM, TRMM, TRSM, HEMM, HERK, HER2K, SYRK and SYR2K.

This library’s primary goal is to assist the end user to enqueue OpenCL kernels to process BLAS functions in an OpenCL-efficient manner, while keeping interfaces familiar to users who know how to use BLAS. All functions accept matrices through buffer objects.

http://clmathlibraries.github.io/clBLAS/[+http://clmathlibraries.github.io/clBLAS/+]

https://github.com/clMathLibraries/clBLAS[+https://github.com/clMathLibraries/clBLAS+]

CLBlast
^^^^^^^

CLBlast is a modern, lightweight, performant and tunable OpenCL BLAS library written in Cxx11. It is designed to leverage the full performance potential of a wide variety of OpenCL devices from different vendors, including desktop and laptop GPUs, embedded GPUs, and other accelerators. CLBlast implements BLAS routines: basic linear algebra subprograms operating on vectors and matrices.

https://github.com/CNugteren/CLBlast[+https://github.com/CNugteren/CLBlast+]

clFFT
^^^^^

clFFT is a software library containing FFT functions written in OpenCL. In addition to GPU devices, the library also supports running on CPU devices to facilitate debugging and heterogeneous programming.

https://github.com/clMathLibraries/clFFT[+https://github.com/clMathLibraries/clFFT+]

gpyfft
^^^^^^

A Python wrapper for the OpenCL FFT library clFFT.
This python wrapper is designed to tightly integrate with PyOpenCL.

https://github.com/geggo/gpyfft[+https://github.com/geggo/gpyfft+]

PyCLBLAS
^^^^^^^^

PyCLBLAS is a wrapper for the clBLAS library. This module can be used to call BLAS routines on OpenCL enabled devices from Python.

https://github.com/jroose/pyclblas[+https://github.com/jroose/pyclblas+]

https://pyclblas.readthedocs.io/en/latest/index.html[+https://pyclblas.readthedocs.io/en/latest/index.html+]

https://github.com/clMathLibraries/clBLAS[+https://github.com/clMathLibraries/clBLAS+]

PyCLBlast
^^^^^^^^^

This Python package provides a straightforward wrapper for CLBast based on PyOpenCL. CLBlast is a modern, lightweight, performant and tunable OpenCL BLAS library written in Cxx11. It is designed to leverage the full performance potential of a wide variety of OpenCL devices from different vendors, including desktop and laptop GPUs, embedded GPUs, and other accelerators. CLBlast implements BLAS routines: basic linear algebra subprograms operating on vectors and matrices.

https://github.com/CNugteren/CLBlast/tree/master/src/pyclblast[+https://github.com/CNugteren/CLBlast/tree/master/src/pyclblast+]

PyOpenCL
^^^^^^^^

PyOpenCL gives you easy, Pythonic access to the OpenCL parallel computation API.
The features include:

* Object cleanup tied to lifetime of objects. This idiom, often called RAII in Cxx, makes it much easier to write correct, leak- and crash-free code.
* Completeness. PyOpenCL puts the full power of OpenCL’s API at your disposal, if you wish. Every obscure get_info() query and all CL calls are accessible.
* Automatic Error Checking. All errors are automatically translated into Python exceptions.
* Speed. PyOpenCL’s base layer is written in Cxx, so all the niceties above are virtually free.
* Helpful Documentation.

https://documen.tician.de/pyopencl/[+https://documen.tician.de/pyopencl/+]

https://github.com/inducer/pyopencl[+https://github.com/inducer/pyopencl+]

Open Compute Stack
~~~~~~~~~~~~~~~~~~

Open Compute Stack (OpenCS) is a framework for modelling of large scale ODE/DAE systems, parallel evaluation of model equations and parallel simulations on shared and distributed memory systems.

The framework provides a platform-independent binary interface for model-exchange with the data structures to describe, store in computer memory and evaluate large scale ODE/DAE systems of equations. This approach differs from the typical model-exchange/co-simulation interfaces in that it does not require a human or a machine readable model definition as in modelling and model-exchange languages (i.e. Modelica, gPROMS and CellML) nor a binary interface (C API) implemented in shared libraries (i.e. Simulink and Functional Mock-up Interface). For instance, in the OpenCS framework model equations are specified in the Reverse Polish (postfix) notation as an array of binary data (a Compute Stack) for direct evaluation by simulators on all platforms/operating systems (including heterogeneous systems) with no additional processing nor compilation steps. Therefore, the same model-specification can be used on any computing platform. 

http://www.daetools.com/opencs-introduction.html[+http://www.daetools.com/opencs-introduction.html+]

https://peerj.com/articles/cs-160/[+https://peerj.com/articles/cs-160/+]

OpenCPN
~~~~~~~

OpenCPN (Open Chart Plotter Navigator) is a free software project to create a concise chart plotter and navigation software, for use underway or as a planning tool. OpenCPN is developed by a team of active sailors using real world conditions for program testing and refinement.

OpenCPN uses GPS input data to determine the ship's own position and data from an AIS receiver to plot the positions of ships in the neighborhood. 

https://github.com/OpenCPN/OpenCPN[+https://github.com/OpenCPN/OpenCPN+]

https://opencpn.org/[+https://opencpn.org/+]

Open Climate Workbench
~~~~~~~~~~~~~~~~~~~~~~

Apache Open Climate Workbench is an effort to develop software that performs climate model evaluation using model outputs from a variety of different sources the Earth System Grid Federation, the Coordinated Regional Climate Downscaling Experiment, the U.S. National Climate Assessment and the North American Regional Climate Change Assessment Program and temporal/spatial scales with remote sensing data from NASA, NOAA and other agencies. The toolkit includes capabilities for rebinning, metrics computation and visualization. 

OCW consists of a Python library for common model evaluation tasks (e.g. area averaging, regridding, bias calculation) as well as a set of user-friendly interfaces for quickly configuring a large-scale regional model evaluation task.

Users can interact with the OCW either by including the Python library directly in their code, or by way of the flexible RESTful Application Programmer Interface (API).

The REST API, also implemented in Python, allows OCW users to integrate the capabilities of the toolkit into their workflow regardless of language and environment, provided they have the ability to make HTTP requests.

https://climate.apache.org/[+https://climate.apache.org/+]

https://rcmes.jpl.nasa.gov/[+https://rcmes.jpl.nasa.gov/+]

https://github.com/apache/climate[+https://github.com/apache/climate+]

https://www.geosci-model-dev.net/11/4435/2018/[+https://www.geosci-model-dev.net/11/4435/2018/+]

OpenCMISS
^^^^^^^^^

OpenCMISS is a set of libraries and applications which provide modelling and visualisation capabilities for complex bioengineering problems.

http://opencmiss.org/[+http://opencmiss.org/+]

https://github.com/OpenCMISS[+https://github.com/OpenCMISS+]

Iron
^^^^

OpenCMISS (Open Continuum Mechanics, Imaging, Signal processing and System identification) is a mathematical modelling environment that enables the application of finite element analysis techniques to a variety of complex bioengineering problems.

The project represents a complete rewrite and overhaul of the existing CMISS computational modelling tool to take advantage of modern programming languages, data structures, and today’s range of available high performance hardware.

This significant re-engineering effort represents a complete upgrade in functionality and modelling capability, particularly in terms of increased ability to optimise simulation performance on high performance, and in particular distributed, architectures.

http://opencmiss.org/documentation/apidoc/iron/latest/fortran/index.html[+http://opencmiss.org/documentation/apidoc/iron/latest/fortran/index.html+]

http://opencmiss.org/documentation/apidoc/iron/latest/python/index.html[+http://opencmiss.org/documentation/apidoc/iron/latest/python/index.html+]

Zinc
^^^^

OpenCMISS-Zinc (‘Zinc’) is a cross-platform software library for building complete modelling and visualisation applications, from rich model representation to high quality OpenGL graphics rendering.

Models are represented in Zinc as mathematical fields defined over domains, including finite elements with support for high-order basis functions, complex parameter mappings and time variation, as well as image-based fields. Further fields can be defined by mathematical expressions and algorithms on existing fields, including image processing filters. Zinc’s model data structures are dynamic, supporting interactive applications which programmatically create, destroy and modify content.

In Zinc, visualisations of models are created by graphics algorithms which assign fields to attributes, including 3-D coordinates, texture coordinates, data/colouring, and specific attributes such as iso-scalar field for contours, vector field for streamlines, and orientation, scale and label fields for points. Graphics are automatically updated following changes to attributes or the underlying model. The graphics approach combined with the general field expression capability means almost any derived result can be visualised.

Zinc graphics are rendered using OpenGL into the client window or canvas, and built-in picking, selection groups and automatic highlighting support the easy development of interactive applications. The Zinc library is UI-independent requiring additional client code to set up and handle rendering and events, as described in the documentation. Zinc also includes utilities such as non-linear optimisation.

https://github.com/OpenCMISS/zinc[+https://github.com/OpenCMISS/zinc+]

http://opencmiss.org/documentation/zinc/index.html[+http://opencmiss.org/documentation/zinc/index.html+]

http://opencmiss.org/about.html[+http://opencmiss.org/about.html+]

OpenDA
~~~~~~

OpenDA is an open interface standard for (and free implementation of) a set of tools to quickly implement data-assimilation and calibration for arbitrary numerical models. OpenDA wants to stimulate the use of data-assimilation and calibration by lowering the implementation costs and enhancing the exchange of software among researchers and end-users. A model that conforms to the OpenDA standard can use all the tools that are available in OpenDA. This allows experimentation with data-assimilation/calibration methods without the need for extensive programming. Reversely, developers of data-assimilation/calibration software that make their implementations compatible with the OpenDA interface will make their new methods usable for all OpenDA users (either for free or on a commercial basis). OpenDA has been designed for high performance. Hence, even large-scale models can use it. Also, OpenDA allows users to optimize the interaction between their model and the data-assimilation/calibration methods. Hence, data-assimilation with OpenDA can be as efficient as with custom-made implementations of data-assimilation methods. OpenDA is an Open Source project. Contributions are welcome from anyone wishing to participate in the further development of the OpenDA toolset.

The available data assimilation methods are:

* Ensemble KF (EnKF)
* Ensemble SquareRoot KF (EnSR)
* Steady State KF
* Particle Filter
* 3DVar
* DudEnKF (still under research)
* DudEnSR (still under research)

The available parameter estimation methods are:

* Dud
* Sparse Dud
* Simplex
* Powell
* Gridded full search
* Shuffled Comples Evolution (SCE)
* Generalized Likelihood Uncertainty Estimation (GLUE)
* (L)BFGS
* Conjugate Gradient: Fleetjer-Reeves, Polak-Ribiere, Steepest Descent
* Uncertainty Analysis methods
* GLUE
* DELSA

There are language interfaces for C/Cxx, Java and Fortran 77/90.

https://github.com/OpenDA-Association/OpenDA[+https://github.com/OpenDA-Association/OpenDA+]

http://www.openda.org/[+http://www.openda.org/+] 

OPeNDAP
~~~~~~~

OPeNDAP is an acronym for "Open-source Project for a Network Data Access Protocol," an endeavor focused on enhancing the retrieval of remote, structured data through a Web-based architecture and a discipline-neutral Data Access Protocol (DAP). Widely used, especially in Earth science, the protocol is layered on HTTP, and its current specification is DAP4, though the previous DAP2 version remains broadly used. Developed and advanced (openly and collaboratively) by the non-profit OPeNDAP, Inc., DAP is intended to enable remote, selective data-retrieval as an easily invoked Web service. OPeNDAP, Inc. also develops and maintains zero-cost (reference) implementations of the DAP protocol in both server-side and client-side software.

"OPeNDAP" often is used in place of "DAP" to denote the protocol but also may refer to an entire DAP-based data-retrieval architecture. Other DAP-centered architectures, such as THREDDS and ERDDAP, the NOAA GEO-IDE UAF ERDDAP exhibit significant interoperability with one another as well as with systems employing OPeNDAP's own (open-source) servers and software. 

A DAP client can be an ordinary browser or even a spreadsheet, though with limited functionality (see OPeNDAP's Web page on Available Client Software). More typically, DAP clients are:

* Data-analysis or data-visualization tools (such as MATLAB, IDL, Panoply, GrADS, Integrated Data Viewer, Ferret and ncBrowse) which their authors have adapted to enable DAP-based data input;
* Similarly adapted Web applications (such as Dapper Data Viewer, aka DChart)
* Similarly adapted end-user programs (in common languages)

Regardless of their types, and whether developed commercially or by an end-user, clients almost universally link to DAP servers through libraries that implement the DAP2 or DAP4 protocol in one language or another. OPeNDAP offers open-source libraries in Cxx and Java, but many clients rely on community developed libraries such as PyDAP or, especially, the NetCDF suite. Developed and maintained by the Unidata Program at the UCAR in multiple programming languages, all NetCDF libraries include embedded capabilities for retrieving (array-style) data from DAP servers. 

OPeNDAP's software for building DAP servers (on top of Apache) is dubbed Hyrax and includes adapters that facilitate serving a wide variety of source data. DAP servers most frequently enable (remote) access to (large) HDF or NetCDF files, but the source data can exist in databases or other formats, including user-defined ones. When source data are organized as files, DAP retrievals enable, via subsetting, finer-grained access than does the FTP. Furthermore, OPeNDAP servers can aggregate subsets from multiple files for delivery in a single retrieval. Taken together, subsetting, aggregation and streaming can yield substantial data-access efficiencies, even in the presence of slow networks. 

https://www.opendap.org/[+https://www.opendap.org/+]

pydap
^^^^^

Pydap is a pure Python library implementing the Data Access Protocol, also known as DODS or OPeNDAP. You can use Pydap as a client to access hundreds of scientific datasets in a transparent and efficient way through the internet; or as a server to easily distribute your data from a variety of formats.

http://pydap.org/en/latest/[+http://pydap.org/en/latest/+]

https://github.com/pydap/pydap[+https://github.com/pydap/pydap+]

OpenDrift
~~~~~~~~~

Open source framework for ocean trajectory modelling.
OpenDrift is a software for modeling the trajectories and fate of objects or substances drifting in the ocean, or even in the atmosphere.

OpenDrift is open source, and is programmed in Python. As the software is very generic, it is rather a "framework" than a "trajectory model" in the traditional sense. Trajectory models for specific purposes (e.g. oil drift, search and rescue, larvae drift etc) may reuse all common functionality from the core model, and need only implement a Python Class describing the purpose-specific processes (physics/biology etc). 

https://github.com/opendrift/opendrift/wiki[+https://github.com/opendrift/opendrift/wiki+]

https://github.com/OpenDrift/opendrift[+https://github.com/OpenDrift/opendrift+]

https://www.geosci-model-dev.net/11/1405/2018/[+https://www.geosci-model-dev.net/11/1405/2018/+]

https://www.ocean-sci.net/14/1581/2018/[+https://www.ocean-sci.net/14/1581/2018/+]

openEMS
~~~~~~~

Welcome! openEMS is a free and open electromagnetic field solver using the FDTD method. Matlab or Octave are used as an easy and flexible scripting interface.  The features include:

* Efficient EC-FDTD method in full 3D cartesian coordinates (x,y,z)
* Efficient EC-FDTD method in full 3D cylindrical coordinates (ρ, φ, z)
* Fully graded mesh
* Multi-threading, SIMD (SSE) and MPI support for high speed parallel FDTD
* Simple engine extensions API to easily introduce new features to the FDTD algorithm
8 Matlab/Octave interface
* Absorbing boundary conditions (MUR, PML)
* Coordinate dependent material definitions
* Coordinate dependent excitation definitions (e.g. mode-profiles)
* Dispersive material (Drude/Lorentz/Debye type)
* Field dumps in time and frequency domain as vtk or hdf5 file format
* Flexible post-processing routines (mostly in Matlab)
* Subgrids to reduce simulation time in cylindrical coordinates
* Remote simulations using SSH (Linux only)

The components are:

* OpenEMS - A free and open source EC-FDTD solver.
* CSXCAD - A Cxx library to describe geometrical objects and their physical or non-physical properties.
* AppCSXCAD - A visualization program, capable of showing the structure defined inside a CSXCAD xml-file.
* QCSXCAD - The visualisation library used by AppCSXCAD.

http://openems.de/start/[+http://openems.de/start/+]

https://fosdem.org/2019/schedule/event/openems/[+https://fosdem.org/2019/schedule/event/openems/+]

https://github.com/dlharmon/pyopenems[+https://github.com/dlharmon/pyopenems+]

https://fosdem.org/2019/schedule/event/openems/[+https://fosdem.org/2019/schedule/event/openems/+]

OpenEO
~~~~~~

Earth Observation data are becoming too large to be downloaded locally for analysis. Also, the way they are organised (as tiles, or granules: files containing the imagery for a small part of the Earth and a single observation date) makes it unnecessary complicated to analyse them. The solution to this is to store these data in the cloud, on compute back-ends, process them there, and browse the results or download resulting figures or numbers. But how do we do that?

The aim of openEO is to develop an open API to connect R, python, javascript and other clients to big Earth observation cloud back-ends in a simple and unified way.

http://openeo.org/[+http://openeo.org/+]

https://github.com/Open-EO[+https://github.com/Open-EO+]

OpenFOAM
~~~~~~~~

OpenFOAM (for "Open source Field Operation And Manipulation") is a Cxx toolbox for the development of customized numerical solvers, and pre-/post-processing utilities for the solution of continuum mechanics problems, including computational fluid dynamics (CFD). The code is released as free and open-source software under the GNU General Public License.

OpenFOAM is constituted by a large base library, which offers the core capabilities of the code:

* Tensor and field operations
* Discretization of partial differential equations using a human-readable syntax
* Solution of linear systems[16]
* Solution of ordinary differential equations[17]
* Automatic parallelization of high-level operations
* Dynamic mesh[18]
* General physical models
** Rheological models[19]
** Thermodynamic models and database[20]
** Turbulence models[21]
** Chemical reaction and kinetics models[22]
** Lagrangian particle tracking methods[23]
** Radiative heat transfer models
** Multi-reference frame and single-reference frame methodologies

The capabilities provided by the library are then used to develop applications. Applications are written using the high-level syntax introduced by OpenFOAM, which aims at reproducing the conventional mathematical notation. Two categories of applications exist:

* Solvers: they perform the actual calculation to solve a specific continuum mechanics problem.
* Utilities: they are used to prepare the mesh, set-up the simulation case, process the results, and to perform operations other than solving the problem under examination.

Each application provides specific capabilities: for example, the application called blockMesh is used to generate meshes from an input file provided by the user, while another application called icoFoam solves the Navier–Stokes equations for an incompressible laminar flow.

Finally, a set of third-party packages are used to provide parallel functionality (OpenMPI) and graphical post-processing (ParaView). 

https://en.wikipedia.org/wiki/OpenFOAM[+https://en.wikipedia.org/wiki/OpenFOAM+]

https://openfoam.org/[+https://openfoam.org/+]

https://github.com/OpenFOAM[+https://github.com/OpenFOAM+]

AeroSolved
^^^^^^^^^^

AeroSolved is a Computational Fluid Dynamics code, based on the OpenFOAM software package, for simulation of the generation, transport, evolution and deposition of multispecies aerosol mixtures. 

https://github.com/pmpsa-cfd/aerosolved[+https://github.com/pmpsa-cfd/aerosolved+]

https://www.intervals.science/resources/aerosolved[+https://www.intervals.science/resources/aerosolved+]

AMMM
^^^^

A repository for the codes used in the Adaptive Moving Mesh Methods NERC Project. This repo is a fork of AtmosFOAM.

https://github.com/AtmosFOAM/AMMM[+https://github.com/AtmosFOAM/AMMM+]

AtmosFOAM
^^^^^^^^^

A collection of OpenFOAM computational fluid dynamics applications and libraries for performing atmospheric experiments. Includes mesh generators, scalar transport and Navier-Stokes solvers, and post-processing and visualisation tools.

https://github.com/AtmosFOAM/AtmosFOAM[+https://github.com/AtmosFOAM/AtmosFOAM+]

Butterfly
^^^^^^^^^

A light python API for creating and running OpenFoam cases for CFD simulation.

https://github.com/ladybug-tools/butterfly[+https://github.com/ladybug-tools/butterfly+]

https://github.com/ladybug-tools/butterfly/wiki[+https://github.com/ladybug-tools/butterfly/wiki+]

CFDEM
^^^^^

CFDEM®coupling provides an open source parallel coupled CFD-DEM framework combining the strengths of LIGGGHTS® DEM code and the Open Source CFD package OpenFOAM®(*). The CFDEM®coupling toolbox allows to expand standard CFD solvers of OpenFOAM®(*) to include a coupling to the DEM code LIGGGHTS®. In this toolbox the particle representation within the CFD solver is organized by “cloud” classes. Key functionalities are organised in sub-models (e.g. force models, data exchange models, etc.) which can easily be selected and combined by dictionary settings.

https://www.cfdem.com/media/CFDEM/docu/CFDEMcoupling_Manual.html[+https://www.cfdem.com/media/CFDEM/docu/CFDEMcoupling_Manual.html+]

Cfd
^^^

An advanced open source CAD integrated CFD preprocessing tool to enable automated one-stop CFD simulation.

https://github.com/qingfengxia/Cfd[+https://github.com/qingfengxia/Cfd+]

CfdOF
^^^^^

This workbench aims to help users set up and run CFD analyses within the FreeCAD modeller. It guides the user in selecting the relevant physics, specifying the material properties, generating a mesh, assigning boundary conditions and setting the solver settings before running the simulation. Where possible, best practices are included to improve the stability of the solvers.

The workbench serves as a front-end to the popular OpenFOAM® CFD toolkit.

https://github.com/jaheyns/CfdOF[+https://github.com/jaheyns/CfdOF+]

cpppo
^^^^^

CPPPO stands for “Compilation of Fluid-Particle Post Processing”, and is a publicly available library to analyze simulation data on the fly, e.g., spatially average (i.e., "filter") fluid velocity fields, draw samples of forces on particle, and compute running averages (i.e., "bin the data"). For example, CPPPO can compute filtered velocities fields and can computed advanced statistical information from this filtered data (e.g., filtered drag coefficients).

CPPPO is available via https://github.com/CFDEMproject, and is designed as an add-on to OpenFOAM and/or CFDEM-based simulations. Specifically, CPPPO is designed as a 'core' library, with various interface libraries that are meant to be instantiated in your simulation software.

https://github.com/CFDEMproject/C3PO-PUBLIC[+https://github.com/CFDEMproject/C3PO-PUBLIC+]

https://www.tugraz.at/institute/ippt/downloads-software/[+https://www.tugraz.at/institute/ippt/downloads-software/+]

dugksFoam
^^^^^^^^^

An OpenFOAM solver for Boltzmann model equation using discrete unified gas kinetic scheme.

https://github.com/zhulianhua/dugksFoam[+https://github.com/zhulianhua/dugksFoam+]

explicitSolidDynamics
^^^^^^^^^^^^^^^^^^^^^

This novel toolkit is based on a cell centred Finite Volume Method to predict large deformation in solids. The governing equations comprise of first order hyperbolic conservation laws for linear momentum and deformation gradient tensor of the system. This helps to bridge the gap between Computational Fluid Dynamics and Computational Solid Dynamics. The governing equations are spatially discretised using a low order cell centred Finite Volume Method along with an upwind Riemann solver.  This is a parallelised Cxx implementation within OpenFOAM code.

https://github.com/jibranhaider/explicitSolidDynamics[+https://github.com/jibranhaider/explicitSolidDynamics+]

edcSMOKE
^^^^^^^^

EDC (Eddy Dissipation Concept) for OpenFOAM based on the OpenSMOKExx Library.

https://github.com/acuoci/edcSMOKE[+https://github.com/acuoci/edcSMOKE+]

fluidfoam
^^^^^^^^^

The fluidfoam package provides Python classes useful to perform some plot with OpenFoam data.

https://bitbucket.org/sedfoam/fluidfoam[+https://bitbucket.org/sedfoam/fluidfoam+]

ITHACA-FEV
~~~~~~~~~~

ITHACA-FV is an implementation in OpenFOAM of several reduced order modelling techniques. ITHACA-FV is designed for OpenFOAM 6.0 and OpenFOAM 5.0 but it can be easily adapted also to other versions of OpenFOAM.

ITHACA-FV can also be used as a basis for more advanced projects that would like to assess the capability of reduced order models in their existing OpenFOAM-based software, thanks to the availability of several reduced order methods and algorithms.

Linear and non-linear algebra operations which are not already implemented in OpenFOAM are performed with the external library Eigen. The source code of Eigen 3.3.4 is provided together with ITHACA-FV and is located in the src/thirdyparty/Eigen folder. For the EigenValue decomposition it is also possible to rely on the Spectra-0.6.1 library and the source code is provided in the src/thirdyparty/spectra-0.6.1 folder.

https://github.com/mathLab/ITHACA-FV[+https://github.com/mathLab/ITHACA-FV+]

https://mathlab.github.io/ITHACA-FV/[+https://mathlab.github.io/ITHACA-FV/+]

rheoThermTool
^^^^^^^^^^^^^

A toolbox to simulate of thermo-rheological fluid flows in OpenFOAM®. It contains models for Viscoelastic fluids (VE) and Generalized Newtonian Fluids (GNF) and the ability to change fluid properties according to temperature-dependent functions.

The rheoThermTool software has a framework which allows a user to choose if a quantity (e.g. reference viscosity or thermal conductivity) is subjected to a thermal law such as the Arrhenius equation.

https://github.com/cvr/rheoThermTool[+https://github.com/cvr/rheoThermTool+]

SedFOAM
^^^^^^^

A three-dimensional two-phase flow solver, SedFoam-3.0, has been developed for sediment transport applications. The solver is extended from twoPhaseEulerFoam available in the 2.1.0 release of the open-source CFD (computational fluid dynamics) toolbox OpenFOAM. In this approach the sediment phase is modeled as a continuum, and constitutive laws have to be prescribed for the sediment stresses. In the proposed solver, two different intergranular stress models are implemented: the kinetic theory of granular flows and the dense granular flow rheology μ(I). For the fluid stress, laminar or turbulent flow regimes can be simulated and three different turbulence models are available for sediment transport: a simple mixing length model (one-dimensional configuration only), a k − ε, and a k − ω model.

https://github.com/sedfoam/sedfoam/[+https://github.com/sedfoam/sedfoam/+]

Yade-OpenFOAM-coupling
^^^^^^^^^^^^^^^^^^^^^^

A coupling module for coupling the DEM solver YADE with the FVM solver OpenFOAM.

https://github.com/dpkn31/Yade-OpenFOAM-coupling[+https://github.com/dpkn31/Yade-OpenFOAM-coupling+]

[[openFrameworks]]
openFrameworks
~~~~~~~~~~~~~~

An open source Cxx toolkit designed to assist the creative process by providing a simple and intuitive framework for experimentation.
The openFrameworks package is designed to work as a general purpose glue, and wraps together several commonly used libraries, including:

* OpenGL, GLEW, GLUT, libtess2 and cairo for graphics;
* rtAudio, PortAudio, OpenAL and Kiss FFT or FMOD for audio input, output and analysis;
* FreeType for fonts;
* FreeImage for image saving and loading;
* Quicktime, GStreamer and videoInput for video playback and grabbing;
* Poco for a variety of utilities;
* OpenCV for computer vision; and
* Assimp for 3D model loading.

The code is written to be massively cross-compatible. Right now we support five operating systems (Windows, OSX, Linux, iOS, Android) and four IDEs (XCode, Code::Blocks, and Visual Studio and Eclipse). The API is designed to be minimal and easy to grasp.

http://www.openframeworks.cc/[+http://www.openframeworks.cc/+]

Open Genera
~~~~~~~~~~~

From the early 1980s to the early 1990s, Symbolics, Inc. produced a line of workstations designed to run a highly advanced Lisp environment called "Genera". What made these Lisp Machines so special was the combination of the powerful software running on top of very specialized hardware. The hardware, for example, performed array bounds checking and operated on data as types rather than as flat fields of bits the way other general-purpose computers did (and still do). The Genera environment, in turn, provided workstation-level functionality with a large, high resolution bitmapped graphics display, overlapping windows, and mouse control. In the early 1980s, before even the Macintosh, this was truly advanced stuff.

In the early 1990s, Symbolics took a bold step by releasing the Genera software environment, renamed "Open Genera", on Digital Equipment Corporation's Alpha workstations. To make this happen, they created a "Virtual Lisp Machine" (VLM) that ran under OSF/1 (later, Digital UNIX) and emulated the full Symbolics Lisp Machine architecture.

The source code for VLM was written in C, and as it happens, it was leaked to the public in the early 2000s. Soon thereafter, it was ported to 64-bit Linux.

And here we reach a grey area. Symbolics was dissolved in the mid 1990s, and the current state of its intellectual property is muddled. The rights holders of VLM and Genera itself are not well understood. There is no longer any "Symbolics, Inc."

With that in mind…

This document is meant to guide you through installing the VLM and Symbolics Open Genera on a modern 64-bit Linux system. If you choose to do so, please be aware that you may or may not be violating someone's copyrights. Have fun and play safe!

https://static.loomcom.com/genera/genera-install.html[+https://static.loomcom.com/genera/genera-install.html+]

https://en.wikipedia.org/wiki/Genera_(operating_system)[+https://en.wikipedia.org/wiki/Genera_(operating_system)+]

OpenHPC
~~~~~~~

OpenHPC is a collaborative, community effort that initiated from a desire to aggregate a number of common ingredients required to deploy and manage High Performance Computing (HPC) Linux clusters including provisioning tools, resource management, I/O clients, development tools, and a variety of scientific libraries. Packages provided by OpenHPC have been pre-built with HPC integration in mind with a goal to provide re-usable building blocks for the HPC community. Over time, the community also plans to identify and develop abstraction interfaces between key components to further enhance modularity and interchangeability. The community includes representation from a variety of sources including software vendors, equipment manufacturers, research institutions, supercomputing sites, and others.

This community works to integrate a multitude of components that are commonly used in HPC systems, and are freely available for open source distribution. We are grateful for the efforts undertaken by the developers and maintainers of these upstream communities that provide key components used in HPC around the world today, and for which this OpenHPC community works to integrate and validate as a cohesive software stack.

Binary downloads are presently available in the form of RPMs. These RPMs are organized into repositories that can be accessed via standard package manager utilities (e.g. yum, zypper). OpenHPC provides builds that are compatible with and tested against CentOS 7.5 as well as SUSE Linux Enterprise Server 12 SP3. A typical deployment on a new system will begin with the installation of the base operating system on a chosen master host identified as the system management server (SMS), followed by enabling access to a compatible OpenHPC repository.

http://openhpc.community/[+http://openhpc.community/+]

https://github.com/openhpc/ohpc[+https://github.com/openhpc/ohpc+]

https://build.openhpc.community/[+https://build.openhpc.community/+]

https://fosdem.org/2019/schedule/event/openhpc/[+https://fosdem.org/2019/schedule/event/openhpc/+]

Open MCT
~~~~~~~~

Open MCT (Mission Control Technologies) is a next-generation mission control framework for visualization of data on desktop and mobile devices. It is developed at NASA's Ames Research Center in collaboration with the Jet Propulsion Laboratory, and is being used by NASA for data analysis of spacecraft missions, as well as planning and operation of experimental rover systems. As a generalizable and open source framework, Open MCT could be used as the basis for building applications for planning, operation, and analysis of any systems producing telemetry data.

Open MCT is designed to meet the rapidly evolving needs of mission control systems. At NASA, the requirements for Open MCT are being driven by a need to support distributed operations, access to data anywhere, data visualization for spacecraft analysis that spans multiple data sources, and flexible reconfiguration to support multiple missions and operator use cases. Open MCT brings together many of the functions of mission operations, alleviating the need for operators to switch between different applications to view all necessary data.

Open MCT can be adapted for planning and operations of any system that produces telemetry. While the framework is developed to support space missions, its core concepts are not unique to that domain. It can display streaming and historical data, imagery, timelines, procedures, and other data visualizations in one place.

https://nasa.github.io/openmct/[+https://nasa.github.io/openmct/+]

https://github.com/nasa/openmct[+https://github.com/nasa/openmct+]

OpenMP
~~~~~~

Blah.

BOTS
^^^^

The Barcelona OpenMP Task Suite is a collection of applications that allow to test OpenMP tasking implementations and compare its behaviour under certain circumstances: task tiedness, throttle and cut-offs mechanisms, single/multiple task generators, etc. 

It currently comes with the following kernels:

* Alignment: Aligns sequences of proteins.
* FFT: Computes a Fast Fourier Transformation.
* Floorplan: Computes the optimal placement of cells in a floorplan.
* Health: Simulates a country health system.
* NQueens: Finds solutions of the N Queens problem.
* Sort: Uses a mixture of sorting algorithms to sort a vector.
* SparseLU: Computes the LU factorization of a sparse matrix.
* Strassen: Computes a matrix multiply with Strassen's method.

https://github.com/bsc-pm/bots[+https://github.com/bsc-pm/bots+]

OpenNebula
~~~~~~~~~~

OpenNebula is a simple yet powerful and flexible turnkey open-source solution to build Private Clouds and manage Data Center virtualization. OpenNebula’s maturity builds upon over a decade of software releases and thousands of enterprise deployments, being widely used by industry and research leaders.

Many of our users use OpenNebula to manage data center virtualization,  consolidate  servers, and  integrate existing IT assets for computing, storage, and networking. In this deployment model, OpenNebula directly integrates with hypervisors (like KVM) and has complete control over virtual and physical resources, providing advanced features for capacity management, resource optimization, high availability and business continuity. Some of these users also enjoy OpenNebula’s cloud management and provisioning features when they additionally want to federate data centers, implement cloudbursting, or offer self-service portals for users.

We also have users that use OpenNebula to provide a multi-tenant, cloud-like provisioning layer on top of an existing infrastructure management solution (like VMware vCenter). These users are looking for provisioning, elasticity and multi-tenancy cloud features like virtual data centers provisioning, datacenter federation or hybrid cloud computing to connect in-house infrastructures with public clouds, while the infrastructure is managed by already familiar tools for infrastructure management and operation.

https://opennebula.org/[+https://opennebula.org/+]

Open Network Linux
~~~~~~~~~~~~~~~~~~

Open Network Linux is a Linux distribution for open hardware switches, i.e. network forwarding devices built from commodity components. ONL uses ONIE to install onto on-board flash memory. 

Open Network Linux supports multiple switch fabric APIs including: OF-DPA, OpenNSL and SAI.
Open Network Linux is compatible with most forwarding agents including: FRRouting, Quagga, BIRD, Facebook FBOSS, Google gNOS and Azure SONiC.

Open Network Linux provides the OS and platform management parts of the ONF's CORD and Stratum projects.

https://github.com/opencomputeproject/OpenNetworkLinux[+https://github.com/opencomputeproject/OpenNetworkLinux+]

http://opennetlinux.org/[+http://opennetlinux.org/+]

https://www.opennetworking.org/cord/[+https://www.opennetworking.org/cord/+]

OpenRAVE
~~~~~~~~

OpenRAVE provides an environment for testing, developing, and deploying motion planning algorithms in real-world robotics applications. The main focus is on simulation and analysis of kinematic and geometric information related to motion planning. OpenRAVE’s stand-alone nature allows is to be easily integrated into existing robotics systems. It provides many command line tools to work with robots and planners, and the run-time core is small enough to be used inside controllers and bigger frameworks. An important target application is industrial robotics automation.

OpenRAVE includes a seamless integration of simulation, visualization, planning, scripting and control. The plugin architecture allows users to easily write custom controllers or extend functionality. Using OpenRAVE plugins, any planning algorithm, robot control, or sensing-based subsystem can be distributed and dynamically loaded at run-time; this distributed nature frees developers from struggling with monolithic code-bases. Users of OpenRAVE can concentrate on the development of planning and scripting aspects of a problem without having to explicitly manage the details of robot kinematics and dynamics, collision detection, world updates, and robot control. OpenRAVE provides a powerful Python API for scripting demos, which makes it simple to control and monitor the demo and environment state. There are also interfaces for Octave and Matlab.

http://openrave.org/[+http://openrave.org/+]

https://github.com/rdiankov/openrave[+https://github.com/rdiankov/openrave+]

OpenSceneGraph
~~~~~~~~~~~~~~

OpenSceneGraph is an open-source 3D graphics application programming interface, used by application developers in fields such as visual simulation, computer games, virtual reality, scientific visualization and modeling. 
The toolkit is written in standard C++ using OpenGL, and supports the major operating systems.
It also supports application development for mobile platforms, namely iOS and Android. 

The features include:

* A feature-rich and widely adopted ihttps://en.wikipedia.org/wiki/Scene_graph[scene graph] implementation
* Support for OpenGL, from 1.1 through 4.x including the latest extensions
* OpenGL ES 1.1, and OpenGL ES 2.0 support
* Tightly coupled support for OpenGL Shading Language
* Support for a wide range of 2D image and 3D database formats, with loaders available for formats such as OpenFlight, TerraPage, OBJ, 3DS, JPEG, PNG and GeoTIFF
* Particle effects
* Support for anti-aliased https://en.wikipedia.org/wiki/TrueType[TrueType] text
* Seamless support for https://en.wikipedia.org/wiki/Framebuffer[framebuffer] objects, pbuffers and frame buffer render-to-texture effects
* Multi-threaded database paging support, which can be used in conjunction with all 3D database and image loaders
* Large scale, whole earth geospatial terrain paged database generation
* Multi-threaded and configurable support for multiple CPU/multiple GPU machines
* 3DS file format export and file conversion
* Integrated FFmpeg support for displaying video file content in 3D applications.
* Support for Android and iOS on tablets and phones
* extensible serializers that provide new .osgb binary, .osgt ascii and .osgx xml file native formats
* a generalized serializable metadata architecture
* an osgQt library that makes it straight forward to integrate !OpenSceneGraph with Qt, including web browsing via QWebKit

The library architecture can be separated into three main subsets: the core OpenSceneGraph library, osgViewer and a set of NodeKits. 

* the core OpenSceneGraph library:
** *osg* - provides classes and methods for construction and manipulation of the scene graph
** *OpenThreads* - a library with a lightweight, cross-platatform thread model intended to
*provide a minimal and complete Object-Oriented (OO) thread interface for Cxx programmers
** *osgUtil* - contains rendering backend functionality and utilities taking care of scene graph traversal, rendering optimisation and transforming the scene into a stream of OpenGL calls
** *osgDB* - a large collection of database loaders and 2D/3D data manipulators for more than 50 formats
* the osgViewer library provides a quick and easy way of visualizing the graphics scene, as well as
a platform-independent abstraction for various window system interfaces
* a set of NodeKits providing solutions of common problems and advanced 3-D application
components and graphics algorithms, including:
** osgAnimation — Skeletal models usage, animating and morphing.
** osgFX — Special effects and image postprocessing.
** osgManipulator — Interactive 3D scene manipulation.
** osgParticle — Advanced particle system usage.
** osgQt — Integration with Qt toolkit and incorporation of QtGUI elements into OSG applications.
** osgShadow — Framework for shadow rendering techniques.
** osgTerrain — Extensive terrain rendering.
** osgText — Quality antialiased fonts, TrueType and FreeType font support.
** osgVolume — Volume rendering and volumetric data manipulation.
** osgWidget — Simple GUI creation.

OpenSceneGraph doesn't provide any functionality for higher "gaming" logic, it is a rendering-only tool. There are several full-scale engines for computer games (or so-called serious games) creation using OSG as a base of graphics rendering, the most common framework being Delta3D. 

http://www.openscenegraph.org/[+http://www.openscenegraph.org/+]

https://en.wikipedia.org/wiki/OpenSceneGraph[+https://en.wikipedia.org/wiki/OpenSceneGraph+]

OpenSensorHub
~~~~~~~~~~~~~

OpenSensorHub (OSH) allows one to easily build interoperable and evolutive sensor networks, based on open-standards for all data exchanges, and providing advanced processing capabilities. The open-standards used are mostly OGC standards from the Sensor Web Enablement (SWE) initiative and are key to design sensor networks that can largely evolve with time (addition of new types of sensors, reconfigurations, etc.).

The Java framework allows one to connect any kind of sensors and actuators to a common bus via a simple yet generic driver API. Sensors can be connected through any available hardware interface such as RS232/422, SPI, I2C, USB, Ethernet, Wifi, Bluetooth, ZigBee, HTTP, etc... Once drivers are available for a specific sensor, it is automatically connected to the bus and it is then trivial to send commands and read data from it. An intuitive user interface allows the user to configure the network to suit its needs and more advanced processing capabilities are available via a plugin system.

SensorHub embeds the full power of OGC web services (Sensor Observation Service or SOS, Sensor Planning Service or SPS) to communicate with all connected sensors in the network and provide robust metadata (owner, location and orientation, calibration, etc.). Through these standards, several SensorHub instances can also communicate with each other to form larger networks.

Low level functions of SensorHub (send commands and read data from sensor) are coded efficiently and can be used on embedded hardware running Java SE®, Java ME® or Android® while more advanced data processing capabilities are fully multi-threaded and can thus benefit from a more powerful hardware platform (e.g. multi-processor servers or even clusters).

SensorHub is pure java software but we have plans to release parts of this software in other languages (Arduino, Cxx) to be used on low power micro-controllers (note that some more powerful ARM micro-controllers can also run the Java version directly using Java ME®).

http://docs.opensensorhub.org/[+http://docs.opensensorhub.org/+]

OpenSoC
~~~~~~~

The OpenSoC Fabric is an ongoing project to create a open source network-on-chip generator capable of creating a synthesizeable network for connecting processors, memory and I/O devices. 

Recent advancements in technology scaling have shown a trend towards greater integration with large-scale chips containing thousands of processors connected to memories and other I/O devices using non-trivial network topologies. Software simulation proves insufficient to study the tradeoffs in such complex systems due to slow execution time, whereas hardware RTL development is too time-consuming. We present OpenSoC Fabric, an on-chip network generation infrastructure which aims to provide a parameterizable and powerful on-chip network generator for evaluating future high performance computing architectures based on SoC technology. OpenSoC Fabric leverages a new hardware DSL, Chisel, which contains powerful abstractions provided by its base language, Scala, and generates both software (Cxx) and hardware (Verilog) models from a single code base. The OpenSoC Fabric infrastructure is modeled after existing state-of-the-art simulators, offers large and powerful collections of configuration options, and follows object-oriented design and functional programming to make functionality extension as easy as possible. 

http://www.opensocfabric.org/home.php[+http://www.opensocfabric.org/home.php+]

OpenSpace
~~~~~~~~~

Funded in part by NASA, OpenSpace brings the latest techniques from data visualization research to the general public. OpenSpace supports interactive presentation of dynamic data from observations, simulations, and space mission planning and operations. OpenSpace works on multiple operating systems, with an extensible architecture powering high resolution tiled displays and planetarium domes, and makes use of the latest graphic card technologies for rapid data throughput. In addition, OpenSpace enables simultaneous connections across the globe, creating opportunity for shared experiences among audiences worldwide.

OpenSpace is an open source, non-commercial, and freely available interactive data visualization software designed to visualize the entire known universe and portray our ongoing efforts to investigate the cosmos. Bringing the latest techniques from data visualization research to the general public, OpenSpace supports interactive presentation of dynamic data from observations, simulations, and space mission planning and operations. The software works on multiple operating systems (Windows, Linux, MacOS) with an extensible architecture powering high resolution tiled displays and planetarium domes, making use of the latest graphic card technologies for rapid data throughput. In addition, OpenSpace enables simultaneous connections across the globe creating opportunity for shared experiences among audiences worldwide.

Current areas of focus within OpenSpace include:

* Visualization of dynamic simulations via interactive volumetric rendering, as a priority for communicating research in astrophysics.
* Utilization of NASA’s SPICE observational geometry system with its Planetary Data Service (PDS) to enable space mission visualization that reveal how missions are designed to gather science.
* Globe browsing techniques across spatial and temporal scales to examine scientific campaigns on multiple planets, including close up surface exploration.

https://github.com/OpenSpace/OpenSpace[+https://github.com/OpenSpace/OpenSpace+]

http://wiki.openspaceproject.com/[+http://wiki.openspaceproject.com/+]

https://www.openspaceproject.com/[+https://www.openspaceproject.com/+]

http://joss.theoj.org/papers/4843fc43db46dff18152e3f55a533b8d[+http://joss.theoj.org/papers/4843fc43db46dff18152e3f55a533b8d+]

OpenStreetMap
~~~~~~~~~~~~~

OpenStreetMap (OSM) is a collaborative project to create a free editable map of the world. Rather than the map itself, the data generated by the project is considered its primary output. The creation and growth of OSM has been motivated by restrictions on use or availability of map information across much of the world, and the advent of inexpensive portable satellite navigation devices.[6] OSM is considered a prominent example of volunteered geographic information.

Created by Steve Coast in the UK in 2004, it was inspired by the success of Wikipedia[7] and the predominance of proprietary map data in the UK and elsewhere.[8] Since then, it has grown to over 2 million registered users,[9] who can collect data using manual survey, GPS devices, aerial photography, and other free sources. This crowdsourced data is then made available under the Open Database License. The site is supported by the OpenStreetMap Foundation, a non-profit organisation registered in England and Wales.

The data from OSM is available for use in both traditional applications, like its usage by Facebook, Craigslist, OsmAnd, Geocaching, MapQuest Open, JMP statistical software, and Foursquare to replace Google Maps, and more unusual roles like replacing the default data included with GPS receivers.[10] OpenStreetMap data has been favourably compared with proprietary datasources.

https://en.wikipedia.org/wiki/OpenStreetMap[+https://en.wikipedia.org/wiki/OpenStreetMap+]

https://wiki.openstreetmap.org/wiki/MapCSS[+https://wiki.openstreetmap.org/wiki/MapCSS+]

https://github.com/openstreetmap/iD[+https://github.com/openstreetmap/iD+]

Merkaartor
^^^^^^^^^^

An opensource OSM editor, written in Cxx and Qt.

https://github.com/openstreetmap/merkaartor[+https://github.com/openstreetmap/merkaartor+]

http://merkaartor.be/[+http://merkaartor.be/+]

OpenTBL
~~~~~~~

A relevant fraction of the energy spent in transportation is lost in a thin layer where the surrounding fluid contacts the surface of the vehicle, called boundary layer. That thin layer, where the fluid is in a turbulent state, is still not fully understood. Describing the structure of the turbulent wall bounded flows, particularly boundary layers, but also pipes and other internal flows, is essential to improve the turbulence models and sub-grid approximation for large-eddy simulations, that are a key ingredient in most industrial CFD applications.

OpenTBL is a fast, highly scalable, and memory efficient code that uses a high resolution numerical scheme to simulate a turbulent boundary layer with a range of possible configurations. This code has provided the largest simulation of a zero-pressure-gradient boundary layer, that is publicly available to researchers and engineers.

http://www.fz-juelich.de/ias/jsc/EN/Expertise/High-Q-Club/OpenTBL/_node.html[+http://www.fz-juelich.de/ias/jsc/EN/Expertise/High-Q-Club/OpenTBL/_node.html+]

https://github.com/guillemborrell/OpenTBL[+https://github.com/guillemborrell/OpenTBL+]

OpenTURNS
~~~~~~~~~

OpenTURNS is an open source initiative for the treatment of uncertainties, risks’n statistics in a structured industrial approach.
The main features are:

* Multivariate probabilistic modeling including dependence
* Numerical tools dedicated to the treatment of uncertainties
* Generic coupling to any type of physical model
* Open source, LGPL licensed, Cxx/Python library

The data analysis features include:

* Visual analysis: QQ-Plot, Cobweb
* Fitting tests: Kolmogorov, Chi2
* Multivariate distribution: kernel smoothing (KDE), maximum likelihood
* Process: covariance models, Welch and Whittle estimators
* Bayesian calibration: Metropolis-Hastings, conditional distribution

The probabilistic modeling features include:

* Dependence modelling: elliptical, archimedian copulas.
* Univariate distribution: Normal, Weibull
* Multivariate distribution: Student, Dirichlet, Multinomial, User-defined
* Process: Gaussian, ARMA, Random walk.
* Covariance models: Matern, Exponential, User-defined

The meta-modeling features include:

* Functional basis methods: orthogonal basis (polynomials, Fourier, Haar, Soize Ghanem)
* Gaussian process regression: General linear model (GLM), Kriging
* Spectral methods: functional chaos (PCE), Karhunen-Loeve, low-rank tensors

The reliability and sensivity features include:

* Sampling methods: Monte Carlo, LHS, low discrepancy sequences
* Variance reduction methods: importance sampling, subset sampling
* Approximation methods: FORM, SORM
* Indices: Spearman, Sobol, ANCOVA
* Importance factors: perturbation method, FORM, Monte Carlo

The functional modeling features include:

* Numerical functions: symbolic, Python-defined, user-defined
* Function operators: addition, product, composition, gradients
* Function transformation: linear combination, aggregation, parametrization
* Polynomials: orthogonal polynomial, algebra

The numerical methods include:

* Integration: Gauss-Kronrod
* Optimization: NLopt, Cobyla, TNC
* Root finding: Brent, Bisection
* Linear algrebra: Matrix, HMat
* Interpolation: piecewise linear, piecewise Hermite
* Least squares: SVD, QR, Cholesky

http://www.openturns.org/[+http://www.openturns.org/+]

https://github.com/openturns/openturns[+https://github.com/openturns/openturns+]

OpenUCX
~~~~~~~

Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications.

UCX utilizes high-speed networks for inter-node communication, and shared memory mechanisms for efficient intra-node communication.

https://github.com/openucx/ucx[+https://github.com/openucx/ucx+]

Open vSwitch
~~~~~~~~~~~~

Open vSwitch is a production quality, multilayer virtual switch licensed under the open source Apache 2.0 license.  It is designed to enable massive network automation through programmatic extension, while still supporting standard management interfaces and protocols (e.g. NetFlow, sFlow, IPFIX, RSPAN, CLI, LACP, 802.1ag).  In addition, it is designed to support distribution across multiple physical servers similar to VMware's vNetwork distributed vswitch or Cisco's Nexus 1000V. 

Open vSwitch can operate both as a soft switch running within the hypervisor, and as the control stack for switching silicon. It has been ported to multiple virtualization platforms and switching chipsets. It is the default switch in XenServer 6.0, the Xen Cloud Platform and also supports Xen, KVM, Proxmox VE and VirtualBox. It has also been integrated into many virtual management systems including OpenStack, openQRM, OpenNebula and oVirt. The kernel datapath is distributed with Linux, and packages are available for Ubuntu, Debian, Fedora and openSUSE. Open vSwitch is also supported on FreeBSD and NetBSD. The Open vSwitch release in development has been ported to DPDK.

https://www.openvswitch.org/[+https://www.openvswitch.org/+]

OpenWhisk
~~~~~~~~~

Apache OpenWhisk (Incubating) is an open source, distributed Serverless platform that executes functions (fx) in response to events at any scale. OpenWhisk manages the infrastructure, servers and scaling using Docker containers so you can focus on building amazing and efficient applications.

The OpenWhisk platform supports a programming model in which developers write functional logic (called Actions), in any supported programming language, that can be dynamically scheduled and run in response to associated events (via Triggers) from external sources ( Feeds) or from HTTP requests. The project includes a REST API-based Command Line Interface (CLI) along with other tooling to support packaging, catalog services and many popular container deployment options. 

Since Apache OpenWhisk builds its components using containers it easily supports many deployment options both locally and within Cloud infrastructures. Options include many of today's popular Container frameworks such as Kubernetes, Mesos, OpenShift and Compose. In general, the community endorses deployment on Kubernetes using Helm charts since it provides many easy and convenient implementations for both Devlopers and Operators alike such as Minikube. 

Work with what you know and love. OpenWhisk supports a growing list of your favorite languages such as NodeJS, Swift, Java, Go, Scala, Python, PHP and Ruby. 

using Packages that are provided either as independently developed projects under the OpenWhisk family or as part of our default Catalog.

Packages offer integrations with general services such as Kafka message queues, databases including Cloudant, Push Notifications from mobile applications, Slack messaging, and RSS feeds. Development pipelines can take advantage of integrations with GitHub, JIRA, or easily connect with custom data services from IBM Watson for Translation or Speech-to-Text, as well as the Weather company. 

https://openwhisk.apache.org/[+https://openwhisk.apache.org/+]

OpenWind
~~~~~~~~

A python package for estimating high resolution wind from SAR image.

https://github.com/nansencenter/openwind[+https://github.com/nansencenter/openwind+]

OpenWrt
~~~~~~~
The OpenWrt Project is a Linux operating system targeting embedded devices. Instead of trying to create a single, static firmware, OpenWrt provides a fully writable filesystem with package management. This frees you from the application selection and configuration provided by the vendor and allows you to customize the device through the use of packages to suit any application. For developers, OpenWrt is the framework to build an application without having to build a complete firmware around it; for users this means the ability for full customization, to use the device in ways never envisioned. 

OpenWrt is a highly extensible GNU/Linux distribution for embedded devices (typically wireless routers). Unlike many other distributions for these routers, OpenWrt is built from the ground up to be a full-featured, easily modifiable operating system for your router. In practice, this means that you can have all the features you need with none of the bloat, powered by a Linux kernel that's more recent than most other distributions. 

OpenWrt is an open source project for embedded operating system based on Linux, primarily used on embedded devices to route network traffic. The main components are Linux, util-linux, musl, and BusyBox. All components have been optimized to be small enough to fit into the limited storage and memory available in home routers.

OpenWrt is configured using a command-line interface (ash shell), or a web interface (LuCI). There are about 3500 optional software packages available for installation via the opkg package management system.

OpenWrt can run on various types of devices, including CPE routers, residential gateways, smartphones, pocket computers (e.g. Ben NanoNote), and laptops. It is also possible to run OpenWrt on personal computers, which are most commonly based on the x86 architecture. 

https://openwrt.org/[+https://openwrt.org/+]

https://en.wikipedia.org/wiki/OpenWrt[+https://en.wikipedia.org/wiki/OpenWrt+]

Opsdroid
~~~~~~~~

An open source chat bot framework written in python. It is designed to be extendable, scalable and simple.
Opsdroid is designed to take messages from chat services and execute python functions (skills) based on their contents. Those functions can be anything you like, from simple conversational responses to running complex tasks. The true power of this project is to act as a glue library to bring the multitude of natural language APIs, chat services and third party APIs together.

Opsdroid has a plug-in architecture that allows various connectors, databases, parsers and 
skills to be connected.  The available modules include:

* Connector modules transfer messages between opsdroid and a particular chat service, including
shell, Telegram, Twitter, websocket, Facebook, Github, Slack and Matrix.
* Database modules allow opsdroid to persist information in a database of your choice, with modules for Mongo, Redis and SQLite.
* Parsers/matchers modules gather meaning of what was said and match a skill, with modules for
regex,  Rasa, Dialogflow, Luis.AI, Wit.ai, crontab, Webhook, Always and Recast.AI.
* Skills are what makes opsdroid tick. They define how opsdroid should respond and what actions to take. There are a metric buttload of modules for various skills.

https://opsdroid.dev/[+https://opsdroid.dev/+]

https://github.com/opsdroid/opsdroid[+https://github.com/opsdroid/opsdroid+]

Orange
~~~~~~

Open source machine learning and data visualization for novice and expert. Interactive data analysis workflows with a large toolbox.

Orange is all about data visualizations that help to uncover hidden data patterns, provide intuition behind data analysis procedures or support communication between data scientists and domain experts. Visualization widgets include scatter plot, box plot and histogram, and model-specific visualizations like dendrogram, silhouette plot, and tree visualizations, just to mention a few. Many other visualizations are available in add-ons and include visualizations of networks, word clouds, geographical maps, and more. 

We take care to make Orange visualizations interactive: you can select data points from a scatter plot, a node in the tree, a branch in the dendrogram. Any such interaction will instruct visualization to send out a data subset that corresponds to the selected part of visualization. Consider the combination of a scatter plot and classification tree below. Scatter plot shows all the data, but highlights the data subset that corresponds to the selected node in the classification tree. 

In Orange, data analysis is done by stacking components into workflows. Each component, called a widget, embeds some data retrieval, preprocessing, visualization, modeling or evaluation task. Combining different widgets in a workflow enables you to build comprehensive data analysis schemas as you go. With a large library of widgets you won't be short for choice. Additional widgets are available through add-ons and allow for a more focused and topic-oriented research. 

Orange widgets communicate with each other. They receive data on the input and send out filtered or processed data, models, or anything the widget does on the output. Say, start with a File widget that reads the data and connect its output to another widget, say, a Data Table, and you have a functioning workflow. Alter any change in one widget, the changes are instantaneously propagated through the downstream workflow. Changing a data file in the File widget will trigger the response in all downstream widgets. This is especially fun if the widgets are open and when you can immediately see the results of any changes in that data, parameters of the methods or selections in interactive visualizations. For example, in a simple workflow below, where selection of the data in the spreadsheet propagates to a scatter plot, which marks the selected data instances. 

Use various add-ons available within Orange to mine data from external data sources, perform natural language processing and text mining, conduct network analysis, infer frequent itemset and do association rules mining. 

https://orange.biolab.si/[+https://orange.biolab.si/+]

OrbitDB
~~~~~~~

OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. It's an eventually consistent database that uses CRDTs for conflict-free database merges making OrbitDB an excellent choice for decentralized apps (dApps), blockchain applications and offline-first web applications.

https://github.com/orbitdb/orbit-db[+https://github.com/orbitdb/orbit-db+]

https://vitriol.co/home[+https://vitriol.co/home+]

Orfeo ToolBox
~~~~~~~~~~~~~

Orfeo ToolBox is an open-source project for state-of-the-art remote sensing, including a fast image viewer, apps callable from Bash, Python or QGIS, and a powerful C++ API.

Orfeo ToolBox (OTB) is an open-source project for state-of-the-art remote sensing. Built on the shoulders of the open-source geospatial community, it can process high resolution optical, multispectral and radar images at the terabyte scale. A wide variety of applications are available: from ortho-rectification or pansharpening, all the way to classification, SAR processing, and much more.

All of OTB’s algorithms are accessible from Monteverdi, QGIS, Python, the command line or Cxx. Monteverdi is an easy to use visualization tool with an emphasis on hardware accelerated rendering for high resolution imagery (optical and SAR). With it, end-users can visualize huge raw imagery products and access all of the applications in the toolbox. From resource limited laptops to high performance MPI clusters, OTB is available on Linux, macOS and Windows. It is community driven, extensible and heavily documented.

https://www.orfeo-toolbox.org/[+https://www.orfeo-toolbox.org/+]

https://www.orfeo-toolbox.org/CookBook/Applications.html[+https://www.orfeo-toolbox.org/CookBook/Applications.html+]

OrthoPoly
~~~~~~~~~

Software that permits to evaluate, efficiently and accurately, finite series of any classical family of orthogonal polynomials (Chebyshev, Legendre, ultraspherical or Gegenbauer, Jacobi, Hermite and Laguerre orthogonal polynomials) and their derivatives. The basic algorithm is the BCS-algorithm (Barrio–Clenshaw–Smith derivative algorithm), that permits to evaluate the th derivative of a finite series of orthogonal polynomials at any point without obtaining before the previous derivatives. Due to the presence of rounding errors, specially in the case of high order derivatives, we introduce the compensated BCS-algorithm, based on Error-Free Transformation techniques, that permits to relegate the influence of the conditioning of the problem up to second order in the round-off unit of the computer. The BCS and compensated BCS algorithms may also give running-error bounds to provide information about the accuracy of the evaluation process. The ORTHOPOLY software includes C and Matlab versions of all the algorithms, and they are designed to be easily used in longer softwares to solve physical, mathematical, chemical or engineering problems (illustrated on the Schrödinger equation for the radial hydrogen atom).

https://www.sciencedirect.com/science/article/pii/S0010465518301577[+https://www.sciencedirect.com/science/article/pii/S0010465518301577+]

https://github.com/j-jith/orthopoly[+https://github.com/j-jith/orthopoly+]

OSGeoLive
~~~~~~~~~

OSGeoLive is a self-contained bootable DVD, USB thumb drive or Virtual Machine based on Lubuntu, that allows you to try a wide variety of open source geospatial software without installing anything. It is composed entirely of free software, allowing it to be freely distributed, duplicated and passed around.
boot select

It provides pre-configured applications for a range of geospatial use cases, including storage, publishing, viewing, analysis and manipulation of data. It also contains sample datasets and documentation.

It can be installed via a live DVD, a USB thumb drive, or run in a virtual machine.

http://live.osgeo.org/en/index.html[+http://live.osgeo.org/en/index.html+]

http://live.osgeo.org/en/overview/overview.html[+http://live.osgeo.org/en/overview/overview.html+]

OSSIA
~~~~~

libossia is a modern Cxx, cross-environment distributed object model for creative coding.

It allows to declare the architecture of your creative coding application's functions as a tree of OSC nodes and parameters. These nodes/parameters can have attributes, which allow to declare many of their properties, such as their values, types, units, ranges, etc.

This OSC tree-based architecture (coined "device" in the OSSIA terminology) can then be exposed over the network under several protocols, some of which allow this architecture, and the properties and values of its nodes, to be fully explored and queried. For now, protocols available in the implemenations are: plain OSC, OSCquery, Midi - more are part of libossia and will be made available in the future.

libossia offers bindings and implementations for several environments: PureData, Max/MSP, Python, Unity3D, QML, Faust, SuperCollider.
The bindings are:

* C : a C89 binding that should work everywhere and has a stable and strong API & ABI guarantee.
* Safe Cxx: a simplified Cxx binding, kept compatible with Cxx98, which makes it hard to have problems at the cost of performance.
* Fast Cxx: the native libossia API, written in modern Cxx14. Subject to frequent updates and changes; use to get maximal performance.
* Python: Python 2 / 3 bindings made with pybind11.
* QML: Bindings for the Qt declarative UI language, QML. Very nice for user interfaces.
* OFX: OpenFrameworks bindings. Requires the ofxOssia addon.
* Unity3D: Bindings for the Unity3D game engine. Required C# files are here.
* Pd: PureData bindings. Requires at least Pd vanilla 0.48/
* Max: Max/MSP bindings. Requires at least Max 7.
* SuperCollider: Requires a custom build of SuperCollider.
* Faust: A Faust architecture file is provided. It should be modified however to tailor your own system.

The interactive sequencer score is an example of software based on libossia. 

https://ossia.github.io/[+https://ossia.github.io/+]

https://ossia.io/[+https://ossia.io/+]

Score
^^^^^

Score is an interactive sequencer for intermedia authoring. It allows to create flexible and interactive scenarios and is especially designed for live performance, art installations, museography or any context requiring a precise and interactive execution of timed events.

Score brings timeline-based precise automation authoring as offerd in modern DAWs as well as flexible and interactive event triggering as offered in cue-based software in a unified environment. These two paradigms can be combined and used in parallel or hierarchically in Score's scenario, which provides a high level of control and of openness, as is required by today’s creation.

https://ossia.github.io/score/[+https://ossia.github.io/score/+]

https://github.com/OSSIA/score/wiki[+https://github.com/OSSIA/score/wiki+]

Ouroboros
~~~~~~~~~

Ouroboros is a prototype Inter-Process Communications (IPC) subsystem for POSIX operating systems. It incorporates a fully decentralised recursive packet switched transport network. The aim is to develop a network that is open and inherently secure, upholding human and societal values, privacy, participation and diversity.

Ouroboros provides a simple and minimal API for synchronous and asynchronous IPC. Ouroboros can use IP, Ethernet LLC, Ethernet DIX or Ethernet PHY (using the experimental raptor NetFPGA implementation) at the lowest layer.

https://ouroboros.ilabt.imec.be/index.html[+https://ouroboros.ilabt.imec.be/index.html+]

https://archive.fosdem.org/2018/schedule/event/ipc/[+https://archive.fosdem.org/2018/schedule/event/ipc/+]

oVirt
~~~~~

oVirt is an open-source distributed virtualization solution, designed to manage your entire enterprise infrastructure. oVirt uses the trusted KVM hypervisor and is built upon several other community projects, including libvirt, Gluster, PatternFly, and Ansible.  oVirt is an open source alternative to VMware™ vSphere™, providing an awesome KVM management interface for multi-node virtualization. 

https://ovirt.org/[+https://ovirt.org/+]

#PPPP

Pachyderm
~~~~~~~~~

Pachyderm is a tool for production data pipelines. If you need to chain together data scraping, ingestion, cleaning, munging, wrangling, processing, modeling, and analysis in a sane way, then Pachyderm is for you. If you have an existing set of scripts which do this in an ad-hoc fashion and you're looking for a way to "productionize" them, Pachyderm can make this easy for you.

The features include:

* Containerized: Pachyderm is built on Docker and Kubernetes. Whatever languages or libraries your pipeline needs, they can run on Pachyderm which can easily be deployed on any cloud provider or on prem.
* Version Control: Pachyderm version controls your data as it's processed. You can always ask the system how data has changed, see a diff, and, if something doesn't look right, revert.
* Provenance (aka data lineage): Pachyderm tracks where data comes from. Pachyderm keeps track of all the code and data that created a result.
* Parallelization: Pachyderm can efficiently schedule massively parallel workloads.
* Incremental Processing: Pachyderm understands how your data has changed and is smart enough to only process the new data.

https://github.com/pachyderm/pachyderm[+https://github.com/pachyderm/pachyderm+]

https://www.pachyderm.io/[+https://www.pachyderm.io/+]

Packer
~~~~~~

Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration. Packer is lightweight, runs on every major operating system, and is highly performant, creating machine images for multiple platforms in parallel. Packer does not replace configuration management like Chef or Puppet. In fact, when building images, Packer is able to use tools like Chef or Puppet to install software onto the image.

A machine image is a single static unit that contains a pre-configured operating system and installed software which is used to quickly create new running machines. Machine image formats change for each platform. Some examples include AMIs for EC2, VMDK/VMX files for VMware, OVF exports for VirtualBox, etc.

The parts of Packer include:

* *Artifacts* are the results of a single build, and are usually a set of IDs or files to represent a machine image. Every builder produces a single artifact. As an example, in the case of the Amazon EC2 builder, the artifact is a set of AMI IDs (one per region). For the VMware builder, the artifact is a directory of files comprising the created virtual machine.

* *Builds* are a single task that eventually produces an image for a single platform. Multiple builds run in parallel to, for example, create builds for VWware, AWS and VirtualBox.

* *Builders* are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, etc. Packer comes with many builders by default, and can also be extended to add new builders.

* *Commands* are sub-commands for the packer program that perform some job. An example command is "build", which is invoked as packer build. Packer ships with a set of commands out of the box in order to define its command-line interface.

* *Provisioners* use builtin and third-party software to install and configure the machine image after booting. Provisioners prepare the system for use.  Common uses for provisioners include installing
packages, patching the kernel, creating users and downloading application code.

* *Post-processors* run after the image is built by the builder and provisioned by the provisioner(s). Post-processors are optional, and they can be used to upload artifacts, re-package, or more.

* *Templates* are JSON files which define one or more builds by configuring the various components of Packer. Packer is able to read a template and use that information to create multiple machine images in parallel.

http://packer.io/[+http://packer.io/+]

https://github.com/hashicorp/packer[+https://github.com/hashicorp/packer+]

PageKite
~~~~~~~~

Fast, reliable, secure: make your localhost part of the Web.

Since 2010, PageKite has led the way in making local servers public. With relays on four continents, it works with any computer, any web server and any Internet connection. 

https://pagekite.net/[+https://pagekite.net/+]

PAGO
~~~~

Inter-comparison of model and gridded observations of the ocean is a challenge when they use different grids. Interpolation from one grid to another brings eventual errors that may affect significantly large scale budgets of tracers (heat, salt, freshwater). This suite of programs offers the possibility to analyze gridded ocean data along physical sections with minimum interpolation. For example, this allows to monitor the circulation across an observed array in various model outputs, whatever their spatial resolution or type of discretization (B- or C- grids). When defining sections that enclose a specific volume, large scale budgets of tracers can be reconstruct and inter-compared among all kinds of gridded ocean data.

The core of PAGO programs consists of finding the suite of west and north grid faces to go from one geographical landmark to another, following the geodesic distance. Temperature and salinity are averaged from the centers of the 2 adjacent grid cells to the center of the grid faces (taking into account missing values in land). Velocities, if available, are either located at the center of the grid faces (in case of C-grids), or at the corners (B-grids). In case of C-grid data, no interpolation is required for the velocities. In case of B-grid data, the volume transport at the corner of the grid cells is computed and then split in 2 to the center of each adjacent faces (see more details in the models page).

Here is the list of the main PAGO functions to be called:

* sections_MODEL reads data regarding grid characteristics only, selects the region of interest when uploading the data, and identifies sections and areas on which circulation, and tracer content will be diagnosed.
* loaddata_* uploads the data output and extracts the information required along preselected sections and areas. This function also interpolates velocity and tracer data at the center of west and north grid faces, when needed.
* indices_MODEL calculates simple or full diagnostics of the circulation (transport of volume, heat, salt and freshwater) across selected sections at each time step.
* volumes_MODEL calculates thermal, haline and freshwater content within selected areas at each time step. It also calculates advective convergence of tracers at the boundaries or areas.

http://www.whoi.edu/science/PO/pago/[+http://www.whoi.edu/science/PO/pago/+]

PyPAGO
^^^^^^

A Python version of PAGO.  This set of programs aims at comparing multiple fields (such as temperature, salinity and velocity) of gridded ocean models along pre-defined sections.

http://pypago.nicolasbarrier.fr/[+http://pypago.nicolasbarrier.fr/+]

PAL
~~~

he Physics Abstraction Layer (PAL) provides a unified interface to a number of different physics engines. This enables the use of multiple physics engines within one application. It is not just a simple physics wrapper, but provides an extensible plug-in architecture for the physics system, as well as extended functionality for common simulation components.

PAL provides a number of benefits to game and simulation developers. First of all PAL is very easy to use, so you can easily integrate physics into your application. Secondly, it does not restrict you to one particular physics engine. This gives you more flexibility, allowing you to easily upgrade your physics system if you decide to pursue a commercial engine, select different engines for alternative platforms, or swap to another engine if the physics engine developers stop development and support for their engine. This flexibility allows you to choose the engine that gives you the best performance for your application. Finally, PAL has an extensive set of common features such as simulating different devices or loading physics configurations from XML, COLLADA and Scythe files. 

http://www.adrianboeing.com/pal/index.html[+http://www.adrianboeing.com/pal/index.html+]

https://sourceforge.net/projects/pal/[+https://sourceforge.net/projects/pal/+]

Pandas
~~~~~~

pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way toward this goal.

pandas is well suited for many different kinds of data:

* Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet
* Ordered and unordered (not necessarily fixed-frequency) time series data.
* Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
* Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. For R users, DataFrame provides everything that R’s data.frame provides and much more. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Things that pandas does well include:

* Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data
* Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
* Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations
* Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
* Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
* Intelligent label-based slicing, fancy indexing, and subsetting of large data sets
* Intuitive merging and joining data sets
* Flexible reshaping and pivoting of data sets
* Hierarchical labeling of axes (possible to have multiple labels per tick)
* Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
* Time series-specific functionality: date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging, etc.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.

Apache Arrow was created to alleviate various pandas shortcomings as explained by its creator in:

http://wesmckinney.com/blog/apache-arrow-pandas-internals/[+http://wesmckinney.com/blog/apache-arrow-pandas-internals/+]

https://pandas.pydata.org/[+https://pandas.pydata.org/+]

https://github.com/apache/arrow[+https://github.com/apache/arrow+]

https://github.com/pydata/pydata-cookbook[+https://github.com/pydata/pydata-cookbook+]

lens
^^^^

lens is a library for exploring data in Pandas DataFrames. It computes single column summary statistics and estimates the correlation between columns. We wrote lens when we realised that the initial steps of acquiring a new data set were almost formulaic: What data type is in this column? How many null values are there? Which columns are correlated? What's the distribution of this value? lens calculates all this for you.

https://github.com/facultyai/lens[+https://github.com/facultyai/lens+]

Modin
^^^^^

Scale your pandas workflows by changing one line of code.
Modin uses Ray to provide an effortless way to speed up your pandas notebooks, scripts, and libraries. Unlike other distributed DataFrame libraries, Modin provides seamless integration and compatibility with existing pandas code. Even using the DataFrame constructor is identical.

To use Modin, you do not need to know how many cores your system has and you do not need to specify how to distribute the data. In fact, you can continue using your previous pandas notebooks while experiencing a considerable speedup from Modin, even on a single machine. Once you’ve changed your import statement, you’re ready to use Modin just like you would pandas.

The modin.pandas DataFrame is an extremely light-weight parallel DataFrame. Modin transparently distributes the data and computation so that all you need to do is continue using the pandas API as you were before installing Modin. Unlike other parallel DataFrame systems, Modin is an extremely light-weight, robust DataFrame. Because it is so light-weight, Modin provides speed-ups of up to 4x on a laptop with 4 physical cores.

https://modin.readthedocs.io/en/latest/[+https://modin.readthedocs.io/en/latest/+]

https://github.com/modin-project/modin[+https://github.com/modin-project/modin+]

pandas-datareader
^^^^^^^^^^^^^^^^^

Up to date remote data access for pandas, works for multiple versions of pandas.

https://github.com/pydata/pandas-datareader[+https://github.com/pydata/pandas-datareader+]

https://pydata.github.io/pandas-datareader/stable/[+https://pydata.github.io/pandas-datareader/stable/+]

Pandashells
^^^^^^^^^^^

For decades, system administrators, dev-ops engineers and data analysts have been piping textual data between unix tools like grep, awk, sed, etc. Chaining these tools together provides an extremely powerful workflow.

The more recent emergence of the "data-scientist" has resulted in the increasing popularity of tools like R, Pandas, IPython, etc. These tools have amazing power for transforming, analyzing and visualizing data-sets in ways that grep, awk, sed, and even the dreaded perl-one-liner could never accomplish.

Pandashells is an attempt to marry the expressive, concise workflow of the shell pipeline with the statistical and visualization tools of the python data-stack.

Pandashells is:

* A set of command-line tools for working with tabular data
* Easily read/write data in CSV, or space delimited formats
* Quickly aggregate, join, and summarize tabular data
* Compute descriptive statistics
* Perform spectral decomposition and linear regression
* Create data visualizations that can be saved to images or rendered interactively using either a native backend or html.
* Easily integrate with unix tools like curl, awk, grep, sed, etc.

If you work with data using Python, you have almost certainly encountered Pandas, SciPy, Matplotlib, Statsmodels and Seaborn. Pandashells opens up a bash API into the python data stack with command syntax closely mirroring the underlying libraries on which it is built. This should allow those familiar with the python data stack to be immediately productive.

https://github.com/robdmc/pandashells[+https://github.com/robdmc/pandashells+]

pandoc
~~~~~~

If you need to convert files from one markup format into another, pandoc is your swiss-army knife. Pandoc can convert documents in (several dialects of) Markdown, reStructuredText, textile, HTML, DocBook, LaTeX, MediaWiki markup, TWiki markup, TikiWiki markup, Creole 1.0, Vimwiki markup, roff man, OPML, Emacs Org-Mode, Emacs Muse, txt2tags, Microsoft Word docx, LibreOffice ODT, EPUB, or Haddock markup to
XHTML, HTML5, and HTML slide shows, Microsoft Word docx, OpenOffice/LibreOffice ODT, OpenDocument XML, Microsoft
PowerPoint, EPUB version 2 or 3, FictionBook2, DocBook version 4 or 5, TEI Simple, GNU TexInfo, roff man, roff ms,
Haddock markup, JATS, InDesign ICML, OPML, LaTeX, ConTeXt, PDF and many flavors of Markdown.

Pandoc understands a number of useful markdown syntax extensions, including document metadata (title, author, date); footnotes; tables; definition lists; superscript and subscript; strikeout; enhanced ordered lists (start number and numbering style are significant); running example lists; delimited code blocks with syntax highlighting; smart quotes, dashes, and ellipses; markdown inside HTML blocks; and inline LaTeX. If strict markdown compatibility is desired, all of these extensions can be turned off.

LaTeX math (and even macros) can be used in markdown documents. Several different methods of rendering math in HTML are provided, including MathJax and translation to MathML. LaTeX math is converted (as needed by the output format) to unicode, native Word equation objects, MathML, or roff eqn.

Pandoc includes a powerful system for automatic citations and bibliographies.  Many forms of bibliography database can be used, including bibtex, RIS, EndNote, ISI, MEDLINE, MODS, and JSON citeproc. Citations work in every output format.

https://pandoc.org/[+https://pandoc.org/+]

pandoc-citeproc
^^^^^^^^^^^^^^^

This package provides a library and executable to facilitate the use of citeproc with pandoc 1.12 and greater.
The pandoc-citeproc executable can be used as a filter with pandoc to resolve and format citations using a bibliography file and a CSL stylesheet. It can also be used (with --bib2yaml or --bib2json options) to convert a bibliography to a YAML format that can be put directly into a pandoc markdown document or to CSL JSON. Bibliographies can be in any of several formats, but bibtex and biblatex are the best supported.

https://github.com/jgm/pandoc-citeproc[+https://github.com/jgm/pandoc-citeproc+]

https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md[+https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md+]

pandoc-scholar
^^^^^^^^^^^^^^

Create beautiful, semantically enriched articles with pandoc. This package provides utilities to make publishing of scientific articles as simple and pleasant as possible. It simplifies setting authors' metadata in YAML blocks, allows to add semantic annotation to citations, and only requires the programs pandoc and make.

Plain pandoc is already excellent at document conversion, but it lacks in metadata handling. Pandoc scholar offers simple ways to include metadata on authors, affiliations, contact details, and citations. The data is included into the final output as document headers. Additionally all entries can be exported as JSON-LD, a standardized format for the semantic web.

https://github.com/pandoc-scholar/pandoc-scholar[+https://github.com/pandoc-scholar/pandoc-scholar+]

https://peerj.com/articles/cs-112/[+https://peerj.com/articles/cs-112/+]

panflute
^^^^^^^^

Panflute is a Python package that makes Pandoc filters fun to write.

https://github.com/sergiocorreia/panflute[+https://github.com/sergiocorreia/panflute+]

http://scorreia.com/software/panflute/[+http://scorreia.com/software/panflute/+]

https://github.com/jgm/pandoc/wiki/Pandoc-Filters[+https://github.com/jgm/pandoc/wiki/Pandoc-Filters+]

Pandore
~~~~~~~

Pandore is a standardized library of image processing operators. The current version contains image processing operators that operate on grayscale, color and multispectral, 1D, 2D and 3D images. The library provides:

* a set of executable operators;
* a Cxx programming environment. 

Pandore is mainly dedicated to image processing experts because the use of the library needs skills on image processing operations and on the way to combine them to perform real world image processing applications.
The use of the image processing library does not require programming skills since all operators are available as executable programs. However, the development of new operators in the library needs Cxx programming skills. 

The library is a set of executable programs performing directly on image files. The development of an image processing application is done from the successive execution of operators where outputs of one operator can be used as inputs of some others. Available images are of Pandore format, but there exists some conversion operators from and into standard formats (e.g., bmp, gif, tiff). 

https://clouard.users.greyc.fr/Pandore/[+https://clouard.users.greyc.fr/Pandore/+]

Panel
~~~~~

Panel provides tools for easily composing widgets, plots, tables, and other viewable objects and controls into control panels, apps, and dashboards. Panel works with visualizations from Bokeh, Matplotlib, HoloViews, and other Python plotting libraries, making them instantly viewable either individually or when combined with interactive widgets that control them. Panel works equally well in Jupyter Notebooks, for creating quick data-exploration tools, or as standalone deployed apps and dashboards, and allows you to easily switch between those contexts as needed.

Panel makes it simple to make:

* Plots with user-defined controls
* Property sheets for editing parameters of objects in a workflow
* Control panels for simulations or experiments
* Custom data-exploration tools
* Dashboards reporting key performance indicators (KPIs) and trends
* Data-rich Python-backed web servers

Panel objects are reactive, immediately updating to reflect changes to their state, which makes it simple to compose viewable objects and link them into simple, one-off apps to do a specific exploratory task. The same objects can then be reused in more complex combinations to build more ambitious apps, while always sharing the same code that works well on its own.

https://github.com/pyviz/panel[+https://github.com/pyviz/panel+]

https://github.com/pyviz[+https://github.com/pyviz+]

papis
~~~~~

Papis is a powerful and highly extensible command-line based document and bibliography manager.
The features include:

* Synchronize your documents using git, dropbox, rsync, OwnCloud, Google Drive etc.
* No sign-up or account on any websites required to share libraries with your colleagues.
* Download directly paper information from DOI number via Crossref.
* Download papers from Sci-Hub and add them to your library with all the relevant information in seconds.
* Import your existing documents from Zotero and other managers.
* Create custom scripts or use existing scripts provided by Papis to help you achieve great tasks easily. For example, you can use papis-mail script to send your papers to fellow researchers or to your guides.
* Export documents into many formats such as bibtex, yaml.
* Free, open source and Command-line based tool.

https://github.com/papis/papis/[+https://github.com/papis/papis/+]

https://www.ostechnix.com/papis-command-line-based-document-bibliography-manager/[+https://www.ostechnix.com/papis-command-line-based-document-bibliography-manager/+]


ParallelAccelerator
~~~~~~~~~~~~~~~~~~~

ParallelAccelerator is a Julia package for speeding up compute-intensive Julia programs. In particular, Julia code that makes heavy use of high-level array operations is a good candidate for speeding up with ParallelAccelerator.

With the @acc macro that ParallelAccelerator provides, users may specify parts of a program to accelerate. ParallelAccelerator compiles these parts of the program to fast native code. It automatically eliminates overheads such as array bounds checking when it is safe to do so. It also parallelizes and vectorizes many data-parallel operations.

ParallelAccelerator is part of the High Performance Scripting (HPS) project at Intel Labs.

https://github.com/IntelLabs/ParallelAccelerator.jl[+https://github.com/IntelLabs/ParallelAccelerator.jl+]

https://parallelacceleratorjl.readthedocs.io/en/latest/[+https://parallelacceleratorjl.readthedocs.io/en/latest/+]

Param
~~~~~

Param is a library providing Parameters: Python attributes extended to have features such as type and range checking, dynamically generated values, documentation strings, default values, etc., each of which is inherited from parent classes if not specified in a subclass. Param lets you program declaratively in Python, by just stating facts about each of your parameters, and then using them throughout your code. With Parameters, error checking will be automatic, which eliminates huge amounts of boilerplate code that would otherwise be required to verify or test user-supplied values.

Param-based programs tend to contain much less code than other Python programs, instead just having easily readable and maintainable manifests of Parameters for each object or function. This way your remaining code can be much simpler and clearer, while users can also easily see how to use it properly.

http://param.pyviz.org/[+http://param.pyviz.org/+]

https://github.com/ioam/param[+https://github.com/ioam/param+]

Paraview
~~~~~~~~

ParaView is an open-source multiple-platform application for interactive, scientific visualization. It has a client–server architecture to facilitate remote visualization of datasets, and generates level of detail (LOD) models to maintain interactive frame rates for large datasets. It is an application built on top of the Visualization Toolkit (VTK) libraries. ParaView is an application designed for data parallelism on shared-memory or distributed-memory multicomputers and clusters. It can also be run as a single-computer application. 

The visualization capabilities include:

* Handles structured (uniform rectilinear, non-uniform rectilinear, and curvilinear grids), unstructured, polygonal, image, multi-block and AMR data types.
* All processing operations (filters) produce datasets. This allows the user to either further process the result of every operation or the results as a data file. For example, the user can extract a cut surface, reduce the number of points on this surface by masking and apply glyphs (i.e. vector arrows) to the result.
* Vectors fields can be inspected by applying glyphs (arrows, cones, lines, spheres, and various 2D glyphs) to the points in a dataset. The glyphs can be scaled by scalars, vector component or vector magnitude and can be oriented using a vector field.
* Contours and isosurfaces can be extracted from all data types using scalars or vector components. The results can be colored by any other variable or processed further. When possible, structured data contours/isosurfaces are extracted with fast and efficient algorithms which make use of the efficient data layout.
* A sub-region of a dataset can be extracted by cutting or clipping with an arbitrary plane (all data types), specifying a threshold criteria to exclude cells (all data types) and/or specifying a VOI (volume of interest - structured data types only).
* Streamlines can be generated using constant step or adaptive integrators. The results can be displayed as points, lines, tubes, ribbons, etc., and can be processed by a multitude of filters. Particle paths can be extracted from temporal datasets.
* The points in a dataset can be warped (displaced) with scalars (given a user defined displacement vector) or with vectors (unavailable for non-linear rectilinear grids).
* With the array calculator, new variables can be computed using existing point or cell field arrays. A multitude of scalar and vector operations are supported.
* Advanced data processing can be done using the Python Programmable filter with VTK, NumPy, SciPy and other Python modules.
* Data can be probed at a point or along a line. The results are displayed either graphically or as text and can be exported for further analysis. Data can also be extracted over time (including statistical information such as minimum, maximum and standard deviation).
* Data can be inspected quantitatively using the powerful selection mechanism and the spreadsheet view: The selection mechanism allows the user to focus on an important subset of a dataset using either interactive selection by picking a point or selecting a rectangular area as well quantitative selection mechanisms.
* The spreadsheet view allows the user to inspect either the whole dataset or the selected subset as raw numbers.
* ParaView provides many other data sources and filters by default. Any VTK source or filter can be added by providing a simple XML description.

https://www.paraview.org/[+https://www.paraview.org/+]

https://en.wikipedia.org/wiki/ParaView[+https://en.wikipedia.org/wiki/ParaView+]

ArcticViewer
^^^^^^^^^^^^

The ParaView ArcticViewer is an example application based on Cinema and intended to show off what you can do with ParaViewWeb and vtk.js. While it was developed to support a growing set of data types, it's still just an example.

The ParaView ArcticViewer is a command line tool that can work anywhere NodeJS or JavaScript is available and lets you serve and or explore large datasets in an intuitive manner.

Mobile clients can connect to a ParaView ArcticViewer server or download the native iOS application on the App Store. The latter approach wraps the JavaScript code of this project into a native Swift application for a better mobile experience.

https://kitware.github.io/arctic-viewer/[+https://kitware.github.io/arctic-viewer/+]

netCDF
^^^^^^

Reads arrays from NetCDF files into structured VTK data sets. In the absence of any other information, the files will be read as image data. This reader will also look for meta information specified by the CF convention that specify things like topology and time. This information can cause the output to be a nonuniform rectilinear grid or curvilinear (structured) grid.

https://kitware.github.io/paraview-docs/latest/python/paraview.simple.NetCDFReader.html[+https://kitware.github.io/paraview-docs/latest/python/paraview.simple.NetCDFReader.html+]

ParaViewWeb
^^^^^^^^^^^

ParaViewWeb, the JavaScript library, is a Web framework to build applications with interactive scientific visualization inside the Web browser. Those applications can leverage a VTK and/or ParaView backend for large data processing and rendering, but can also be used on a static Web server like Apache or NGINX. You can even build local command line tools and use your browser to interact with your application.

The components and features include:

* A data model that defines how data and components are associated with each other.
The common module provides core data model and helpers to intelligently enhance the data model in the presence of certain components and capabilities, which include: color handling, offscreen canvas, WebGL utilities and the various data model extensions to hold the state for a number of visualization components and UI widgets.
* A collection of visualization components to illustrate patterns and structure in large data sets. Each component highlights one of the many possible ways of viewing datasets,\.
These visualization components can be integrated into a Web-based workbench-like environment that provides new interfaces to support discovery, exploration, filtering and analysis. The visualization components module is a set of interactive tools for exploring and visualizing data that share the same API.
* Interactive tools that enable end-users to: focus on interesting details; customize the content; and modify the visualization, all while enhancing the exploration of large amounts of data. 
ParaViewWeb’s interaction module, consisting of utilities to aid with user input/interaction, mouse and touch handling, makes it easier for the developer to create captivating Web-based applications with scientific visualization.
* An IO module that provides download helpers for querying various types of data including
image, text, CSV, JSON, the VTK and ParaView web backends, and the Girder data management system.

https://kitware.github.io/paraviewweb/[+https://kitware.github.io/paraviewweb/+]

https://github.com/Kitware/paraviewweb[+https://github.com/Kitware/paraviewweb+]

pv_atmos
^^^^^^^^

Python scripting for scientific visualization software ParaView. Historically, pv_atmos has been developed to work with geophysical, and in particular, atmospheric model data (hence the name). However, pv_atmos has evolved into a very general package, and contains routines for visualizing netCDF data, and the capability to show arbitrary axes and labels in a large variety of geometries (linear and logarithmic axes, spherical geometry).

https://github.com/mjucker/pv_atmos[+https://github.com/mjucker/pv_atmos+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.al/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.al/+]

PVGeo
^^^^^

PVGeo is a Python package that contains VTK powered tools for data visualization in geophysics which are wrapped for direct use within the application ParaView by Kitware. These tools are tailored to data visualization in the geosciences with a heavy focus on structured data sets like 2D or 3D time-varying grids.

http://pvgeo.org/[+http://pvgeo.org/+]

PVGeo-HDF5
^^^^^^^^^^

An offshoot of PVGeo for HDF5 and netCDF4 data formats.

https://github.com/OpenGeoVis/PVGeo-HDF5[+https://github.com/OpenGeoVis/PVGeo-HDF5+]

PyEVTK
^^^^^^

PyEVTK (Python Export VTK) exports data to binary VTK files for visualization/analysis with packages like Paraview, VisIt, and Mayavi.

https://bitbucket.org/pauloh/pyevtk[+https://bitbucket.org/pauloh/pyevtk+]

https://github.com/pyscience-projects/pyevtk[+https://github.com/pyscience-projects/pyevtk+]

Python
^^^^^^

ParaView offers rich scripting support through Python. This support is available as part of the ParaView client (paraview), an MPI-enabled batch application (pvbatch), the ParaView python client (pvpython), or any other Python-enabled application. Using Python, users and developers can gain access to the ParaView visualization engine.

https://kitware.github.io/paraview-docs/latest/python/index.html[+https://kitware.github.io/paraview-docs/latest/python/index.html+]

TTK
^^^

The Topology ToolKit (TTK) is an open-source library and software collection for topological data analysis in scientific visualization.
It can handle scalar data defined either on regular grids or triangulations, either in 2D or in 3D. It provides a substantial collection of generic, efficient and robust implementations of key algorithms in topological data analysis. It includes:
* For scalar data: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, topological simplification;
* For bivariate scalar data: fibers, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces;
* For uncertain scalar data: mandatory critical points;
* For time-varying scalar data: critical point tracking;
* For high-dimensional / point cloud data: dimension reduction

TTK makes topological data analysis accessible to end users thanks to easy-to-use plugins for the visualization front end ParaView. Thanks to ParaView, TTK supports a variety of input data formats.

https://topology-tool-kit.github.io/[+https://topology-tool-kit.github.io/+]

https://github.com/topology-tool-kit/ttk[+https://github.com/topology-tool-kit/ttk+]

VCGLibForParaview
^^^^^^^^^^^^^^^^^

VCGLibForParaview is an university project for the Scientific and Large Data Visualization course of the University of Pisa.

The main goal is bringing the VCG (http://vcg.isti.cnr.it/vcglib/) functionalities inside Paraview but also providing a set of utilities to make really confortable moving between the VCG world and the VTK one or viceversa. The result is a Paraview plugin with the many advanced VCG algorithms encapsulated in single place and completely available through the pipeline-based framework.

https://github.com/korut94/VCGLibForParaview[+https://github.com/korut94/VCGLibForParaview+]

Parcels
~~~~~~~

The OceanParcels project develops Parcels (Probably A Really Computationally Efficient Lagrangian Simulator), a set of Python classes and methods to create customisable particle tracking simulations using output from Ocean Circulation models. Parcels can be used to track passive and active particulates such as water, plankton, plastic and fish. 

As ocean general circulation models (OGCMs) move into the petascale age, where the output of single simulations exceeds petabytes of storage space, tools to analyse the output of these models will need to scale up too. Lagrangian ocean analysis, where virtual particles are tracked through hydrodynamic fields, is an increasingly popular way to analyse OGCM output, by mapping pathways and connectivity of biotic and abiotic particulates. However, the current software stack of Lagrangian ocean analysis codes is not dynamic enough to cope with the increasing complexity, scale and need for customization of use-cases. Furthermore, most community codes are developed for stand-alone use, making it a nontrivial task to integrate virtual particles at runtime of the OGCM. Here, we introduce the new Parcels code, which was designed from the ground up to be sufficiently scalable to cope with petascale computing. We highlight its API design that combines flexibility and customization with the ability to optimize for HPC workflows, following the paradigm of domain-specific languages. Parcels is primarily written in Python, utilizing the wide range of tools available in the scientific Python ecosystem, while generating low-level C code and using just-in-time compilation for performance-critical computation.

http://oceanparcels.org/[+http://oceanparcels.org/+]

https://www.geosci-model-dev.net/10/4175/2017/gmd-10-4175-2017.html[+https://www.geosci-model-dev.net/10/4175/2017/gmd-10-4175-2017.html+]

Parquet
~~~~~~~

Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.

Parquet is built from the ground up with complex nested data structures in mind, and uses the record shredding and assembly algorithm described in the Dremel paper. We believe this approach is superior to simple flattening of nested name spaces.

Parquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented.

Parquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies.

https://parquet.apache.org/[+https://parquet.apache.org/+]

fastparquet
^^^^^^^^^^^

A Python interface to the Parquet file format.

Since it was developed as part of the Hadoop ecosystem, Parquet’s reference implementation is written in Java. This package aims to provide a performant library to read and write Parquet files from Python, without any need for a Python-Java bridge. This will make the Parquet format an ideal storage mechanism for Python-based big data workflows.

The tabular nature of Parquet is a good fit for the Pandas data-frame objects, and we exclusively deal with data-frameto and from Parquet.

The features include:

* read and write Parquet files, in single- or multiple-file format. The latter is commonly found in hive/Spark usage.
* choice of compression per-column and various optimized encoding schemes; ability to choose row divisions and partitioning on write.
* acceleration of both reading and writing using numba
* ability to read and write to arbitrary file-like objects, allowing interoperability with s3fs, hdfs3, adlfs and possibly others.
* can be called from dask, to enable parallel reading and writing with Parquet files, possibly distributed across a cluster.

https://fastparquet.readthedocs.io/en/latest/[+https://fastparquet.readthedocs.io/en/latest/+]

Parrot
~~~~~~

Parrot is a tool for attaching old programs to new storage systems. Parrot makes a remote storage system appear as a file system to a legacy application. Parrot does not require any special privileges, any recompiling, or any change whatsoever to existing programs. It can be used by normal users doing normal tasks. For example, an anonymous FTP service is made available to vi like so:

`% parrot_run vi /anonftp/ftp.gnu.org/pub/README`

Parrot is particularly handy for distributed computing, because it allows your application to access remote software and data, regardless of where it is actually executing. For example, it is commonly used in the high energy physics community to obtain remote access to the CVMFS distributed software repository.

Almost any application - whether static or dynmically linked, standard or commercial, command-line or GUI - should work with Parrot. There are a few exceptions. Because Parrot relies on the Linux ptrace interface any program that relies on the ptrace interface cannot run under Parrot. This means Parrot cannot run a debugger, nor can it run itself recursively. In addition, Parrot cannot run setuid programs, as the operating system system considers this a security risk.

Parrot also provide a new experimental features called identity boxing. This feature allows you to securely run a visiting application within a protection domain without become root or creating a new account.

http://ccl.cse.nd.edu/software/parrot/[+http://ccl.cse.nd.edu/software/parrot/+]

Parrot Virtual Machine
~~~~~~~~~~~~~~~~~~~~~~

Parrot is a register-based process virtual machine designed to run dynamic languages efficiently. It is possible to compile Parrot assembly language and PIR (an intermediate language) to Parrot bytecode and execute it. Parrot is free and open source software.

Parrot was started by the Perl community and is developed with help from the open source and free software communities. As a result, it is focused on license compatibility with Perl (Artistic License 2.0), platform compatibility across a broad array of systems, processor architecture compatibility across most modern processors, speed of execution, small size (around 700k depending on platform), and the flexibility to handle the varying demands made by Perl 6 and other modern dynamic languages. 

The differing properties of statically and dynamically typed languages have motivated the design of Parrot. Current popular virtual machines such as the Java virtual machine and the Common Language Runtime, for the .NET platform, have been designed for statically typed languages, while the languages targeted by Parrot are dynamically typed.

Virtual machines such as the Java virtual machine and the current Perl 5 virtual machine are also stack based. Parrot developers see Parrot's inclusion of registers as an advantage, as it therefore more closely resembles a hardware design, allowing the vast literature on compiler optimization to be used in generating bytecode for the Parrot virtual machine that could run at speeds closer to machine code. Other register-based virtual machines have inspired parts of Parrot's design, including LLVM, the Lua VM and Inferno's Dis. 

http://www.parrot.org/[+http://www.parrot.org/+]

http://perl6.cz/wiki/Perl_6_and_Parrot_links[+http://perl6.cz/wiki/Perl_6_and_Parrot_links+]

https://en.wikipedia.org/wiki/Parrot_virtual_machine[+https://en.wikipedia.org/wiki/Parrot_virtual_machine+]

Parsl
~~~~~

Parsl provides an intuitive, pythonic way of parallelizing codes by annotating "apps": Python functions or external applications that run concurrently.
Parsl creates a dynamic graph of tasks and their data dependencies. Tasks are only executed when their dependencies are met.
It works seamlessly with Jupyter notebooks allowing apps within a notebook to be executed in parallel and on remote resources.
Scripts are independent of the execution environment. A single script can be executed on one or more execution resources without modifying the script.
Parsl handles the complexity of ensuring data is in the right place at the right time for computation.

http://parsl-project.org/[+http://parsl-project.org/+]

https://parsl.readthedocs.io/en/stable/[+https://parsl.readthedocs.io/en/stable/+]

ParTI
~~~~~

A Parallel Tensor Infrastructure (ParTI!), is to support fast essential sparse tensor operations and tensor decompositions on multicore CPU and GPU architectures. These basic tensor operations are critical to the overall performance of tensor analysis algorithms (such as tensor decomposition). ParTI! is formerly known as SpTOL.

The supported operations are:

* Scala-tensor mul/div [CPU]
* Kronecker product [CPU]
* Khatri-Rao product [CPU]
* Sparse tensor matricization [CPU]
* Element-wise tensor add/sub/mul/div [CPU, Multicore, GPU]
* Sparse tensor-times-dense matrix (SpTTM) [CPU, Multicore, GPU]
* Sparse matricized tensor times Khatri-Rao product (SpMTTKRP) [CPU, Multicore, GPU]
* Sparse tensor matricization [CPU]
* Sparse CANDECOMP/PARAFAC decomposition
* Sparse Tucker decomposition (refer to branch JPDC)

The supported formats are:

* Coordinate (COO) format
* Hierarchical coordinate (HiCOO) format

https://github.com/hpcgarage/ParTI[+https://github.com/hpcgarage/ParTI+]

Pascal
~~~~~~

Blah.

Vector Pascal
^^^^^^^^^^^^^

Vector Pascal is a language targeted at SIMD instruction-sets such as the MMX and the AMD 3d Now

http://www.dcs.gla.ac.uk/%7Ewpc/reports/compilers/compilerindex/Doc2.html[+http://www.dcs.gla.ac.uk/%7Ewpc/reports/compilers/compilerindex/Doc2.html+]

patsy
~~~~~

patsy is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S.

For instance, if we have some variable y, and we want to regress it against some other variables x, a, b, and the interaction of a and b, then we simply write:

`patsy.dmatrices("y ~ x + a + b + a:b", data)`

and Patsy takes care of building appropriate matrices. Furthermore, it:

* Allows data transformations to be specified using arbitrary Python code: instead of x, we could have written log(x), (x > 0), or even log(x) if x > 1e-5 else log(1e-5),
* Provides a range of convenient options for coding categorical variables, including automatic detection and removal of redundancies,
* Knows how to apply ‘the same’ transformation used on original data to new data, even for tricky transformations like centering or standardization (critical if you want to use your model to make predictions),
* Has an incremental mode to handle data sets which are too large to fit into memory at one time,
* Provides a language for symbolic, human-readable specification of linear constraint matrices,
* Has a thorough test suite (>97% statement coverage) and solid underlying theory, allowing it to correctly handle corner cases that even R gets wrong, and
* Features a simple API for integration into statistical packages.

What Patsy won’t do is, well, statistics — it just lets you describe models in general terms. It doesn’t know or care whether you ultimately want to do linear regression, time-series analysis, or fit a forest of decision trees, and it certainly won’t do any of those things for you — it just gives a high-level language for describing which factors you want your underlying model to take into account. It’s not suitable for implementing arbitrary non-linear models from scratch; for that, you’ll be better off with something like Theano, SymPy, or just plain Python. But if you’re using a statistical package that requires you to provide a raw model matrix, then you can use Patsy to painlessly construct that model matrix; and if you’re the author of a statistics package, then I hope you’ll consider integrating Patsy as part of your front-end.

https://patsy.readthedocs.io/en/latest/overview.html[+https://patsy.readthedocs.io/en/latest/overview.html+]

https://github.com/pydata/patsy[+https://github.com/pydata/patsy+]

PCOCC
~~~~~

pcocc (pronounced like "peacock") stands for Private Cloud On a Compute Cluster. It allows users of an HPC cluster to host their own clusters of VMs on compute nodes, alongside regular jobs. Users are thus able to fully customize their software environments for development, testing, or facilitating application deployment. Compute nodes remain managed by the batch scheduler as usual since the clusters of VMs are seen as regular jobs. For each virtual cluster, pcocc allocates the necessary resources to host the VMs, including private Ethernet and/or Infiniband networks, creates temporary disk images from the selected templates and instantiates the requested VMs.

https://github.com/cea-hpc/pcocc[+https://github.com/cea-hpc/pcocc+]

PCL
~~~

The Point Cloud Library (PCL) is an open-source library of algorithms for point cloud processing tasks and 3D geometry processing, such as occur in three-dimensional computer vision. The library contains algorithms for feature estimation, surface reconstruction, 3D registration[4], model fitting, and segmentation. It is written in Cxx and released under the BSD license.

These algorithms have been used, for example, for perception in robotics to filter outliers from noisy data, stitch 3D point clouds together, segment relevant parts of a scene, extract keypoints and compute descriptors to recognize objects in the world based on their geometric appearance, and create surfaces from point clouds and visualize them.

PCL is cross-platform, and has been successfully compiled and deployed on Linux, MacOS, Windows, and Android/iOS. To simplify development, PCL is split into a series of smaller code libraries, that can be compiled separately. This modularity is important for distributing PCL on platforms with reduced computational or size constraints (for more information about each module see the documentation page). Another way to think about PCL is as a graph of code libraries, similar to the Boost set of Cxx libraries.

A point cloud is a data structure used to represent a collection of multi-dimensional points and is commonly used to represent three-dimensional data. In a 3D point cloud, the points usually represent the X, Y, and Z geometric coordinates of an underlying sampled surface. When color information is present (see the figures below), the point cloud becomes 4D. 

Point clouds can be acquired from hardware sensors such as stereo cameras, 3D scanners, or time-of-flight cameras, or generated from a computer program synthetically. PCL supports natively the OpenNI 3D interfaces, and can thus acquire and process data from devices such as the PrimeSensor 3D cameras, the Microsoft Kinect, or the Asus XTionPRO.

http://www.pointclouds.org/[+http://www.pointclouds.org/+]

https://github.com/PointCloudLibrary/pcl[+https://github.com/PointCloudLibrary/pcl+]

pclpy
^^^^^

Python bindings for the Point Cloud Library (PCL). Generated from headers using CppHeaderParser and pybind11.

Many other python libraries tried to bind PCL. The most popular one being python-pcl, which uses Cython. While Cython is really powerful, binding Cxx templates isn't one of its strenghts (and PCL uses templates heavily). The result for python-pcl is a lot of code repetition, which is hard to maintain and to add features to, and incomplete bindings of PCL's classes and point types.

Using pybind11, we use Cxx directly. Templates, boost::smart_ptr and the buffer protocol are examples of things that are simpler to implement.

The results so far are very promising. A large percentage of PCL is covered.

https://github.com/davidcaron/pclpy[+https://github.com/davidcaron/pclpy+]

PCRaster
~~~~~~~~

PCRaster is a Geographical Information System which consists of a set of computer tools for storing, manipulating, analyzing and retrieving geographic information. It is a raster-based system that uses a strict data type checking mechanism. This means that data type information is added to all spatial data, based upon the kind of attribute that the data represent. The use of data types controls the way the data are stored in the database and the possibilities for manipulation and analysis of the data. This strict data type checking mechanism has the advantage that it helps the user to organize the data and prevents the execution of operations that will make a nonsense result. The spatial data types implemented discriminate between various types of continuous fields and classified objects.

PCRaster has a relatively open database. The architecture of the system permits the integration of environmental modelling functions with classical GIS functions such as database maintenance, screen display and hard copy output. The modules for Cartographic and Dynamic Modelling are integrated with the GIS at a high level, which means that the GIS functions and modelling functions are incorporated in a single GIS and modelling language for performing both GIS and modelling operations.

The module for geostatistical modelling (gstat module) are integrated at a medium level with the GIS part of the package: both use a separate set of functions for manipulating the data; the map format of the central database is used and files can be automatically exchanged between the modules and the GIS part of the package. Exchange of ascii files with any other modelling or GIS package can be done using PCRaster conversion operations.

The central concept of PCRaster is a discretization of the landscape in space, resulting in cells of information. Each cell can be regarded as a set of attributes defining its properties, but one which can receive and transmit information to and from neighbouring cells. This representation of the landscape is often referred to as 2.5 D: the lateral directions in a landscape are represented by a set of neighbouring cells making up a map; relations in vertical directions, for instance between soil layers, are implemented using several attributes stored in each cell. GIS operations or operations used in modelling can be regarded as functions that induce a change in the properties of the cells on the basis of the relations within cells (between attributes on one cell location) or between cells, see figure below. In PCRaster, each functional module of the package represents a group of operations that change the properties of the cells in a specific way.

http://pcraster.geo.uu.nl/pcraster/4.2.1/documentation/index.html[+http://pcraster.geo.uu.nl/pcraster/4.2.1/documentation/index.html+]

http://pcraster.geo.uu.nl/[+http://pcraster.geo.uu.nl/+]

wflow
^^^^^

A set of python programs that can be run on the command line and perform hydrological simulations. The models are based on the PCRaster python framework.
In wflow this framework is extended (the wf_DynamicFramework class) so that models build using the framework can be controlled using the API. Links to BMI and OpenDA (www.openda.org) have been established.

The wflow distributed hydrological model platform currently includes the following models:

* the wflow_sbm model (derived from topog_sbm )
* the wflow_hbv model (a distributed version of the HBV96 model).
* the wflow_gr4 model (a distributed version of the gr4h/d models).
* the wflow_W3RA model (a global hydrological model)
* the wflow_routing model (a kinematic wave model that can run on the output of one of the hydrological models optionally including a floodplain for more realistic simulations in areas that flood).
* the wflow_wave model (a dynamic wave model that can run on the output of the wflow_routing model).
* the wflow_floodmap model (a flood mapping model that can use the output of the wflow_wave model or de wflow_routing model).

wflow is part of the Deltares OpenStreams project (http://www.openstreams.nl). The OpenStreams project is a work in progress. Wflow functions as a toolkit for distributed hydrological models within OpenStreams.

As part of the eartH2Observe project global dataset of forcing data has been compiled that can also be used with the wflow models. A set of tools is available that can work with wflow (the wflow_dem.map file) to extract data from the server and downscale these for your wflow model. Check https://github.com/earth2observe/downscaling-tools for the tools. A description of the project can be found at http://www.earth2observe.eu and the data server can be access via http://wci.earth2observe.eu

The different wflow models share the same structure but are fairly different with respect to the conceptualisation. The shared software framework includes the basic maps (dem, landuse, soil etc) and the hydrological routing via the kinematic wave. The Python class framework also exposes the models as an API and is based on the PCRaster/Python version 4.0 Beta (www.pcraster.eu).

https://www.openda.org/[+https://www.openda.org/+]

https://github.com/csdms/bmi[+https://github.com/csdms/bmi+]

PDAF
~~~~

The Parallel Data Assimilation Framework - PDAF - is a software environment for ensemble data assimilation. PDAF simplifies the implementation of the data assimilation system with existing numerical models. With this, users can obtain a data assimilation system with less work and can focus on applying data assimilation.

PDAF provides fully implemented and optimized data assimilation algorithms, in particular ensemble-based Kalman filters like LETKF and LSEIK. It allows users to easily test different assimilation algorithms and observations. PDAF is optimized for the application with large-scale models that usually run on big parallel computers and is applicable for operational applications. However, it is also well suited for smaller models and even toy models.

PDAF provides a standardized interface that separates the numerical model from the assimilation routines. This allows to perform the further development of the assimilation methods and the model independently. New algorithmic developments can be readily made available through the interface such that they can be immediately applied with existing implementations. The test suite of PDAF provides small models for easy testing of algorithmic developments and for teaching data assimilation.

PDAF is an open-source project. Its functionality will be further extended by input from research projects. In addition, users are welcome to contribute to the further enhancement of PDAF, e.g. by contributing additional assimilation methods or interface routines for different numerical models. 

http://pdaf.awi.de/trac/wiki[+http://pdaf.awi.de/trac/wiki+]

PDAL
~~~~

PDAL is a Cxx BSD library for translating and manipulating point cloud data. It is very much like the GDAL library which handles raster and vector data. The About page provides high level overview of the library and its philosophy. Visit Readers and Writers to list data formats it supports, and see Filters for filtering operations that you can apply with PDAL.

In addition to the library code, PDAL provides a suite of command-line applications that users can conveniently use to process, filter, translate, and query point cloud data. Applications provides more information on that topic.

Finally, PDAL speaks Python by both embedding and extending it. Visit Python to find out how you can use PDAL with Python to process point cloud data.

https://pdal.io/[+https://pdal.io/+]

https://github.com/PDAL/PDAL[+https://github.com/PDAL/PDAL+]

pdelia2
~~~~~~~

pdelib2 is a collection of software components which are useful to create simulators based on solving partial differential equations. The design is aimed at modularity, portability, ability to integrate with other code, and straigthforward parallelization on shared memory architectures.

Array based design of the main data structures allows interoperability with  a large number of legacy packages and leaves freedom for the choice of the programming language. pdelib2 uses computational kernels written in FORTRAN. Main data structures are implemented in Cxx. Scripting is available in Lua.

The features include:

* Data structures based on multi-dimensional arrays to store solutions, grids, matrices and other objects.
* Safe memory management based on Cxx smart pointers.
* Library access to the grid generators Triangle and TetGen.
* Grid partitioning for parallel computations and cache efficient data access using METIS
* Linear algebra subroutines, matrix assembly and nonlinear operator application designed for parallel computations (more...).
* Integrated visualization of the computed data using OpenGL.
* Linear problems are solved by iterative and direct solvers, e.g. PARDISO.
* Large part of the library accessible from the extension language Lua by using the SWIG interface generator tool.
* Graphical user interfaces for specialized problems using the FLTK toolkit.

http://www.wias-berlin.de/software/pdelib/?lang=1[+http://www.wias-berlin.de/software/pdelib/?lang=1+]

PDSLin
~~~~~~

PDSLin is based on a non-overlapping domain decomposition technique called the Schur complement method. In this method, the global system is first partitioned into smaller interior subdomain systems, which are connected only through separators. To compute the solution of the global system, the unknowns associated with the interior subdomain systems are first eliminated to form the Schur complement system, which is defined only on the separators. Since most of the fill occurs in the Schur complement, to obtain the solution on the separators, the Schur complement is solved using a preconditioned iterative method. Then, the solution on the subdomains is computed by using this solution on the separators and solving another set of subdomain systems. These unknowns associated with the mutually-independent interior subdomains are eliminated in parallel using multiple processors per subdomain. PDSLin is implemented in C with Fortran interface, and uses MPI for message passing on distributed memory machine. 

http://portal.nersc.gov/project/sparse/pdslin/[+http://portal.nersc.gov/project/sparse/pdslin/+]

https://fastmath-scidac.llnl.gov/software-catalog.html[+https://fastmath-scidac.llnl.gov/software-catalog.html+]

PeachPy
~~~~~~~

PeachPy is a Python framework for writing high-performance assembly kernels.

PeachPy aims to simplify writing optimized assembly kernels while preserving all optimization opportunities of traditional assembly. Some PeachPy features:

* Universal assembly syntax for Windows, Unix, and Golang assembly.
** PeachPy can directly generate ELF, MS COFF and Mach-O object files and assembly listings for Golang toolchain
* Automatic adaption of function to different calling conventions and ABIs.
** Functions for different platforms can be generated from the same assembly source
** Supports Microsoft x64 ABI, System V x86-64 ABI (Linux and OS X), Linux x32 ABI, Native Client x86-64 SFI ABI, Golang AMD64 ABI, Golang AMD64p32 ABI
* Automatic register allocation.
** PeachPy is flexible and lets mix auto-allocated and hardcoded registers in the same code.
* Automation of routine tasks in assembly programming:
** Function prolog and epilog and generated by PeachPy
** De-duplication of data constants (e.g. Constant.float32x4(1.0))
** Analysis of ISA extensions used in a function
* Supports x86-64 instructions up to AVX-512 and SHA
** Including 3dnow!+, XOP, FMA3, FMA4, TBM and BMI2.
** Excluding x87 FPU and most system instructions.
** Rigorously tested with auto-generated tests to produce the same opcodes as binutils.
* Auto-generation of metadata files
** Makefile with module dependencies (-MMD and -MF options)
** C header for the generated functions
** Function metadata in JSON format
* Python-based metaprogramming and code-generation.
* Multiplexing of multiple instruction streams (helpful for software pipelining).
* Compatible with Python 2 and Python 3, CPython and PyPy.

https://github.com/Maratyszcza/PeachPy[+https://github.com/Maratyszcza/PeachPy+]

Peano
~~~~~

Peano is an open source Cxx solver framework. It is based upon the fact that spacetrees, a generalisation of the classical octree concept, yield a cascade of adaptive Cartesian grids. Consequently, any spacetree traversal is equivalent to an element-wise traversal of the hierarchy of the adaptive Cartesian grids. The software Peano realises such a grid traversal and storage algorithm, and it provides hook-in points for applications performing per-element, per-vertex, and so forth operations on the grid. It also provides interfaces for dynamic load balancing, sophisticated geometry representations, and other features. Some properties are enlisted below.

Peano is plain Cxx code and depends only on MPI and Intel's TBB or OpenMP if you want to run it with distributed or shared memory support. Cxx 11 is used. There are no further dependencies or libraries required. If you intend to use Peano, we provide a small Java tool to facilitate rapid prototyping and to get rid of writing glue code. This Peano Development Toolkit (PDT) is pure Java and uses DaStGen. While we provide the PDT's sources, there's also a jar file available that comprises all required Java libraries and runs stand alone.

http://www.peano-framework.org/hpcsoftware/peano[+http://www.peano-framework.org/hpcsoftware/peano+]

Perl
~~~~

Perl modules of interest.

math::prime::util
~~~~~~~~~~~~~~~~~

A module for number theory in Perl. This includes prime sieving, primality tests, primality proofs, integer factoring, counts / bounds / approximations for primes, nth primes, and twin primes, random prime generation, and much more.

This module is the fastest on CPAN for almost all operations it supports. This includes Math::Prime::XS, Math::Prime::FastSieve, Math::Factor::XS, Math::Prime::TiedArray, Math::Big::Factors, Math::Factoring, and Math::Primality (when the GMP module is available). For numbers in the 10-20 digit range, it is often orders of magnitude faster. Typically it is faster than Math::Pari for 64-bit operations.

All operations support both Perl UV's (32-bit or 64-bit) and bignums. If you want high performance with big numbers (larger than Perl's native 32-bit or 64-bit size), you should install Math::Prime::Util::GMP and Math::BigInt::GMP. This will be a recurring theme throughout this documentation -- while all bignum operations are supported in pure Perl, most methods will be much slower than the C+GMP alternative.

The module is thread-safe and allows concurrency between Perl threads while still sharing a prime cache. It is not itself multi-threaded.

https://metacpan.org/pod/Math::Prime::Util[+https://metacpan.org/pod/Math::Prime::Util+]

https://archive.fosdem.org/2015/schedule/event/design_and_implementation_of_a_perl_number_theory_module/[+https://archive.fosdem.org/2015/schedule/event/design_and_implementation_of_a_perl_number_theory_module/+]

Perl 11
~~~~~~~

Perl 11 is not (yet) an actual version of Perl; rather, Perl 11 is currently a philosophy with 3 primary tenets:

* 1. Pluggability Of Perl On All Levels
* 2. Reunification Of Perl 5 & Perl 6
* 3. Runtime Performance Of C/Cxx Or Faster

Perl 11 promotes ideas which will make Perl 5 pluggable at the following levels:

* Runtime Virtual Machine
* Compilation Unit Format / AST
* Source Code Syntax / Compilers

This will open up the doors to many kinds of language / technology experimentation, without endangering the existing Perl 5 / CPAN code bases that we depend on every day.

http://perl11.org/[+http://perl11.org/+]

https://github.com/perl11[+https://github.com/perl11+]

cperl
^^^^^

An improved variant of perl5, running all of perl5 and CPAN code. With many perl6 features, just faster.
Faster than perl5 and perl6. It is stable and usable, but still in development with many more features being added soon.

cperl tries to follow the old perl5 spirit and principles, unlike recent perl5 changes, which wildly deviate from it.
cperl can parse and run 99.9% all of perl5 code. But there are a few incompatibilities
CPAN works. Some modules currently need patches.

http://perl11.org/cperl/[+http://perl11.org/cperl/+]

https://github.com/perl11/cperl[+https://github.com/perl11/cperl+]

gperl
^^^^^

This compiles to heavily optimized LLVM and is ~3x faster than p2. Currrently it is being decoupled to different perl5 compiler modules.

https://github.com/goccy/gperl[+https://github.com/goccy/gperl+]

MoarVM
^^^^^^

Short for "Metamodel On A Runtime", MoarVM is a modern virtual machine built for the Rakudo Perl 6 compiler and the NQP Compiler Toolchain. MoarVM is used by the majority of Perl 6 programmers. Highlights include:

* Great Unicode support, with strings represented at grapheme level
* Dynamic analysis of running code to identify hot functions and loops, and perform a range of optimizations, including type specialization and inlining
* Support for threads, a range of concurrency control constructs, and asynchronous sockets, timers, processes, and more
* Generational, parallel, garbage collection
* Support for numerous language features, including first class functions, exceptions, continuations, runtime loading of code, big integers and interfacing with native libraries

https://moarvm.org/[+https://moarvm.org/+]

https://github.com/MoarVM/[+https://github.com/MoarVM/+]

NQP
^^^

This is "Not Quite Perl" -- a lightweight Perl 6-like environment for virtual machines. The key feature of NQP is that it's designed to be a very small environment (as compared with, say, perl6 or Rakudo) and is focused on being a high-level way to create compilers and libraries for virtual machines like MoarVM [1], the JVM, and others.

Unlike a full-fledged implementation of Perl 6, NQP strives to have as small a runtime footprint as it can, while still providing a Perl 6 object model and regular expression engine for the virtual machine.

https://github.com/perl6/nqp/[+https://github.com/perl6/nqp/+]

p2
^^

p2 is a perl5 and possible perl6/nqp backend, based on Potion.

https://github.com/perl11/p2[+https://github.com/perl11/p2+]

http://perl11.org/p2/[+http://perl11.org/p2/+]

Perlito
^^^^^^^

A compiler collection that implements a subset of Perl 5 and Perl 6 programming languages.
It is a Perl to Java and Perl to Javascript compiler.

https://github.com/fglock/Perlito[+https://github.com/fglock/Perlito+]

http://fglock.github.io/Perlito/[+http://fglock.github.io/Perlito/+]

http://blogs.perl.org/users/flavio_s_glock/perlito/[+http://blogs.perl.org/users/flavio_s_glock/perlito/+]

Potion
^^^^^^

A virtual machine (VM) on which p2 runs.
Potion is an object- and mixin-oriented (traits) language.  The features include:

* Just-in-time compilation to x86 and x86-64 machine code function pointers. This means she's a speedy one. Who integrates very well with C extensions.
* Intermediate bytecode format and VM. Load and dump code. Decent speed and cross- architecture. Heavily based on Lua's VM.
* A lightweight extremely fast generational copying Cheney GC
* Bootstrapped "id" object model, based on Ian Piumarta's soda languages. This means everything in the language, including object allocation and interpreter state are part of the object model.
* Interpreter is thread-safe and reentrant.

http://perl11.org/potion/[+http://perl11.org/potion/+]

https://github.com/perl11/potion[+https://github.com/perl11/potion+]

RPerl
^^^^^

A Restricted Perl which translates a medium-magic subset of Perl 5 into C/Cxx using Inline::C and Inline::CPP.

Dynamic languages like Perl are fast at running some kinds of computational actions, such as regular expressions (text data pattern matching) and reading from a database.
Unfortunately, dynamic languages are slow at running general-purpose computations, such as arithmetic and moving data around in memory. Sometimes very slow.
Dynamic languages like Perl are also flexible, powerful, and relatively easy to learn. Sometimes too flexible.
RPerl's goal is to keep all of Perl's power and ease-of-use, while removing the redundant parts of Perl's flexibility in order to gain a major runtime speed boost.
The most complex and flexible parts of Perl are called "high magic", so RPerl is focused on supporting the "low magic" parts of Perl which can be made to run fast.`

RPerl is a general-purpose programming language, which means you can use RPerl to efficiently and effectively implement virtually any kind of software you can imagine.
RPerl is especially well-suited for building software which benefits from speed, such as scientific simulations and graphical video games.
RPerl is also good for building software which utilizes Perl's strong-suit of string manipulation; RPerl currently supports basic string operators, with full regular expression support to be added in an upcoming version.

Using RPerl, low-magic Perl 5 software may be compiled in serial mode to run 200x, 300x, maybe even 400x as fast as normal high-magic Perl. In serial mode, RPerl moves Perl from being (arguably) the slowest general-purpose language to being among Fortran, C, and Cxx as one of the very fastest. In auto-parallel mode, there is theoretically no upper limit to the speed of RPerl, and it may be (arguably) considered the fastest software on Earth.

RPerl works by implementing a restricted low-magic (low-complexity) subset of the Perl 5 language, which is then converted into Cxx and compiled into ultra-fast binary form. The resulting compiled low-magic RPerl code can be called seamlessly from existing high-magic pure Perl code. RPerl is primarily meant to be used for performance optimization, although as a true source-to-binary compiler it can also be used for strong source code obfuscation and IP protection.

http://rperl.org/[+http://rperl.org/+]

http://rperl.org/learning/[+http://rperl.org/learning/+]

https://github.com/wbraswell/rperl[+https://github.com/wbraswell/rperl+]

https://fosdem.org/2019/schedule/event/perl11/[+https://fosdem.org/2019/schedule/event/perl11/+]

SPVM
^^^^

The Station Perl Virtual Machine (SPVM) provides fast calculation and an easy C/Cxx binding.

https://metacpan.org/pod/SPVM[+https://metacpan.org/pod/SPVM+]

https://github.com/yuki-kimoto/SPVM[+https://github.com/yuki-kimoto/SPVM+]

std
^^^

This is the home of STD.pm6, the canonical
Perl 6 grammar, which is written in Perl 6 (of course).  It is also the
home of viv, a retargettable Perl 6 metacompiler which can translate
STD.pm6 into Perl 5 (and eventually other things).  The metacompiler
relies on STD.pm6 to parse itself, requiring a stored bootstrap version
of the compiled STD.pm.  This is also the home of Cursor, the canonical
implementation of the Perl 6 rules engine; it co-evolves with viv and
executes the actual rules.  This is also the home of a collection of
minor tools used in the development of STD.pm6 and viv.

https://github.com/perl6/std/[+https://github.com/perl6/std/+]

WebPerl
^^^^^^^

WebPerl uses the power of WebAssembly and Emscripten to let you run Perl in the browser.
It does not translate your Perl code to JavaScript, instead, it is a port of the perl binary to WebAssembly, so that you have the full power of Perl at your disposal.

https://webperl.zero-g.net/[+https://webperl.zero-g.net/+]

https://github.com/haukex/webperl[+https://github.com/haukex/webperl+]

PETSc
~~~~~

PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. It supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU parallelism. PETSc (sometimes called PETSc/Tao) also contains the Tao optimization software library. 

PETSc is intended for use in large-scale application projects, many ongoing computational science projects are built around the PETSc libraries. PETSc is easy to use for beginners. Moreover, its careful design allows advanced users to have detailed control over the solution process. PETSc includes a large suite of parallel linear, nonlinear equation solvers and ODE integrators that are easily used in application codes written in C, Cxx, Fortran and Python. PETSc provides many of the mechanisms needed within parallel application codes, such as simple parallel matrix and vector assembly routines that allow the overlap of communication and computation. In addition, PETSc includes support for parallel distributed arrays useful for finite difference methods. 

https://www.mcs.anl.gov/petsc/[+https://www.mcs.anl.gov/petsc/+]

https://github.com/petsc/petsc[+https://github.com/petsc/petsc+]

SLEPc
^^^^^

SLEPc the Scalable Library for Eigenvalue Problem Computations, is a software library for the solution of large sparse eigenproblems on parallel computers. It can be used for the solution of linear eigenvalue problems formulated in either standard or generalized form, as well as other related problems such as the singular value decomposition. It can also be used to solve nonlinear eigenvalue problems, either those formulated as polynomial eigenproblems or more general nonlinear problems. Finally, SLEPc provides solvers for the computation of the action of a matrix function on a vector.

The emphasis of the software is on methods and techniques appropriate for problems in which the associated matrices are sparse, for example, those arising after the discretization of partial differential equations. Therefore, most of the methods offered by the library are projection methods or other methods with similar properties. Examples of these methods are Krylov-Schur, Jacobi-Davidson, and Conjugate Gradient, to name a few. SLEPc provides implementations of these methods. It also provides built-in support for spectral transformations such as the shift-and-invert technique. SLEPc is a general library in the sense that it covers both Hermitian and non-Hermitian problems, with either real or complex arithmetic.

SLEPc is built on top of PETSc, the Portable, Extensible Toolkit for Scientific Computation. It can be considered an extension of PETSc providing all the functionality necessary for the solution of eigenvalue problems. This means that PETSc must be previously installed in order to use SLEPc. PETSc users will find SLEPc very easy to use, since it enforces the same programming paradigm. For those users which are not familiar with PETSc yet, our recommendation is to fully understand its basic concepts before proceeding with SLEPc. Parallelism in SLEPc is obtaned by means of MPI, and in addition there is some support for GPU computing.

SLEPc interfaces to some external software packages such as ARPACK or BLOPEX. 

http://slepc.upv.es/[+http://slepc.upv.es/+]

PFLOTRAN
~~~~~~~~

PFLOTRAN is an open source, state-of-the-art massively parallel subsurface flow and reactive transport code. PFLOTRAN solves a system of generally nonlinear partial differential equations describing multiphase, multicomponent and multiscale reactive flow and transport in porous materials. The code is designed to run on massively parallel computing architectures as well as workstations and laptops. Parallelization is achieved through domain decomposition using the PETSc (Portable Extensible Toolkit for Scientific Computation) libraries. PFLOTRAN has been developed from the ground up for parallel scalability and has been run on up to 2^18 processor cores with problem sizes up to 2 billion degrees of freedom. PFLOTRAN is written in object oriented, free formatted Fortran 2003. The choice of Fortran over C/C++ was based primarily on the need to enlist and preserve tight collaboration with experienced domain scientists, without which PFLOTRAN's sophisticated process models would not exist.The reactive transport equations can be solved using either a fully implicit Newton-Raphson algorithm or the less robust operator splitting method. 

The capabilities include:

* Richard's Equation
* Thermo-Hydro-Chemical
* Multiphase Water-Supercritical CO2
* Surface Flow
* Discrete Fracture Network
* Aqueous Complexation
* Sorption
* Mineral Precipitation and Dissolution
* Multiple Continuum for Heat
* Subsurface Flow-Reactive Transport Coupling
* Multiphase Ice-Water-Vapor Flow
* Structured and Unstructured Grids
* Multiple Realizations
* Multiple Inputs
* Parallel I/O

https://www.pflotran.org/[+https://www.pflotran.org/+]

https://lagrit.lanl.gov/[+https://lagrit.lanl.gov/+]

pFUnit
~~~~~~

pFUnit is a unit testing framework enabling JUnit-like testing of serial and MPI-parallel software written in Fortran. Initial support for OPENMP has been implemented. pFUnit makes use of modern Fortran programming techniques, including object oriented programming, offering a convenient, lightweight mechanism for Fortran developers to create and run software tests that specify the desired behavior for a given piece of code. The framework was originally created by developers from NASA and NGC TASC.

http://pfunit.sourceforge.net/[+http://pfunit.sourceforge.net/+]

http://www.fortran.bcs.org/2017/fortran_verification_cam.pdf[+http://www.fortran.bcs.org/2017/fortran_verification_cam.pdf+]

PHAML
~~~~~

The primary goal of the PHAML (Parallel Hierarchical Adaptive MultiLevel method) project is to develop new methods and software for the efficient solution of 2D elliptic partial differential equations (PDEs) on distributed memory parallel computers and multicore computers using adaptive mesh refinement and multigrid solution techniques.

The main accomplishments and features of PHAML are:

* low and high order finite elements on triangle grids
a*  novel approach to parallel data distribution (the Full Domain Partition)
* h-, p-, and hp-adaptive mesh refinement based on newest node bisection
* multiple choices for a posteriori error indicators/estimators
* multiple choices for hp-adaptive strategies
* parallel multigrid solver based on h- and p-hierarchical basis functions
* optional hooks into popular linear system solver packages (PETSc, hypre, SuperLU, MUMPS) as alternatives to the built-in multigrid solver
* a refinement-tree based partitioning method for dynamic load balancing
* optional hooks into popular partitioning packages (Zoltan, ParMETIS) as alternatives to the built-in partitioner
* solution of scalar, linear, self-adjoint, 2D, elliptic PDEs
* solution of other classes of PDEs including systems of equations (a.k.a. multiple component solutions), eigenvalue problems (using SLEPc, ARPACK, BLOPEX), and, with external looping, parabolic and nonlinear problems.
* boundary conditions: Dirichlet, natural (usually Neumann), mixed, and periodic
* arbitrary 2D connected, bounded domains, including curved boundaries and holes
* use of Fortran 90 features such as modules for data abstraction and optional arguments for simplifying calls to PHAML procedures
* message passing parallelism through MPI
* shared memory parallelism through OpenMP
* hybrid MPI/OpenMP parallelism for clusters of multicore computers
* extensive visualization capabilities using OpenGL for portability 

https://math.nist.gov/\~WMitchell/phaml/phaml.html[+https://math.nist.gov/~WMitchell/phaml/phaml.html+]

Pharo
~~~~~

Pharo is an open source dynamic and reflective language inspired from the programming language and integrated development environment (IDE) Smalltalk. Pharo offers strong live programming features such as immediate object manipulation, live update and hot recompiling. The live programming environment is at the heart of the system.

Pharo is a pure object-oriented dynamically typed and reflective language. It was influenced by Smalltalk. The goal of Pharo is to revisit Smalltalk design and enhance it. Pharo syntax fits on a single postcard. The object model is simple: only objects. Methods are virtual public (dynamically looked up). Fields are protected (only visible from class and subclasses). There is single inheritance between classes. A class can also be composed of traits, which are a group of methods. Fields are first class entities, named slots. 

https://en.wikipedia.org/wiki/Pharo[+https://en.wikipedia.org/wiki/Pharo+]

https://pharo.org/[+https://pharo.org/+]

http://blog.khinsen.net/posts/2018/12/19/exploring-pharo/[+http://blog.khinsen.net/posts/2018/12/19/exploring-pharo/+]

*Pharo by Example* - http://books.pharo.org/updated-pharo-by-example/[+http://books.pharo.org/updated-pharo-by-example/+]

*Deep Into Pharo* - http://deepintopharo.com/[+http://deepintopharo.com/+]

*Enterprise Pharo* - http://books.pharo.org/enterprise-pharo/[+http://books.pharo.org/enterprise-pharo/+]

Glamorous Toolkit
^^^^^^^^^^^^^^^^^

Glamorous Toolkit is the moldable integrated development environment for Pharo.
It offers a fundamentally new perspective on programming called moldable development.
We want the environment to fit the context of the current system and when it does not, we want to mold it seamlessly. This change is transformational. 

Glamorous Toolkit has cool widgets and fancy code analyses, yet the emphasis is not on the features, but on the way you experience software.

The components of GT are:

* *Inspector* - lets you define custom presentations for each and every object and navigate through them
* *Playground* - powers up the inspector and makes the code part of the live inspection flow
* *Debugger* - a moldable debugger that lets you build custom debuggers that can be switched to during debugging time
* *Coder* - for manipulating the static code
* *Transcript* - a live visual environment that enables in-place interaction with the logged events
* *Documenter* - makes creating and consuming code documentation and tutorials a beautiful experience directly in the environment, and transforms the environment into a data science tool
* *Visualizer* - makes visualization a first-class citizen in the environment, and includes the programmable engines Mondrian  (for drawing graphs) and Connector (for identifying connections between objects)
* *Examples* - an engine that lets you define examples throughout the code and use them for documentation or testing
* *Releaser* - a utility engine that allows you to release deeply nested projects
* *Bloc* - a vector-based graphical framework for creating new kinds of visual interfaces
* *XDoc* - a a container that contains content files and meta-data based on which a player can decide what tool to execute the content with, which is useful for sharing and publishing content such as live documents

https://gtoolkit.com/[+https://gtoolkit.com/+]

http://blog.khinsen.net/posts/2019/02/11/the-computational-notebook-of-the-future/[+http://blog.khinsen.net/posts/2019/02/11/the-computational-notebook-of-the-future/+]

Grafoscopio
^^^^^^^^^^^

A moldable tool for literate computing and reproducible research.

http://mutabit.com/grafoscopio/[+http://mutabit.com/grafoscopio/+]

http://joss.theoj.org/papers/10.21105/joss.00251[+http://joss.theoj.org/papers/10.21105/joss.00251+]

http://mutabit.com/repos.fossil/grafoscopio/doc/tip/intro.md[+http://mutabit.com/repos.fossil/grafoscopio/doc/tip/intro.md+]

Moose
^^^^^

Moose is a platform for software and data analysis.
It helps programmers craft custom analyses cheaply.
It's based on Pharo.
The capabilities include visualizations, browsers, parsers, models,
queries and scripting.

http://moosetechnology.org/[+http://moosetechnology.org/+]

http://themoosebook.org/[+http://themoosebook.org/+]

PolyMath
^^^^^^^^

PolyMath is a Smalltalk project, similar to existing scientific libraries like NumPy, SciPy for Python or SciRuby for Ruby. PolyMath already provide the following basic functionalities:

* complex and quaternions extensions,
* random number generators,
* fuzzy algorithms,
* KDE-trees,
* Didier Besset's numerical methods,
* Ordinary Differential Equation (ODE) solvers.

https://github.com/PolyMathOrg/PolyMath[+https://github.com/PolyMathOrg/PolyMath+]

https://github.com/SquareBracketAssociates/NumericalMethods/releases/tag/snapshot-2016-01-17[+https://github.com/SquareBracketAssociates/NumericalMethods/releases/tag/snapshot-2016-01-17+]

https://github.com/PolyMathOrg/PolyMath/wiki[+https://github.com/PolyMathOrg/PolyMath/wiki+]

Roassal
^^^^^^^

Roassal is a visualization engine, written in the Pharo and VisualWorks programming languages.
It is a scripting system for advanced interactive visualizations.
Roassal is intimate with XML, CSV, JSON. Roassal will make you an empowered data cruncher to paint and paint your data structures at will.
Many software engineering tasks involves advanced visualization techniques. Roassal, when coupled with Moose, will turn you into a surgeon for software.

http://agilevisualization.com/[+http://agilevisualization.com/+]

https://vimeo.com/search?q=roassal[+https://vimeo.com/search?q=roassal+]

PHCpack
~~~~~~~

PHCpack implements a collection of algorithms to solve polynomial systems by homotopy continuation methods.

On input is a sequence of polynomials in several variables, on output are the solutions to the polynomial system given on input. The computational complexity of this problem is NP-hard because of the exponential growth of the number of solutions as the number of input polynomials increases. For example, ten polynomials of degree two may intersect in 1,024 isolated points (that is two to the power ten). Twenty quadratic polynomials may lead to 1,048,576 solutions (that is 1,024 times 1,024). So it is not too difficult to write down small input sequences that lead to a huge output.

Even as the computation of the total number of solutions may take a long time, numerical homotopy continuation methods have the advantage that they compute one solution after the other. A homotopy is a family of polynomial systems, connecting the system we want to solve with an easier to solve system, which is called the start system. Numerical continuation methods track the solution paths starting at known solutions of an easier system to the system we want to solve. We have an optimal homotopy if every path leads to a solution, that is: there are no divergent paths.

PHCpack offers optimal homotopies for systems that resemble linear-product structures, for geometric problems in enumerative geometry, and for sparse polynomial systems with sufficiently generic choices of the coefficients. While mathematically this sounds all good, most systems arising in practical applications have their own peculiar structure and so most homotopies will lead to diverging solution paths. In general, a polynomial system may have solution sets of many different dimensions, which renders the solving process challenging but at the same time still very interesting.

https://github.com/janverschelde/PHCpack[+https://github.com/janverschelde/PHCpack+]

http://www.phcpack.org[+http://www.phcpack.org+]

https://fosdem.org/2019/schedule/event/python_solving_polynomial_systems/[+https://fosdem.org/2019/schedule/event/python_solving_polynomial_systems/+]

http://homepages.math.uic.edu/\~jan/phcpack_doc_html/index.html[+http://homepages.math.uic.edu/~jan/phcpack_doc_html/index.html+]

PHIST
~~~~~

PHIST provides implementations of and interfaces to block iterative solvers for sparse linear and eigenvalue problems. In contrast to other libraries we support multiple backends (e.g. Trilinos, PETSc and our own optimized kernels), and interfaces in multiple languages such as C, C++, Fortran 2003 and Python. PHIST has a clear focus on portability and hardware performance: in particular we support row-major storage of block vectors and using GPUs (via GHOST or Trilinos/Tpetra).

The 'Pipelined Hybrid-parallel Iterative Solver Toolkit' was developed as a sparse iterative solver framework within the ESSEX project (Equipping Sparse Solvers for Exascale). The aim of PHIST is to provide an environment for developing iterative solvers for sparse linear systems and eigenvalue problems that can tackle the challenges of today's increasingly complex CPUs and arithmetic coprocessors.

Pipelined indicates in the broadest sense that algorithms are optimized for processors with wide 'vector units' (SIMD/SIMT on GPUs). Standard schemes may not expose sufficient parallelism to allow performing the same operation on 4, 8 or 32 elements independently. Hence PHIST may for instance solve multiple systems with different shifts and right-hand sides (but the same matrix) simultaneously, replacing those that converge. Likewise, pipelining of operations during block orthogonalization allows using faster fused kernels.

Hybrid parallel means that we assume an 'MPI+X' programming model, where only MPI communication between processes is assumed and 'X' may be any additional accelerator, CPU or core level programming scheme. The 'X' depends on the underlying kernel library used, for instance, ghost uses OpenMP, SIMD intrinsics and CUDA.

https://bitbucket.org/essex/phist/wiki/Home[+https://bitbucket.org/essex/phist/wiki/Home+]

http://www.sppexa.de/sppexa-activities/software.html[+http://www.sppexa.de/sppexa-activities/software.html+]

Phylanx
~~~~~~~

Phylanx is a platform that takes a step toward providing a general purpose system for computations on distributed arrays for applied statistics on commodity cloud systems. Phylanx builds upon Spartan, Theano, and Tensorflow in an effort to generalize array operations specifically to support distributed computing. The system will decompose array computations into a predefined set of parallel operations and employ algorithms which optimize execution and data layout from of a user provided expression graph. Using hints from the user and the optimization step the expression graph will be passed to the HPX runtime which schedules work and infers the data layout on each compute locale arguments.

Phylanx improves upon previous solutions by adding more sophisticated algorithms to optimize data layout, distribution, and tiling on HPC systems, and by using cache-oblivious data layouts to improve the overall performance and scalability of the provided expression evaluations.

http://phylanx.stellar-group.org/[+http://phylanx.stellar-group.org/+]

https://github.com/STEllAR-GROUP/phylanx[+https://github.com/STEllAR-GROUP/phylanx+]

https://stellar-group.github.io/phylanx/docs/sphinx/branches/master/html/index.html[+https://stellar-group.github.io/phylanx/docs/sphinx/branches/master/html/index.html+]

https://arxiv.org/abs/1810.07591[+https://arxiv.org/abs/1810.07591+]

PhysX
~~~~~

PhysX is a library for representing three dimensional worlds made of discrete entities named actors which can in turn be composed of multiple shapes. PhysX lets the user create and destroy such actors, and tracks their explicit or proximity based interactions. Actors can either be static, be moved around by the user, or be moved by PhysX according to the laws of classical mechanics. PhysX' dynamics simulation capability includes support for collision, joints and actuation using maximal and/or reduced coordinates. Furthermore, the world may be queried by the user using a number of different tools ranging from simple ray-casts to sweep and overlap tests. PhysX provides extensions for special purpose functionality such as vehicle simulation.

PhysX is designed to be robust, high performance, scalable, portable, as well as easy to integrate and use. These capabilities make PhysX suitable as a foundation technology for game engines and other real time simulation systems.

It is important to note that PhysX does not run any code on GPUs by default. However PhysX can be configured to take advantage of CUDA capable GPUs, which provides a performance benefit proportional to the arithmetic complexity of a scene. GPU acceleration extensions are provided as an optional binary.

http://gameworksdocs.nvidia.com/simulation.html[+http://gameworksdocs.nvidia.com/simulation.html+]

https://github.com/NVIDIAGameWorks/PhysX-3.4[+https://github.com/NVIDIAGameWorks/PhysX-3.4+]

Pike
~~~~

Pike is a dynamic programming language with a syntax similar to Java and C. It is simple to learn, does not require long compilation passes and has powerful built-in data types allowing simple and really fast data manipulation.

Pike is an interpreted, object-oriented programming language. It looks a bit like C and Cxx, but it is much easier to learn and use. It can be used for small scripts as well as for large programs.

Pike is

* high-level and powerful, which means that even very complex things are easy to do in Pike,
* object-oriented, which means that you can use modern programming techniques to divide a large program into small pieces, which are much easier to write than it would be to write the entire program at once,
* interpreted, which means that you don’t have to wait for a program to compile and link when you want to run it,
* one of the fastest “scripting” languages available,
* garbage-collected, which makes programming much simpler, and also removes the risk for memory leaks and other memory-related bugs,
* easy to extend, which means that you can create plug-ins, written in Pike as well as in C or Cxx, and integrate them with the rest of Pike.

Pike can be used to write small and simple scripts, and also for very large programs: the World Wide Web servers Roxen WebServer and Caudium are both written in Pike. Pike’s advanced data types and built-in support for sockets makes it ideal for use in Internet applications.

https://pike.lysator.liu.se/[+https://pike.lysator.liu.se/+]

http://modules.gotpike.org/[+http://modules.gotpike.org/+]

https://github.com/pikelang[+https://github.com/pikelang+]

https://en.wikipedia.org/wiki/Pike_(programming_language)[+https://en.wikipedia.org/wiki/Pike_(programming_language)+]

Pint
~~~~

Pint is a Python package to define, operate and manipulate physical quantities: the product of a numerical value and a unit of measurement. It allows arithmetic operations between them and conversions from and to different units.

It is distributed with a comprehensive list of physical units, prefixes and constants. Due to its modular design, you can extend (or even rewrite!) the complete list without changing the source code. It supports a lot of numpy mathematical operations without monkey patching or wrapping numpy.

https://pint.readthedocs.io/en/0.9/[+https://pint.readthedocs.io/en/0.9/+]

https://github.com/hgrecco/pint[+https://github.com/hgrecco/pint+]

PIPS
~~~~

The goal of the PIPS project is to develop a free, open and extensible workbench for automatically analyzing and transforming scientific and signal processing applications. The PIPS workbench is especially relevant for people interested in whole program compilation, a.k.a. application compilation or interprocedural compilation, reverse-engineering, program verification, source-to-source program optimization and parallelization. Its interprocedural analyses help with program understanding and with checking legality and impact of automatic program transformations. These transformations are used to reduce the execution cost and latency, as well as the optimization cost itself. They can also be used to support or ease code debugging and maintenance, and to check and enforce coding rules.

Techniques developed for PIPS can be re-used for signal processing code written in C, because pointers, data structures and dynamic allocation are not used much, and for Java code optimization because array boundary checking must be minimized conservatively.

PIPS is not primarily designed to support compiler back-end research like SUIF but is much better as a C and Fortran source-to-source tool because the data types (e.g. complex) the code structures, the comments and, to some extent, the initial statement numbers are preserved. However, PIPS can be used to generate assembly code by lowering and specializing the interal representation.

https://pips4u.org/[+https://pips4u.org/+]

PISM
~~~~

The Parallel Ice Sheet Model PISM v1.1 is open source and capable of high resolution. It has been widely adopted as a tool for doing science. 
The features include:

* extensible atmospheric/ocean coupling
* hybrid shallow stress balance
* marine ice sheet physics
* polythermal energy conservation
* subglacial hydrology and till model
* complete documentation
* parallel simulations using MPI & PETSc
* reads and writes CF-compliant NetCDF
* inversion toolbox in Python
* verification and validation tools

http://www.pism-docs.org/wiki/doku.php[+http://www.pism-docs.org/wiki/doku.php+]

https://github.com/pism/pism[+https://github.com/pism/pism+]

PISTON
~~~~~~

The Portable Data-Parallel Visualization and Analysis Library (also referred to as PISTON) is a cross-platform software library providing frequently used operations for scientific visualization and analysis. The algorithms for these operations are specified in a data-parallel way. By using nVidia's freely downloadable Thrust library and our own tools, we can generate executable codes for different acceleration hardware architectures (GPUs and multi-core CPUs) from a single version of source code. The library is designed to be extensible and is intended to be integrated into other visualization applications.

https://github.com/kmorel/PISTON[+https://github.com/kmorel/PISTON+]

https://viz.lanl.gov/projects/PISTON.html[+https://viz.lanl.gov/projects/PISTON.html+]

pixiedust
~~~~~~~~~

PixieDust is an open source Python helper library that works as an add-on to Jupyter notebooks to improve the user experience of working with data. It also fills a gap for users who have no access to configuration files when a notebook is hosted on the cloud.

PixieDust greatly simplifies working with Python display libraries like matplotlib, but works just as effectively in Scala notebooks too. You no longer have compromise your love of Scala to generate great charts. PixieDust lets you bring robust Python visualization options to your Scala notebooks.

The capabilities include:

* packageManager lets you install Spark packages inside a Python notebook. This is something that you can't do today on hosted Jupyter notebooks, which prevents developers from using a large number of spark package add-ons.

* One single API called display() lets you visualize your Spark object in different ways: table, charts, maps, etc.... This module is designed to be extensible, providing an API that lets anyone easily contribute a new visualization plugin.

* Embedded apps. Let nonprogrammers actively use notebooks. Transform a hard-to-read notebook into a polished graphic app for business users.

* Create your own visualizations or apps using the PixieDust extensibility APIs. If you know html and css, you can write and deliver amazing graphics without forcing notebook users to type one line of code. Use the shape of the data to control when PixieDust shows your visualization in a menu.

* Notebook users can download data to .csv, HTML, JSON, etc. locally on your laptop or into a variety of back-end data sources, like Cloudant, dashDB, GraphDB, etc...

https://pixiedust.github.io/pixiedust/[+https://pixiedust.github.io/pixiedust/+]

https://github.com/pixiedust/pixiedust[+https://github.com/pixiedust/pixiedust+]

pkgsrc
~~~~~~

pkgsrc is a framework for building over 20,000 open source software packages. It is the native package manager on SmartOS, NetBSD, and Minix, and is portable across 23 different operating systems. Use one package manager across all of your systems.

Joyent provide binary packages for SmartOS/illumos, macOS, and Linux. 

Packages for Linux are currently built for Enterprise Linux (i.e. CentOS, Oracle, RedHat, Scientific) versions 6.x and 7.x. Both sets are built in 64-bit from pkgsrc trunk and are updated with the latest packages every couple of days. 

https://pkgsrc.joyent.com/install-on-linux/[+https://pkgsrc.joyent.com/install-on-linux/+]

https://www.pkgsrc.org/[+https://www.pkgsrc.org/+]

Plan 9
~~~~~~

Plan 9 from Bell Labs is a distributed operating system, originating in the Computing Sciences Research Center (CSRC) at Bell Labs in the mid-1980s, and building on UNIX concepts first developed there in the late 1960s. The final official release was in early 2015.

Under Plan 9, UNIX's "everything is a file" metaphor was to be extended via a pervasive network-centric filesystem, and graphical user interface assumed as a basis for almost all functionality, though retaining a heavily text-centric ideology.

The name Plan 9 from Bell Labs is a reference to the Ed Wood 1959 cult science fiction Z-movie Plan 9 from Outer Space.[5] Also, Glenda, the Plan 9 Bunny, is presumably a reference to Wood's film Glen or Glenda. The system continues to be used and developed by operating system researchers and hobbyists.

Plan 9 is a distributed operating system, designed to make a network of heterogeneous and geographically separated computers function as a single system.[21] In a typical Plan 9 installation, users work at terminals running the window system rio, and they access CPU servers which handle computation-intensive processes. Permanent data storage is provided by additional network hosts acting as file servers and archival storage.

https://9p.io/plan9/[+https://9p.io/plan9/+]

https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs[+https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs+]

9base
^^^^^

9base is a port of various original Plan 9 tools for Unix, based on plan9port.

https://tools.suckless.org/9base/[+https://tools.suckless.org/9base/+]

plan9port
^^^^^^^^^

Plan 9 from User Space (aka plan9port) is a port of many Plan 9 programs from their native Plan 9 environment to Unix-like operating systems.

https://github.com/9fans/plan9port[+https://github.com/9fans/plan9port+]

https://9fans.github.io/plan9port/[+https://9fans.github.io/plan9port/+]

https://9fans.github.io/plan9port/man/man1/intro.html[+https://9fans.github.io/plan9port/man/man1/intro.html+]

https://github.com/gdiazlo/acme[+https://github.com/gdiazlo/acme+]

PLANETOPLOT
~~~~~~~~~~~

PLANETOPLOT is a plotting/mapping tool based on popular Python librairies.

https://github.com/aymeric-spiga/planetoplot[+https://github.com/aymeric-spiga/planetoplot+]

https://nbviewer.jupyter.org/github/aymeric-spiga/planetoplot/blob/master/tutorial/planetoplot_tutorial.ipynb[+https://nbviewer.jupyter.org/github/aymeric-spiga/planetoplot/blob/master/tutorial/planetoplot_tutorial.ipynb+]

platform-tools
~~~~~~~~~~~~~~

Android SDK Platform-Tools is a component for the Android SDK. It includes tools that interface with the Android platform, such as adb, fastboot, and systrace. These tools are required for Android app development. They're also needed if you want to unlock your device bootloader and flash it with a new system image.

Although some new features in these tools are available only for recent versions of Android, the tools are backward compatible, so you need only one version of the SDK Platform-Tools.

The tools include:

* *adb* - A versatile command-line tool that lets you communicate with a device. The adb command facilitates a variety of device actions, such as installing and debugging apps, and it provides access to a Unix shell that you can use to run a variety of commands on a device.

* *systrace* - Allows you to collect and inspect timing information across all processes running on your device at the system level. It combines data from the Android kernel, such as the CPU scheduler, disk activity, and app threads, to generate an HTML report.

* *fastboot* - A tool/protocol for writing data directly to your phone's flash memory. In practical use, it is used to flash images such as recoveries, bootloaders, and kernels to your Android device. Outside of the obvious, you can also restore nandroid backups, change your splash screen, and flash system updates.  Fastboot can do very little without a rooted device and an engineering SPL (secondary program loader), meaning the bootloader, or HBOOT for HTC.

https://developer.android.com/studio/releases/platform-tools[+https://developer.android.com/studio/releases/platform-tools+]

https://android.gadgethacks.com/how-to/know-your-android-tools-what-is-fastboot-do-you-use-it-0155640/[+https://android.gadgethacks.com/how-to/know-your-android-tools-what-is-fastboot-do-you-use-it-0155640/+]

https://www.makeuseof.com/tag/use-adb-fastboot-android/[+https://www.makeuseof.com/tag/use-adb-fastboot-android/+]

PMIx
~~~~

The Process Management Interface (PMI) has been used for quite some time as a means of exchanging wireup information needed for interprocess communication. Two versions (PMI-1 and PMI-2) have been released as part of the MPICH effort. While PMI-2 demonstrates better scaling properties than its PMI-1 predecessor, attaining rapid launch and wireup of the roughly 1M processes executing across 100k nodes expected for exascale operations remains challenging.

PMI Exascale (PMIx) represents an attempt to resolve these questions by providing an extended version of the PMI standard specifically designed to support clusters up to and including exascale sizes. The overall objective of the project is not to branch the existing pseudo-standard definitions - in fact, PMIx fully supports both of the existing PMI-1 and PMI-2 APIs - but rather to (a) augment and extend those APIs to eliminate some current restrictions that impact scalability, and (b) provide a reference implementation of the PMI-server that demonstrates the desired level of scalability.

PMIx is designed to be particularly easy for resource managers to adopt, thus facilitating a rapid uptake into that community for application portability. Both client and server libraries are included, along with reference examples of client usage and server-side integration. A list of supported environments and versions is provided here - please check regularly as the list is changing!

PMIx targets support for the Linux operating system. A reasonable effort is made to support all major, modern Linux distributions; however, validation is limited to the most recent 2-3 releases of RedHat Enterprise Linux (RHEL), Fedora, CentOS, and SUSE Linux Enterprise Server (SLES). Support for vendor-specific operating systems is included as provided by the vendor.

The very breadth of PMIx’s scope can present a challenge to adoption by SMS vendors and programming library developers. Accordingly, the PMIx community has developed and released a PMIx “reference implementation” containing a complete implementation of the PMIx standard, each release being tied directly to a corresponding revision level of the standard.

Similarly, the PMIx community has released a “Reference RunTime Environment” — i.e., a runtime environment containing the reference implementation and capable of operating within a host system management stack (SMS). The reference RTE therefore provides an easy way of exploring PMIx capabilities and testing PMIx-based applications outside of a PMIx-enabled environment.

https://github.com/pmix/pmix[+https://github.com/pmix/pmix+]

https://pmix.org/[+https://pmix.org/+]

points2grid
~~~~~~~~~~~

points2grid generates Digital Elevation Models (DEM) using a local gridding method. The local gridding algorithm computes grid cell elevation using a circular neighbourhood defined around each grid cell based on a radius provided by the user. This neighbourhood is referred to as a bin, while the grid cell is referred to as a DEM node. Several values, including minimum, maximum, mean, and inverse distance weighted (IDW) mean, are computed for points that fall within the bin. These values are then assigned to the corresponding DEM node and used to represent the elevation variation over the neighbourhood represented by the bin. If no points are found within a given bin, the DEM node receives a value of null. points2grid also provides a null filling option, which applies an inverse distance weighted focal mean via a square moving window of 3, 5, or 7 pixels to fill cells in the DEM that have null values.

https://github.com/CRREL/points2grid[+https://github.com/CRREL/points2grid+]

poliastro
~~~~~~~~~

poliastro is an open source (MIT) collection of Python functions useful in Astrodynamics and Orbital Mechanics, focusing on interplanetary applications. It provides a simple and intuitive API and handles physical quantities with units.
The features include:

* Analytical and numerical orbit propagation
* Conversion between position and velocity vectors and classical orbital elements
* Coordinate frame transformations
* Hohmann and bielliptic maneuvers computation
* Trajectory plotting
* Initial orbit determination (Lambert problem)
* Planetary ephemerides (using SPICE kernels via Astropy)
* Computation of Near-Earth Objects (NEOs)

https://docs.poliastro.space/en/stable/[+https://docs.poliastro.space/en/stable/+]

https://github.com/poliastro/poliastro/[+https://github.com/poliastro/poliastro/+]

postmarketOS
~~~~~~~~~~~~

We are sick of not receiving updates shortly after buying new phones. Sick of the walled gardens deeply integrated into Android and iOS. That's why we are developing a sustainable, privacy and security focused free software mobile OS that is modeled after traditional Linux distributions. With privilege separation in mind. Let's keep our devices useful and safe until they physically break.

postmarketOS is an experimental, touch-optimized and pre-configured Alpine Linux. It can be installed on smartphones and other devices. 

https://postmarketos.org/[+https://postmarketos.org/+]

https://hackaday.com/2018/01/09/postmarketos-saves-old-smartphones/[+https://hackaday.com/2018/01/09/postmarketos-saves-old-smartphones/+]

https://hackaday.com/2019/01/17/postmarketos-turns-600-days-old/[+https://hackaday.com/2019/01/17/postmarketos-turns-600-days-old/+]

https://lineageos.org/[+https://lineageos.org/+]

https://e.foundation/[+https://e.foundation/+]

ppci
~~~~

The ppci (pure python compiler infrastructure) project is a compiler written entirely in python.

The project contains:

* A compiler, an assembler, a linker and a build system
* Language front-ends: Brainfuck, c3, C
* Backends for various target architectures, such as 6500, arm, avr, m68k, Microblaze, msp430, Open risc, ricv-v, stm8, x86_64, xtensa
* Support for working with WebAssembly, java JVM and OCaml bytecode.
* Support for various file formats, such as hexfile, s-records, ELF files and exe files.
* A pythonic api and a set of command line utilities build around it.
* A simple intermediate language
* Machine independent code generation algorithms for register allocation and instruction selection
* A simple way to describe an instruction set

https://ppci.readthedocs.io/en/latest/[+https://ppci.readthedocs.io/en/latest/+]

https://bitbucket.org/windel/ppci[+https://bitbucket.org/windel/ppci+]

https://godbolt.org/g/eooaPP[+https://godbolt.org/g/eooaPP+]

preCICE
~~~~~~~

preCICE (Precise Code Interaction Coupling Environment) is a coupling library for partitioned multi-physics simulations, including, but not restricted to fluid-structure interaction and conjugate heat transfer simulations. Partitioned means that preCICE couples existing programs (solvers) capable of simulating a subpart of the complete physics involved in a simulation. This allows for the high flexibility that is needed to keep a decent time-to-solution for complex multi-physics scenarios.

The software offers methods for transient equation coupling, communication means, and data mapping schemes. Ready-to-use adapters for well-known commercial and open-source solvers, such as OpenFOAM, SU2, or CalculiX, are available. Adapters for in-house codes can be implemented and validated in only a few weeks.

https://www.precice.org/[+https://www.precice.org/+]

http://www.sppexa.de/sppexa-activities/software.html[+http://www.sppexa.de/sppexa-activities/software.html+]

premake
~~~~~~~

Premake is a command line utility which reads a scripted definition of a software project and, most commonly, uses it to generate project files for toolsets like Visual Studio, Xcode, or GNU Make.
The features include:

* Maximize your potential audience by allowing developers to use the platforms and toolsets they prefer.
* Allow developers to customize the build, and output project files specific to that configuration.
* Keep builds in sync across toolsets by generating project from the Premake scripts on demand.
* Quickly update large codebases with many workspaces and projects: make the change once in your Premake script and then regenerate.
* Create project files for toolsets you don't own.
* Quickly upgrade to newer versions of your chosen toolset.
* Script common configuration and build maintenance tasks.
* Add-on modules can extend Premake with support for additional languages, frameworks, and toolsets.

In addition to its project generation capabilities, Premake also provides a complete Lua scripting environment, enabling the automation of complex configuration tasks such as setting up new source tree checkouts or creating deployment packages. These scripts will run on any platform, ending batch/shell script duplication.

https://premake.github.io/index.html[+https://premake.github.io/index.html+]

https://github.com/premake/premake-core[+https://github.com/premake/premake-core+]

https://github.com/premake/premake-core/wiki/Modules[+https://github.com/premake/premake-core/wiki/Modules+]

PRIMME
~~~~~~

PReconditioned Iterative MultiMethod Eigensolver for solving symmetric/Hermitian eigenvalue problems and singular value problems.

PRIMME, pronounced as prime, computes a few eigenvalues and their corresponding eigenvectors of a real symmetric or complex Hermitian matrix. It can also compute singular values and vectors of a square or rectangular matrix. It can find largest, smallest, or interior singular/eigenvalues and can use preconditioning to accelerate convergence. It is especially optimized for large, difficult problems, and can be a useful tool for both non-experts and experts. PRIMME is written in C99, but complete interfaces are provided for Fortran 77, MATLAB, Python, and R.

https://github.com/primme/primme[+https://github.com/primme/primme+]

http://www.cs.wm.edu\/~andreas/software/[+http://www.cs.wm.edu/~andreas/software/+]

http://www.cs.wm.edu/\~andreas/publications/primmeTOMS.pdf[+http://www.cs.wm.edu/~andreas/publications/primmeTOMS.pdf+]

https://pypi.org/project/primme/[+https://pypi.org/project/primme/+]

probabilistic programming
~~~~~~~~~~~~~~~~~~~~~~~~~

Probabilistic graphical models provide a formal lingua franca for modeling and a common target for efficient inference algorithms. Their introduction gave rise to an extensive body of work in machine learning, statistics, robotics, vision, biology, neuroscience, artificial intelligence (AI) and cognitive science. However, many of the most innovative and useful probabilistic models published by the AI, machine learning, and statistics community far outstrip the representational capacity of graphical models and associated inference techniques. Models are communicated using a mix of natural language, pseudo code, and mathematical formulae and solved using special purpose, one-off inference methods. Rather than precise specifications suitable for automatic inference, graphical models typically serve as coarse, high-level descriptions, eliding critical aspects such as fine-grained independence, abstraction and recursion.

PROBABILISTIC PROGRAMMING LANGUAGES aim to close this representational gap, unifying general purpose programming with probabilistic modeling; literally, users specify a probabilistic model in its entirety (e.g., by writing code that generates a sample from the joint distribution) and inference follows automatically given the specification. These languages provide the full power of modern programming languages for describing complex distributions, and can enable reuse of libraries of models, support interactive modeling and formal verification, and provide a much-needed abstraction barrier to foster generic, efficient inference in universal model classes. 

http://probabilistic-programming.org/wiki/Home[+http://probabilistic-programming.org/wiki/Home+]

https://arxiv.org/abs/1809.10756[+https://arxiv.org/abs/1809.10756+]

https://en.wikipedia.org/wiki/Probabilistic_programming_language[+https://en.wikipedia.org/wiki/Probabilistic_programming_language+]

https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/[+https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers.+]

http://dippl.org/[+http://dippl.org/+]

Anglican
^^^^^^^^

Anglican is a probabilistic programming language integrated with Clojure and ClojureScript.

While Anglican incorporates a sophisticated theoretical background that you are invited to explore, its value proposition is to allow intuitive modeling in a stochastic environment. It is not an academic exercise, but a practical everyday machine learning tool that makes probabilistic reasoning effective for you.

Do you interact with users or remote systems? Then you often make the unfortunate experience that they act unpredictably. Mathematically speaking you observe undeterministic or stochastic behaviour. Anglican allows you to express random variables capturing all this stochasticity for you and helps you to learn from data to execute informed decisions in your Clojure programs.

http://probprog.ml/anglican/index.html[+http://probprog.ml/anglican/index.html+]

https://github.com/probprog/anglican[+https://github.com/probprog/anglican+]

https://github.com/probprog/bopp[+https://github.com/probprog/bopp+]

https://arxiv.org/abs/1707.04314[+https://arxiv.org/abs/1707.04314+]

ArviZ
^^^^^

ArviZ is a Python package for exploratory analysis of Bayesian models. Includes functions for posterior analysis, sample diagnostics, model checking, and comparison.

The goal is to provide backend-agnostic tools for diagnostics and visualizations of Bayesian inference in Python, by first converting inference data into xarray objects. See here for more on xarray and ArviZ.

ArviZ will plot NumPy arrays, dictionaries of arrays, xarray datasets, and has built-in support for PyMC3, PyStan, Pyro, and emcee objects. Support for PyMC4, TensorFlow Probability, Edward2, and Edward are on the roadmap.

https://arviz-devs.github.io/arviz/[+https://arviz-devs.github.io/arviz/+]

https://github.com/arviz-devs/arviz[+https://github.com/arviz-devs/arviz+]

BLOG
^^^^

Bayesian Logic (BLOG) is a probabilistic modeling language. It is designed for representing relations and uncertainties among real world objects. For instance, tracking multiple targets in a video. BLOG makes it easy and concise to represet - uncertainty about the existence (and the number) of underlying objects - uncertain relations among objects - dependencies among relations and functions - observed evidence

BLOG also provides a query language to ask questions about what the world could possibly be after making observations.

BLOG also refers to the default inference system for models specified in BLOG language. 

http://bayesianlogic.github.io/[+http://bayesianlogic.github.io/+]

https://github.com/lileicc/swift[+https://github.com/lileicc/swift+]

https://bair.berkeley.edu/software.html[+https://bair.berkeley.edu/software.html+]

BUGS
^^^^

BUGS is a software package for performing Bayesian inference Using Gibbs Sampling. The user specifies a statistical model, of (almost) arbitrary complexity, by simply stating the relationships between related variables. The software includes an ‘expert system’, which determines an appropriate MCMC (Markov chain Monte Carlo) scheme (based on the Gibbs sampler) for analysing the specified model. The user then controls the execution of the scheme and is free to choose from a wide range of output types. 

The specified model belongs to a class known as Directed Acyclic Graphs (DAGs), for which there exists an elegant underlying mathematical theory. This allows us to break down the analysis of arbitrarily large and complex structures into a sequence of relatively simple computations. BUGS includes a range of algorithms that its expert system can assign to each such computational task. 

http://www.openbugs.net/w/FrontPage[+http://www.openbugs.net/w/FrontPage+]

Edward
^^^^^^

Edward is a Python library for probabilistic modeling, inference, and criticism. It is a testbed for fast experimentation and research with probabilistic models, ranging from classical hierarchical models on small data sets to complex deep probabilistic models on large data sets. Edward fuses three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming.

Edward is built on TensorFlow. It enables features such as computational graphs, distributed training, CPU/GPU integration, automatic differentiation, and visualization with TensorBoard.

http://edwardlib.org/[+http://edwardlib.org/+]

https://github.com/blei-lab/edward[+https://github.com/blei-lab/edward+]

https://arxiv.org/abs/1610.09787[+https://arxiv.org/abs/1610.09787+]

https://arxiv.org/abs/1701.03757[+https://arxiv.org/abs/1701.03757+]

JAGS
^^^^

Just another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC), developed by Martyn Plummer. JAGS has been employed for statistical work in many fields, for example ecology, management, and genetics.

JAGS aims for compatibility with WinBUGS/OpenBUGS through the use of a dialect of the same modeling language (informally, BUGS), but it provides no GUI for model building and MCMC sample postprocessing, which must therefore be treated in a separate program (for example calling JAGS from R through a library such as rjags and post-processing MCMC output in R).

The main advantage of JAGS in comparison to the members of the original BUGS family (WinBUGS and OpenBUGS) is its platform independence. It is written in Cxx, while the BUGS family is written in Component Pascal, a less widely known programming language.

JAGS can be used via the command line or run in batch mode through script files. This means that there is no need to redo the settings with every run and that the program can be called and controlled from within another program (e.g. from R via rjags as outlined above). 

http://mcmc-jags.sourceforge.net/[+http://mcmc-jags.sourceforge.net/+]

https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler[+https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler+]

LibBi
^^^^^

LibBi is used for state-space modelling and Bayesian inference on high-performance computer hardware, including multi-core CPUs, many-core GPUs (graphics processing units) and distributed-memory clusters.

The staple methods of LibBi are based on sequential Monte Carlo (SMC), also known as particle filtering. These methods include particle Markov chain Monte Carlo (PMCMC) and SMC2. Other methods include the extended Kalman filter and some parameter optimisation routines.

LibBi consists of a Cxx template library, as well as a parser and compiler, written in Perl, for its own modelling language.

http://libbi.org/[+http://libbi.org/+]

NIMBLE
^^^^^^

NIMBLE adopts and extends BUGS as a modeling language and lets you program with the models you create.

Other packages that use the BUGS language are only for Markov chain Monte Carlo (MCMC). With NIMBLE, you can turn BUGS code into model objects and use them for whatever algorithm you want. That includes algorithms provided with NIMBLE and algorithms you write using nimbleFunctions. NIMBLE extends BUGS by allowing multiple parameterizations for distributions, user-written functions and distributions, and more.

NIMBLE algorithms are written so they can adapt to different statistical models. For MCMC, NIMBLE can assign a default set of sampler choices, but you can customize the samplers from R. For example, you can choose what parameters to sample in a block, and you can easily write your own samplers and include them.  

You don't need to know anything about Cxx to use NIMBLE's compiler. NIMBLE provides R functions to call the compiled algorithms, and you get the output back in R. (You do need to have a Cxx compiler and related tools installed. See installation instructions.)

If you have a method you’d like to implement, you can program it using nimbleFunctions. The syntax is very similar to R, but you’ll need to learn some details to get started. The nimbleFunction system allows programmers to control how a particular algorithm should adapt to each model and/or variables it is applied to. The NIMBLE compiler can make nimbleFunctions run very efficiently.

nimbleFunctions don’t need to use BUGS models, so you can use them to speed up many kinds of numerical computations for any other purpose. The NIMBLE compiler can handle math, including linear algebra and distributions.  It also supports basic iteration, flow control, and data structures.

https://r-nimble.org/[+https://r-nimble.org/+]

https://cran.r-project.org/web/packages/nimble/index.html[+https://cran.r-project.org/web/packages/nimble/index.html+]

PPX
^^^

PPX is a cross-platform Probabilistic Programming eXecution protocol and API based on flatbuffers. It is intended as an open interoperability protocol between models and inference engines implemented in different probabilistic programming languages.

Probabilistic programming is about the execution probabilistic models under the control of inference engines, and PPX allows the model and the inference engine to be

* implemented in different programming languages and
* executed in separate processes and on separate machines across networks.

https://github.com/probprog/ppx[+https://github.com/probprog/ppx+]

https://google.github.io/flatbuffers/[+https://google.github.io/flatbuffers/+]

pyprob
^^^^^^

pyprob is a PyTorch-based library for probabilistic programming and inference compilation. The main focus of this library is on coupling existing simulation codebases with probabilistic inference with minimal intervention.

The main advantage of pyprob, compared against other probabilistic programming languages like Pyro, is a fully automatic amortized inference procedure based on importance sampling. pyprob only requires a generative model to be specified. Particularly, pyprob allows for efficient inference using inference compilation which trains a recurrent neural network as a proposal network.

https://github.com/probprog/pyprob[+https://github.com/probprog/pyprob+]

Stan
^^^^

Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation. Thousands of users rely on Stan for statistical modeling, data analysis, and prediction in the social, biological, and physical sciences, engineering, and business.

Users specify log density functions in Stan’s probabilistic programming language and get:

* full Bayesian statistical inference with MCMC sampling (NUTS, HMC)
* approximate Bayesian inference with variational inference (ADVI)
* penalized maximum likelihood estimation with optimization (L-BFGS)

Stan’s math library provides differentiable probability functions & linear algebra (Cxx autodiff). Additional R packages provide expression-based linear modeling, posterior visualization, and leave-one-out cross-validation.

Stan interfaces with the most popular data analysis languages (R, Python, shell, MATLAB, Julia, Stata) and runs on all major platforms (Linux, Mac, Windows).

https://mc-stan.org/[+https://mc-stan.org/+]

https://github.com/stan-dev/stan[+https://github.com/stan-dev/stan+]

https://github.com/stan-dev/pystan[+https://github.com/stan-dev/pystan+]

Turing.jl
^^^^^^^^^

Turing.jl is a Julia library for (universal) probabilistic programming. Current features include:

* Universal probabilistic programming with an intuitive modelling interface
* Hamiltonian Monte Carlo (HMC) sampling for differentiable posterior distributions
* Particle MCMC sampling for complex posterior distributions involving discrete variables and stochastic control flows
* Gibbs sampling that combines particle MCMC, HMC and many other MCMC algorithms

https://github.com/TuringLang/Turing.jl[+https://github.com/TuringLang/Turing.jl+]

http://turing.ml/docs/get-started/[+http://turing.ml/docs/get-started/+]

https://github.com/tpapp/DynamicHMC.jl[+https://github.com/tpapp/DynamicHMC.jl+]

https://arxiv.org/abs/1701.02434[+https://arxiv.org/abs/1701.02434+]

http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html[+http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html+]

http://deeplearning.net/tutorial/hmc.html[+http://deeplearning.net/tutorial/hmc.html+]

Yaps
^^^^

Stan is a popular probabilistic programming language with a self-contained syntax and semantics that is close to graphical models. Unfortunately, existing embeddings of Stan in Python use multi-line strings. That approach forces users to switch between two different language styles, with no support for syntax highlighting or simple error reporting within the Stan code. This paper tackles the question of whether Stan could use Python syntax while retaining its self-contained semantics. The answer is yes, that can be accomplished by reinterpreting the Python syntax. This paper introduces Yaps, a new frontend to Stan based on reinterpreted Python. We tested Yaps on over a thousand Stan models and made it available open-source.

https://github.com/IBM/yaps[+https://github.com/IBM/yaps+]

https://yaps.readthedocs.io/en/latest/[+https://yaps.readthedocs.io/en/latest/+]

https://arxiv.org/abs/1812.04125[+https://arxiv.org/abs/1812.04125+]

prompt_toolkit
~~~~~~~~~~~~~~

prompt_toolkit is a library for building powerful interactive command line and terminal applications in Python.
It can be a very advanced pure Python replacement for GNU readline, but it can also be used for building full screen applications.

The features include:

* Syntax highlighting of the input while typing. (For instance, with a Pygments lexer.)
* Multi-line input editing.
* Advanced code completion.
* Both Emacs and Vi key bindings. (Similar to readline.)
* Even some advanced Vi functionality, like named registers and digraphs.
* Reverse and forward incremental search.
* Runs on all Python versions from 2.6 up to 3.7.
* Works well with Unicode double width characters. (Chinese input.)
* Selecting text for copy/paste. (Both Emacs and Vi style.)
* Support for bracketed paste.
* Mouse support for cursor positioning and scrolling.
* Auto suggestions. (Like fish shell.)
* Multiple input buffers.
* No global state.
* Lightweight, the only dependencies are Pygments, six and wcwidth.

https://github.com/prompt-toolkit/python-prompt-toolkit[+https://github.com/prompt-toolkit/python-prompt-toolkit+]

https://python-prompt-toolkit.readthedocs.io/en/stable/[+https://python-prompt-toolkit.readthedocs.io/en/stable/+]

ptpython
^^^^^^^^

Ptpython is an advanced Python REPL. It should work on all Python versions from 2.6 up to 3.7 and work cross platform (Linux, BSD, OS X and Windows).

https://github.com/prompt-toolkit/ptpython/[+https://github.com/prompt-toolkit/ptpython/+]

pymux
^^^^^

A terminal multiplexer (like tmux) in Python.

https://github.com/prompt-toolkit/pymux[+https://github.com/prompt-toolkit/pymux+]

xonsh
^^^^^

Xonsh is a Python-powered, cross-platform, Unix-gazing shell language and command prompt. The language is a superset of Python 3.4+ with additional shell primitives that you are used to from Bash and IPython. It works on all major systems including Linux, Mac OSX, and Windows. Xonsh is meant for the daily use of experts and novices alike.

https://xon.sh/[+https://xon.sh/+]

protobuf
~~~~~~~~

Protocol Buffers are a method of serializing structured data. It is useful in developing programs to communicate with each other over a wire or for storing data. The method involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.

Google developed Protocol Buffers for use internally and has provided a code generator for multiple languages under an open source license (see below).

The design goals for Protocol Buffers emphasized simplicity and performance. In particular, it was designed to be smaller and faster than XML

A software developer defines data structures (called messages) and services in a proto definition file (.proto) and compiles it with protoc. This compilation generates code that can be invoked by a sender or recipient of these data structures. For example, example.proto will produce example.pb.cc and example.pb.h, which will define Cxx classes for each message and service that example.proto defines.

Canonically, messages are serialized into a binary wire format which is compact, forward- and backward-compatible, but not self-describing (that is, there is no way to tell the names, meaning, or full datatypes of fields without an external specification). There is no defined way to include or refer to such an external specification (schema) within a Protocol Buffers file. 

Though the primary purpose of Protocol Buffers is to facilitate network communication, its simplicity and speed make Protocol Buffers an alternative to data-centric Cxx classes and structs, especially where interoperability with other languages or systems might be needed in the future. 

https://developers.google.com/protocol-buffers/[+https://developers.google.com/protocol-buffers/+]

https://github.com/protocolbuffers/protobuf[+https://github.com/protocolbuffers/protobuf+]

https://en.wikipedia.org/wiki/Protocol_Buffers[+https://en.wikipedia.org/wiki/Protocol_Buffers+]

Prune
~~~~~

Prune is designed to preserve the evolution of scientific workflows so that they can be easily verified or expanded upon by other researchers. Execution of the workflow is also performed through Prune to ensure that it has all information necessary to restore the workflow as it was at any point in time, such as when results were used in a publication.

Some other preservation solutions force the user to use specific low-level operations to make up the workflow or automatically preserve those low-level operations. Prune gives the user full control over the granularity by which operations are defined in the workflow. This makes it much easier for a human to understand the workflow after the fact.

Despite this flexibility Prune enables preservation by providing a framework for the researcher to explicitly state (in advance) all data, software, and hardware dependencies for any given operation in the workflow. As a workflow executes, intermediate data can be deleted with the knowledge that it could be re-generated later if needed, which allows Prune to execute workflows with reduced storage requirements. Even final published results can be deleted as the workflow evolves because the data required to re-generate those published results is retained. All this can be done in the background with no worry about accidentally deleting some data that might be needed in the future. Prune assumes that the data about the workflow and the software used to execute it consume much less space than the actual data generated at each stage of the workflow. If this is the case Prune could store a workflow as it evolves over many years.

Both content based and derivation based identifiers are stored in the Prune repository. They are used to detect and prevent duplicate execution and storage. This can be done in an ad hoc distributed manner across repositories, and in some cases they can even detect logical equivalence when files are bitwise disparate due to timestamps or intentional randomness based on statistical models. Additional naming designed to be readable to the user can be done in a Python script which describes the Prune operations that make up a workflow. 

http://ccl.cse.nd.edu/software/prune/[+http://ccl.cse.nd.edu/software/prune/+]

PSUADE
~~~~~~

PSUADE is a acronym for Problem Solving environment for Uncertainty Analysis and Design Exploration. It is a software toolkit to facilitate the UQ tasks described above. PSUADE has a rich set of tools for performing uncertainty analysis, global sensitivity analysis, design optimization, model calibration, etc. In particular, PSUADE supports a global sensitivity methodology for models with large number of parameters and complex constraints.  It has three components:

* *Sampling Methods* - Monte Carlo, quasi-Monte Carlo, Latin hypercube and variants, orthogonal arrays, factorial and fractional factorial, Morris method, Fourier Amplitude Sampling Test (FAST), Box-Behnken, Plackett-Burman, central composite, and methods based on spatial decomposition. In addition, a few uniform and adaptive sampling refinements are supported. Basic probability distributions such as uniform, normal, lognormal, and triangular are available.
* *Simulator Execution Environment* - Once a sample design has been created, the sample points are propagated through the simulator to generate the corresponding outputs of interest. Since PSUADE supports an integrated design and analysis environment, it also provides an automated system for launching the simulators and collecting results. There are several parallel simulation modes supported by PSUADE. Details can be found in the PSUADE user manual.
* *Analysis Methods*
** basic statistical analysis (moments and correlations)
** many methods for main, second-order, and total-order effect analyses
** several dimension reduction analysis methods
** Markov Chain Monte Carlo for parameter estimation
** principal component analysis and some hypothesis testing
** response surface analysis (many different types of response surfaces)

https://computation.llnl.gov/projects/psuade/software[+https://computation.llnl.gov/projects/psuade/software+]

https://computation.llnl.gov/projects/psuade-uncertainty-quantification[+https://computation.llnl.gov/projects/psuade-uncertainty-quantification+]

PsychroLib
~~~~~~~~~~

Psychrometrics are the study of physical and thermodynamic properties of moist air. These properties include, for example, the air's dew point temperature, its wet bulb temperature, relative humidity, humidity ratio, enthalpy.

The estimation of these properties is critical in several engineering and scientific applications such as heating, ventilation, and air conditioning (HVAC) and meteorology. Although formulae to calculate the psychrometric properties of air are widely available in the literature, heir implementation in computer programs or spreadsheets can be challenging and time consuming.

PsychroLib is a library of functions to enable calculating psychrometric properties of moist and dry air. The library is available for Python, C, Fortran, JavaScript, and Microsoft Excel Visual Basic for Applications (VBA). The functions are based of formulae from the 2017 ASHRAE Handbook — Fundamentals, Chapter 1. Functions can be grouped into two categories:

* Functions for the calculation of dew point temperature, wet-bulb temperature, partial vapour pressure of water, humidity ratio or relative humidity, knowing any other of these and dry bulb temperature and atmospheric pressure.

* Functions for the calculation of other moist air properties. All these use the humidity ratio as input.

https://github.com/psychrometrics/psychrolib[+https://github.com/psychrometrics/psychrolib+]

http://joss.theoj.org/papers/cc72219ce76f5569cb1541b8e3b8527c[+http://joss.theoj.org/papers/cc72219ce76f5569cb1541b8e3b8527c+]

PSyclone
~~~~~~~~

PSyclone, the PSy code generator, is being developed for use in finite element, finite volume and finite difference codes. PSyclone development started with the aim to support the emerging API in the GungHo project for a finite element dynamical core.

The GungHo project was initiated in 2011 to address challenges of weather and climate prediction on the next generation of supercomputers. 
GungHo also proposed a novel separation of concerns for the software implementation of the dynamical core. This approach distinguishes between three layers: the Algorithm layer, the Kernel layer and the Parallelisation System (PSy) layer. Together this separation is termed PSyKAl.

The Algorithm layer specifies the algorithm that the scientist would like to run (in terms of calls to kernel routines and built-in operations) and logically operates on full fields.

The Kernel layer provides the implementation of the code kernels as subroutines. These subroutines operate on local fields (a set of elements, a vertical column, or a set of vertical columns, depending on the kernel).

The PSy layer sits in-between the algorithm and kernel layers and its primary role is to provide node-based parallel performance for the target architecture. The PSy layer can be optimised for a particular hardware architecture, such as multi-core, many-core, GPGPUs, or some combination thereof with no change to the algorithm or kernel layer code. This approach therefore offers the potential for portable performance.

Rather than writing the PSy layer manually, the PSyclone code generation system can help a user to optimise the code for a particular architecture (by providing optimisations such as blocking, loop merging, inlining etc), or alternatively, generate the PSy layer automatically.

https://psyclone.readthedocs.io/en/stable/[+https://psyclone.readthedocs.io/en/stable/+]

https://github.com/stfc/PSyclone[+https://github.com/stfc/PSyclone+]

https://puma.nerc.ac.uk/trac/GOcean[+https://puma.nerc.ac.uk/trac/GOcean+]

PTXdist
~~~~~~~

PTXdist is a build system for firmware images. It is developed by an Open Source Community including Pengutronix since 2001. The configuration system Kconfig, known from the Linux kernel, is used to select and configure each package. The collection of recipes is based on GNU Make and Bash.

PTXdist is an easy-to-use Linux distribution build tool that allows you to compose your embedded Linux out of a huge set of standard components with a simple kernel-like configuration interface.

Despite being rather simple in the basic configuration PTXdist also allows you to perform truly complex and very fine-grained customization of the packages and system configuration you need. 

PTXdist is not a distribution: Our experience is that binary distributions are too inflexible for embedded systems. That’s why PTXdist builds the target system directly from the original source code. Here, PTXdist is intended as “executable documentation”: the necessary steps for generating the target system are stored in a form in which they can be reviewed and reproduced as required, but can also be executed by a less savvy user at the push of a button.

https://www.ptxdist.org/[+https://www.ptxdist.org/+]

https://git.pengutronix.de/cgit/ptxdist[+https://git.pengutronix.de/cgit/ptxdist+]

barebox
^^^^^^^

barebox is a bootloader designed for embedded systems. It runs on a variety of architectures including x86, ARM, MIPS, PowerPC and others.

barebox aims to be a versatile and flexible bootloader, not only for booting embedded Linux systems, but also for initial hardware bringup and development. barebox is highly configurable to be suitable as a full-featured development binary as well as for lean production systems. Just like busybox is the Swiss Army Knife for embedded Linux, barebox is the Swiss Army Knife for bare metal, hence the name. 

https://www.barebox.org/[+https://www.barebox.org/+]

https://git.pengutronix.de/cgit/barebox[+https://git.pengutronix.de/cgit/barebox+]

RAUC
^^^^

RAUC is a lightweight update client that runs on your Embedded Linux device and reliably controls the procedure of updating your device with a new firmware revision. RAUC is also the tool on your host system that lets you create, inspect and modify update artifacts for your device.

https://www.rauc.io/[+https://www.rauc.io/+]

https://github.com/rauc[+https://github.com/rauc+]

https://rauc.readthedocs.io/en/latest/[+https://rauc.readthedocs.io/en/latest/+]

https://github.com/rauc/meta-rauc[+https://github.com/rauc/meta-rauc+]

PUMI
~~~~

An efficient distributed mesh data structure is needed to support parallel adaptive analysis since it strongly influences the overall performance of adaptive mesh-based simulations. In addition to the general mesh-based operations, such as mesh entity creation/deletion, adjacency and geometric classification, iterators, arbitrary attachable data to mesh entities, etc., the distributed mesh data structure must support (i) efficient communication between entities duplicated over multiple processors, (ii) migration of mesh entities between processors, and (iii) dynamic load balancing. Issues associated with supporting parallel adaptive analysis on unstructured meshes include dynamic mesh load balancing techniques, and data structure and algorithms for parallel mesh adaptation. 

The Parallel Unstructured Mesh Infrastructure (PUMI) is an unstructured, distributed mesh data management system that is capable of handling general non-manifold models and effectively supporting automated adaptive analysis. PUMI supports a full range of operations on unstructured meshes on massively parallel computers consisiting of five libraries:

* https://scorec.rpi.edu/pcu/[PCU] for phased message passing and thread management
* https://scorec.rpi.edu/pumi/pumi_geom.php[GMI] for geometric model interface
* https://scorec.rpi.edu/\~dibanez/core/mds.html[MDS] for unstructured mesh representation
* https://scorec.rpi.edu/\~dibanez/core/apfMesh_8h.html[APF_Mesh] for partition model and distributed mesh management
* https://scorec.rpi.edu/apf/[APF_Field] for field management

The PUMI provides a core capability used in all the automated adaptive simulation software developed at RPI’s Scientific Computation Research Center.

https://scorec.rpi.edu/pumi/[+https://scorec.rpi.edu/pumi/+]

https://github.com/CEED/PUMI[+https://github.com/CEED/PUMI+]

https://github.com/SCOREC/core[+https://github.com/SCOREC/core+]

Puppet
~~~~~~

Puppet, an automated administrative engine for your Linux, Unix, and Windows systems, performs administrative tasks (such as adding users, installing packages, and updating server configurations) based on a centralized specification.

https://puppet.com/[+https://puppet.com/+]

https://github.com/puppetlabs/puppet[+https://github.com/puppetlabs/puppet+]

Pure Data
~~~~~~~~~

Pure Data (Pd) is a visual programming language developed by Miller Puckette in the 1990s for creating interactive computer music and multimedia works. While Puckette is the main author of the program, Pd is an open-source project with a large developer base working on new extensions. It is released under a license similar to the BSD license. It runs on GNU/Linux, Mac OS X, iOS, Android and Windows.

With the addition of the Graphics Environment for Multimedia (GEM) external, and externals designed to work with it (like Pure Data Packet / PiDiP for Linux, Mac OS X), framestein for Windows, GridFlow (as n-dimensional matrix processing, for Linux, Mac OS X, Windows), it is possible to create and manipulate video, OpenGL graphics, images, etc., in realtime with extensive possibilities for interactivity with audio, external sensors, etc.

Pd is natively designed to enable live collaboration across networks or the Internet, allowing musicians connected via LAN or even in disparate parts of the globe to create music together in real time. Pd uses FUDI as a networking protocol. 

Pd is a dataflow programming language. As with most DSP software, there are two primary rates at which data is passed: sample (audio) rate, usually at 44,100 samples per second, and control rate, at 1 block per 64 samples. Control messages and audio signals generally flow from the top of the screen to the bottom between "objects" connected via inlets and outlets.

Pd supports four basic types of text entities: messages, objects, atoms, and comments. Atoms are the most basic unit of data in Pd, and they consist of either a float, a symbol, or a pointer to a data structure (in Pd, all numbers are stored as 32-bit floats). Messages are composed of one or more atoms and provide instructions to objects. A special type of message with null content called a bang is used to initiate events and push data into flow, much like pushing a button.

Pd's native objects range from the basic mathematical, logical, and bitwise operators found in every programming language to general and specialized audio-rate DSP functions (designated by a tilde (~) symbol), such as wavetable oscillators, the Fast Fourier transform (fft~), and a range of standard filters. Data can be loaded from file, read in from an audio board, MIDI, via Open Sound Control (OSC) through a Firewire, USB, or network connection, or generated on the fly, and stored in tables, which can then be read back and used as audio signals or control data. 


https://puredata.info/[+https://puredata.info/+]

https://puredata.info/docs/StartHere[+https://puredata.info/docs/StartHere+]

https://github.com/pure-data/pure-data[+https://github.com/pure-data/pure-data+]

https://www.soundonsound.com/techniques/pure-data-introduction[+https://www.soundonsound.com/techniques/pure-data-introduction+]

https://en.wikipedia.org/wiki/Pure_Data[+https://en.wikipedia.org/wiki/Pure_Data+]

PVS
~~~

The Prototype Verification System (PVS) is a specification language integrated with support tools and an automated theorem prover, developed at the Computer Science Laboratory of SRI International in Menlo Park, California.

PVS is based on a kernel consisting of an extension of Church's theory of types with dependent types, and is fundamentally a classical typed higher-order logic. The base types include uninterpreted types that may be introduced by the user, and built-in types such as the booleans, integers, reals, and the ordinals. Type-constructors include functions, sets, tuples, records, enumerations, and abstract data types. Predicate subtypes and dependent types can be used to introduce constraints; these constrained types may incur proof obligations (called type-correctness conditions or TCCs) during typechecking. PVS specifications are organized into parameterized theories. 

The specification language of PVS is based on classical, typed higher-order logic. The base types include uninterpreted types that may be introduced by the user, and built-in types such as the booleans, integers, reals, and the ordinals up to epsilon_0; the type-constructors include functions, sets, tuples, records, enumerations, and inductively-defined abstract data types, such as lists and binary trees, and coinductively-defined abstract data types such as streams. Predicate subtypes and dependent types can be used to introduce constraints, such as the type of prime numbers or order-preserving maps. These constrained types may incur proof obligations (called type-correctness conditions or TCCs) during typechecking, but greatly increase the expressiveness and naturalness of specifications. In practice, most of the obligations are discharged automatically by the theorem prover. PVS specifications are organized into parameterized theories that may contain assumptions, definitions, axioms, and theorems. Parameters can include constants, types, and theory instances. Theory interpretations are used to instantiate the declared types and constants in a theory with definitions. Theory interpretations can be used for exhibiting models for an axiomatic theory and to refine a abstract theory in terms of a concrete one. Definitions are guaranteed to be conservative extensions; to ensure this, recursive function definitions generate termination proof obligations. Inductively-defined relations are also supported. PVS expressions provide the usual arithmetic and logical operators, function application, lambda abstraction, and quantifiers, within a natural syntax. Names may be freely overloaded, including those of the built-in operators such as AND and +. Tabular specifications of the kind advocated by Parnas are supported, with automated checks for disjointness and coverage of conditions. An extensive prelude of built-in theories provides hundreds of useful definitions and lemmas; user-contributed libraries provide many more. 

The PVS theorem prover provides a collection of powerful primitive inference procedures that are applied interactively under user guidance within a sequent calculus framework. The primitive inferences include propositional and quantifier rules, induction, rewriting, simplification using decision procedures for equality and linear arithmetic, data and predicate abstraction, and symbolic model checking. The implementations of these primitive inferences are optimised for large proofs: for example, propositional simplification uses BDDs, and auto-rewrites are cached for efficiency. User-defined procedures can combine these primitive inferences to yield higher-level proof strategies. There is a synergistic interaction between the PVS language features and the prover so that the type information associated with a term is exploited by the inference mechanisms, and conversely, the automation in the prover is helpful in automatically discharging TCCs. Proofs yield scripts that can be edited, attached to additional formulas, and rerun. This allows many similar theorems to be proved efficiently, permits proofs to be adjusted economically to follow changes in requirements or design, and encourages the development of readable proofs.

PVS includes a BDD-based decision procedure for the relational mu-calculus and thereby provides an experimental integration between theorem proving and CTL model checking. PVS has recently been extended to use the Yices SMT solver as an endgame prover and an infinite-state bounded model checker, the PVSio framework for evaluating ground PVS expressions, and a random testing capability that can be used during proofs. 

PVS uses Gnu or X Emacs to provide an integrated interface to its specification language and prover. Commands can be selected either by pull-down menus or by extended Emacs commands. Extensive help, status-reporting and browsing tools are available, as well as the ability to generate typeset specifications (in user-defined notation) using LaTeX. Proof trees and theory hierarchies can be displayed graphically using Tcl/Tk. 

http://pvs.csl.sri.com/index.shtml[+http://pvs.csl.sri.com/index.shtml+]

NASA PVS Library
^^^^^^^^^^^^^^^^

The NASA PVS Library is a collection of formal PVS developments maintained by the NASA Langley Formal Methods Team. The NASA PVS library is part of the research on theorem proving sponsored by NASA Langley.

https://github.com/nasa/pvslib[+https://github.com/nasa/pvslib+]

https://github.com/nasa/WellClear[+https://github.com/nasa/WellClear+]

https://github.com/nasa/PRECiSA[+https://github.com/nasa/PRECiSA+]

https://github.com/nasa/PolyCARP[+https://github.com/nasa/PolyCARP+]

PyAbel
~~~~~~

PyAbel is a Python package that provides functions for the forward and inverse Abel transforms. The forward Abel transform takes a slice of a cylindrically symmetric 3D object and provides the 2D projection of that object. The inverse abel transform takes a 2D projection and reconstructs a slice of the cylindrically symmetric 3D distribution.

Inverse Abel transforms play an important role in analyzing the projections of angle-resolved photoelectron/photoion spectra, plasma plumes, flames, and solar occultation.

PyAbel provides efficient implementations of several Abel transform algorithms, as well as related tools for centering images, symmetrizing images, and calculating properties such as the radial intensity distribution and the anisotropy parameters.

https://github.com/PyAbel/PyAbel[+https://github.com/PyAbel/PyAbel+]

https://arxiv.org/abs/1902.09007[+https://arxiv.org/abs/1902.09007+]

pyccel
~~~~~~

Pyccel is a new static compiler for Python that uses Fortran as backend language while enabling High-Performance Computing HPC capabilities.

Fortran is a computer language for scientific programming that is tailored for efficient run-time execution on a wide variety of processors. Even if the 2003 and 2008 standards added major improvements like OOP, Coarrays, Submodules, do concurrent, etc ... they are not covered by all available compilers. Moreover, the Fortran developer still suffers from the lack of meta-programming compared to Cxx ones. Therefore, it is more and more difficult for applied mathematicians and computational physicists to write applications at the state of art (targeting CPUs, GPUs, MICs) while implementing complicated algorithms or numerical schemes.

Pyccel can be used in two cases: accelerate Python code by converting it to Fortran and calling f2py, and generate portable HPC Fortran codes from a DSL using the Python syntax.

In order to achieve the second point, we developed an internal DSL for types and macros. The later is used to map sentences based on mpi4py, scipy.linalg.blas or lapack onto the appropriate calls in Fortran. Moreover, two parsers, for OpenMP and OpenACC, were added too, allowing for explicit parallelism through the use of pragmas.

Last but not least, Pyccel is an extension of Sympy. Actually, it converts a Python code to symbolic expressions/trees, from a Full Syntax Tree (RedBaron), then annotates the new AST using types or different settings provided by the user.

https://github.com/pyccel/pyccel[+https://github.com/pyccel/pyccel+]

https://pyccel.readthedocs.io/[+https://pyccel.readthedocs.io/+]

PyCWT
~~~~~

A Python module for continuous wavelet spectral analysis. It includes a collection of routines for wavelet transform and statistical analysis via FFT algorithm. In addition, the module also includes cross-wavelet transforms, wavelet coherence tests and sample scripts.

https://github.com/regeirk/pycwt[+https://github.com/regeirk/pycwt+]

https://pycwt.readthedocs.io/en/latest/[+https://pycwt.readthedocs.io/en/latest/+]

PyDEC
~~~~~

Exterior calculus is the generalization of vector calculus to manifolds. PyDEC is a Python library for computations related to the discretization of exterior calculus which includes numerical solution of partial differential equations. It is also useful for purely topological computations. Thus PyDEC facilitates inquiry into both physical problems on manifolds as well as purely topological problems on abstract complexes. It uses efficient algorithms for constructing the operators and objects and related topological problems. Our algorithms are formulated in terms of high-level matrix operations which extend to arbitrary dimension. As a result, our implementations map well to the facilities of numerical libraries such as NumPy and SciPy. The availability of such libraries makes Python suitable for prototyping numerical methods. The code and the companion paper includes examples where we demonstrate how PyDEC is used to solve physical and topological problems.

https://github.com/hirani/pydec[+https://github.com/hirani/pydec+]

https://arxiv.org/abs/1103.3076[+https://arxiv.org/abs/1103.3076+]

PyDMD
~~~~~

PyDMD is a Python package that uses Dynamic Mode Decomposition for a data-driven model simplification based on spatiotemporal coherent structures.

Dynamic Mode Decomposition (DMD) is a model reduction algorithm developed by Schmid (see "Dynamic mode decomposition of numerical and experimental data"). Since then has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. DMD relies only on the high-fidelity measurements, like experimental data and numerical simulations, so it is an equation-free algorithm. Its popularity is also due to the fact that it does not make any assumptions about the underlying system. See Kutz ("Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems") for a comprehensive overview of the algorithm and its connections to the Koopman-operator analysis, initiated in Koopman ("Hamiltonian systems and transformation in Hilbert space"), along with examples in computational fluid dynamics.

In the last years many variants arose, such as multiresolution DMD, compressed DMD, forward backward DMD, and higher order DMD among others, in order to deal with noisy data, big dataset, or spurius data for example.

In PyDMD we implemented the majority of the variants mentioned above with a user friendly interface.

The research in the field is growing both in computational fluid dynamic and in structural mechanics, due to the equation-free nature of the model.

https://github.com/mathLab/PyDMD[+https://github.com/mathLab/PyDMD+]

https://mathlab.github.io/PyDMD/[+https://mathlab.github.io/PyDMD/+]

PyDy
~~~~

PyDy, short for Python Dynamics, is a tool kit written in the Python programming language that utilizes an array of scientific programs to enable the study of multibody dynamics. The goal is to have a modular framework and eventually a physics abstraction layer which utilizes a variety of backends that can provide the user with their desired workflow, including model specification, equation of motion generation,
simulation, visualization and publication.

We started by building the SymPy mechanics package which provides an API for building models and generating the symbolic equations of motion for complex multibody systems. More recently we developed two packages, pydy.codegen and pydy.viz, for simulation and visualization of the models, respectively. This Python package contains these two packages and other tools for working with mathematical models generated from SymPy mechanics. The remaining tools currently used in the PyDy workflow are popular scientific Python packages such as NumPy, SciPy, IPython, Jupyter, ipywidgets, and matplotlib (i.e. the SciPy stack) which provide additional code for numerical analyses, simulation, and visualization.

https://github.com/pydy/pydy[+https://github.com/pydy/pydy+]

pyEMU
~~~~~

pyEMU is a set of python modules for model-independent, user-friendly, computer model uncertainty analysis. pyEMU is tightly coupled to the open-source suite PEST and PESTxx,
which are tools for model-independent parameter estimation. However, pyEMU can be used with generic array objects, such as numpy ndarrays.

Several equations are implemented, including Schur's complement for conditional uncertainty propagation (a.k.a. Bayes Linear estimation) (the foundation of the PREDUNC suite from PEST) and error variance analysis (the foundation of the PREDVAR suite of PEST). pyEMU has easy-to-use routines for parmaeter and data worth analyses, which estimate how increased parameter knowledge and/or additional data effect forecast uncertainty in linear, Bayesian framework. Support is also provided for Monte Carlo analyses via an Ensemble and MonteCarlo class, including the null-space monte carlo approach.

pyEMU also includes lots of functionality for dealing with PEST(xx) datasets, such as:

* manipulation of PEST control files, including the use of pandas for sophisticated editing of the parameter data and observation data sections
* creation of PEST control files from instruction and template files
* going between site sample files and pandas dataframes - really cool for observation processing
* easy-to-use observation (re)weigthing via residuals or user-defined functions
* handling Jacobian and covariance matrices, including functionality to go between binary and ASCII matrices, reading and writing PEST uncertaity files. Covariance matrices can be instaniated from relevant control file sections, such as parameter bounds or observation weights. The base Matrix class overloads most common linear algebra operators so that operations are automatically aligned by row and column name. Builtin SVD is also included in all Matrix instances.
* geostatistics including geostatistical structure support, reading and writing PEST structure files and creating covariance matrices implied by nested geostatistical structures, and ordinary kriging (in the utils.geostats.OrdrinaryKrige object), which replicates the functionality of pest utility ppk2fac. See test/utils.py for an example of how the OrdinaryKrige class functions.
* a prototype, model-independent iterative ensemble smoother, based on the Levenburg-Marquardt algorithm of Chen and Oliver (2013). See autotests/smoother.py for examples of how this prototype works.
* composite scaled sensitivity calculations
* calculation of correlation coefficient matrix from a given covariance matrix
* prototype Karhunen-Loeve-based parameterization as an alternative to pilot points for spatially-distributed parameter fields
* a helper function to start a group of tcp/ip workers on a local machine for parallel PEST++/BeoPEST runs
* full support for prior information equations in control files
* preferred differencing prior information equations where the weights are based on the Pearson correlation coefficient
* verification-based tests based on results from several PEST utilities

https://github.com/jtwhite79/pyemu[+https://github.com/jtwhite79/pyemu+]

https://github.com/jtwhite79/pestpp[+https://github.com/jtwhite79/pestpp+]

http://www.pesthomepage.org/[+http://www.pesthomepage.org/+]

PyEphem
~~~~~~~

PyEphem provides an ephem Python package for performing high-precision astronomy computations. The underlying numeric routines are coded in C and are the same ones that drive the popular XEphem astronomy application, whose author, Elwood Charles Downey, generously gave permission for their use in PyEphem. The name ephem is short for the word ephemeris, which is the traditional term for a table giving the position of a planet, asteroid, or comet for a series of dates.

The author says "I recommend using Skyfield instead of PyEphem if it’s possible for your new project to do so!"

https://rhodesmill.org/skyfield/[+https://rhodesmill.org/skyfield/+]

https://github.com/brandon-rhodes/pyephem[+https://github.com/brandon-rhodes/pyephem+]

https://rhodesmill.org/pyephem/[+https://rhodesmill.org/pyephem/+]

PyGeM
~~~~~

PyGeM is a python package using Free Form Deformation, Radial Basis Functions and Inverse Distance Weighting to parametrize and morph complex geometries. It is ideally suited for actual industrial problems, since it allows to handle:

* Computer Aided Design files (in .iges, .step, and .stl formats)
* Mesh files (in .unv and OpenFOAM formats)
* Output files (in .vtk format)
* LS-Dyna Keyword files (.k format)

By now, it has been used with meshes with up to 14 milions of cells. 

https://github.com/mathLab/PyGeM[+https://github.com/mathLab/PyGeM+]

http://mathlab.github.io/PyGeM/[+http://mathlab.github.io/PyGeM/+]

pyGMCALab
~~~~~~~~~

Toolbox for solving sparse matrix factorization problems.
It is composed of the following modules:

* GMCA is the building block of the pyGMCALab toolbox. This algorithm basically tackles sparse BSS problems :
* Building upon GMCA, AMCA is an extension that specifically deals with partially correlated sources
* nGMCA allows solving sparse non-negative matrix factorization problems (sparse NMF). Sparse modelling can be done either in the sample domain or in a transformed domain. For that purpose, a python/C wrapper for wavelets (RedWave) is also provided as an external module (see ./redwave_toolbox).
* rGMCA copes with sparse BSS problems in the presence of outliers. Tr_rGMCA is an extension that can benefit from the morphological diversity between the outliers and the sources.

For all these algorithms, we strongly advise the interested user to have a close look at the jupyter notebooks.

https://github.com/jbobin/pyGMCALab[+https://github.com/jbobin/pyGMCALab+]

http://www.cosmostat.org/software/gmcalab[+http://www.cosmostat.org/software/gmcalab+]

http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html[+http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html+]

PyGMO
~~~~~

PyGMO (the Python Parallel Global Multiobjective Optimizer) is a scientific library providing a large number of optimisation problems and algorithms under the same powerful parallelization abstraction built around the generalized island-model paradigm. What this means to the user is that the available algorithms are all automatically parallelized (asynchronously, coarse-grained approach) thus making efficient use of the underlying multicore architecture. The user can also program his own solvers ... they also will be parallelized by PyGMO!! PyGMO’s implementation of the generalized migration operator allows the user to easily define “migration paths” (topologies) between a large number of “islands” (CPU cores).

Efficient implementantions of state-of-the-art bio-inspired algorithms are sided to state-of the art optimization algorithms (Simplex Methods, SQP methods ....) and can be easily mixed (also with your newly invented algorithms) to build a super-algorithm exploiting cooperation via the asynchronous, generalized island model.

Many complex-networks topologies (Hypercube, Ring, Barabasi-Albert, Watts-Strogatz, Erdos-Renyi, etc.) are built-in and may be used to define the migration pathways of good solutions among islands. Custom topologies are also possible.

PyGMO can be used to solve constrained, unconstrained, single objective, multiple objective, continuous, mixed int optimization problem, or to perform research on novel algorithms and paradigms and easily compare them to state of the art implementations of established ones.

PyGMO is interfaced with SciPy optimization algorithms, NLOPT algorithms, GSL algorithms, SNOPT, IPOPT and, hopefully .... more to come. Packages such as networkx and vpython enhance functionalities allowing advanced visualization options.

http://esa.github.io/pygmo/[+http://esa.github.io/pygmo/+]

https://github.com/esa/pagmo2[+https://github.com/esa/pagmo2+]

https://esa.github.io/pagmo2/[+https://esa.github.io/pagmo2/+]

PyGNOME
~~~~~~~

GNOME (General NOAA Operational Modeling Environment) is a modeling tool developed by the National Oceanic and Atmospheric Administration (NOAA), Office of Response and Restoration (ORR), Emergency Response Division.

It is designed to support oil and other hazardous material spills in the coastal environment.

And this is a python package that encapsulates GNOME's functionality.

https://github.com/NOAA-ORR-ERD/PyGnome[+https://github.com/NOAA-ORR-ERD/PyGnome+]

pyKML
~~~~~

pyKML is a Python package for creating, parsing, manipulating, and validating
KML, a language for encoding and annotating geographic data.

pyKML is based on the lxml.objectify API which provides a Pythonic API for
working with XML documents. pyKML adds additional functionality specific to
the KML language.

KML comes in several flavors. pyKML can be used with KML documents that follow
the base OGC KML specification, the Google Extensions Namespace, or a
user-supplied extension to the base KML specification (defined by an XML
Schema document).

https://pythonhosted.org/pykml/[+https://pythonhosted.org/pykml/+]

PyLops
~~~~~~

This Python library is inspired by the MATLAB Spot – A Linear-Operator Toolbox project.

Linear operators and inverse problems are at the core of many of the most used algorithms in signal processing, image processing, and remote sensing. When dealing with small-scale problems, the Python numerical scientific libraries numpy and scipy allow to perform many of the underlying matrix operations (e.g., computation of matrix-vector products and manipulation of matrices) in a simple and compact way.

Many useful operators, however, do not lend themselves to an explicit matrix representation when used to solve large-scale problems. PyLops operators, on the other hand, still represent a matrix and can be treated in a similar way, but do not rely on the explicit creation of a dense (or sparse) matrix itself. Conversely, the forward and adjoint operators are represented by small pieces of codes that mimic the effect of the matrix on a vector or another matrix.

Luckily, many iterative methods (e.g. cg, lsqr) do not need to know the individual entries of a matrix to solve a linear system. Such solvers only require the computation of forward and adjoint matrix-vector products as done for any of the PyLops operators.

https://github.com/equinor/pylops[+https://github.com/equinor/pylops+]

PyMap3d
~~~~~~~

3-D geographic coordinate conversions, with API similar to Matlab mapping toolbox.
PyMap3D is intended for non-interactive use on massively parallel (HPC) and embedded systems. Includes some relevant Vallado algorithms.

Popular mapping toolbox functions ported to Python include the following, where the source coordinate system (before the "2") is converted to the desired coordinate system:

-----
aer2ecef  aer2enu  aer2geodetic  aer2ned
ecef2aer  ecef2enu  ecef2enuv  ecef2geodetic  ecef2ned  ecef2nedv
ecef2eci  eci2ecef  eci2aer  aer2eci
enu2aer  enu2ecef   enu2geodetic
geodetic2aer  geodetic2ecef  geodetic2enu  geodetic2ned
ned2aer  ned2ecef   ned2geodetic
azel2radec radec2azel
vreckon vdist
lookAtSpheroid
track2
-----

https://github.com/scivision/pymap3d[+https://github.com/scivision/pymap3d+]

https://scivision.github.io/pymap3d/index.html[+https://scivision.github.io/pymap3d/index.html+]

http://celestrak.com/software/vallado-sw.php[+http://celestrak.com/software/vallado-sw.php+]

PyMDPToolbox
~~~~~~~~~~~~

The MDP toolbox provides classes and functions for the resolution of descrete-time Markov Decision Processes. The list of algorithms that have been implemented includes backwards induction, linear programming, policy iteration, q-learning and value iteration along with several variations.

The classes and functions were developped based on the MATLAB MDP toolbox by the Biometry and Artificial Intelligence Unit of INRA Toulouse (France). There are editions available for MATLAB, GNU Octave, Scilab and R.

The features include:

* Eight MDP algorithms implemented
* Fast array manipulation using NumPy
* Full sparse matrix support using SciPy's sparse package
* Optional linear programming support using cvxopt

https://github.com/sawcordwell/pymdptoolbox/[+https://github.com/sawcordwell/pymdptoolbox/+]

https://github.com/nasa/pymdptoolbox[+https://github.com/nasa/pymdptoolbox+]

https://github.com/sawcordwell/MDPs.jl[+https://github.com/sawcordwell/MDPs.jl+]

pyMOR
~~~~~

pyMOR is a software library for building model order reduction applications with the Python programming language. Its main focus lies on the application of reduced basis methods to parameterized partial differential equations. All algorithms in pyMOR are formulated in terms of abstract interfaces for seamless integration with external high-dimensional PDE solvers. Moreover, pure Python implementations of finite element and finite volume discretizations using the NumPy/SciPy scientific computing stack are provided for getting started quickly.

https://pymor.org/[+https://pymor.org/+]

pyMultiscale
~~~~~~~~~~~~

PyMultiscale is a collection of 1D, 2D, and 3D wavelet transforms for Python.
The following families of transforms are supported:

* MODWT - Maximal Overlap Discrete Wavelet Transform. This is a Python port of portions of the http://cran.r-project.org/web/packages/waveslim/index.html package for R 
* Starlet Transform (i.e. undecimated isotropic wavelet transform)
* Curvelet Transform. This is a thin wrapper around PyCurvelab, which must be installed seperately. First install Curvelab, and the PyCurvelab.

https://github.com/broxtronix/pymultiscale[+https://github.com/broxtronix/pymultiscale+]

http://www.curvelet.org/[+http://www.curvelet.org/+]

https://github.com/slimgroup/PyCurvelab[+https://github.com/slimgroup/PyCurvelab+]

http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html[+http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html+]

Pynac
~~~~~

Pynac is a derivative of the Cxx library GiNaC, which allows manipulation of symbolic expressions. It currently provides the backend for symbolic expressions in Sage. In order to use Pynac, one needs to have the Sage library available.

The main difference between Pynac and GiNaC is that Pynac relies on Sage to provide the operations on numerical types, while GiNaC depends on CLN for this purpose.

http://pynac.org/[+http://pynac.org/+]

https://github.com/pynac/pynac[+https://github.com/pynac/pynac+]

PyNomo
~~~~~~

A Python program to create nomographs/nomograms.

https://github.com/lefakkomies/pynomo[+https://github.com/lefakkomies/pynomo+]

http://pynomo.org/wiki/index.php?title=Main_Page[+http://pynomo.org/wiki/index.php?title=Main_Page+]

Pyomo
~~~~~

Pyomo is a Python-based open-source software package that supports a diverse set of optimization capabilities for formulating, solving, and analyzing optimization models.

A core capability of Pyomo is modeling structured optimization applications.  Pyomo can be used to define general symbolic problems, create specific problem instances, and solve these instances using commercial and open-source solvers.  Pyomo's modeling objects are embedded within a full-featured high-level programming language providing a rich set of supporting libraries, which distinguishes Pyomo from other algebraic modeling languages like AMPL, AIMMS and GAMS.

Pyomo supports a wide range of problem types, including:

* Linear programming
* Quadratic programming
* Nonlinear programming
* Mixed-integer linear programming
* Mixed-integer quadratic programming
* Mixed-integer nonlinear programming
* Stochastic programming
* Generalized disjunctive programming
* Differential algebraic equations
* Bilevel programming
* Mathematical programs with equilibrium constraints

Pyomo also supports iterative analysis and scripting capabilities within a full-featured programming language.  Further, Pyomo has also proven an effective framework for developing high-level optimization and analysis tools.  For example, the PySP package provides generic solvers for stochastic programming.  PySP leverages the fact that Pyomo's modeling objects are embedded within a full-featured high-level programming language, which allows for transparent parallelization of subproblems using Python parallel communication libraries.

https://github.com/Pyomo/pyomo[+https://github.com/Pyomo/pyomo+]

http://www.pyomo.org/[+http://www.pyomo.org/+]

https://www.researchgate.net/publication/255061517_PySP_modeling_and_solving_stochastic_mixed-integer_programs_in_Python[+https://www.researchgate.net/publication/255061517_PySP_modeling_and_solving_stochastic_mixed-integer_programs_in_Python+]

https://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud1/[+https://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud1/+]

https://www.coin-or.org/projects/[+https://www.coin-or.org/projects/+]

PyOP2
~~~~~

Many numerical algorithms and scientific computations on unstructured meshes can be viewed as the independent application of a local operation everywhere on a mesh. This local operation is often called a computational kernel and its independent application lends itself naturally to parallel computation. An unstructured mesh can be described by sets of entities (vertices, edges, cells) and the connectivity between those sets forming the topology of the mesh.

PyOP2 is a domain-specific language (DSL) for the parallel executions of computational kernels on unstructured meshes or graphs.

http://op2.github.io/PyOP2/[+http://op2.github.io/PyOP2/+]

https://github.com/OP2/PyOP2[+https://github.com/OP2/PyOP2+]

https://ieeexplore.ieee.org/document/6495916[+https://ieeexplore.ieee.org/document/6495916+]

PyReshaper
~~~~~~~~~~

The PyReshaper is a tool for converting time-slice (or history-file or synoptically) formatted NetCDF files into time-series (or single-field) format. The PyReshaper package is designed to run in parallel (MPI) to maximize performance, with the parallelism implemented over variables (i.e., task parallelism). This means that the maximum parallelism achieveable for a given operation is one core/processor per variables in the time-slice NetCDF files.

The PyReshaper directly depends upon the ASAP Python Toolbox (ASAPTools) and either PyNIO or netcdf4-python. Access and manipulation of the NetCDF files is done through PyNIO or netcdf4-python, and the parallelism is implimented using the ASAPTools SimpleComm, which uses mpi4py. Implicit dependencies exist as a result of these direct dependencies.

https://github.com/NCAR/PyReshaper[+https://github.com/NCAR/PyReshaper+]

xarray-pyreshaper
^^^^^^^^^^^^^^^^^

PyReshaper-like operation with Xarray.

https://github.com/NCAR/xarray-pyreshaper[+https://github.com/NCAR/xarray-pyreshaper+]

Pyro
~~~~

Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.

Probability is the mathematics of reasoning under uncertainty, much as calculus is the mathematics for reasoning about rates of change. Models built in the language of probability can capture complex reasoning, know what they do not know, and uncover structure in data without supervision. Further, probability provides a way for human experts to provide knowledge to AI systems in the form of a priori beliefs.

Specifying probabilistic models directly can be cumbersome and implementing them can be very error-prone. Probabilistic programming languages (PPLs) solve these problems by marrying probability with the representational power of programming languages. A probabilistic program is a mix of ordinary deterministic computation and randomly sampled values; this stochastic computation represents a generative story about data. The probabilities are implicit in this representation—there is no need to derive formulas—and yet this specification is also universal: any computable probabilistic model can be written this way. Pyro builds on full Python as its base language, making it clear and familiar to many.

By observing the outcome of a probabilistic program, we can describe an inference problem, roughly translated as: “what must be true if this random choice had a certain observed value?” Probabilistic programming systems provide universal inference algorithms that can perform inference with little intervention from the user. Think of this as the compiler for a PPL: it allows us to divide labor between the modeler and the inference expert.

Yet inference is the key challenge for probabilistic modeling, and non-scalable inference is the main failure mode of PPLs. Leveraging the power of deep learning, recent advances have introduced a new approach to probabilistic inference and PPL implementation. The key idea is to describe inference in a model via a second model called an inference model, or guide in Pyro. (This is actually an idea that goes back as far as at least the Helmholtz machine.) Just as a model is a generative story for the data, a guide is a generative story for translating the data into latent choices.

Of course, we cannot simply write down the correct guide (that is why inference is hard). Instead, we use the variational approach, specifying a parameterized family of guides and then solving an optimization problem to move the guide toward the posterior distribution of the model. This optimization can be automated thanks to automatic differentiation, a technique for efficiently computing the gradient of a program, and several tricks for estimating the gradient of an expectation.

Pyro builds on the excellent PyTorch library, which includes automatic differentiation using very fast, GPU-accelerated tensor math. PyTorch constructs gradients dynamically, which enables Pyro programs to include stochastic control structure, that is, random choices in a Pyro program can control the presence of other random choices in the program. Stochastic control structure is crucial to make a PPL universal. Hence, Pyro can represent any probabilistic model, while providing automatic optimization-based inference that is flexible and scalable to large data sets.

In Pyro, both the generative models and the inference guides can include deep neural networks as components. The resulting deep probabilistic models have shown great promise in recent work, especially for unsupervised and semi-supervised machine learning problems.

https://eng.uber.com/pyro/[+https://eng.uber.com/pyro/+]

http://pyro.ai/[+http://pyro.ai/+]

pyro2
~~~~~

pyro is a simple framework for implementing and playing with hydrodynamics solvers. It is designed to provide a tutorial for students in computational astrophysics (and hydrodynamics in general) and for easily prototyping new methods. We introduce simple implementations of some popular methods used in the field, with the code written to be easily understandable. All simulations use a single grid (no domain decomposition).

pyro builds off of a finite-volume framework for solving PDEs. There are a number of solvers in pyro, allowing for the solution of hyperbolic (wave), parabolic (diffusion), and elliptic (Poisson) equations. In particular, the following solvers are developed:

    linear advection
    compressible hydrodynamics
    shallow water hydrodynamics
    multigrid
    implicit thermal diffusion
    incompressible hydrodynamics
    low Mach number atmospheric hydrodynamics
    shallow water hydrodynamics

Runtime visualization shows the evolution as the equations are solved.

https://pyro2.readthedocs.io/en/latest/intro.html[+https://pyro2.readthedocs.io/en/latest/intro.html+]

https://github.com/python-hydro/pyro2/[+https://github.com/python-hydro/pyro2/+]

PyTables
~~~~~~~~

PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data. You can download PyTables and use it for free. You can access documentation, some examples of use and presentations here.

PyTables is built on top of the HDF5 library, using the Python language and the NumPy package. It features an object-oriented interface that, combined with C extensions for the performance-critical parts of the code (generated using Cython), makes it a fast, yet extremely easy to use tool for interactively browse, process and search very large amounts of data. One important feature of PyTables is that it optimizes memory and disk resources so that data takes much less space (specially if on-flight compression is used) than other solutions such as relational or object oriented databases.

PyTables comes with out-of-box support for the Blosc compressor. This allows for extremely high compression speed, while keeping decent compression ratios. By doing so, I/O can be accelerated by a large extent, and you may end achieving higher performance than the bandwidth provided by your I/O subsystem.

PyTables is not designed to work as a relational database replacement, but rather as a teammate. If you want to work with large datasets of multidimensional data (for example, for multidimensional analysis), or just provide a categorized structure for some portions of your cluttered RDBS, then give PyTables a try. It works well for storing data from data acquisition systems (DAS), simulation software, network data monitoring systems (for example, traffic measurements of IP packets on routers), or as a centralized repository for system logs, to name only a few possible uses.

http://www.pytables.org/[+http://www.pytables.org/+]

https://github.com/PyTables/PyTables[+https://github.com/PyTables/PyTables+]

ViTables
^^^^^^^^

ViTables is a component of the PyTables family. It is a GUI for browsing and editing files in both PyTables and HDF5 formats. It is developed using Python and PyQt5 (the Python bindings to Qt, so it can run on any platform that supports these components.

ViTables capabilities include easy navigation through the data hierarchy, displaying of real data and its associated metadata, a simple, yet powerful, browsing of multidimensional data and much more.

As a viewer, one of the greatest strengths of ViTables is its ability to display very large datasets. Tables with one thousand millions of rows (and beyond) are navigated stunningly fast and with very low memory requirements. So, if you ever need to browse huge tables, don’t hesitate, ViTables is your choice.

If you need a customized browser for managing your HDF5 data, ViTables is an excellent starting point.

http://vitables.org/[+http://vitables.org/+]

Python
~~~~~~

Blah. BLART.

https://www.python.org/[+https://www.python.org/+]

https://en.wikipedia.org/wiki/Python_(programming_language)[+https://en.wikipedia.org/wiki/Python_(programming_language)+]

*Enthought Python* - https://www.enthought.com/product/enthought-Python-distribution[+https://www.enthought.com/product/enthought-Python-distribution+]

*Intel Python* - https://software.intel.com/en-us/distribution-for-Python[+https://software.intel.com/en-us/distribution-for-Python+]

*Python for HPC* - https://github.com/betterscientificsoftware/python-for-hpc[+https://github.com/betterscientificsoftware/python-for-hpc+]

*Python for High Performance Computing* - https://github.com/wscullin/ecp_Python_tutorial/blob/master/slides/ECP_Python_Tutorial_2018.pdf[+https://github.com/wscullin/ecp_Python_tutorial/blob/master/slides/ECP_Python_Tutorial_2018.pdf+]

*XSD Python Training* - https://confluence.aps.anl.gov/display/XSDPT/XSD+Python+Training+Home[+https://confluence.aps.anl.gov/display/XSDPT/XSD+Python+Training+Home+]

Brython
^^^^^^^

A Python 3 implementation for client-side web programming.
Brython (Browser Python) is an implementation of Python 3 running in the browser, with an interface to the DOM elements and events.
It is designed to replace Javascript as the scripting language for the Web. As such, it is a Python 3 implementation adapted to the HTML5 environment.

Brython supports most of the syntax of Python 3, including comprehensions, generators, metaclasses, imports, etc. and many modules of the CPython distribution.

It includes libraries to interact with DOM elements and events, and with existing Javascript libraries such as jQuery, 3D, Highcharts, Raphael etc. It supports lastest specs of HTML5/CSS3, and can use CSS Frameworks like Bootstrap3, LESS, SASS etc.

http://www.brython.info/[+http://www.brython.info/+]

https://github.com/brython-dev/brython[+https://github.com/brython-dev/brython+]

http://www.brython.info/gallery/gallery_en.html[+http://www.brython.info/gallery/gallery_en.html+]

https://github.com/brython-dev/brython/wiki/Brython%20in%20the%20wild[+https://github.com/brython-dev/brython/wiki/Brython%20in%20the%20wild+]

pythonOCC
~~~~~~~~~

pythonOCC is a 3D CAD/CAE/PLM development framework for the Python programming language. It provides features such as advanced topological and geometrical operations, data exchange (STEP, IGES, STL import/export), GUI based visualization (wx, Qt), jupyter notebook rendering.

pythonOCC is built upon free and open source 3D CAD kernel OCE project.

https://github.com/tpaviot/pythonocc[+https://github.com/tpaviot/pythonocc+]

http://www.pythonocc.org/[+http://www.pythonocc.org/+]

https://readthedocs.org/projects/pythonocc-core/[+https://readthedocs.org/projects/pythonocc-core/+]

https://github.com/tpaviot/oce/[+https://github.com/tpaviot/oce/+]

pythonpy
~~~~~~~~

The Swiss army knife of the command line.
With pythonpy you can evaluate Python expressions, import modules and do just about
anything you can do in Python from the shell command line.

https://github.com/Russell91/pythonpy[+https://github.com/Russell91/pythonpy+]

PythonTex
~~~~~~~~~

PythonTeX provides fast, user-friendly access to Python from within LaTeX. It allows Python code entered within a LaTeX document to be executed, and the results to be included within the original document. It also provides syntax highlighting for code within LaTeX documents via the Pygments syntax highlighter.

PythonTeX also provides support for Ruby, Julia, Octave, Sage, Bash, Rust, and R. Support for additional languages is coming soon.

See pythontex_quickstart.pdf to get started, and pythontex_gallery.pdf for examples of what is possible with PythonTeX. PythonTeX is included in TeX Live and MiKTeX and may be installed via the package manager. See pythontex.pdf for detailed installation instructions if you want to install the current development version, or use the installation script for TeX Live and MiKTeX.

The depythontex utility creates a copy of a PythonTeX document in which all Python code has been replaced by its output. This plain LaTeX document is more suitable for journal submission, sharing, or conversion to other document formats. See pythontex_gallery.html and the accompanying conversion script for an example of a PythonTeX document that was converted to HTML via depythontex and Pandoc.

https://github.com/gpoore/pythontex[+https://github.com/gpoore/pythontex+]

Pythran
~~~~~~~

Pythran is an ahead of time compiler for a subset of the Python language, with a focus on scientific computing. It takes a Python module annotated with a few interface description and turns it into a native Python module with the same interface, but (hopefully) faster.

It is meant to efficiently compile scientific programs, and takes advantage of multi-cores and SIMD instruction units.

Pythran supports Python 2.7 and also has a decent Python 3 support.

https://pythran.readthedocs.io/en/latest/[+https://pythran.readthedocs.io/en/latest/+]

https://github.com/serge-sans-paille/pythran[+https://github.com/serge-sans-paille/pythran+]

https://serge-sans-paille.github.io/pythran-stories/pythran-tutorial.html[+https://serge-sans-paille.github.io/pythran-stories/pythran-tutorial.html+]

https://archive.fosdem.org/2018/schedule/event/pythran/[+https://archive.fosdem.org/2018/schedule/event/pythran/+]

http://quantstack.net/xsimd.html[+http://quantstack.net/xsimd.html+]

PyTROLL
~~~~~~~

Pytroll is an easy to use, modular, free and open source python framework for the processing of earth observation satellite data. The provided python packages are designed to be used both in R&D environments and in 24/7 operational production.

The focus is on atmospheric applications and imaging sensors, but as seen from the list of supported satellite sensors below the data that can be handled by Pytroll allows the usage in a wide range of earth sciences.

http://pytroll.github.io/[+http://pytroll.github.io/+]

https://github.com/pytroll[+https://github.com/pytroll+]

PyVisFile
~~~~~~~~~

Pyvisfile allows you to write a variety of visualization file formats, including

* Kitware’s XML-style Vtk data files.
* Silo visualization files, as introduced by LLNL’s MeshTV and more recently used by the VisIt large-scale visualization program.

pyvisfiles supports many mesh geometries, such such as unstructured and rectangular structured meshes, particle meshes, as well as scalar and vector variables on them. In addition, pyvisfile allows the semi-automatic writing of parallelization-segmented visualization files in both Silo and Vtk formats. For Silo files, pyvisfile also supports the writing of expressions as visualization variables.

pyvisfile can write Vtk files without any extra software installed.

PyVisfile allows you to write Silo and Vtk (XML-style) visualization files from the Python programming language, more specifically from data contained in numpy arrays.

https://documen.tician.de/pyvisfile/[+https://documen.tician.de/pyvisfile/+]

PyWI
~~~~

PyWI is a Python image filtering library aimed at removing additive background noise from raster graphics images.

The image filter relies on multiresolution analysis methods (Wavelet transforms) that remove some scales (frequencies) locally in space. These methods are particularly efficient when signal and noise are located at different scales (or frequencies). Optional features improve the SNR ratio when the (clean) signal constitute a single cluster of pixels on the image (e.g. electromagnetic showers produced with Imaging Atmospheric Cherenkov Telescopes). This library is written in Python and is based on the existing Cosmostat tools iSAp (Interactive Sparse Astronomical data analysis Packages http://www.cosmostat.org/software/isap/).

The PyWI library also contains a dedicated package to optimize the image filter parameters for a given set of images (i.e. to adapt the filter to a specific problem). From a given training set of images (containing pairs of noised and clean images) and a given performance estimator (a function that assess the image filter parameters comparing the cleaned image to the actual clean image), the optimizer can determine the optimal filtering level for each scale.

The PyWI library contains:

* wavelet transform and wavelet filtering functions for image multiresolution analysis and filtering;
* additional filter to remove some image components (non-significant pixels clusters);
* a set of generic filtering performance estimators (MSE, NRMSE, SSIM, PSNR, image moment’s difference), some relying on the scikit-image Python library (supplementary estimators can be easily added to meet particular needs);
* a graphical user interface to visualize the filtering process in the wavelet transformed space;
* an Evolution Strategies (ES) algorithm known in the mathematical optimization community for its good convergence rate on generic derivative-free continuous global optimization problems (Beyer, H. G. (2013) “The theory of evolution strategies”, Springer Science & Business Media);
* additional tools to manage and monitor the parameter optimization.

http://www.pywi.org/docs/intro.html[+http://www.pywi.org/docs/intro.html+]

http://www.cosmostat.org/software/isap[+http://www.cosmostat.org/software/isap+]

#QQQQ

QGIS
~~~~

QGIS (previously known as Quantum GIS) is a free and open-source cross-platform desktop geographic information system (GIS) application that supports viewing, editing, and analysis of geospatial data.

QGIS functions as geographic information system (GIS) software, allowing users to analyze and edit spatial information, in addition to composing and exporting graphical maps.[3] QGIS supports both raster and vector layers; vector data is stored as either point, line, or polygon features. Multiple formats of raster images are supported, and the software can georeference images.

QGIS supports shapefiles, coverages, personal geodatabases, dxf, MapInfo, PostGIS, and other formats.[4] Web services, including Web Map Service and Web Feature Service, are also supported to allow use of data from external sources.[5]

QGIS integrates with other open-source GIS packages, including PostGIS, GRASS GIS, and MapServer.[5] Plugins written in Python or Cxx extend QGIS's capabilities. Plugins can geocode using the Google Geocoding API, perform geoprocessing functions similar to those of the standard tools found in ArcGIS, and interface with PostgreSQL/PostGIS, SpatiaLite and MySQL databases. 

https://en.wikipedia.org/wiki/QGIS[+https://en.wikipedia.org/wiki/QGIS+]

https://qgis.org/en/site/[+https://qgis.org/en/site/+]

QGIS-Meshing
^^^^^^^^^^^^

QGIS plugins for meshing geophysical domains.
This project contains four plugins for the generation of surface meshes in QGIS:

* mesh surface plugin;
* boundary identification plugin;
* Rasterise Polygons plugin, and
* Raster Calculator plugin.

https://github.com/adamcandy/QGIS-Meshing[+https://github.com/adamcandy/QGIS-Meshing+]

http://gismeshing.org/[+http://gismeshing.org/+]

QGModel
~~~~~~~

This is Python code that solves the nonlinear QG equations for PV anomalies on a doubly-periodic domain. It consists of two parts:

* a fully-dealiased pseudo-spectral code that steps forward the PV conservation equations and
* implementations of specific model types that provide an inversion relation.

At the moment, seven model types have been implemented:

* two-dimensional dynamics (TwoDim),
* surface QG dynamics (Surface),
* multi-layer dynamics (Layered),
* Eady dynamics (Eady),
* floating Eady dynamics (FloatingEady),
* two-Eady dynamics (TwoEady),
* two-Eady dynamics with buoyancy jump (TwoEadyJump).

See run.py for an example of how a model is initialized and run.

This model makes use of PyFFTW, a Python wrapper of FFTW.

https://github.com/joernc/QGModel[+https://github.com/joernc/QGModel+]

QUARK
~~~~~

QUARK (QUeuing And Runtime for Kernels) provides a library that enables the dynamic execution of tasks with data dependencies in a multi-core, multi-socket, shared-memory environment.  QUARK infers data dependencies and precedence constraints between tasks from the way that the data is used, and then executes the tasks in an asynchronous, dynamic fashion in order to achieve a high utilization of the available resources. 
 
QUARK is designed to be easy to use and it is intended to scale to large numbers of cores.  It should enable the efficient expression and implementation of complex algorithms.  The driving application behind the development of QUARK is the PLASMA linear algebra library, and the QUARK runtime contains several optimizations inspired by the algorithms in PLASMA. 
 
An early release of QUARK is being prepared, with a well-stressed and robust implementation and an initial Users’ Guide and Reference Guide.  Additional documentation will be provided in future releases.

http://icl.cs.utk.edu/quark/index.html[+http://icl.cs.utk.edu/quark/index.html+]

Qubes
~~~~~

Qubes OS is a security-oriented operating system (OS). 

Qubes takes an approach called security by compartmentalization, which allows you to compartmentalize the various parts of your digital life into securely isolated compartments called qubes.

This approach allows you to keep the different things you do on your computer securely separated from each other in isolated qubes so that one qube getting compromised won’t affect the others. For example, you might have one qube for visiting untrusted websites and a different qube for doing online banking. This way, if your untrusted browsing qube gets compromised by a malware-laden website, your online banking activities won’t be at risk. Similarly, if you’re concerned about malicious email attachments, Qubes can make it so that every attachment gets opened in its own single-use disposable qube. In this way, Qubes allows you to do everything on the same physical computer without having to worry about a single successful cyberattack taking down your entire digital life in one fell swoop.

Moreover, all of these isolated qubes are integrated into a single, usable system. Programs are isolated in their own separate qubes, but all windows are displayed in a single, unified desktop environment with unforgeable colored window borders so that you can easily identify windows from different security levels. Common attack vectors like network cards and USB controllers are isolated in their own hardware qubes while their functionality is preserved through secure networking, firewalls, and USB device management. Integrated file and clipboard copy and paste operations make it easy to work across various qubes without compromising security. The innovative Template system separates software installation from software use, allowing qubes to share a root filesystem without sacrificing security (and saving disk space, to boot). Qubes even allows you to sanitize PDFs and images in a few clicks. Users concerned about privacy will appreciate the integration of Whonix with Qubes, which makes it easy to use Tor securely, while those concerned about physical hardware attacks will benefit from Anti Evil Maid.

https://www.qubes-os.org/[+https://www.qubes-os.org/+]

QUESO
~~~~~

The Parallel Cxx Statistical Library for the Quantification of Uncertainty for Estimation, Simulation and Optimization, Queso, is a collection of statistical algorithms and programming constructs supporting research into the quantification of uncertainty of models and their predictions. Queso is primarily focused on solving statistical inverse problems using Bayes's theorem, which expresses a distribution of possible values for a set of uncertain parameters (the posterior distribution) in terms of the existing knowledge of the system (the prior) and noisy observations of a physical process, represented by a likelihood distribution. The posterior distribution is not often known analytically, and so requires computational methods. It is typical to compute probabilities and moments from the posterior distribution, but this is often a high-dimensional object and standard Reimann-type methods for quadrature become prohibitively expensive. The approach Queso takes in this regard is to rely on Markov chain Monte Carlo (MCMC) methods which are well suited to evaluating quantities such as probabilities and moments of high-dimensional probability distributions. Queso's intended use is as tool to assist and facilitate coupling uncertainty quantification to a specific application called a forward problem. While many libraries presently exist that solve Bayesian inference problems, Queso is a specialized piece of software primarily designed to solve such problems by utilizing parallel environments demanded by large-scale forward problems.

Statistical inverse theory reformulates inverse problems as problems of statistical inference by means of Bayesian statistics: all quantities are modeled as random variables, and probability distribution of the quantities encapsulates the uncertainty observed in their values. The solution to the inverse problem is then the probability distribution of the quantity of interest when all information available has been incorporated in the model. This (posterior) distribution describes the degree of confidence about the quantity after the measurement has been performed \cite{KaSo05}.

Thus, the solution to the statistical inverse problem may be given by Bayes' formula, which express the posterior distribution as a function of the prior distribution and the data represented through the likelihood function.

The likelihood function has an open form and its evaluation is highly computationally expensive.  Moreover, simulation-based posterior inference requires a large number of forward calculations to be performed, therefore fast and efficient sampling techniques are required for posterior inference.

It is often not straightforward to obtain explicit posterior point estimates of the solution, since it usually involves the evaluation of a high-dimensional integral with respect to a possibly non-smooth posterior distribution. In such cases, an alternative integration technique is the Markov chain Monte Carlo method: posterior means may be estimated using the sample mean from a series of random draws from the posterior distribution.

QUESO is designed in an abstract way so that it can be used by any computational model, as long as a likelihood function (in the case of statistical inverse problems) and a quantity of interest (QoI) function (in the case of statistical forward problems) is provided by the user application.

QUESO provides tools for both sampling algorithms for statistical inverse problems, following Bayes' formula, and statistical forward problems. It contains Monte Carlo solvers (for autocorrelation, kernel density estimation and accuracy assessment), MCMC, as well as the DRAM (for sampling from probability distributions); it also has the capacity to handle many chains or sequences in parallel, each chain or sequence itself demanding many computing nodes because of the computational model being statistically explored.

https://github.com/libqueso/queso[+https://github.com/libqueso/queso+]

https://arxiv.org/abs/1611.07521[+https://arxiv.org/abs/1611.07521+]

https://arxiv.org/abs/1507.00398[+https://arxiv.org/abs/1507.00398+]

QUIC
~~~~

QUIC (Quick UDP Internet Connections, pronounced 'quick') is an experimental transport layer[1] network protocol initially designed, implemented, and deployed by Google [2] in 2012,[3] and announced publicly in 2012 as experimentation broadened.

QUIC's main goal is to improve perceived performance of connection-oriented web applications that are currently using TCP.[1][7] It does this by establishing a number of multiplexed connections between two endpoints over User Datagram Protocol (UDP). This works hand-in-hand with HTTP/2's multiplexed connections, allowing multiple streams of data to reach all the endpoints independently. In contrast, HTTP hosted on Transmission Control Protocol (TCP) can be blocked if any of the multiplexed data streams has an error.

QUIC's secondary goals include reduced connection and transport latency, and bandwidth estimation in each direction to avoid congestion. It also moves control of the congestion avoidance algorithms into the application space at both endpoints, rather than the kernel space, which it is claimed will allow these algorithms to improve more rapidly. Additionally, the protocol can be extended with forward error correction (FEC) to further improve performance when errors are expected, and this is seen as the next step in the protocol's evolution. 

https://quicwg.org/[+https://quicwg.org/+]

https://en.wikipedia.org/wiki/QUIC[+https://en.wikipedia.org/wiki/QUIC+]

https://www.chromium.org/quic[+https://www.chromium.org/quic+]

https://quiche.googlesource.com/quiche/[+https://quiche.googlesource.com/quiche/+]

https://github.com/h2o/quicly[+https://github.com/h2o/quicly+]

https://github.com/litespeedtech/lsquic-client[+https://github.com/litespeedtech/lsquic-client+]

https://cwiki.apache.org/confluence/display/TS/QUIC[+https://cwiki.apache.org/confluence/display/TS/QUIC+]

https://github.com/djc/quinn[+https://github.com/djc/quinn+]

https://github.com/lucas-clemente/quic-go[+https://github.com/lucas-clemente/quic-go+]

https://github.com/NTAP/quant[+https://github.com/NTAP/quant+]

https://github.com/cloudflare/quiche[+https://github.com/cloudflare/quiche+]

Quinoa
~~~~~~

Quinoa is a set of computational tools that enables research and numerical analysis in fluid dynamics. Using the Charm++ runtime system, we employ asynchronous (or non-blocking) parallel programming and decompose computational problems into a large number of work units (that may be more than the available number of processors) enabling arbitrary overlap of parallel computation, communication, input, and output. Then the runtime system dynamically and automatically homogenizes computational load across the simulation distributed across many computers.

Our ultimate goal is to simulate large and complex engineering multiphysics problems with a production-quality code that is extensible and maintainable, using hardware resources efficiently, even for problems with a priori unknown, heterogeneous, and dynamic load distribution.

Quinoa consists of the following tools:

* Walker — Time-integrator for stochastic differential equations
* Inciter — Navier-Stokes solver for complex domains
* RNGTest — Random number generators test suite
* UnitTest — Test suite for synchronous and asynchronous functions
* MeshConv — Tetrahedron-mesh converter

https://github.com/quinoacomputing/quinoa[+https://github.com/quinoacomputing/quinoa+]

https://quinoacomputing.github.io/[+https://quinoacomputing.github.io/+]

QuIP
~~~~

QuIP is an interactive environment designed to provide similar functionality to general-purpose scientific computing platforms such as Matlab, Octave, etc.

The QuIP interpreter, a software environment for QUick image processing, uses an interactive scripting language designed to facilitate use by non-expert users, through features such as context-sensitive automatic response completion and integrated documentation. The package includes a number of script packages that implement high-, medium-, and low-level functions (e.g., analysis of eye images for human gaze tracking, feature tracking, and image filtering). The environment also includes facilities for displaying images on screen, drawing and overlaying graphics, and constructing graphical user interfaces using the scripting language. Currently supported platforms are *NIX (tested on Mac OS X and Linux), and Apple iOS.

https://github.com/nasa/QuIP[+https://github.com/nasa/QuIP+]

QVox
~~~~

QVox is a visualization/edition tool for 3D volumetric data sets with a Qt based GUI.
The features include:

* Read several3D volume files: RAW, VFF, 3DZ (own format with basic RLE compression), PAN (Pandore library from the GREYC), VOL, NPZ.
* Manipulate volumes with voxels of several types (sizes): boolean (1 bit) unsigned char (8-bit), signed short (16-bit), signed/unsigned long (32-bit), float or double.
* Import 2D images from file formats recognized by the Qt lib.
* Display and edit the volume using a slice by slice view (following one of the planes of coordinates).
* Export 3D views to bitmap files.
* Exports 3D views to vector graphics files: EPS, SVG and FIG(Xfig).
* Build and export smooth triangular meshes as .obj and .off (Geomview) files.
* Export the surfels/surface as a .off (Geomview) file.
* Let the user draw on or remove surfels or voxels in both 3D and 2D views.
* Let the user remove selected voxels that are simple only.
* Apply homotopic thinning algorithms (sequential and parallel) using (6,18),(6,26),(18,6) and (26,6) adjacency pairs.
* Use several colormaps: gray levels, shades (red, green or blue), hue scale and random colors. If the volume has size 3 in the fourth dimension, it is considered as a color (RGB) volume with true colors.
* Apply few operations on volumes: re-sampling x3, binarization, inversion, mirrors, cropping to fit the bounding box, merge volumes, fill all cavities, hollow out a volume...
* Convert voxel type from one type to another.
* Launch external programs on the currently displayed volume directly from the interface. A simple XML coinfiguration file describes the command parameters and a dynamically built dialog prompts the user for them. External program may act like filter, data sources or data sinks.
* Import a triangular mesh trough a basic 3D rasterization method.
* Shade the surface according to a diffuse and specular reflection model using a digital surface normal estimator. 

http://qvox.sourceforge.net/[+http://qvox.sourceforge.net/+]

#RRRR

R Language
~~~~~~~~~~

Schtoff.

bigmemory
^^^^^^^^^

Create, store, access, and manipulate massive matrices. Matrices are allocated to shared memory and may use memory-mapped files. Packages 'biganalytics', 'bigtabulate', 'synchronicity', and 'bigalgebra' provide advanced functionality.
Access to and manipulation of a ‘big.matrix’ object is exposed in by an S4 class whose interface is simlar to that of an ‘matrix’. Use of these packages in parallel environments can provide substantial speed and memory efficiencies. ‘bigmemory’ also provides a Cxx framework for the development of new tools that can work both with ‘big.matrix’ and native ‘matrix’ objects.

https://github.com/kaneplusplus/bigmemory[+https://github.com/kaneplusplus/bigmemory+]

https://cran.r-project.org/web/packages/bigmemory/index.html[+https://cran.r-project.org/web/packages/bigmemory/index.html+]

https://cran.r-project.org/web/packages/synchronicity/index.html[+https://cran.r-project.org/web/packages/synchronicity/index.html+]

https://cran.r-project.org/web/packages/bigalgebra/index.html[+https://cran.r-project.org/web/packages/bigalgebra/index.html+]

https://cran.r-project.org/web/packages/bigtabulate/index.html[+https://cran.r-project.org/web/packages/bigtabulate/index.html+]

https://cran.r-project.org/web/packages/biganalytics/index.html[+https://cran.r-project.org/web/packages/biganalytics/index.html+]

infotheo
^^^^^^^^

This package implements various measures of information theory based on several entropy estimators.

https://cran.r-project.org/web/packages/infotheo/index.html[+https://cran.r-project.org/web/packages/infotheo/index.html+]

pbdR
^^^^

The "Programming with Big Data in R" project (pbdR) is a set of highly scalable R packages for distributed computing and profiling in data science.

Our packages include high performance, high-level interfaces to MPI, ZeroMQ, ScaLAPACK, NetCDF4, PAPI, and more. While these libraries shine brightest on large distributed platforms, they also work rather well on small clusters and usually, surprisingly, even on a laptop with only two cores.

https://pbdr.org/[+https://pbdr.org/+]

https://github.com/RBigData[+https://github.com/RBigData+]
regtooals
^^^^^^^^

Novel tools tools for linear, nonlinear and nonparametric regression.
These tools are associated with my forthcoming book, From Linear Models to Machine Learning: Modern Statistical Regression and Classification, N. Matloff, CRC, 2017.

The features include:

* Nonparametric regression for general dimensions in predictor and response variables, using k-Nearest Neighbors. Local-linear option. Allows for user-specified smoothing method. Allows for accelerated exploration of multiple values of k at once. Tool to aid in choosing k.
* Innovative tools for assessing fit in linear and nonlinear parametric models, via nonparametric methods. Model evaluation, examination of quadratic effects, investigation of nonhomogeneity of variance.
** Tools for multiclass classification, parametric and nonparametric. One vs. All and All vs. All. Novel adjustment for artificially balanced data.
* Linear regression, PCA and log-linear model estimation in missing-data setting, via the Available Cases method.
* Nicer implementation of ridge regression, with more meaningful scaling and better plotting.
* Extension to nonlinear parametric regression with of Eickert-White technique to handle heteroscedasticity.
* Misc. tools, e.g. Method of Moments estimation (including for nonregression settings).

https://github.com/matloff/regtools[+https://github.com/matloff/regtools+]

https://github.com/matloff/polyreg[+https://github.com/matloff/polyreg+]

https://arxiv.org/abs/1806.06850[+https://arxiv.org/abs/1806.06850+]

https://matloff.wordpress.com/2018/06/20/neural-networks-are-essentially-polynomial-regression/[+https://matloff.wordpress.com/2018/06/20/neural-networks-are-essentially-polynomial-regression/+]

VLMC
^^^^

Functions, Classes & Methods for estimation, prediction, and simulation (bootstrap) of Variable Length Markov Chain ('VLMC') Models.

https://cran.r-project.org/web/packages/VLMC/[+https://cran.r-project.org/web/packages/VLMC/+]

https://cran.r-project.org/web/packages/VLMC/VLMC.pdf[+https://cran.r-project.org/web/packages/VLMC/VLMC.pdf+]

https://projecteuclid.org/euclid.aos/1018031204[+https://projecteuclid.org/euclid.aos/1018031204+]

https://www.jstor.org/stable/1391185[+https://www.jstor.org/stable/1391185+]

RabbitMQ
~~~~~~~~

RabbitMQ is an open-source message-broker software (sometimes called message-oriented middleware) that originally implemented the Advanced Message Queuing Protocol (AMQP) and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol (STOMP), Message Queuing Telemetry Transport (MQTT), and other protocols.

The RabbitMQ server program is written in the Erlang programming language and is built on the Open Telecom Platform framework for clustering and failover. Client libraries to interface with the broker are available for all major programming languages.

The RabbitMQ project consists of:

* The RabbitMQ exchange server itself
* Gateways for AMQP, HTTP, STOMP, and MQTT protocols
* AMQP client libraries for Java, .NET Framework, and Erlang. (AMQP clients for other languages are available from other vendors.)
* A plug-in platform for custom additions, with a pre-defined collection of supported plug-ins, including:
** A "Shovel" plug-in that takes care of moving or copying (replicating) messages from one broker to another.
** A "Federation" plug-in that enables efficient sharing of messages between brokers (at the exchange level).
** A "Management" plug-in that enables monitoring and control of brokers and clusters of brokers.

RabbitMQ is officially supported on a number of operating systems and several languages. In addition, the RabbitMQ community has created numerous clients, adaptors and tools.

https://www.rabbitmq.com/[+https://www.rabbitmq.com/+]

https://github.com/rabbitmq[+https://github.com/rabbitmq+]

Celery
^^^^^^

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.
It’s a task queue with focus on real-time processing, while also supporting task scheduling.

Task queues are used as a mechanism to distribute work across threads or machines.
A task queue’s input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.

Celery communicates via messages, usually using a broker to mediate between clients and workers. To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.
A Celery system can consist of multiple workers and brokers, giving way to high availability and horizontal scaling.

Celery requires a message transport to send and receive messages. The RabbitMQ and Redis broker transports are feature complete, but there’s also support for a myriad of other experimental solutions, including using SQLite for local development.

http://docs.celeryproject.org/en/latest/[+http://docs.celeryproject.org/en/latest/+]

https://github.com/celery/celery[+https://github.com/celery/celery+]

Pika
^^^^

Pika is a pure-Python implementation of the AMQP 0-9-1 protocol that tries to stay fairly independent of the underlying network support library.
Pika is a RabbitMQ (AMQP-0-9-1) client library for Python.

https://pypi.org/project/pika/[+https://pypi.org/project/pika/+]

https://pika.readthedocs.io/en/stable/[+https://pika.readthedocs.io/en/stable/+]

Racket
~~~~~~

Racket (formerly PLT Scheme) is a general-purpose, multi-paradigm programming language based on the Scheme dialect of Lisp. It is designed to be a platform for programming language design and implementation. Racket is also used for scripting, computer science education, and research.

The Racket platform provides an implementation of the Racket language (including a run-time system, libraries, and JIT compiler) along with the DrRacket integrated development environment (IDE) written in Racket. Racket is used by the ProgramByDesign outreach program, which aims to turn computer science into "an indispensable part of the liberal arts curriculum".

The core Racket language is known for its extensive macro system which enables creating embedded and domain-specific languages, language constructs such as classes or modules, and separate dialects of Racket with different semantics.

Racket's core language includes macros, modules, lexical closures, tail calls, delimited continuations, parameters (fluid variables), software contracts, green and OS threads, and more. The language also comes with primitives, such as eventspaces and custodians, which control resource management and enables the language to act like an operating system for loading and managing other programs.Further extensions to the language are created with the powerful macro system, which together with the module system and custom parsers can control all aspects of a language. Unlike programming languages that lack macro systems, most language constructs in Racket are written on the base language using macros. These include a mixin class system, a component (or module) system as expressive as ML's opaque ascription, and pattern matching.

Further, the language features the first contract system for a higher-order programming language. Racket's contract system is inspired by the Design by Contract work for Eiffel and extends it to work for higher-order values such as first-class functions, objects, reference cells, and so on. For example, an object that is checked by a contract can be ensured to make contract checks when its methods are eventually invoked.

Racket includes both bytecode and JIT (JIT) compilers. The bytecode compiler that translates to an internal bytecode format that is run by the Racket virtual machine, with the JIT compiler transtating bytecoder to native code on x86, x86-64, ARM and PowerPC platforms at runtime. 

http://racket-lang.org/[+http://racket-lang.org/+]

https://en.wikipedia.org/wiki/Racket_(programming_language)[+https://en.wikipedia.org/wiki/Racket_(programming_language)+]

Radare
~~~~~~

Radare is a portable reverse engineering framework that can:

* Disassemble (and assemble for) many different architectures
* Debug with local native and remote debuggers (gdb, rap, webui, r2pipe, winedbg, windbg)
* Run on Linux, *BSD, Windows, OSX, Android, iOS, Solaris and Haiku
* Perform forensics on filesystems and data carving
* Be scripted in Python, Javascript, Go and more
* Support collaborative analysis using the embedded webserver
* Visualize data structures of several file types
* Patch programs to uncover new features or fix vulnerabilities
* Use powerful analysis capabilities to speed up reversing
* Aid in software exploitation

The supported architectures are i386, x86-64, ARM, MIPS, PowerPC, SPARC, RISC-V, SH, m68k, AVR, XAP, System Z, XCore, CR16, HPPA, ARC, Blackfin, Z80, H8/300, V810, V850, CRIS, XAP, PIC, LM32, 8051, 6502, i4004, i8080, Propeller, Tricore, Chip8 LH5801, T8200, GameBoy, SNES, MSP430, Xtensa, NIOS II, Dalvik, WebAssembly, MSIL, EBC, TMS320 (c54x, c55x, c55+, c66), Hexagon, Brainfuck, Malbolge, DCPU16.

The supported file formats are ELF, Mach-O, Fatmach-O, PE, PE+, MZ, COFF, OMF, TE, XBE, BIOS/UEFI, Dyldcache, DEX, ART, CGC, Java class, Android boot image, Plan9 executable, ZIMG, MBN/SBL bootloader, ELF coredump, MDMP (Windows minidump), WASM (WebAssembly binary), Commodore VICE emulator, Game Boy (Advance), Nintendo DS ROMs and Nintendo 3DS FIRMs, various filesystems.

https://www.radare.org/r/[+https://www.radare.org/r/+]

http://beta.rada.re/en/latest/[+http://beta.rada.re/en/latest/+]

https://github.com/radare/radare2[+https://github.com/radare/radare2+]

RadVel
~~~~~~

RadVel is an open-source Python package for modeling Keplerian orbits in radial velocity (RV) timeseries. RadVel provides a convenient framework to fit RVs using maximum a posteriori optimization and to compute robust confidence intervals by sampling the posterior probability density via Markov Chain Monte Carlo (MCMC). RadVel allows users to float or fix parameters, impose priors, and perform Bayesian model comparison. We have implemented real-time MCMC convergence tests to ensure adequate sampling of the posterior. RadVel can output a number of publication-quality plots and tables. Users may interface with RadVel through a convenient command-line interface or directly from Python. The code is object-oriented and thus naturally extensible.

https://github.com/California-Planet-Search/radvel/[+https://github.com/California-Planet-Search/radvel/+]

https://radvel.readthedocs.io/en/latest/[+https://radvel.readthedocs.io/en/latest/+]

http://adsabs.harvard.edu/abs/2018PASP..130d4504F[+http://adsabs.harvard.edu/abs/2018PASP..130d4504F+]

Raftlib
~~~~~~~

RaftLib is a C\xx Library for enabling stream/data-flow parallel computation. Using simple right shift operators (just like the Cxx streams that you would use for string manipulation), you can link parallel compute kernels together. With RaftLib, we do away with explicit use of pthreads, std::thread, OpenMP, or any other parallel "threading" library. These are often mis-used, creating non-deterministic behavior. RaftLib's model allows lock-free FIFO-like access to the communications channels connecting each compute kernel. The full system has many auto-parallelization, optimization, and convenience features that enable relatively simple authoring of performant applications. This project is currently in the alpha stage. The beta release will bring back multi-node support, along with (planned) container support for the remote machines. 

https://github.com/RaftLib/RaftLib[+https://github.com/RaftLib/RaftLib+]

http://www.raftlib.io/[+http://www.raftlib.io/+]

RAJA
~~~~

RAJA is a collection of Cxx software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to:

* Make existing (production) applications portable with minimal disruption
* Provide a model for new applications so that they are portable from inception.

RAJA uses standard C\xx11 -- Cxx is the predominant programming language in which many LLNL codes are written. RAJA is rooted in a perspective based on substantial experience working on production mesh-based multiphysics applications at LLNL. Another goal of RAJA is to enable application developers to adapt RAJA concepts and specialize them for different code implementation patterns and Cxx usage, since data structures and algorithms vary widely across applications.

RAJA shares goals and concepts found in other Cxx portability abstraction approaches, such as Kokkos and Thrust. However, it includes concepts that are absent in other models and which are fundamental to LLNL codes.

https://github.com/LLNL/RAJA[+https://github.com/LLNL/RAJA+]

RandNLA
~~~~~~~

Randomized matrix algorithms have been a hot topic in research the last years. Recent developments have shown their utility in large-scale machine learning and statistical data analysis applications.
RandNLA is an implementation of many Randomized algorithms for Numerical Linear Algebra on top of Numpy/Scipy.

Sketching is a way to compress matrices that preserve essential matrix properties. For some problems, sketches can be used to get faster ways to find high-precision solutions to the original problem. This tool can be used for least-squares and robust regression, eigenvector analysis, non-negative matrix factorization, etc.

https://github.com/jomsdev/randNLA[+https://github.com/jomsdev/randNLA+]

RAPIDS
~~~~~~

The RAPIDS suite of open source software libraries gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. RAPIDS is incubated by NVIDIA® based on years of accelerated data science experience. RAPIDS relies on NVIDIA CUDA® primitives for low-level compute optimization, and exposes GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.

RAPIDS also focuses on common data preparation tasks for analytics and data science. This includes a familiar DataFrame API that integrates with a variety of machine learning algorithms for end-to-end pipeline accelerations without paying typical serialization costs. RAPIDS also includes support for multi-node, multi-GPU deployments, enabling vastly accelerated processing and training on much larger dataset sizes.

The components of RAPIDS include:

* *Apache Arrow* - a columnar, in-memory data structure that delivers efficient and fast data interchange with flexibility to support complex data models.
* *cuDF* - a DataFrame manipulation library based on Apache Arrow that accelerates loading, filtering, and manipulation of data for model training data preparation. The Python bindings of the core-accelerated CUDA DataFrame manipulation primitives mirror the pandas interface for seamless onboarding of pandas users
* *cuML* - a collection of GPU-accelerated machine learning libraries that will provide GPU versions of all machine learning algorithms available in scikit-learn
* *cuGRAPH* - a framework and collection of graph analytics libraries that seamlessly integrate into the RAPIDS data science platform
* RAPIDS provides native array_interface support. This means data stored in Apache Arrow can be seamlessly pushed to deep learning frameworks that accept array_interface such as PyTorch and Chainer.
* RAPIDS will include tightly integrated data visualization libraries based on Apache Arrow. Native GPU in-memory data format provides high-performance, high-FPS data visualization, even with very large datasets

https://rapids.ai/[+https://rapids.ai/+]

https://github.com/rapidsai[+https://github.com/rapidsai+]

https://ngc.nvidia.com/catalog/containers/nvidia:rapidsai:rapidsai[+https://ngc.nvidia.com/catalog/containers/nvidia:rapidsai:rapidsai+]

RapydScript
~~~~~~~~~~~

RapydScript (pronounced 'RapidScript') is a pre-compiler for JavaScript, similar to CoffeeScript, but with cleaner, more readable syntax. The syntax is almost identical to Python, but RapydScript has a focus on performance and interoperability with external JavaScript libraries. This means that the JavaScript that RapydScript generates is performant and quite close to hand written JavaScript.

RapydScript allows to write your front-end in Python without the overhead that other similar frameworks introduce (the performance is the same as with pure JavaScript). To those familiar with CoffeeScript, RapydScript is like CoffeeScript, but inspired by Python's readability rather than Ruby's cleverness. To those familiar with Pyjamas, RapydScript brings many of the same features and support for Python syntax without the same overhead. Don't worry if you've never used either of the above-mentioned compilers, if you've ever had to write your code in pure JavaScript you'll appreciate RapydScript. RapydScript combines the best features of Python as well as JavaScript, bringing you features most other Pythonic JavaScript replacements overlook. Here are a few features of RapydScript:

* classes that work and feel similar to Python
* an import system for modules/packages that works just like Python's
* optional function arguments that work similar to Python
* inheritance system that's both, more powerful than Python and cleaner than JavaScript
* support for object literals with anonymous functions, like in JavaScript
* ability to invoke any JavaScript/DOM object/function/method as if it's part of the same framework, without the need for special syntax
*variable and object scoping that make sense (no need for repetitive 'var' or 'new' keywords)
* ability to use both, Python's methods/functions and JavaScript's alternatives
* similar to above, ability to use both, Python's and JavaScript's tutorials (as well as widgets)
* it's self-hosting, that means the compiler is itself written in RapydScript and compiles into JavaScript

https://github.com/kovidgoyal/rapydscript-ng[+https://github.com/kovidgoyal/rapydscript-ng+]

Rasdaman
~~~~~~~~

Rasdaman ("raster data manager") allows storing and querying massive multi-dimensional ​arrays, such as sensor, image, simulation, and statistics data appearing in domains like earth, space, and life science. This worldwide leading array analytics engine distinguishes itself by its flexibility, performance, and scalability. Rasdaman can process arrays residing in file system directories as well as in databases. 

From simple geo imagery services up to complex analytics, rasdaman provides the whole spectrum of functionality on spatio-temporal raster data - both regular and irregular grids. And it does so with an unprecedented performance and scalability, as recent scientific benchmarks show. To leverage this enabling technology, users do not necessarily have to learn new interfaces: rasdaman integrates smoothly with R, OpenLayers, Leaflet, NASA WorldWind, GDAL, MapServer, ESRI ArcGIS, and many more. 

Rasdaman is brought to you by the guys writing the Big Datacube standards, such as ISO DIS SQL/MDA (Multi-Dimensional Arrays), integrating n-D arrays seamlessly into relational world. This standard will be domain-independent and can serve all of Earth, Space, Life sciences, and beyond. Further, rasdaman is the blueprint for Earth datacubes with OGC WCS and WCPS, the OGC raster query language. No surprise, rasdaman supports OGC WMS, WCS core and all extensions including WCS-T, WCPS, and WPS. Further, rasdaman is INSPIRE WCS reference implementation.

http://www.rasdaman.org/[+http://www.rasdaman.org/+]

RASM
~~~~

The Regional Arctic System Model (RASM) has been developed to advance capability in simulating critical physical processes, feedbacks and their impact on the Arctic climate system and to reduce uncertainty in its prediction. RASM is a limited-area, fully coupled ice-ocean-atmosphere-land model that uses the Community Earth System Model (CESM) framework. It includes the Weather Research and Forecasting (WRF) model, the LANL Parallel Ocean Program (POP) and Community Ice Model (CICE) and the Variable Infiltration Capacity (VIC) land hydrology model. In addition, a streamflow routing (RVIC) model was recently implemented in RASM to transport the freshwater flux from the land surface to the Arctic Ocean. Finally, marine biogeochemistry components are currently being implemented in the ocean and sea ice components to expand RASM capability into Arctic ecosystem studies. The model domain is configured at horizontal resolution of 1/12° (or ~9km) for the ice-ocean and 50 km for the atmosphere-land model components. It covers the entire Northern Hemisphere marine cryosphere, terrestrial drainage to the Arctic Ocean and its major inflow and outflow pathways, with optimal extension into the North Pacific / Atlantic to model the passage of cyclones into the Arctic. All RASM components are coupled at high frequency to realistically represent interactions among model components at inertial and longer time scales.

https://www.oc.nps.edu/NAME/RASM_PhaseIII.html[+https://www.oc.nps.edu/NAME/RASM_PhaseIII.html+]

https://www.geosci-model-dev.net/11/4817/2018/[+https://www.geosci-model-dev.net/11/4817/2018/+]

Rasterio
~~~~~~~~

Geographic information systems use GeoTIFF and other formats to organize and store gridded raster datasets such as satellite imagery and terrain models. Rasterio reads and writes these formats and provides a Python API based on Numpy N-dimensional arrays and GeoJSON.

Before Rasterio there was one Python option for accessing the many different kind of raster data files used in the GIS field: the Python bindings distributed with the Geospatial Data Abstraction Library, GDAL. These bindings extend Python, but provide little abstraction for GDAL’s C API. This means that Python programs using them tend to read and run like C programs. For example, GDAL’s Python bindings require users to watch out for dangling C pointers, potential crashers of programs. This is bad: among other considerations we’ve chosen Python instead of C to avoid problems with pointers.

What would it be like to have a geospatial data abstraction in the Python standard library? One that used modern Python language features and idioms? One that freed users from concern about dangling pointers and other C programming pitfalls? Rasterio’s goal is to be this kind of raster data library – expressing GDAL’s data model using fewer non-idiomatic extension classes and more idiomatic Python types and protocols, while performing as fast as GDAL’s Python bindings.

High performance, lower cognitive load, cleaner and more transparent code. This is what Rasterio is about.

https://rasterio.readthedocs.io/en/latest/[+https://rasterio.readthedocs.io/en/latest/+]

RasterSmith
~~~~~~~~~~~

RasterSmith is a package to preprocess different NASA Earth observing satellite data products into common resolution, spatial reference, and format for easy analysis and processing across sensors. 
The core strength of RasterSmith is the ability to read in geographic raster data and format it in a way that is internally consistent.

The RasterSmith data structure is an xarray DataArray object with five dimensions which are labeled as lat, lon, z, band, and time. RasterSmith uses these five dimensions to account for the full dimensionality of all satellite data products (see RasterSmith data structure section for full details). Furthermore, we can see that there are coordinates associated with the labeled dimensions where we can use the internal xarray API for sub-sampling and manipulation. Lastly, we have common metadata properties provided as xarray attributes. For example, every satellite product has a projection String, or projStr (using the Proj4 string representaion), to decribe the native coordinate system as well as extent, acquisition date, resolution, and scaling factors.

Each xarray DataArray that RasterSmith creates has a mask band. This is a band that describes good data pixels (value of 1) and poor data pixels to not use (value of 0) based on internal satellite data product QA information. RasterSmith has a method to mask poor data within the object so any future operations will use only good data.

Many satellite data products provided come in different (1) data formats, (2) number of bands, (3) spatial resolution/extent, and (4) geographic projections making the combined use of the data product often difficult to handle and use for non-experts. RasterSmith is used to take the differing data product and make a common format with a set of helper functions for use in analysis.

Take two commonly used remote sensing data products from LandSat and the Visible Infrared Imaging Radiometer Suite (VIIRS). LandSat is distributed as GeoTIFF files, one file for each band, along with associated metadata in a separate file, where as VIIRS data is distributed as a single HDF5 file with embedded metadata. While individual LandSat GeoTIFF files can be easily used in GIS software, preprocessing is needed to use multiple bands together as a single variable. On the other hand, VIIRS data distributed as HDF5 data is difficult to read in and use within traditional GIS software. Ultimately, using the two datasets in conjunction with ArcGIS or QGIS has proven difficult due to the varying formats that the data is distributed in.

https://github.com/KMarkert/rastersmith[+https://github.com/KMarkert/rastersmith+]

Ray
~~~

Ray is a flexible, high-performance distributed execution framework.

Ray is a high-performance distributed execution framework targeted at large-scale machine learning and reinforcement learning applications. It achieves scalability and fault tolerance by abstracting the control state of the system in a global control store and keeping all other components stateless. It uses a shared-memory distributed object store to efficiently handle large data through shared memory, and it uses a bottom-up hierarchical scheduling architecture to achieve low-latency and high-throughput scheduling. It uses a lightweight API based on dynamic task graphs and actors to express a wide range of applications in a flexible manner.

https://github.com/ray-project/ray[+https://github.com/ray-project/ray+]

https://ray.readthedocs.io/en/latest/[+https://ray.readthedocs.io/en/latest/+]

https://rise.cs.berkeley.edu/projects/ray/[+https://rise.cs.berkeley.edu/projects/ray/+]

https://arxiv.org/abs/1712.05889[+https://arxiv.org/abs/1712.05889+]

https://github.com/modin-project/modin[+https://github.com/modin-project/modin+]

rclone
~~~~~~

Rclone is a command line program to sync files and directories to and from literally every cloud storage solution in the universe including Google Drive.
The features include:

* MD5/SHA1 hashes checked at all times for file integrity
* Timestamps preserved on files
* Partial syncs supported on a whole file basis
* Copy mode to just copy new/changed files
* Sync (one way) mode to make a directory identical
* Check mode to check for file hash equality
* Can sync to and from network, eg two different cloud accounts
* (Encryption) backend
* (Cache) backend
* (Union) backend
* Optional FUSE mount (rclone mount)

https://rclone.org/[+https://rclone.org/+]

https://github.com/ncw/rclone[+https://github.com/ncw/rclone+]

https://github.com/restic/restic[+https://github.com/restic/restic+]

RDF
~~~

The Resource Description Framework (RDF) is a family of World Wide Web Consortium (W3C) specifications[1] originally designed as a metadata data model. It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications. 

The RDF data model[2] is similar to classical conceptual modeling approaches (such as entity–relationship or class diagrams). It is based on the idea of making statements about resources (in particular web resources) in expressions of the form subject–predicate–object, known as triples. The subject denotes the resource, and the predicate denotes traits or aspects of the resource, and expresses a relationship between the subject and the object. 

RDF is an abstract model with several serialization formats (i.e. file formats), so the particular encoding for resources or triples varies from format to format. 

A collection of RDF statements intrinsically represents a labeled, directed multi-graph. This in theory makes an RDF data model better suited to certain kinds of knowledge representation than are other relational or ontological models. However, in practice, RDF data is often stored in relational database or native representations (also called Triplestores—or Quad stores, if context such as the named graph is also stored for each RDF triple).[

protege
^^^^^^^

A free, open-source ontology editor and framework for building intelligent systems.
Protégé’s plug-in architecture can be adapted to build both simple and complex ontology-based applications. Developers can integrate the output of Protégé with rule systems or other problem solvers to construct a wide range of intelligent systems.

Protégé fully supports the latest OWL 2 Web Ontology Language and RDF specifications from the World Wide Web Consortium. 
It has direct interfaces to the FaCTxx and Pellet reasoners, and a plug-in architecture to add
others.

https://protege.stanford.edu/[+https://protege.stanford.edu/+]

https://protegewiki.stanford.edu/wiki/Main_Page[+https://protegewiki.stanford.edu/wiki/Main_Page+]

https://github.com/stardog-union/pellet[+https://github.com/stardog-union/pellet+]

http://owl.cs.manchester.ac.uk/tools/fact/[+http://owl.cs.manchester.ac.uk/tools/fact/+]


Redland
^^^^^^^

Redland is a set of free software C libraries that provide support for the Resource Description Framework (RDF).

* Modular, object based libraries and APIs for manipulating the RDF graph, triples, URIs and Literals.
* Storage for graphs in memory and persistently with Oracle Berkeley DB, MySQL 3-5, PostgreSQL, OpenLink Virtoso, SQLite, files or URIs.
* Support for multiple syntaxes for reading and writing RDF as RDF/XML, N-Triples and Turtle, RSS and Atom syntaxes via the Raptor RDF Syntax Library.
* Querying with SPARQL and RDQL using the Rasqal RDF Query Library.
* Data aggregation and recording provenance support with Redland contexts.
* Language Bindings in Perl, PHP, Python and Ruby via the Redland Bindings package.
* Command line utility programs rdfproc (RDF), rapper (parsing) and roqet (query).
* Portable, fast and with no known memory leaks.

http://librdf.org/[+http://librdf.org/+]

Silk
^^^^

Silk is an open source framework for integrating heterogeneous data sources. The primary uses cases of Silk include:

* Generating links between related data items within different Linked Data sources.
* Linked Data publishers can use Silk to set RDF links from their data sources to other data sources on the Web.
* Applying data transformations to structured data sources.

Silk is based on the Linked Data paradigm, which is built on two simple ideas: First, RDF provides an expressive data model for representing structured information. Second, RDF links are set between entities in different data sources. Background information about Linked Data and the vision of the Web of Data can be found in the overview article Linked Data - The Story So Far and the Linked Data book. 

Using the declarative Silk - Link Specification Language (Silk-LSL), developers can specify which types of RDF links should be discovered between data sources as well as which conditions data items must fulfill in order to be interlinked. These link conditions may combine various similarity metrics and can take the graph around a data item into account, which is addressed using an RDF path language. Silk accesses the data sources that should be interlinked via the SPARQL protocol and can thus be used against local as well as remote SPARQL endpoints. Link Specifications can be created using the Silk Workbench graphical user interface or manually in XML.

http://silkframework.org/[+http://silkframework.org/+]

Records
~~~~~~~

Records is a very simple, but powerful, library for making raw SQL queries to most relational databases.
Just write SQL. No bells, no whistles. This common task can be surprisingly difficult with the standard tools available. This library strives to make this workflow as simple as possible, while providing an elegant interface to work with your query results.
Database support includes RedShift, Postgres, MySQL, SQLite, Oracle, and MS-SQL (drivers not included).

Records also features full Tablib integration, and allows you to export your results to CSV, XLS, JSON, HTML Tables, YAML, or Pandas DataFrames with a single line of code. Excellent for sharing data with friends, or generating reports.

https://github.com/kennethreitz/records[+https://github.com/kennethreitz/records+]

Redis
~~~~~

Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes with radius queries and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.

You can run atomic operations on these types, like appending to a string; incrementing the value in a hash; pushing an element to a list; computing set intersection, union and difference; or getting the member with highest ranking in a sorted set.

In order to achieve its outstanding performance, Redis works with an in-memory dataset. Depending on your use case, you can persist it either by dumping the dataset to disk every once in a while, or by appending each command to a log. Persistence can be optionally disabled, if you just need a feature-rich, networked, in-memory cache.

Redis also supports trivial-to-setup master-slave asynchronous replication, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split.

https://redis.io/[+https://redis.io/+]

rediSQL
^^^^^^^

RediSQL is a module for Redis that embed a completely functional SQLite database.

RediSQL enables new paradigm where is possible to have several smaller decentralized databases instead of a single giant one.

RediSQL adds simple commands to your Redis giving it the ability to store, manage and retrieve more structured data.

https://github.com/RedBeardLab/rediSQL[+https://github.com/RedBeardLab/rediSQL+]

ReJSON
^^^^^^

ReJSON is a Redis module that implements ECMA-404 The JSON Data Interchange Standard as a native data type. It allows storing, updating and fetching JSON values from Redis keys (documents).

Primary features:

* Full support of the JSON standard
* JSONPath-like syntax for selecting element inside documents
* Documents are stored as binary data in a tree structure, allowing fast access to sub-elements
* Typed atomic operations for all JSON values types

https://github.com/RedisLabsModules/ReJSON[+https://github.com/RedisLabsModules/ReJSON+]

REDUCE
~~~~~~

REDUCE is an interactive system for general algebraic computations of interest to mathematicians, scientists and engineers. It can be used interactively for simple calculations but also provides a flexible and expressive user programming language.

The development of the REDUCE computer algebra system was started in the 1960s by Anthony C. Hearn. Since then, many scientists from all over the world have contributed to its development. REDUCE has a long and distinguished place in the history of computer algebra systems. Other systems that address some of the same issues but sometimes with rather different emphasis are Axiom, Derive, Macsyma (Maxima), Maple, Mathematica and MuPAD.

REDUCE primarily runs on either Portable Standard Lisp (PSL) or Codemist Standard Lisp (CSL), both of which are included in the SourceForge distribution. By modern standards, REDUCE is a surprisingly small and compact application, which runs well on all major operating systems.

https://sourceforge.net/projects/reduce-algebra/[+https://sourceforge.net/projects/reduce-algebra/+]

https://reduce-algebra.sourceforge.io/[+https://reduce-algebra.sourceforge.io/+]

ATENSOR
^^^^^^^

A REDUCE program for tensor simplification.

http://cpc.cs.qub.ac.uk/summaries/ADDQ_v1_0.html[+http://cpc.cs.qub.ac.uk/summaries/ADDQ_v1_0.html+]

https://arxiv.org/abs/1811.05409[+https://arxiv.org/abs/1811.05409+]

RefactorF4Acc
~~~~~~~~~~~~~

An automatic refactoring tool for Fortran code that converts FORTRAN77 into Fortran-95, and also
has a backend to translate modules to C/OpenCL.

Most code for numerical simulation of weather/climate models is written in Fortran.
In particular the older codebases, written in FORTRAN77, are difficult to maintain or modify.
With the advent of multicore CPUs, GPGPUs and FPGAs and technologies such as OpenMP and OpenCL, there is a growing interest in acceleration of numerical simulations.
With the current state of affairs, this usually requires a considerable manual rewrite of the code.
Our compiler tries to automate this process as much as possible.

Automatic parallelisation and offloading of legacy code to accelerators is the ultimate aim of the project, and already works for many cases. However, the work flow is more complicated and requires an additional compiler, AutoParallel-Fortran. This compiler is written in Haskell, which is not yet a mainstream programming language. Furthermore, the generated OpenCL code relies on the OclWrapper Fortran OpenCL API, written in Cxx, and uses scons, a Python-based build system. For these reasons, it is harder to install this autoparallelising compiler. However, if you have installed it, a test case for the full flow is provided in the tests folder.

https://github.com/wimvanderbauwhede/RefactorF4Acc[+https://github.com/wimvanderbauwhede/RefactorF4Acc+]

http://joss.theoj.org/papers/5ced707dc2be213ad7ee02291e2c6e3c[+http://joss.theoj.org/papers/5ced707dc2be213ad7ee02291e2c6e3c+]

https://www.itl.nist.gov/div897/ctg/fortran_form.htm[+https://www.itl.nist.gov/div897/ctg/fortran_form.htm+]

http://www.fortran-2000.com/ArnaudRecipes/fcvs21_f95.html[+http://www.fortran-2000.com/ArnaudRecipes/fcvs21_f95.html+]

ReFRESCO
~~~~~~~~

ReFRESCO is an acronym for REliable&Fast Rans Equations (solver for) Ships (and) Constructions Offshore. ReFRESCO resembles in several regards a general commercial CFD code, but it is optimized, verified and validated exclusively for maritime applications.

ReFRESCO  is currently being developed, verified and (its applications) validated at MARIN (www.marin.nl) in collaboration with several other organizations around the world (see http://www.refresco.org/community/users/). Being community-based open-source, the developments done by all partners go into one code (yes, the same for everyone!), where the code quality control and assessment is done at MARIN using modern software development tools, software management procedures and modern CFD V&V(Verification&Validation, http://www.refresco.org/verification-validation/) techniques.

ReFRESCO is a viscous-flow CFD code that solves multiphase (unsteady) incompressible flows using the Navier-Stokes equations, complemented with turbulence models, cavitation models and volume-fraction transport equations for different phases. The equations are discretised using a finite-volume approach with cell-centered collocated variables, in strong-conservation form, and a pressure-correction equation based on the SIMPLE algorithm is used to ensure mass conservation. Time integration is performed implicitly with first or second-order backward schemes. At each implicit time step, the non-linear system for velocity and pressure is linearised with Picard’s method and either a segregated or coupled approach is used. In the latter, the coupled linear system is solved with a matrix-free Krylov subspace method using a SIMPLE-type preconditioner. A segregated approach is always adopted for the solution of all other transport equations. The implementation is face-based, which permits grids with elements consisting of an arbitrary number of faces (hexahedrals, tetrahedrals, prisms, pyramids, etc.), and if needed h-refinement (hanging nodes).

State-of-the-art CFD features (for more detail see CFD Features page) such as moving, sliding and deforming grids, as well automatic grid adaptation (refinement and/or coarsening) are also available. Coupling with structural equations-of-motion (rigid-body 6DOF), and flexible-body FSI) is possible For turbulence modelling, both RANS/URANS and Scale-Resolving Simulations (SRS) models such as SAS, DDES/IDDES, XLES, PANS and LES approaches can be used. Acoustic models are also available. Couplings with propeller models (RANS-BEM coupling), fast-time simulation tools (XMF) and wave generation codes (ComFLOW, OceanWave3D, SWASH) are also available.

ReFRESCO runs on Linux workstations and HPC clusters. The code is parallelised using MPI and subdomain decomposition, and has been tested to have good speed-up up to 4000 cores. In order to guarantee the quality and easy maintenance of a complex code such as ReFRESCO, automatic testing, verification and validation procedures are used, as well as modern version control systems.

http://www.refresco.org/[+http://www.refresco.org/+]

RegCM
~~~~~

The Regional Climate Model system RegCM , originally developed at the National Center for Atmospheric Research (NCAR), is maintained in the Earth System Physics (ESP) section of the ICTP. The first version of the model, RegCM1, was developed in 1989 and since then it has undergone major updates in 1993 (RegCM2), 1999 (RegCM2.5), 2006 (RegCM3) and most recently 2010 (RegCM4). The latest version of the model, RegCM4, is now fully supported by the ESP, while previous versions are no longer available. This version includes major upgrades in the structure of the code and its pre- and post- processors, along with the inclusion of some new physics parameterizations. The model is flexible, portable and easy to use. It can be applied to any region of the World, with grid spacing of up to about 10 km (hydrostatic limit),  and for a wide range of studies, from process studies to paleoclimate and future climate simulation.

Model improvements currently under way include the development of a new microphysical cloud scheme (to be released by the end of 2014), coupling with a regional ocean model, inclusion of full gas-phase chemistry, upgrades of some physics schemes (convection, PBL, cloud microphysics) and development of a non-hydrostatic dynamical core.

https://www.ictp.it/research/esp/models/regcm4.aspx[+https://www.ictp.it/research/esp/models/regcm4.aspx+]

http://gforge.ictp.it/gf/project/regcm/[+http://gforge.ictp.it/gf/project/regcm/+]

RELATE
~~~~~~

RELATE is a web-based courseware package. It is set apart by the following features:

* Focus on easy content creation
** Simple, text-based format for reusable course content
** Based on standard YAML, Markdown
* See example content.
* Flexible rules for participation, access, and grading
* Versioning of content through deep integration with git. Instructors can preview newly-authored content while students work with prior versions, all from the same instance of RELATE.
* Multiple courses can be hosted on the same installation
* Code questions:
** Allow students to write code into a text box (with syntax highlighting)
** Sandboxed execution
** Automatic grading
** Plotting support
** Optional second-stage grading by a human
* Class calendar and grade book functionality.
* Statistics/analytics of student answers.
* Facilitates live quizzes in the classroom.
* In-class instant messaging via XMPP. Works well with xmpp-popup.
* Built-in support for VideoJS offers easy-to-use support for integrating HTML5 video into course content without the need for third-party content hosting.

RELATE is a based on the popular Django web framework for Python. It lets students participate in online activities, each of which is (generically) called a “flow”, which allows a sequence of pages, each of which can be both static or interactive content, for exapmle a video, a quiz question, a page of text, or, within the confines of HTML, something completely different.

https://documen.tician.de/relate/[+https://documen.tician.de/relate/+]

https://github.com/inducer/relate[+https://github.com/inducer/relate+]

resampling statistics
~~~~~~~~~~~~~~~~~~~~~

In statistics, resampling is any of a variety of methods for doing one of the following:

* Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)
* Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests)
* Validating models by using random subsets (bootstrapping, cross validation)

Common resampling techniques include bootstrapping, jackknifing and permutation tests. 

Bootstrapping is a statistical method for estimating the sampling distribution of an estimator by sampling with replacement from the original sample, most often with the purpose of deriving robust estimates of standard errors and confidence intervals of a population parameter like a mean, median, proportion, odds ratio, correlation coefficient or regression coefficient. It may also be used for constructing hypothesis tests. It is often used as a robust alternative to inference based on parametric assumptions when those assumptions are in doubt, or where parametric inference is impossible or requires very complicated formulas for the calculation of standard errors. Bootstrapping techniques are also used in the updating-selection transitions of particle filters, genetic type algorithms and related resample/reconfiguration Monte Carlo methods used in computational physics.

Jackknifing, which is similar to bootstrapping, is used in statistical inference to estimate the bias and standard error (variance) of a statistic, when a random sample of observations is used to calculate it.
The basic idea behind the jackknife variance estimator lies in systematically recomputing the statistic estimate, leaving out one or more observations at a time from the sample set. From this new set of replicates of the statistic, an estimate for the bias and an estimate for the variance of the statistic can be calculated. 
Instead of using the jackknife to estimate the variance, it may instead be applied to the log of the variance. This transformation may result in better estimates particularly when the distribution of the variance itself may be non normal. 
For many statistical parameters the jackknife estimate of variance tends asymptotically to the true value almost surely.

A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. In other words, the method by which treatments are allocated to subjects in an experimental design is mirrored in the analysis of that design. If the labels are exchangeable under the null hypothesis, then the resulting tests yield exact significance levels; see also exchangeability. Confidence intervals can then be derived from the tests.

https://en.wikipedia.org/wiki/Resampling_(statistics)[+https://en.wikipedia.org/wiki/Resampling_(statistics)+]

*Everything you wanted to know about Data Analysis and Fitting but were afraid to ask* - https://arxiv.org/abs/1210.3781[+https://arxiv.org/abs/1210.3781+]

boot
^^^^

R functions and datasets for bootstrapping from the book "Bootstrap Methods and Their Application" by A. C. Davison and D. V. Hinkley (1997, CUP).
The boot package provides extensive facilities for bootstrapping and related resampling methods. You can bootstrap a single statistic (e.g. a median), or a vector (e.g., regression weights).

https://cran.r-project.org/web/packages/boot/index.html[+https://cran.r-project.org/web/packages/boot/index.html+]

https://www.statmethods.net/advstats/bootstrapping.html[+https://www.statmethods.net/advstats/bootstrapping.html+]

resampling
^^^^^^^^^^

Python module for doing resampling analysis (jackknife and bootstrap).

https://github.com/bkrueger/resampling[+https://github.com/bkrueger/resampling+]

http://bkrueger.github.io/resampling/[+http://bkrueger.github.io/resampling/+]

Reticulated Python
~~~~~~~~~~~~~~~~~~

Reticulated Python is a tool for experimenting with gradual typing in Python 3. Reticulated Python adds optional static and runtime typechecking to Python. It lets programmers annotate functions and classes with types, and it enforces these types both before and during the execution of the program, providing early detection of errors. Crucially, Reticulated does not require that all parts of a program be given static types, or even any of it. In places where typed and untyped code interact - for example, when an untyped variable is passed into a function whose argument type is Int - Reticulated can perform runtime checks, to ensure that the values in variables always correspond to their expected types, even when this can't be guaranteed statically.

Reticulated Python runs on Python 3.5 and lower, using the syntactic annotations provided by Python 3 as type annotations. The absence of an annotation implies that the parameters or return values are dynamically typed, as in standard Python. We also provide "type functions" for creating types that correspond to higher-order values in Python, like functions and lists, and we provide the type Any (for dynamic) to allow for, for example, heterogeneous lists, which have the type List[Any].

Reticulated Python itself is written in Python 3. Its static, compile-time component is both a "linter" or static analyzer, and a source-to-source translator. This component parses source files (using the Python ast package) and searches for type errors, rejecting programs that have statically detectable errors and syntactically inserting runtime checks or casts at boundaries between typed and untyped code, where type errors may occur.

https://github.com/mvitousek/reticulated[+https://github.com/mvitousek/reticulated+]

https://arxiv.org/abs/1902.07808[+https://arxiv.org/abs/1902.07808+]

RINGMesh
~~~~~~~~

RINGMesh is a Cxx open-source platform for manipulating meshes of geological models. RINGMesh is neither a geomodeler nor a mesh generation tool, but it can be used to develop geomodeling applications, such as meshing and numerical simulation tools.

The features include:

* Efficient data structure to handle geological models
* Validity checks of geometric, geological and topological consistency.
* Meshes representing the geometry of geological model entities can be extented using your own data structure
* Complete and user-friendly API to use meshes in physical simulators
* Lightweight geological model viewer
* Utilities to convert geological models into another file format

http://ringmesh.org/[+http://ringmesh.org/+]

Ripser
~~~~~~

Ripser.py is a lean persistent homology package for Python. Building on the blazing fast Cxx Ripser package as the core computational engine, Ripser.py provides an intuitive interface for

* computing persistence cohomology of sparse and dense data sets,

* visualizing persistence diagrams,

* computing lowerstar filtrations on images, and

* computing representative cochains.

Ripser.py is an evolution of the original Cxx Ripser project. We have put extensive work into making the package available to Python developers across all major platforms.

The original motivation for defining homology groups was the observation that two shapes can be distinguished by examining their holes. For instance, a circle is not a disk because the circle has a hole through it while the disk is solid, and the ordinary sphere is not a circle because the sphere encloses a two-dimensional hole while the circle encloses a one-dimensional hole. However, because a hole is "not there", it is not immediately obvious how to define a hole or how to distinguish different kinds of holes. Homology was originally a rigorous mathematical method for defining and categorizing holes in a manifold.

In topological data analysis, data sets are regarded as a point cloud sampling of a manifold or algebraic variety embedded in Euclidean space. By linking nearest neighbor points in the cloud into a triangulation, a simplicial approximation of the manifold is created and its simplicial homology may be calculated. Finding techniques to robustly calculate homology using various triangulation strategies over multiple length scales is the topic of persistent homology.

https://ripser.scikit-tda.org/[+https://ripser.scikit-tda.org/+]

https://github.com/scikit-tda/ripser.py[+https://github.com/scikit-tda/ripser.py+]

http://joss.theoj.org/papers/a8774cc066969ab17e9a0e9c39c6a81c[+http://joss.theoj.org/papers/a8774cc066969ab17e9a0e9c39c6a81c+]

https://en.wikipedia.org/wiki/Homology_(mathematics)[+https://en.wikipedia.org/wiki/Homology_(mathematics)+]

Robinhood
~~~~~~~~~

Robinhood Policy Engine is a versatile tool to manage contents of large file systems. It maintains a replicate of filesystem medatada in a database that can be queried at will. It makes it possible to schedule mass action on filesystem entries by defining attribute-based policies, provides fast 'find' and 'du' enhanced clones, gives to administrators an overall view of filesystem contents through its web UI and command line tools.

t supports any POSIX filesystem and implements advanced features for Lustre filesystems (list/purge files per OST or pool, read MDT changelogs...)

Originally developped for HPC, it has been designed to perform all its tasks in parallel, so it is particularly adapted for running on large filesystems with millions of entries and petabytes of data. But of course, you can take benefits of all its features for managing smaller filesystems, like '/tmp' of workstations.

https://github.com/cea-hpc/robinhood/wiki[+https://github.com/cea-hpc/robinhood/wiki+]

https://github.com/cea-hpc/robinhood[+https://github.com/cea-hpc/robinhood+]

RoboSat
~~~~~~~

RoboSat is an end-to-end pipeline written in Python 3 for feature extraction from aerial and satellite imagery. Features can be anything visually distinguishable in the imagery for example: buildings, parking lots, roads, or cars.

The tools RoboSat comes with can be categorized as follows:

* data preparation: creating a dataset for training feature extraction models
* training and modeling: segmentation models for feature extraction in images
* post-processing: turning segmentation results into cleaned and simple geometries

Tools work with the Slippy Map tile format to abstract away geo-referenced imagery behind tiles of the same size.

The data preparation tools help you with getting started creating a dataset for training feature extraction models. Such a dataset consists of aerial or satellite imagery and corresponding masks for the features you want to extract. We provide convenient tools to automatically create these datasets downloading aerial imagery from the Mapbox Maps API and generating masks from OpenStreetMap geometries but we are not bound to these sources.

The modelling tools help you with training fully convolutional neural nets for segmentation. We recommend using (potentially multiple) GPUs for these tools: we are running RoboSat on AWS p2/p3 instances and GTX 1080 TI GPUs. After you trained a model you can save its checkpoint and run prediction either on GPUs or CPUs.

The post-processing tools help you with cleaning up the segmentation model's results. They are responsible for denoising, simplifying geometries, transforming from pixels in Slippy Map tiles to world coordinates (GeoJSON features), and properly handling tile boundaries.

https://github.com/mapbox/robosat[+https://github.com/mapbox/robosat+]

ROMS
~~~~

ROMS is a free-surface, terrain-following, primitive equations ocean model widely used by the scientific community for a diverse range of applications.
The algorithms that comprise ROMS computational nonlinear kernel are described in detail in Shchepetkin and McWilliams (2003, 2005), and the tangent linear and adjoint kernels and platforms are described in Moore et al. (2004). ROMS includes accurate and efficient physical and numerical algorithms and several coupled models for biogeochemical, bio-optical, sediment, and sea ice applications. The sea ice model is described in Budgell (2005). It also includes several vertical mixing schemes (Warner et al., 2005a), multiple levels of nesting and composed grids. 

For computational economy, the hydrostatic primitive equations for momentum are solved using a split-explicit time-stepping scheme which requires special treatment and coupling between barotropic (fast) and baroclinic (slow) modes. A finite number of barotropic time steps, within each baroclinic step, are carried out to evolve the free-surface and vertically integrated momentum equations. In order to avoid the errors associated with the aliasing of frequencies resolved by the barotropic steps but unresolved by the baroclinic step, the barotropic fields are time averaged before they replace those values obtained with a longer baroclinic step. A cosine-shape time filter, centered at the new time level, is used for the averaging of the barotropic fields (Shchepetkin and McWilliams, 2005). In addition, the separated time-stepping is constrained to maintain exactly both volume conservation and consistancy preservation properties which are needed for the tracer equations (Shchepetkin and McWilliams, 2005). Currently, all 2D and 3D equations are time-discretized using a third-order accurate predictor (Leap-Frog) and corrector (Adams-Molton) time-stepping algorithm which is very robust and stable. The enhanced stability of the scheme allows larger time steps, by a factor of about four, which more than offsets the increased cost of the predictor-corrector algorithm. 

http://myroms.org/[+http://myroms.org/+]

https://github.com/dcherian/ROMS[+https://github.com/dcherian/ROMS+]

https://github.com/kshedstrom/roms[+https://github.com/kshedstrom/roms+]

https://github.com/ESMG/pyroms[+https://github.com/ESMG/pyroms+]

https://github.com/bjornaa/xroms[+https://github.com/bjornaa/xroms+]

https://github.com/AustralianAntarcticDivision/angstroms[+https://github.com/AustralianAntarcticDivision/angstroms+]

ROOT
~~~~

ROOT is an object-oriented program and library developed by CERN. It was originally designed for particle physics data analysis and contains several features specific to this field, but it is also used in other applications such as astronomy and data mining.
It provides platform independent access to a computer's graphics subsystem and operating system using abstract layers. Parts of the abstract platform are: a graphical user interface and a GUI builder, container classes, reflection, a Cxx script and command line interpreter (CINT in version 5, cling in version 6), object serialization and persistence. 

The packages provided by ROOT include those for

* Histogramming and graphing to view and analyze distributions and functions,
* curve fitting (regression analysis) and minimization of functionals,
* statistics tools used for data analysis,
* matrix algebra,
* four-vector computations, as used in high energy physics,
* standard mathematical functions,
* multivariate data analysis, e.g. using neural networks,
* image manipulation, used, for instance, to analyze astronomical pictures,
* access to distributed data (in the context of the Grid),
* distributed computing, to parallelize data analyses,
* persistence and serialization of objects, which can cope with changes in class definitions of persistent data,
* access to databases,
* 3D visualizations (geometry),
* creating files in various graphics formats, like PDF, PostScript, PNG, SVG, LaTeX, etc.
* interfacing Python and Ruby code in both directions,
* interfacing Monte Carlo event generators.

https://en.wikipedia.org/wiki/ROOT[+https://en.wikipedia.org/wiki/ROOT+]

https://root.cern.ch/[+https://root.cern.ch/+]

https://arxiv.org/abs/1812.03145[+https://arxiv.org/abs/1812.03145+]

ROS
~~~

Robot Operating System (ROS) is robotics middleware (i.e. collection of software frameworks for robot software development). Although ROS is not an operating system, it provides services designed for a heterogeneous computer cluster such as hardware abstraction, low-level device control, implementation of commonly used functionality, message-passing between processes, and package management. Running sets of ROS-based processes are represented in a graph architecture where processing takes place in nodes that may receive, post and multiplex sensor, control, state, planning, actuator, and other messages. Despite the importance of reactivity and low latency in robot control, ROS itself is not a real-time OS (RTOS). It is possible, however, to integrate ROS with real-time code.[2] The lack of support for real-time systems has been addressed in the creation of ROS 2.0.[3][4][5]

Software in the ROS Ecosystem[6] can be separated into three groups:

* language-and platform-independent tools used for building and distributing ROS-based software;
* ROS client library implementations such as roscpp,[7] rospy,[8] and roslisp;[9]
* packages containing application-related code which uses one or more ROS client libraries.[10]

Both the language-independent tools and the main client libraries (Cxx, Python, and Lisp) are released under the terms of the BSD license, and as such are open source software and free for both commercial and research use. The majority of other packages are licensed under a variety of open source licenses. These other packages implement commonly used functionality and applications such as hardware drivers, robot models, datatypes, planning, perception, simultaneous localization and mapping, simulation tools, and other algorithms. 

https://en.wikipedia.org/wiki/Robot_Operating_System[+https://en.wikipedia.org/wiki/Robot_Operating_System+]

http://www.ros.org/[+http://www.ros.org/+]

ROS-Industrial
~~~~~~~~~~~~~~

ROS-Industrial is an open-source project that extends the advanced capabilities of ROS to manufacturing automation and robotics. The ROS-Industrial repository includes interfaces for common industrial manipulators, grippers, sensors, and device networks. It also provides software libraries for automatic 2D/3D sensor calibration, process path/motion planning, applications like Scan-N-Plan, developer tools like the Qt Creator ROS Plugin, and training curriculum that is specific to the needs of manufacturers. ROS-I is supported by an international Consortium of industry and research members. ROS-Industrial:

* Provides a one-stop location for manufacturing-related ROS software.
* Striving towards software robustness and reliability that meets the needs of industrial applications.
* Combines the relative strengths of ROS and existing technology, combining ROS high-level functionality with the low-level reliability and safety of an industrial robot controller, as opposed to replacing any one technology entirely.
* Stimulates the development of hardware-agnostic software by standardizing interfaces.
* Provides an "easy" path to apply cutting-edge research to industrial applications by using a common ROS architecture.
* Provides simple, easy-to-use, well-documented application programming interfaces.

https://rosindustrial.org/[+https://rosindustrial.org/+]

http://wiki.ros.org/Industrial[+http://wiki.ros.org/Industrial+]

https://github.com/ros-industrial[+https://github.com/ros-industrial+]

ROSE
~~~~

ROSE is an open source compiler infrastructure to build source-to-source program transformation and analysis tools for large-scale C (C89 and C98), Cxx (Cxx98 and Cxx11), UPC, Fortran (77/95/2003), OpenMP, Java, Python and PHP applications. 

ROSE is a library providing users access to compiler technology that was hitherto inaccessible to non-experts.

What is compiler technology? Compilers are sophisticated software that translate source code into machine binaries. Compiler developers have created powerful techniques to parse, analyze, transform and optimize the input source code.

Traditional compilers like GCC use these techniques, but they are essentially inaccessible to the user. However, even if the user had these capabilities in gcc, evaluating the results would be extremely difficult. The user inputs source code, the compiler outputs machine code. The user has access to his own code and perhaps the code generation at the assembly level, but comparing the two is extremely difficult as the code generation involved is not for clarity, it is for the needs of the compiler developer.

But what if a compiler returned a faithful source code representation of the post-transformation changes? Taking the original source code to transformed source code. A source-to-source compiler.

ROSE is a source-to-source compiler.

ROSE gives the user a library of compiler techniques. It also gives the user access to the building blocks of source analysis, allowing the user to create their own compilers, analyzers, translators, preprocessors, and so on. 

http://rosecompiler.org/[+http://rosecompiler.org/+]

https://github.com/rose-compiler/rose[+https://github.com/rose-compiler/rose+]

https://en.wikibooks.org/wiki/ROSE_Compiler_Framework[+https://en.wikibooks.org/wiki/ROSE_Compiler_Framework+]

https://computation.llnl.gov/projects/rose-compiler[+https://computation.llnl.gov/projects/rose-compiler+]

Rosetta
~~~~~~~

Field data obtained from dataloggers often take the form of comma separated value (CSV) ASCII text files. While ASCII based data formats have some positive aspects, such as the ability to open them with a text editor or spreadsheet software and "see" the data effortlessly, there are some drawbacks, especially when viewing the situation through the lens of data interoperability and stewardship.

Issues regarding ASCII data and their integration, interoperability, and stewardship have become especially urgent for the NSF-funded next-generation Advanced Cooperative Arctic Data and Information Service (ACADIS) project. The goal of ACADIS is to allow scientists to more easily access, share, integrate and work with Arctic data spanning multiple disciplines. These goals become quite challenging when one considers the large number of ASCII datasets that are either currently part of ACADIS or are being routinely submitted to the project, as those ASCII data are stored in a multitude of layouts, and nearly all metadata reside in non-standard README files, completely disjointed from the actual data they describe.

The Unidata Data Transformation Tool, Rosetta, is a web-based service that provides an easy, wizard-based interface for data collectors to transform their datalogger generated ASCII output into Climate and Forecast (CF) compliant netCDF files, complete with metadata describing what data are contained in the file, the instruments used to collect the data, and other critical information that otherwise may be lost in one of many dreaded README files. However, with the understanding that the observational community appreciates the ease of use of ASCII files, methods for transforming the netCDF back into a user defined CSV or spreadsheet formats are also built-in. We anticipate that Rosetta and the associated services will be of value to a broader community users who have similar needs for transforming the data they have collected or stored in non-standard formats.

https://github.com/Unidata/rosetta[+https://github.com/Unidata/rosetta+]

Rosie
~~~~~

Rosie Pattern Language (RPL) and the Rosie Pattern Engine, including librosie, CLI, REPL, and standard pattern library.

RPL is a variant of modern Regular Expressions (regex) that is designed
to scale to big data, many developers, and large collections of patterns.  If
you use regex, you already know a lot of RPL.  Additional features over regex
found in RPL:

* Looks like a programming language, and plays well with development tools
* Comes with a library of dozens of useful patterns (timestamps, network addresses, and more)
* Has development tools: tracing, REPL, color-coded match output
* Produces JSON output (and other formats)

Rosie searches all given input files for lines that match a pattern. The pattern language is similar to regular expressions, but more powerful. Also, a set of predefined (named) patterns are provided. The pattern library is extensible. Rosie patterns are written in Rosie Pattern Language (RPL).

A shared library, librosie, provides a programmatic interface to rosie in C, Go, Python, and other languages that support libffi. Rosie can be used for data mining on large data sets, or for the kind of smaller tasks that Unix grep and other regex tools are used for. The output can be plain text, like grep produces, or can be structured JSON, in the form of a parse tree. A number of other output options are available.

https://gitlab.com/rosie-pattern-language/rosie[+https://gitlab.com/rosie-pattern-language/rosie+]

https://rosie-lang.org/[+https://rosie-lang.org/+]

https://www.thestrangeloop.com/2018/rosie-pattern-language-improving-on-50-year-old-regular-expression-technology.html[+https://www.thestrangeloop.com/2018/rosie-pattern-language-improving-on-50-year-old-regular-expression-technology.html+]

rowan
~~~~~

Welcome to the documentation for rowan, a package for working with quaternions! Quaternions, which form a number system with various interesting properties, were originally developed for classical mechanics. Although they have since been largely displaced from this application by vector mathematics, they have become a standard method of representing rotations in three dimensions. Quaternions are now commonly used for this purpose in various fields, including computer graphics and attitude control.

This package provides tools for standard algebraic operations on quaternions as well as a number of additional tools for e.g. measuring distances between quaternions, interpolating between them, and performing basic point-cloud mapping. A particular focus of the rowan package is working with unit quaternions, which are a popular means of representing rotations in 3D. In order to provide a unified framework for working with the various rotation formalisms in 3D, rowan allows easy interconversion between these formalisms.

Core features of rowan include (but are not limited to):

* Algebra (multiplication, exponentiation, etc).
* Derivatives and integrals of quaternions.
* Rotation and reflection operations, with conversions to and from matrices, axis angles, etc.
* Various distance metrics for quaternions.
* Basic point set registration, including solutions of the Procrustes problem and the Iterative Closest Point algorithm.
* Quaternion interpolation (slerp, squad).

https://rowan.readthedocs.io/en/latest/[+https://rowan.readthedocs.io/en/latest/+]

https://bitbucket.org/glotzer/rowan[+https://bitbucket.org/glotzer/rowan+]

http://joss.theoj.org/papers/9c781f6fd89b36396abbabd26d6d9ef6[+http://joss.theoj.org/papers/9c781f6fd89b36396abbabd26d6d9ef6+]

rpe
~~~

This software provides a Fortran library for emulating the effect of low-precision real numbers. It is intended to be used as an experimental tool to understand the effect of reduced precision within numerical simulations.

The library contains a derived type: rpe_var. This type can be used in place of real-valued variables to perform calculations with floating-point numbers represented with a reduced number of bits in the floating-point significand.

The precision used by reduced precision types can be controlled at two different levels. Each reduced precision variable has an sbits attribute which controls the number of explicit bits in its significand. This can be set independently for different variables, and comes into effect after it is explicitly set.

For variables whose sbits attribute has not been explicitly set, there is a default global precision level, set by RPE_DEFAULT_SBITS. This will stand-in for the number of explicit significand bits in any variable where sbits has not been set. Setting RPE_DEFAULT_SBITS once to define the global default precision, and setting the precision of variables that require other precision manually using the sbits attribute is generally a good strategy.

https://github.com/aopp-pred/rpe[+https://github.com/aopp-pred/rpe+]

https://rpe.readthedocs.io/en/stable/[+https://rpe.readthedocs.io/en/stable/+]

https://github.com/aopp-pred/rpe-examples[+https://github.com/aopp-pred/rpe-examples+]

https://github.com/aopp-pred/fp-truncate[+https://github.com/aopp-pred/fp-truncate+]

RxPY
~~~~

Reactive Extensions for Python (RxPY) is a set of libraries for composing asynchronous and event-based programs using observable sequences and LINQ-style query operators in Python. Using Rx, developers represent asynchronous data streams with Observables, query asynchronous data streams using operators, and parameterize concurrency in data/event streams using Schedulers.

Using Rx, you can represent multiple asynchronous data streams (that come from diverse sources, e.g., stock quote, Tweets, computer events, web service requests, etc.), and subscribe to the event stream using the Observer object. The Observable notifies the subscribed Observer instance whenever an event occurs. You can put various transformations in-between the source Observable and the consuming Observer as well.

Because Observable sequences are data streams, you can query them using standard LINQ-like query operators implemented by the Observable type. Thus you can filter, map, reduce, compose and perform time-based operations on multiple events easily by using these static LINQ operators. In addition, there are a number of other reactive stream specific operators that allow powerful queries to be written. Cancellation, exceptions, and synchronization are also handled gracefully by using the methods on the Observable object.

https://github.com/ReactiveX/RxPY[+https://github.com/ReactiveX/RxPY+]

https://github.com/ReactiveX/RxPY/blob/develop/notebooks/Getting%20Started.ipynb[+https://github.com/ReactiveX/RxPY/blob/develop/notebooks/Getting%20Started.ipynb+]

http://reactivex.io/[+http://reactivex.io/+]

rsync
~~~~~

rsync is a utility for efficiently transferring and synchronizing files across networked computers by comparing the modification times and sizes of files. It is commonly found on Unix-like operating systems. The rsync algorithm is a type of delta encoding, and is used for minimizing network usage. Zlib may be used for additional data compression, and SSH or stunnel can be used for security.

Rsync is typically used for synchronizing files and directories between two different systems. For example, if the command rsync local-file user@remote-host:remote-file is run, rsync will use SSH to connect as user to remote-host. Once connected, it will invoke the remote host's rsync and then the two programs will determine what parts of the local file need to be transferred so that the remote file matches the local one.

Rsync can also operate in a daemon mode, serving and receiving files in the native rsync protocol.

An rsync process operates by communicating with another rsync process, a sender and a receiver. At startup, an rsync client connects to a peer process. If the transfer is local (that is, between file systems mounted on the same host) the peer can be created with fork, after setting up suitable pipes for the connection. If a remote host is involved, rsync starts a process to handle the connection, typically Secure Shell. Upon connection, a command is issued to start an rsync process on the remote host, which uses the connection thus established. As an alternative, if the remote host runs an rsync daemon, rsync clients can connect by opening a socket on TCP port 873, possibly using a proxy.

Rsync has numerous command line options and configuration files to specify alternative shells, options, commands, possibly with full path, and port numbers. Besides using remote shells, tunnelling can be used to have remote ports appear as local on the server where an rsync daemon runs. Those possibilities allow adjusting security levels to the state of the art, while a naive rsync daemon can be enough for a local network. 

By default rsync determines which files differ between the sending and receiving systems by checking the modification time and size of each file. If time or size is different between the systems, it transfers the file from the sending to the receiving system. As this only requires reading file directory information, it is quick, but it will miss unusual modifications which change neither.[3]

Rsync performs a slower but comprehensive check if invoked with --checksum. This forces a full checksum comparison on every file present on both systems. Barring rare checksum collisions, this avoids the risk of missing changed files at the cost of reading every file present on both systems. 

https://rsync.samba.org/[+https://rsync.samba.org/+]

https://en.wikipedia.org/wiki/Rsync[+https://en.wikipedia.org/wiki/Rsync+]

https://code.google.com/archive/p/drsync/[+https://code.google.com/archive/p/drsync/+]

https://github.com/LabAdvComp/UDR[+https://github.com/LabAdvComp/UDR+]

http://www.math.ualberta.ca/imaging/rlbackup/[+http://www.math.ualberta.ca/imaging/rlbackup/+]

https://rclone.org/[+https://rclone.org/+]

http://duplicity.nongnu.org/[+http://duplicity.nongnu.org/+]

https://www.tecmint.com/rsync-local-remote-file-synchronization-commands/[+https://www.tecmint.com/rsync-local-remote-file-synchronization-commands/+]

https://medium.com/@sethgoldin/a-gentle-introduction-to-rsync-a-free-powerful-tool-for-media-ingest-86761ca29c34[+https://medium.com/@sethgoldin/a-gentle-introduction-to-rsync-a-free-powerful-tool-for-media-ingest-86761ca29c34+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

parsyncfp
^^^^^^^^^

For transferring large, deep file trees, rsync will pause while it generates
lists of files to process. Since Version 3, it does this pretty fast, but on
sluggish filesystems, it can take hours or even days before it will start to
actually exchange rsync data.

Second, due to various bottlenecks, rsync will tend to use less than the
available bandwidth on high speed networks. Starting multiple instances of
rsync can improve this significantly. However, on such transfers, it is also
easy to overload the available bandwidth, so it would be nice to both limit
the bandwidth used if necessary and also to limit the load on the system.

Parsyncfp tries to satisfy all these conditions and more by:

* using the fpart file partitioner which can produce lists of files very
rapidly
* allowing re-use of the cache files so generated
* doing crude loadbalancing of the number of active rsyncs, suspending and
unsuspending the processes as necessary
* using rsync’s own bandwidth limiter (--bwlimit) to throttle the total
bandwidth
* using rsync’s own vast option selection is available as a pass-thru

As a warning, the main use case for parsyncfp is really only very large data transfers thru fairly fast network connections (>1Gb). Below this speed, rsync itself can saturate the connection, so there’s little reason to use parsyncfp and in fact the overhead of testing the existence of and starting more rsyncs tends to worsen its performance on small transfers to slightly less than rsync alone.

http://moo.nac.uci.edu/\~hjm/parsync/[+http://moo.nac.uci.edu/~hjm/parsync/+]

Rump
~~~~

Rump kernels enable you to build the software stack you need without forcing you to reinvent the wheels. The key observation is that a software stack needs driver-like components which are conventionally tightly-knit into operating systems — even if you do not desire the limitations and infrastructure overhead of a given OS, you do need drivers.

We solve the problem by providing free, reusable, componentized, kernel quality drivers such as file systems, POSIX system calls, PCI device drivers and TCP/IP and SCSI protocol stacks. For examples of what you can achieve with rump kernels, see the repositories we provide. There is also a wiki page for 3rd party projects using rump kernels.

The article Rise and Fall of the Operating System provides an extended high-level motivation for rump kernels. The book Design and Implementation of the Anykernel and Rump Kernels gives a technical description of the fundamental operating principles and terminology. Further information is available on the wiki or interactively via the community.

http://rumpkernel.org/[+http://rumpkernel.org/+]

https://github.com/rumpkernel[+https://github.com/rumpkernel+]

#SSSS

SaC
~~~

Single-Assignment C is an array programming language predominantly suited for application areas such as numerically intensive applications and signal processing. Its distinctive feature is that it combines high-level program specifications with runtime efficiency similar to that of hand-optimized low-level specifications. Key to the optimization process that facilitates these runtimes is the underlying functional model which also constitutes the basis for implicit parallelisation. This makes SAC ideally suited for harnessing the full potential of a wide variety of modern architectures ranging from a few symmetric cores with shared memory to massively parallel systems that host heterogeneous components including GPUs and FPGAs.

The overall philosophy of the project is to combine high performance, high productivity and high portability under the hood of one compiler: Being able to write a program, or at least the compute intensive part of a program in a high-level style, quickly and leaving a compiler to figure out the details of the underlying architecture and details of the code transformation, leads to performance competitiveness with hand-optimised low-level codes. This vision drives a number of research activities around SaC. 

http://www.sac-home.org/doku.php[+http://www.sac-home.org/doku.php+]

https://github.com/SacBase[+https://github.com/SacBase+]

*Tutorial*: http://www.sac-home.org/doku.php?id=docs:tutorial[+http://www.sac-home.org/doku.php?id=docs:tutorial+]

SageMath
~~~~~~~~

SageMath is a free open-source mathematics software system licensed under the GPL. It builds on top of many existing open-source packages: NumPy, SciPy, matplotlib, Sympy, Maxima, GAP, FLINT, R and many more. Access their combined power through a common, Python-based language or directly via interfaces or wrappers. 

 SageMath is built out of nearly 100 open-source packages and features a unified interface. SageMath can be used to study elementary and advanced, pure and applied mathematics. This includes a huge range of mathematics, including basic algebra, calculus, elementary to very advanced number theory, cryptography, numerical computation, commutative algebra, group theory, combinatorics, graph theory, exact linear algebra and much more. It combines various software packages and seamlessly integrates their functionality into a common experience. It is well-suited for education and research.

The user interface is a notebook in a web browser or the command line. Using the notebook, SageMath connects either locally to your own SageMath installation or to a SageMath server on the network. Inside the SageMath notebook you can create embedded graphics, beautifully typeset mathematical expressions, add and delete input, and share your work across the network. 

https://www.sagemath.org/[+https://www.sagemath.org/+]

Salt
~~~~

Salt is a new approach to infrastructure management built on a dynamic communication bus. Salt can be used for data-driven orchestration, remote execution for any infrastructure, configuration management for any app stack, and much more.

Running commands on remote systems is the core function of Salt. Salt can execute commands across thousands of systems in seconds.

Salt contains a robust and flexible configuration management framework that allows effortless, simultaneous configuration of tens of thousands of systems.

Salt is built around an event infrastructure that can drive reactive provisioning, configuration, and management across all systems in your infrastructure.

https://github.com/saltstack/salt[+https://github.com/saltstack/salt+]

https://repo.saltstack.com/[+https://repo.saltstack.com/+]

https://crate.io/a/infrastructure-as-code-configuration-with-salt/[+https://crate.io/a/infrastructure-as-code-configuration-with-salt/+]

https://salt.tips/[+https://salt.tips/+]

Sapaclisp
~~~~~~~~~

Sapaclisp is the Common-Lisp code for the book "Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques" (Cambridge University Press, 1993), by Donald B. Percival and Andrew T. Walden. The code is not only focused on spectral analysis, but contains many usefull numerical routines for a variety of purposes (linear systems, least squares, FFT, etc).

The code is now (as of 15.04.2005) under the BSD ("sans advertising clause") licence.

https://common-lisp.net/project/sapaclisp/[+https://common-lisp.net/project/sapaclisp/+]

https://www.cliki.net/asdf[+https://www.cliki.net/asdf+]

Intel MKL
^^^^^^^^^

Intel® Math Kernel Library implements routines from the ScaLAPACK package for distributed-memory architectures. Routines are supported for both real and complex dense and band matrices to perform the tasks of solving systems of linear equations, solving linear least-squares problems, eigenvalue and singular value problems, as well as performing a number of related computational tasks.

Intel MKL ScaLAPACK routines are written in FORTRAN 77 with exception of a few utility routines written in C to exploit the IEEE arithmetic. All routines are available in all precision types: single precision, double precision, complexm, and double complex precision.

https://software.intel.com/en-us/mkl-developer-reference-c-scalapack-routines[+https://software.intel.com/en-us/mkl-developer-reference-c-scalapack-routines+]

https://www.scivision.co/intel-mkl-scalapack/[+https://www.scivision.co/intel-mkl-scalapack/+]

SCALE
~~~~~

SCALE (Scalable Computing for Advanced Library and Environment) is a basic library for weather and climate model developed principally by scientists at the RIKEN Center for Computational Science. The SCALE library was developed as a scalable package that could be used on next-generation supercomputers as well as normal desktops. It was developed jointly by scientists specializing in weather and climate science and by researchers in computational and computer science.

The SCALE-RM and SCALE-GM, numerical models fully using SCALE-library, are available. The SCALE-library manages the parallel processes, file I/O, and inner-communication. It also provides the solver for atmospheric flow (dynamical core) and physical processes such as micro-physics and radiation processes. SCALE-RM and SCALE-GM themselves conduct time-integration by keeping the atmospheric status as prognostic variable and combining functions provided by SCALE-library. Users can select a scheme in every component according to simulations they want.

https://www.r-ccs.riken.jp/software_center/software/scale/overview/[+https://www.r-ccs.riken.jp/software_center/software/scale/overview/+]

http://r-ccs-climate.riken.jp/scale/download/index.html[+http://r-ccs-climate.riken.jp/scale/download/index.html+]

https://www.r-ccs.riken.jp/software_center/software/oacis/overview/[+https://www.r-ccs.riken.jp/software_center/software/oacis/overview/+]

Schnek
~~~~~~

Schnek is a new library for Cxx that makes it easy to develop large parallel simulation codes. The library is intended mainly for use in physics simulations on regular grids but some of its features might be useful for other types of simulation codes.

When writing a new simulation code from scratch one is confronted with a choice. One can either keep the code simple and focus on the physics and the numerical side. For this kind of code, most time is spent on actually developing the interesting stuff. The downside of this type of code is that is not much use to anybody else except the developer of the code herself and maybe the immediate collaborators in a research group.

The other option is to develop a code that might be useful for others, that can be configured using a configuration file and that is written to be versatile and adaptable to problems other than the one it was initially developed for. The code should be easy to use by others and hopefully should also be extendible by others. Writing this type of code requires a lot of time and effort. Only very few projects get to a stage where this investment of time and work is worthwhile.

In addition, if the code handles large amounts of data and should be able to run on multi-processor clusters then additional code has to be written. For simulations on regular grids this means that one has to use multi-dimensional arrays, define boundary values, synchronise grid values across multiple processes and write data into files that can be used by other software.

Schnek provides a single library that aids in all these tasks and thus massively reduces the code that needs to be written by the developer.

http://www.notjustphysics.com/schnek/[+http://www.notjustphysics.com/schnek/+]

https://github.com/holgerschmitz/Schnek[+https://github.com/holgerschmitz/Schnek+]

SciDB
~~~~~

SciDB is a database management system (DBMS) designed for multidimensional data management and analytics common to scientific, geospatial, financial, and industrial applications. 

https://www.paradigm4.com/try_scidb/[+https://www.paradigm4.com/try_scidb/+]

https://github.com/Paradigm4[+https://github.com/Paradigm4+]

scidb4gdal
^^^^^^^^^^

This GDAL driver implements read and write access to SciDB arrays. SciDB is an open-source data management and analytics system developed by Paradigm4.

To interface SciDB with most earth-observation products the driver supports representing multi-tiled and multi-temporal imagery as single multidimensional arrays. Single images can be added to an existing array as tile or temporal slice where array indexes are automatically computed. For advanced functionality like this, SciDB instances need to run the extension for geographic reference scidb4geo.

https://github.com/appelmar/scidb4gdal[+https://github.com/appelmar/scidb4gdal+]

https://github.com/appelmar/scidb4geo[+https://github.com/appelmar/scidb4geo+]

scikit-learn
~~~~~~~~~~~~

Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language.[3] It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.

Scikit-learn is largely written in Python, with some core algorithms written in Cython to achieve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. 

https://scikit-learn.org/dev/index.html[+https://scikit-learn.org/dev/index.html+]

sklearn-pandas
^^^^^^^^^^^^^^

This module provides a bridge between Scikit-Learn's machine learning methods and pandas-style Data Frames.

https://github.com/scikit-learn-contrib/sklearn-pandas[+https://github.com/scikit-learn-contrib/sklearn-pandas+]

sklearn-xarray
^^^^^^^^^^^^^^

sklearn-xarray is an open-source python package that combines the n-dimensional labeled arrays of xarray with the machine learning and model selection tools of scikit-learn. The package contains wrappers that allow the user to apply scikit-learn estimators to xarray types without losing their labels.

https://github.com/phausamann/sklearn-xarray/[+https://github.com/phausamann/sklearn-xarray/+]

Scikit-TDA
~~~~~~~~~~

Scikit-TDA is a home for compatible TDA (Topological Data Analysis) Python libraries intended for non-topologists.
This project aims to provide a curated library of TDA Python tools that are widely usable and easily approachable.
 
It is structured so that each package can stand alone or be used as part of the scikit-tda bundle.

Current packages available:

* Ripser - Data to diagrams in one line
* Persim - Easy Persistence Images
* UMAP - Mathematically justified dimensionality reduction
* Kepler Mapper - Mapper framework integrated into sklearn
* TaDAsets - Constructors for common data sets for demonstrating TDa.

https://github.com/scikit-tda/scikit-tda[+https://github.com/scikit-tda/scikit-tda+]

Scilab
~~~~~~

Scilab is a free and open-source, cross-platform numerical computational package and a high-level, numerically oriented programming language. It can be used for signal processing, statistical analysis, image enhancement, fluid dynamics simulations, numerical optimization, and modeling, simulation of explicit and implicit dynamical systems and (if the corresponding toolbox is installed) symbolic manipulations. 

Scilab is a high-level, numerically oriented programming language. The language provides an interpreted programming environment, with matrices as the main data type. By using matrix-based computation, dynamic typing, and automatic memory management, many numerical problems may be expressed in a reduced number of code lines, as compared to similar solutions using traditional languages, such as Fortran, C, or Cxx. This allows users to rapidly construct models for a range of mathematical problems. While the language provides simple matrix operations such as multiplication, the Scilab package also provides a library of high-level operations such as correlation and complex multidimensional arithmetic. The software can be used for signal processing, statistical analysis, image enhancement, fluid dynamics simulations, and numerical optimization.

Scilab also includes a free package called Xcos (a fork of Scicos based on Modelica language) for modeling and simulation of explicit and implicit dynamical systems, including both continuous and discrete sub-systems. Xcos is the open source equivalent to Simulink from the MathWorks.

As the syntax of Scilab is similar to MATLAB, Scilab includes a source code translator for assisting the conversion of code from MATLAB to Scilab. Scilab is available free of cost under an open source license.

https://www.scilab.org/[+https://www.scilab.org/+]

http://forge.scilab.org/index.php/projects/[+http://forge.scilab.org/index.php/projects/+]

SCIRun
~~~~~~

SCIRun is a problem solving environment or "computational workbench" in which
a user selects software modules that can be connected in a visual programing
environment to create a high level workﬂow for experimentation. Each module
exposes all the available parameters necessary for scientists to adjust the
outcome of their simulation or visualization. The networks in SCIRun are
flexible enough to enable duplication of networks and creation of new modules.

Many SCIRun users find this software particularly useful for their bioelectric
ﬁeld research. Their topics of investigation include cardiac
electro-mechanical simulation, ECG and EEG forward and inverse calculations,
modeling of deep brain stimulation, electromyography calculation, and
determination of the electrical conductivity of anisotropic heart tissue.
Users have also made use of SCIRun for the visualization of breast tumor
brachytherapy, computer aided surgery, teaching, and a number of
non-biomedical applications.

http://www.sci.utah.edu/cibc-software/scirun.html[+http://www.sci.utah.edu/cibc-software/scirun.html+]

sci-wms
~~~~~~~

sci-wms is an open-source Python implementation of a WMS (Web Mapping Service) server designed for oceanographic, atmospheric, climate and weather data. It achieves real-time, on-demand visualization of internal (file access) and externally hosted (DAP) CF-compliant data. The features include:

* Visualize grids adhering to CF-UGRID or CF-SGRID conventions
* Abstracts each dataset into two objects: a topology and corresponding model data
* Topologies are stored locally for quick and efficient spatial queries (RTree indexes for each grid)
* External data (over DAP) is subset, downloaded, and rendered for each request
* Supports arbitrary cartographic projections
* Web-based management console for managing datasets and variables
* Lightweight client for testing

http://sci-wms.github.io/sci-wms/[+http://sci-wms.github.io/sci-wms/+]

https://github.com/sci-wms/sci-wms[+https://github.com/sci-wms/sci-wms+]

SCOREC
~~~~~~

The SCOREC Core is a set of C/Cxx libraries for unstructured mesh simulations on supercomputers.  It includes:

* https://scorec.rpi.edu/pumi/index.php[PUMI]: parallel unstructured mesh infrastructure API User's Guide
* https://scorec.rpi.edu/pcu/[PCU]: Communication and file IO built on MPI
* https://scorec.rpi.edu/apf/[APF]: Abstract definition of meshes, fields, and related operations
* https://redmine.scorec.rpi.edu/projects/gmi[GMI]: Common interface for geometric modeling kernels
* https://scorec.rpi.edu/\~dibanez/core/mds.html[MDS]: Compact but flexible array-based mesh data structure
* https://scorec.rpi.edu/parma/[PARMA]: Scalable partitioning and load balancing procedures
* https://scorec.rpi.edu/\~dibanez/core/spr.html[SPR]: Superconvergent Patch Recovery error estimator
* https://github.com/SCOREC/core/tree/master/ma[MA]: Anisotropic mixed mesh adaptation and solution transfer
* SAM: Sizing anisotropic meshes
* https://github.com/SCOREC/core/tree/master/stk[STK]: Conversion from APF meshes to Sandia's STK meshes
* http://www.cs.sandia.gov/zoltan/[ZOLTAN]: Interface to run Sandia's Zoltan code on APF meshes
* https://wiki.scorec.rpi.edu/wiki/PHASTA[PHASTA]: Tools and file formats related to the PHASTA fluid solver
* MTH: Math containers and routines
* CRV: Support for curved meshes with Bezier Shapes

https://github.com/SCOREC/core[+https://github.com/SCOREC/core+]

https://epubs.siam.org/doi/10.1137/16M1063496[+https://epubs.siam.org/doi/10.1137/16M1063496+]

SDR
~~~

Software-defined radio (SDR) is a radio communication system where components that have been traditionally implemented in hardware (e.g. mixers, filters, amplifiers, modulators/demodulators, detectors, etc.) are instead implemented by means of software on a personal computer or embedded system.[1] While the concept of SDR is not new, the rapidly evolving capabilities of digital electronics render practical many processes which were once only theoretically possible.

A basic SDR system may consist of a personal computer equipped with a sound card, or other analog-to-digital converter, preceded by some form of RF front end. Significant amounts of signal processing are handed over to the general-purpose processor, rather than being done in special-purpose hardware (electronic circuits). Such a design produces a radio which can receive and transmit widely different radio protocols (sometimes referred to as waveforms) based solely on the software used.

Software radios have significant utility for the military and cell phone services, both of which must serve a wide variety of changing radio protocols in real time. 

https://wiki.opendigitalradio.org/Main_Page[+https://wiki.opendigitalradio.org/Main_Page+]

GNU Radio
^^^^^^^^^

GNU Radio is a free software development toolkit that provides signal processing blocks to implement software-defined radios and signal-processing systems. It can be used with external RF hardware to create software-defined radios, or without hardware in a simulation-like environment. It is widely used in hobbyist, academic, and commercial environments to support both wireless communications research and real-world radio systems. 

The GNU Radio software provides the framework and tools to build and run software radio or just general signal-processing applications. The GNU Radio applications themselves are generally known as "flowgraphs", which are a series of signal processing blocks connected together, thus describing a data flow. As with all software-defined radio systems, reconfigurability is a key feature. Instead of using different radios designed for specific but disparate purposes, a single, general-purpose, radio can be used as the radio front-end, and the signal-processing software (here, GNU Radio), handles the processing specific to the radio application.

These flowgraphs can be written in either Cxx or the Python programming language. The GNU Radio infrastructure is written entirely in Cxx, and many of the user tools are written in Python. 

https://www.gnuradio.org/[+https://www.gnuradio.org/+]

https://en.wikipedia.org/wiki/GNU_Radio[+https://en.wikipedia.org/wiki/GNU_Radio+]

https://github.com/gnuradio[+https://github.com/gnuradio+]

https://wiki.gnuradio.org/index.php/GNURadioCompanion[+https://wiki.gnuradio.org/index.php/GNURadioCompanion+]

https://hackaday.com/2015/11/11/getting-started-with-gnu-radio/[+https://hackaday.com/2015/11/11/getting-started-with-gnu-radio/+]

http://cgran.hopto.org/[+http://cgran.hopto.org/+]

liquid-dsp
^^^^^^^^^^

liquid-dsp is a free and open-source digital signal processing (DSP) library designed specifically for software-defined radios on embedded platforms. The aim is to provide a lightweight DSP library that does not rely on a myriad of external dependencies or proprietary and otherwise cumbersome frameworks. All signal processing elements are designed to be flexible, scalable, and dynamic, including filters, filter design, oscillators, modems, synchronizers, complex mathematical operations, and much more.

The project was created as a lightweight library to be used in embedded platforms were minimizing overhead is critical. You will notice that liquid lacks any sort of underlying framework for connecting signal processing "blocks" or "components." The design was chosen because each application requires the signal processing block to be redesigned and recompiled for each application anyway so the notion of a reconfigurable framework is, for the most part, a flawed concept. 

In liquid there is no model for passing data between structures, no generic interface for data abstraction, no customized/proprietary data types, no framework for handling memory management; this responsibility is left to the designer, and as a consequence the library provides very little computational overhead. This package does not provide graphical user interfaces, component models, or debugging tools; liquid is simply a collection raw signal processing modules providing flexibility in algorithm development for wireless communications at the physical layer. 

http://liquidsdr.org/[+http://liquidsdr.org/+]

https://github.com/jgaeddert/liquid-dsp[+https://github.com/jgaeddert/liquid-dsp+]

https://github.com/michelp/pyliquid-dsp[+https://github.com/michelp/pyliquid-dsp+]

mmbTools
^^^^^^^^

The DAB+ transmission chain composed of ODR-DabMux and ODR-DabMod and the Ettus Research USRP or LimeSDR software-defined radio platforms is a very interesting, flexible and low-cost way to set up a DAB+ transmitter. An encoder for DAB and DAB+ is available on ODR-AudioEnc. ODR-PadEnc can encode DLS and Slideshow as Programme Associated Data.

The ODR-mmbTools are a fork of the CRC-mmbTools whose development has ceased. Opendigitalradio pursues this development, and takes care to create a set of tools that are ready for production environments.

The tools are:

* http://wiki.opendigitalradio.org/ODR-DabMux[ODR-DabMux] implements a DAB multiplexer that combines all audio and data
inputs into an ETI output. It can be used off-line (i.e. not real-time) to generate
ETI data for later processing, or in a real-time streaming scenario (e.g. in a
transmitter). 
* http://wiki.opendigitalradio.org/ODR-DabMod[ODR-DabMod] is a software-defined DAB modulator that receives or reads ETI,
and generates modulated I/Q data usable for transmission. It can directly interface the Ettus USRP devices, and can also be used with other SDR signal sources.
* https://github.com/Opendigitalradio/ODR-AudioEnc[ODR-AudioEnc] contains a MPEG-1 Layer II audio encoder that is used to encode audio for the DAB standard and uses fdk-aac to encode for DAB+.
* https://github.com/Opendigitalradio/ODR-PadEnc[ODR-PadEnc] can read DLS from a text file and slides from a folder, and prepare the PAD data stream for injection into the audio encoder.

https://www.opendigitalradio.org/mmbtools[+https://www.opendigitalradio.org/mmbtools+]

The tools can be installed using PyBOMBS recipes.

https://github.com/Opendigitalradio/DAB-recipes[+https://github.com/Opendigitalradio/DAB-recipes+]

NGSoftFM
^^^^^^^^

NGSoftFM is a software-defined radio receiver for FM broadcast radio. Stereo decoding is supported. It is written in C++. It is a derivative work of SoftFM (https://github.com/jorisvr/SoftFM) with a new application design and new features. It also corrects wrong de-emphasis scheme for stereo signals.

Hardware supported:

* RTL-SDR based (RTL2832-based) hardware is suppoeted and uses the librtlsdr library to interface with the RTL-SDR hardware.
* HackRF One and variants are supported with libhackrf library.
* Airspy is supported with libairspy library.
* BladeRF is supported with libbladerf library.

The purposes of NGSoftFM are:

* experimenting with digital signal processing and software radio;
* investigating the stability of the 19 kHz pilot;
* doing the above while listening to my favorite radio station.

NGSoftFM actually produces pretty good stereo sound when receiving a strong radio station.

https://github.com/f4exb/ngsoftfm[+https://github.com/f4exb/ngsoftfm+]

PyBOMBS
^^^^^^^

PyBOMBS (Python Build Overlay Managed Bundle System) is the GNU Radio install management system for resolving dependencies and pulling in out-of-tree projects. 

https://github.com/gnuradio/pybombs[+https://github.com/gnuradio/pybombs+]

SoapySDR
~~~~~~~~

SoapySDR is an open-source generalized C/Cpp API and runtime library for interfacing with SDR devices. With SoapySDR, you can instantiate, configure, and stream with an SDR device in a variety of environments. Both osmosdr and uhd devices are available within SoapySDR. In addition, vendors can directly support their hardware using SoapySDR device modules. There are wrappers for both gr-osmosdr, uhd, and gr-uhd to bring an ecosystem of existing applications to SoapySDR devices. And SoapySDR has support for powerful platforms like GNU Radio and Pothos.

There are a family of devices known as SDRs with RF frontends, ADCs, DACs, and a PC interface. The project goal is to support this general type of device known as SDR. SoapySDR is not intended to be a generalized hardware abstraction library. However, with interfaces for registers, generalized settings, spi, i2c, etc; its feasible to wrap arbitrary hardware or SoC devices in a SoapySDR plugin as a convenient way to get network support and python language bindings.

SoapySDR is not tied to any particular SDR hardware vendor. We try to generalize the SoapySDR API to support most devices out there, while providing hooks for users to access vendor-specific funtionalities.

SoapySDR supports SDR devices through runtime-loadable modules. This allows users to build and install device support against an existing SoapySDR installation without disturbing the existing installation or requiring recompilation of the platform. This allows vendors to maintain a SoapySDR module along with their driver build package.

Use any SoapySDR supported device transparently over a local network link. The remote support feature can turn any SDR into a network peripheral, to work around software-related issues, to share a device among multiple computers, or to ease embedded device development. 

https://github.com/pothosware/SoapySDR/wiki[+https://github.com/pothosware/SoapySDR/wiki+]

SDRangel
^^^^^^^^

SDRangel is an Open Source Qt5 / OpenGL 3.0+ SDR and signal analyzer frontend to various hardware.
It is intended for the power user. We expect you to already have some experience with SDR applications and digital signal processing in general. 

SDRangel is a near real time application that is demanding on CPU power and clock speeds for low latency. Recent (2015 or later) Core i7 class CPU is recommended preferably with 4 HT CPU cores (8 logical CPUs or more) with nominal clock over 2 GHz and at least 8 GB RAM. Modern Intel processors will include a GPU suitable for proper OpenGL support. On the other hand SDRangel is not as demanding as recent computer games for graphics and CPU integrated graphics are perfectly fine. USB-3 ports are also preferable for high speed, low latency USB communication. 

https://github.com/f4exb/sdrangel[+https://github.com/f4exb/sdrangel+]

https://github.com/f4exb/sdrangel-docker[+https://github.com/f4exb/sdrangel-docker+]

secushare
~~~~~~~~~

Imagine Facebook, Whatsapp, Gmail and Skype rolled into one, without the centralized surveillance and control. Crazy? Well, it hasn't been tried before, at least not our way. So let's give it a try.

secushare employs GNUnet for end-to-end encryption and anonymizing mesh routing (because it has a more suitable architecture than Tor or I2P) and applies PSYC on top (because it performs better than XMPP, JSON or OStatus) to create a distributed social graph. Together, these technologies allow for distributed private social networking including more straightforward and secure e-mail, chat, exchange of content and a private web. It could even work out as a safer choice for the Internet of Things.

The resulting new Internet enjoys the speed of servers that help without knowing much about us, the authenticity of social relationships between users without becoming transparent, the privacy of advanced obfuscation without becoming cumbersome or a threat to society. This way, we can become independent of centralized infrastructure and expect that only the designated recipients can read our communications. Read more about it in the introduction. 

https://secushare.org/[+https://secushare.org/+]

sed
~~~

sed (stream editor) is a Unix utility that parses and transforms text, using a simple, compact programming language. sed was developed from 1973 to 1974 by Lee E. McMahon of Bell Labs,[1] and is available today for most operating systems.[2] sed was based on the scripting features of the interactive editor ed ("editor", 1971) and the earlier qed ("quick editor", 1965–66). sed was one of the earliest tools to support regular expressions, and remains in use for text processing, most notably with the substitution command. Popular alternative tools for plaintext string manipulation and "stream editing" include AWK and Perl. 

sed is a line-oriented text processing utility: it reads text, line by line, from an input stream or file, into an internal buffer called the pattern space. Each line read starts a cycle. To the pattern space, sed applies one or more operations which have been specified via a sed script. sed implements a programming language with about 25 commands that specify the operations on the text. For each input line, after running the script sed ordinarily outputs the pattern space (the line as modified by the script) and begins the cycle again with the next line. Other end-of-script behaviors are available through sed options and script commands, e.g. d to delete the pattern space, q to quit, N to add the next line to the pattern space immediately, and so on. Thus a sed script corresponds to the body of a loop that iterates through the lines of a stream, where the loop itself and the loop variable (the current line number) are implicit and maintained by sed. 

https://www.gnu.org/software/sed/[+https://www.gnu.org/software/sed/+]

https://en.wikipedia.org/wiki/Sed[+https://en.wikipedia.org/wiki/Sed+]

http://sed.sourceforge.net/[+http://sed.sourceforge.net/+]

http://www.pement.org/sed/[+http://www.pement.org/sed/+]

*sed & awk* - https://www.amazon.com/_/dp/1565922255[+https://www.amazon.com/_/dp/1565922255+]

*sed - An Introduction and Tutorial* - http://www.grymoire.com/Unix/Sed.html[+http://www.grymoire.com/Unix/Sed.html+]

*FAQ* - http://sed.sourceforge.net/sedfaq.html[+http://sed.sourceforge.net/sedfaq.html+]

*Sed One-Liners Explained* - https://catonmat.net/sed-book[+https://catonmat.net/sed-book+]

SEG-Y
~~~~~

The SEG-Y (sometimes SEG Y) file format is one of several standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data. It is an open standard, and is controlled by the SEG Technical Standards Committee, a non-profit organization.

The format was originally developed in 1973 to store single-line seismic reflection digital data on magnetic tapes. The specification was published in 1975.[1]

The format and its name evolved from the SEG "Ex" or Exchange Tape Format.[1][2] However, since its release, there have been significant advancements in geophysical data acquisition, such as 3-dimensional seismic techniques and high speed, high capacity recording.

The most recent revision of the SEG-Y format was published in 2017, named the rev 2.0 specification.[3] It still features certain legacies of the original format (referred as rev 0), such as an optional SEG-Y tape label, the main 3200 byte textual EBCDIC character encoded tape header and a 400 byte binary header. 

http://www.subsurfwiki.org/wiki/SEG_Y[+http://www.subsurfwiki.org/wiki/SEG_Y+]

https://en.wikipedia.org/wiki/SEG-Y[+https://en.wikipedia.org/wiki/SEG-Y+]

netcdf_segy
^^^^^^^^^^^

Convert between SEG-Y and NetCDF

This is currently only a research/demonstration tool. It is not "industrial strength". In particular, it does not run in parallel, so will likely be slow on very large datasets (if it runs at all). Also, only the SEG-Y -> NetCDF direction is implemented.

I have created a Jupyter Notebook to discuss the advantages of NetCDF compared to SEG-Y, show an example of segy2netcdf being used, and demonstrate the attractions of loading the resulting NetCDF file with xarray: Alternatives to SEG-Y.

One of the "additional options" mentioned above is to use specified headers as dimensions. This allows you to use 'FieldRecord' as a dimension if your data is stored as shot gathers, for example (as in the Notebook). If you don't do this, the NetCDF file will store the data as a 2D array with Time/Depth/SampleNumber and Traces as the dimensions. As netcdf_segy currently uses SegyIO to read the SEG-Y file, the header names are those used by that package.

https://github.com/ar4/netcdf_segy[+https://github.com/ar4/netcdf_segy+]

ObsPy
^^^^^

ObsPy is an open-source project dedicated to provide a Python framework for processing seismological data. It provides parsers for common file formats, clients to access data centers and seismological signal processing routines which allow the manipulation of seismological time series.

https://github.com/obspy/obspy[+https://github.com/obspy/obspy+]

http://docs.obspy.org/packages/obspy.io.segy.html#module-obspy.io.segy[+http://docs.obspy.org/packages/obspy.io.segy.html#module-obspy.io.segy+]

https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014003/meta[+https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014003/meta+]

segpy
^^^^^

A Python package for reading and writing SEG Y files. 

The SEG-Y file format is one of several standards developed by the Society of Exploration Geophysicists for storing geophysical seismic data. It is an open standard, and is controlled by the SEG Technical Standards Committee, a non-profit organization.

This project aims to implement an open SEG-Y module in Python 3 for transporting seismic data between SEG-Y files and Python data structures in pure Python.

https://github.com/sixty-north/segpy[+https://github.com/sixty-north/segpy+]

segyio
^^^^^^

Segyio is a small LGPL licensed C library for easy interaction with SEG-Y formatted seismic data, with language bindings for Python and Matlab. Segyio is an attempt to create an easy-to-use, embeddable, community-oriented library for seismic applications.  The features include:

* A low-level C interface with few assumptions; easy to bind to other languages
* Read and write binary and textual headers
* Read and write traces and trace headers
* Simple, powerful, and native-feeling Python interface with numpy integration
* xarray integration with netcdf_segy
* Some simple applications with unix philosophy

https://segyio.readthedocs.io/en/latest/[+https://segyio.readthedocs.io/en/latest/+]

https://github.com/equinor/segyio[+https://github.com/equinor/segyio+]

https://github.com/equinor/segyviewer[+https://github.com/equinor/segyviewer+]

SeisIO.jl
^^^^^^^^^

SeisIO is a Julia package for reading and writing SEGY Rev 1 files. In addition to providing tools for reading/writing entire files, SeisIO provides a parallel scanner that reduces any number of files into a single object with direct out-of-core access to the underlying data.

https://github.com/slimgroup/SeisIO.jl[+https://github.com/slimgroup/SeisIO.jl+]

SELFE
~~~~~

SELFE is an open-source community-supported code. It is based on unstructured triangular grids, and designed for the effective simulation of 3D baroclinic circulation. Originally developed to meet stringent modeling challenges for the Columbia River estuary and plume, SELFE has now been extensively applied to study circulation in coastal margins around the world. Several related codes constitute an interdisciplinary modeling system that is growing organically around SELFE.

http://www.stccmop.org/knowledge_transfer/software/selfe[+http://www.stccmop.org/knowledge_transfer/software/selfe+]

http://www.stccmop.org/search/node/selfe%20download[+http://www.stccmop.org/search/node/selfe%20download+]

HyosPy
^^^^^^

HyosPy (Hydrodynamic and oil spill Python) is a model coupling system developed by Dr. Ben R. Hodges' research team in CRWR at UT-Austin. The HyosPy is able to automate multiple simulations of a hydrodynamic model (SELFE) using real-time downloaded hindcast/foreast data, then couples the results to an oil spill transport model (PyGNOME or RK4) and provides visualization/animation in the 2D Google Map and 3D Google Earth GIS.

https://github.com/utcivil/HyosPy[+https://github.com/utcivil/HyosPy+]

pyselfe
^^^^^^^

Tools to work with SELFE hydrodynamic model.

https://github.com/twdb/pyselfe[+https://github.com/twdb/pyselfe+]

SELEN
~~~~~

The open source program SELEN solves numerically the so-called "Sea Level Equation" (SLE) for a spherical, layered, non-rotating Earth with Maxwell viscoelastic rheology. The SLE is an integral equation that was introduced in the 70s to model the sea level variations in response to the melting of late Pleistocene ice sheets, but it can also be employed to present-day melting of continental ice-sheets. SELEN can compute vertical and horizontal surface displacements, gravity variations and sea level changes on a global and regional scale. SELEN (acronym of SEa Level EquatioN solver) is particularly oriented to scientists at their first approach to the glacial isostatic adjustment (GIA) problem and, according to our experience, it can be successfully used in teaching. The current release (2.9) considerably improves the previous version of the code in terms of computational efficiency, portability and versatility. 

https://geodynamics.org/cig/software/selen/[+https://geodynamics.org/cig/software/selen/+]

https://github.com/geodynamics/selen[+https://github.com/geodynamics/selen+]

SGeMS
~~~~~

The Stanford Geostatistical Modeling Software (SGeMS) is an open-source computer package for solving problems involving spatially related variables. It provides geostatistics practitioners with a user-friendly interface, an interactive 3-D visualization, and a wide selection of algorithms. 

http://sgems.sourceforge.net/[+http://sgems.sourceforge.net/+]

ftp://ftp.ige.unicamp.br/pub/geoestat/sgems_manual.pdf[+ftp://ftp.ige.unicamp.br/pub/geoestat/sgems_manual.pdf+]

https://pubs.usgs.gov/of/2009/1103/ofr2009-1103-rev-jan2010.pdf[+https://pubs.usgs.gov/of/2009/1103/ofr2009-1103-rev-jan2010.pdf+]

https://github.com/fnavarrov/SGeMS[+https://github.com/fnavarrov/SGeMS+]

https://github.com/SCRFpublic/SGEMS-UQ[+https://github.com/SCRFpublic/SGEMS-UQ+]

https://github.com/sayantanx/ar2tech-SGeMS-public[+https://github.com/sayantanx/ar2tech-SGeMS-public+]

sh
~~

sh is a full-fledged subprocess replacement for Python 2.6 - 3.5, PyPy and PyPy3 that allows you to call any program as if it were a function.

Note that these aren’t Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are easily available to you from within Python.

http://amoffat.github.io/sh/[+http://amoffat.github.io/sh/+]

https://github.com/amoffat/sh[+https://github.com/amoffat/sh+]

Shaarli
~~~~~~~

The personal, minimalist, super-fast, database free, bookmarking service.
Do you want to share the links you discover? Shaarli is a minimalist link sharing service that you can install on your own server. It is designed to be personal (single-user), fast and handy.

https://github.com/shaarli/Shaarli[+https://github.com/shaarli/Shaarli+]

SHAVEL
~~~~~~

A method for performing a spherical harmonic analysis, using observed horizontal components of a tangent vector on a sphere, is presented. The vector data samples are assumed to be provided in an equiangular grid, which essentially simplifies the least-squares analysis by making use of (1) the block diagonal structure of the normal equations of least squares, (2) the even–odd symmetry of the associated Legendre functions, and (3) the fast Fourier transform of mix-radix. The correct function of the program and its numerical precision is verified by applying it to a data set, derived by evaluating a given set of vector spherical harmonic coefficients. That the program works correctly is demonstrated by the excellent agreement between the input and output spherical harmonic coefficients.

https://www.sciencedirect.com/science/article/pii/S0010465518302236[+https://www.sciencedirect.com/science/article/pii/S0010465518302236+]

https://data.mendeley.com/datasets/nppz4y7wg7/1[+https://data.mendeley.com/datasets/nppz4y7wg7/1+]

shenfun
~~~~~~~

Shenfun is a high performance computing platform for solving partial differential equations (PDEs) by the spectral Galerkin method. The user interface to shenfun is very similar to FEniCS, but applications are limited to multidimensional tensor product grids. The code is parallelized with MPI through the mpi4py-fft package.

Shenfun enables fast development of efficient and accurate PDE solvers (spectral order and accuracy), in the comfortable high-level Python language. The spectral accuracy is ensured by using high-order global orthogonal basis functions (Fourier, Legendre and Chebyshev), as opposed to finite element codes that are using low-order local basis functions. Efficiency is ensured through vectorization (Numpy), parallelization (mpi4py) and by moving critical routines to Cython or Numba. Shenfun has been used to run turbulence simulations (Direct Numerical Simulations) on thousands of processors on high-performance supercomputers, see the spectralDNS repository.

The spectral Galerkin method solves partial differential equations through a special form of the method of weighted residuals (WRM). As a Galerkin method it is very similar to the finite element method (FEM). The most distinguishable feature is that it uses global shape functions, where FEM uses local. This feature leads to highly accurate results with very few shape functions, but the downside is much less flexibility when it comes to computational domain than FEM.

https://shenfun.readthedocs.io/en/latest/[+https://shenfun.readthedocs.io/en/latest/+]

https://github.com/spectralDNS/shenfun[+https://github.com/spectralDNS/shenfun+]

http://joss.theoj.org/papers/43f64b8a0ef42408c72acead37717ec6[+http://joss.theoj.org/papers/43f64b8a0ef42408c72acead37717ec6+]

Sherpa
~~~~~~

Sherpa is a Python package for modeling and fitting data. It was originally developed by the Chandra X-ray Center for use in analysing X-ray data (both spectral and imaging) from the Chandra X-ray telescope, but it is designed to be a general-purpose package, which can be enhanced with domain-specific tasks (such as X-ray Astronomy). Sherpa contains an expressive and powerful modeling language, coupled with a range of statistics and robust optimisers.

The features and capabilities of Sherpa include:

* Model generic 1D/2D (N-D) data arrays.
* Fit 1D (multiple) data including: spectra, surface brightness profiles, light curves, arrays.
* Fit 2D images/surfaces in Poisson/Gaussian regime.
* Build complex model expressions.
* Import, define and use your own models.
* Simulate predicted data based on defined models.
* Use appropriate statistics for modeling Poisson or Gaussian data
* Use Classic Maximum Likelihood or Bayesian Framework.
* Import, define the new statistics, with priors if required by analysis.
* Visualize a parameter space with simulations or using 1D/2D cuts of the parameter space
* Calculate confidence levels on the best fit model parameters
* Use a robust optimization method for the fit: Levenberg-Marquardt, Nelder-Mead Simplex or Monte Carlo/Differential Evolution.
* Sherpa supports wcs, responses, psf, convolution.

https://github.com/sherpa/sherpa/[+https://github.com/sherpa/sherpa/+]

https://sherpa.readthedocs.io/en/4.11.0/[+https://sherpa.readthedocs.io/en/4.11.0/+]

http://cxc.cfa.harvard.edu/contrib/sherpa/[+http://cxc.cfa.harvard.edu/contrib/sherpa/+]

Shift
~~~~~

In high-end computing environments, remote file transfers of very large data
sets to and from computational resources are commonplace as users are
typically widely distributed across different organizations and must transfer
in data to be processed and transfer out results for further analysis. Local
transfers of this same data across file systems are also frequently performed
by administrators to optimize resource utilization when new file systems come
on-line or storage becomes imbalanced between existing file systems. In both
cases, files must traverse many components on their journey from source to
destination where there are numerous opportunities for performance
optimization as well as failure. A number of tools exist for providing
reliable and/or high performance file transfer capabilities, but most either
do not support local transfers, require specific security models and/or
transport applications, are difficult for individual users to deploy, and/or
are not fully optimized for highest performance.

Shift is a framework for Self-Healing Independent File Transfer that provides
high performance and resilience for local and remote transfers through a
variety of techniques. These include end-to-end integrity via cryptographic
hashes, throttling of transfers to prevent resource exhaustion, balancing
transfers across resources based on load and availability, and parallelization
of transfers across multiple source and destination hosts for increased
redundancy and performance. In addition, Shift was specifically designed to
accommodate the diverse heterogeneous environments of a widespread user base
with minimal assumptions about operating environments. In particular, Shift is
unique in its ability to provide advanced reliability and automatic single and
multi-file parallelization to any stock command-line transfer application
while being easily deployed by both individual users as well as entire
organizations.

The features include:

* support for local, LAN, and WAN transfers
* drop-in replacement for both cp and scp (basic options only)
* tracking of individual file operations with on-demand status
* transfer stop and restart
* email notification of completion, errors, and warnings
* local and remote tar creation/extraction
* rsync-like synchronization based on modification times and checksums
* integrity verification of transfers with partial retransfer/resum to
  rectify corruption
* detection of silent corruption between transfers of the same file
* throttling based on local and remote resource utilization
* automatic retrieval/release of files residing on DMF-managed file systems
* automatic striping of files transferred to Lustre file systems
* fully self-contained besides perl core and ssh
* automatic detection and selection of higher performance transports and
  hash utilities when available including bbcp, bbftp, gridftp, mcp, msum,
  and rsync
* automatic many-to-many parallelization of single and multi-file
  transfers with file system equivalence detection and rewriting

http://ti.arc.nasa.gov/opensource/projects/shift/[+http://ti.arc.nasa.gov/opensource/projects/shift/+]

https://github.com/pkolano/shift[+https://github.com/pkolano/shift+]

SHMEM
~~~~~

SHMEM (from Cray Research’s “shared memory” library) is a family of parallel programming libraries, providing one-sided, RDMA, parallel-processing interfaces for low-latency distributed-memory supercomputers. The SHMEM acronym was subsequently reverse engineered to mean "Symmetric Hierarchical MEMory”. Later it was expanded to distributed memory parallel computer clusters, and is used as parallel programming interface or as low-level interface to build partitioned global address space (PGAS) systems and languages.

SHMEM laid the foundations for low-latency (sub-microsecond) one-sided communication. After its use on the CRAY T3E, its popularity waned as few machines could deliver the near-microsecond latencies necessary to maintain efficiency for its hallmark individual-word communication. With the advent of popular sub-microsecond interconnects, SHMEM has been used to address the necessity of hyper-efficient, portable, parallel-communication methods for exascale computing.

https://en.wikipedia.org/wiki/SHMEM[+https://en.wikipedia.org/wiki/SHMEM+]

OpenSHMEM
^^^^^^^^^

OpenSHMEM is a standard for SHMEM library implementations. Many SHMEM libraries exist but they do not conform to a particular standard and have similar but not identical APIs and behavior, which hinders portability.

OpenSHMEM is an effort to create a specification for a standardized API for parallel programming in the Partitioned Global Address Space. Along with the specification the project is also creating a reference implementation of the API. This implementation attempts to be portable, to allow it to be deployed in multiple environments, and to be a starting point for implementations targeted to particular hardware platforms. It will also serve as a springboard for future development of the API.

http://www.openshmem.org/site/[+http://www.openshmem.org/site/+]

https://github.com/openshmem-org[+https://github.com/openshmem-org+]

osss-ucx
^^^^^^^^

An OpenSHMEM Implementation on top of OpenUCX (UCX) and PMIx.

https://github.com/openshmem-org/osss-ucx[+https://github.com/openshmem-org/osss-ucx+]

http://www.openucx.org/[+http://www.openucx.org/+]

https://pmix.org/[+https://pmix.org/+]

SHOC
~~~~

The Scalable Heterogeneous Computing Benchmark Suite (SHOC) is a collection of benchmark programs testing the performance and stability of systems using computing devices with non-traditional architectures for general purpose computing, and the software used to program them. Its initial focus is on systems containing Graphics Processing Units (GPUs) and multi-core processors, and on the OpenCL programming standard. It can be used on clusters as well as individual hosts.

In addition to OpenCL-based benchmark programs, SHOC also includes a Compute Unified Device Architecture (CUDA) version of many of its benchmarks for comparison with the OpenCL version.

https://github.com/vetter/shoc/wiki[+https://github.com/vetter/shoc/wiki+]

https://github.com/vetter/shoc[+https://github.com/vetter/shoc+]

Shroud
~~~~~~

Shroud is a tool for creating a Fortran or Python interface to a C or Cxx library. It can also create a C API for a Cxx library.

The user creates a YAML file with the C/Cxx declarations to be wrapped along with some annotations to provide semantic information and code generation options. Shroud produces a wrapper for the library. The generated code is highly-readable and intended to be similar to code that would be hand-written to create the bindings.

Input is read from a YAML file which describes the types, variables, enumerations, functions, structures and classes to wrap. This file must be created by the user. Shroud does not parse Cxx code to extract the API. That was considered a large task and not needed for the size of the API of the library that inspired Shroud’s development. In addition, there is a lot of semantic information which must be provided by the user that may be difficult to infer from the source alone. However, the task of creating the input file is simplified since the Cxx declarations can be cut-and-pasted into the YAML file.

In some sense, Shroud can be thought of as a fancy macro processor. It takes the function declarations from the YAML file, breaks them down into a series of contexts (library, class, function, argument) and defines a dictionary of format macros of the form key=value. There are then a series of macro templates which are expanded to create the wrapper functions. The overall structure of the generated code is defined by the classes and functions in the YAML file as well as the requirements of Cxx and Fortran syntax.

Each declaration can have annotations which provide semantic information. This information is used to create more idiomatic wrappers. Shroud started as a tool for creating a Fortran wrapper for a Cxx library. The declarations and annotations in the input file also provide enough information to create a Python wrapper.

https://github.com/LLNL/shroud[+https://github.com/LLNL/shroud+]

https://shroud.readthedocs.io/en/develop/[+https://shroud.readthedocs.io/en/develop/+]

SHYFEM
~~~~~~

The finite element program SHYFEM is a program package that can be used to resolve the hydrodynamic equations in lagoons, coastal seas, estuaries and lakes. The program uses finite elements for the resolution of the hydrodynamic equations. These finite elements, together with an effective semi-implicit time resolution algorithm, makes this program especially suitable for application to a complicated geometry and bathymetry.

Finite elements are superior to finite differences when dealing with complex bathymetric situations and geometries. Finite differences are limited to a regular outlay of their grids.This will be a problem if only parts of a basin need high resolution. The finite element method has an advantage in this case allowing more flexibility with its subdivision of the system in triangles varying in form and size.

https://sites.google.com/site/shyfem/[+https://sites.google.com/site/shyfem/+]

https://github.com/SHYFEM-model/shyfem/[+https://github.com/SHYFEM-model/shyfem/+]

SIAL
~~~~

The Super Instruction Architecture (SIA) is an environment comprising a
programming language, SIAL, and
a runtime system, SIP, with the goal of providing portable and efficient code
related to tensor computations
for a wide array of computing environments including distributed-memory
environments [20]. SIAL exposes
commonly used abstractions in scientific computing, such as blocking,
providing the user a useful method
of describing how an algorithm proceeds without unnecessarily complicating the
code. Programs written
in SIAL are compiled to a bytecode which is then interpreted by an SIP virtual
machine which handles
the execution of the program. Additionally, the SIP handles difficulties
associated with parallelism, thus
hiding this aspect of the program from the user. Distributed-memory
parallelism in the SIP is handled
through the use of asynchronous communication routines to aid in effectively
overlapping computation with
communication.

http://www.qtp.ufl.edu/aces/[+http://www.qtp.ufl.edu/aces/+]

http://www.qtp.ufl.edu/aces/downloads/SIALProgrammerGuide.pdf[+http://www.qtp.ufl.edu/aces/downloads/SIALProgrammerGuide.pdf+]

http://www.qtp.ufl.edu/aces/downloads/SIPDesign.pdf[+http://www.qtp.ufl.edu/aces/downloads/SIPDesign.pdf+]

https://ieeexplore.ieee.org/document/6569869[+https://ieeexplore.ieee.org/document/6569869+]

SigPy
~~~~~

SigPy is a package for signal processing, with emphasis on iterative methods. It is built to operate directly on numpy arrays on CPU and cupy arrays on GPU. Its main features include:

* A unified CPU/GPU interface to signal processing functions, including convolution, FFT, NUFFT, wavelet transform, and thresholding functions.
* Linear operator classes (Linop) that can do adjoint, addition, composing, and stacking.
* Proximal operator classes (Prox) that can do stacking, and conjugation.
* Iterative algorithm classes (Alg), including conjugate gradient, (accelerated/proximal) gradient method, and primal dual hybrid gradient.
* Application classes (App) that wrap Alg, Linop, and Prox to form a final deliverable for each application.

SigPy also provides a submodule sigpy.mri for MRI iterative reconstruction methods. Its main features include:

* Commonly used MRI reconstruction methods as an App: SENSE reconstruction, l1-wavelet reconstruction, total-variation reconstruction, and JSENSE reconstruction
* Convenient simulation and sampling functions, including poisson-disc sampling function, and shepp-logan phantom generation function.

Finally, SigPy provides a preliminary submodule sigpy.learn that implements convolutional sparse coding, and linear regression, using the core module.

https://github.com/mikgroup/sigpy[+https://github.com/mikgroup/sigpy+]

http://people.eecs.berkeley.edu/\~mlustig/Software.html[+http://people.eecs.berkeley.edu/~mlustig/Software.html+]

SILE
~~~~

SILE is a system for creating beautiful printed documents. It borrows extensively from TeX, but brings some of TeX's ideas into the 21st century with frame-based layouts, native support for Unicode, PDF, Opentype and XML processing, and extensibility and programmability in a modern, high-level language.

SILE is a typesetter designed to meet the needs of a particular translation community, with an emphasis on extensibility and layout flexibility. Existing solutions (TeX and friends) were not able to provide the required features in these areas, and so translators and publishers have been looking around for an alternative solution for many years. SILE takes a lot of inspiration from TeX, but updates many of its ideas to match the changes in the software ecosystem in the past 35 years.

SILE does not, however, aim to be a replacement for TeX, but is a separate system for document layout and rendering. Because of this, functionality which is a challenge for TeX users - for instance, laying out text on a grid, or magazine-style frame based layouts - becomes very easy in SILE. SILE is written in an interpreted language, so parts of the system's operation, including core typesetting algorithms, can be redefined at run time.

SILE is also designed to meet the realities of today's data processing. Documents these days are often prepared in authoring software and stored in XML format. This applies both to document-specific XML DTDs, such as DocBook and the Text Encoding Initiative, and also tagged text database formats, such as those for storing linguistic and other annotation data such as the LIFT standard for dictionary data and the USX format used by translators. SILE classes can define processing expectations for XML elements, which means that XML files can then be read in and typeset directly. We will look at examples of complex book layouts driven by XML data sources.

https://github.com/simoncozens/sile[+https://github.com/simoncozens/sile+]

http://sile-typesetter.org/[+http://sile-typesetter.org/+]

https://archive.fosdem.org/2015/schedule/event/introducing_sile/[+https://archive.fosdem.org/2015/schedule/event/introducing_sile/+]

CaSILE
^^^^^^

The CaSILE toolkit is a collection of tools designed to automate book publishing from start to finish. The concept is to take very simple input and turn it into a finished product with as little manual intervention as possible. It transforms plain text document formats and meta data into press ready PDFs, E-Books, and rendered promotional materials.

https://github.com/alerque/casile[+https://github.com/alerque/casile+]

Simflowny
~~~~~~~~~

Simflowny is a cloud-based open environment for scientific dynamical models, composed by a semantic Domain Specific Language and a friendly Integrated Development Environment, which automatically generates parallel code for simulation frameworks.

Currently supported scientific model paradigms are:

* Partial Differential Equations, such as Navier-Stokes and Einstein Equations. This family is conformed by two branches:
** Balance Law PDE, the PDEs are written as an evolution system of first order in time and space, which allow us to use schemes based on Finite Volume Methods to deal with shocks and discontinuities. This model also allows parabolic terms like the ones appearing, for instance, in the Navier-Stokes Equations.
** PDE, allowing almost arbitrary forms of evolution equations, including spatial derivatives of any order. The only restriction is that the system must be still first order in time. Typical examples of systems naturally in this form would be the Einstein Equations.
* Spatial Agent-Based Models, such as Ising and Collective Motion (flocking) models
* Graph-based Models, such as the Voter model and the Elementary Cellular Automaton

SimML, Simulation Markup Language, is a set of tags used in Simflowny to describe algorithms within a Simulation domain for Agent Based Models and Partial Differential Equations. The language is integrated in Simflowny and, in most cases, self-descriptive.

https://bitbucket.org/iac3/simflowny/wiki/Home[+https://bitbucket.org/iac3/simflowny/wiki/Home+]

https://www.sciencedirect.com/science/article/pii/S0010465518300870[+https://www.sciencedirect.com/science/article/pii/S0010465518300870+]

https://arxiv.org/abs/1702.04715[+https://arxiv.org/abs/1702.04715+]

SimGrid
~~~~~~~

SimGrid is a scientific instrument to study the behavior of large-scale distributed systems such as Grids, Clouds, HPC or P2P systems. It can be used to evaluate heuristics, prototype applications or even assess legacy MPI applications. 
The capabilities include:

* SimGrid provides ready to use models and API to simulate many different distributed systems: clusters, wide-area and local-area networks, peers over DSL connexions, data centers, etc.
* SimGrid as a Grid Simulator: Accurate yet fast simulation models
* SimGrid as a P2P Simulator: Highly scalable simulations (several milions of nodes on a single machine)
* SimGrid as an MPI Simulator: Realistically simulates unmodified MPI programs
* SimGrid as a Cloud Simulator: SimGrid provides an libvirt-like interface for your Cloud simulation
* SimGrid in other domains: It was used by others as a Volunteer Computing Simulator, Fog Computing Simulator or MapReduce Simulator.

https://simgrid.org/[+https://simgrid.org/+]

SingleDop
~~~~~~~~~

SingleDop is a software module, written in the Python programming language, that will retrieve two-dimensional low-level winds from either real or simulated Doppler radar data.
The interface is simplified to a single line of code in the end user's Python scripts, making implementation of the algorithm in their research analyses very easy. The software package also interfaces well with other open source radar packages, such as the Python ARM Radar Toolkit (Py-ART). Simple visualization (including vector and contour plots) and save/load routines (to preserve analysis results) are also provided.

https://github.com/nasa/SingleDop[+https://github.com/nasa/SingleDop+]

Sirius
~~~~~~

Sirius is an Eclipse project which allows you to easily create your own graphical modeling workbench by leveraging the Eclipse Modeling technologies, including EMF and GMF.

Sirius has been created by Obeo and Thales to provide a generic workbench for model-based architecture engineering that could be easily tailored to fit specific needs. 

A modeling workbench created with Sirius is composed of a set of Eclipse editors (diagrams, tables and trees) which allow the users to create, edit and visualize EMF models.

The editors are defined by a model which defines the complete structure of the modeling workbench, its behavior and all the edition and navigation tools. This description of a Sirius modeling workbench is dynamically interpreted by a runtime within the Eclipse IDE.

https://www.eclipse.org/sirius/[+https://www.eclipse.org/sirius/+]

https://www.siriuscon.org/[+https://www.siriuscon.org/+]

sirocco-genesis
~~~~~~~~~~~~~~~

sirocco-genesis is an interactive graphic user interface for model pre/post-processing (structured and unstructured grids) and database editing. It has basics Geographical Information System capabilities linked with numerical models pre/post-processing utilities. The most useful sections allows you to:

* build quality structured/unstructured meshes from scratch (automated processing and interactive editing)

* display many popular netcdf gridded database in their geographical context (MERCATOR-Ocean, Symphonie, ROMS, MARS, WW3, ECMWF, WRF, ...)

* display layers, vertical profiles and sections, horizontal sections, time series, etc.

* display/edit shorelines and ocean bathymetry dataset

* create quality printing files (true postscript, GIF, JPEG, etc...)

http://sirocco.omp.obs-mip.fr/other_tools/processing/genesis[+http://sirocco.omp.obs-mip.fr/other_tools/processing/genesis+]

SkyField
~~~~~~~~

Skyfield computes positions for the stars, planets, and satellites in orbit around the Earth. Its results should agree with the positions generated by the United States Naval Observatory and their Astronomical Almanac to within 0.0005 arcseconds (which equals half a “mas” or milliarcsecond).

https://github.com/skyfielders/python-skyfield/[+https://github.com/skyfielders/python-skyfield/+]

https://rhodesmill.org/skyfield/[+https://rhodesmill.org/skyfield/+]

SkyView
~~~~~~~

This application supports the functionality of the SkyView Web pages for generating files. It includes almost all the surveys available through the standard SkyView web interface (the one current exception is the Mellinger low-resolution optical survey). You can generate FITS and graphic images. The program lets you treat your own files as surveys and you can add other surveys linking tp external data you might be interested in. It is especially easy to add a survey when the data is available through the Virtual Observatory Simple Image Access protocol.

Features include:

* more than 100 surveys (DSS, SDSS, WISE, FIRST, UKIDSS, Fermi, ROSAT, ....)
* Grid overlays, contours, smoothing
* Catalogs from HEASARC, VizieR, NED and other VO sources
* ImageJ data visualization capabilities
* Outputs in FITS and several graphic image formats including JPEG, GIF, BMP and TIFF.
* High order resamplers including Lanczos and Spline resampling
*  An efficient exact-area flux-conserving reampler.
*  Several de-edging algorithms to match image backgrounds when mosaics are made from images with disparate backgrounds.
* Several ways of choosing the source image to sample.
* Multi-order samplers (using a low order sampler near image defects)
* Ability for users to easily add in their own surveys.
* Easy linking to datasets described by the Virtual Obsevatory Simple Image Access protocol
* Extensible design that allows users to plug in their own samplers, projections, coordinate systems, mosaickers, ... to deal with any special requirements they might have.
* Independent processing so that users do not share the limited resources of the SkyView Web server. The program can run on many clients simultaneously.
* Java source files included in the JAR for users who wish to modify or extend functionality.
* The ability to add overlapping observations to generate deeper images
* Support for finding exactly which input is sampled at a given point
* Support for astronomical projection used by major surveys, include the WWT's TOAST projection HEALPix and many standard projections. 

https://skyview.gsfc.nasa.gov/current/docs/jar.html[+https://skyview.gsfc.nasa.gov/current/docs/jar.html+]

https://skyview.gsfc.nasa.gov/current/cgi/titlepage.pl[+https://skyview.gsfc.nasa.gov/current/cgi/titlepage.pl+]

SLATE
~~~~~

The objective of the Software for Linear Algebra Targeting Exascale (SLATE) project is to provide fundamental dense linear algebra capabilities to the US Department of Energy and to the high-performance computing (HPC) community at large. To this end, SLATE will provide basic dense matrix operations (e.g., matrix multiplication, rank-k update, triangular solve), linear systems solvers, least square solvers, singular value and eigenvalue solvers.

The ultimate objective of SLATE is to replace the venerable Scalable Linear Algebra PACKage (ScaLAPACK) library, which has become the industry standard for dense linear algebra operations in distributed memory environments. However, after two decades of operation, ScaLAPACK is past the end of its lifecycle and overdue for a replacement, as it can hardly be retrofitted to support hardware accelerators, which are an integral part of today’s HPC hardware infrastructure.

Primarily, SLATE aims to extract the full performance potential and maximum scalability from modern, many-node HPC machines with large numbers of cores and multiple hardware accelerators per node. For typical dense linear algebra workloads, this means getting close to the theoretical peak performance and scaling to the full size of the machine (i.e., thousands to tens of thousands of nodes). This is to be accomplished in a portable manner by relying on standards like MPI and OpenMP.

SLATE functionalities will first be delivered to the ECP applications that most urgently require SLATE capabilities (e.g., EXascale Atomistics with Accuracy, Length, and Time [EXAALT], NorthWest computational Chemistry for Exascale [NWChemEx], Quantum Monte Carlo PACKage [QMCPACK], General Atomic and Molecular Electronic Structure System [GAMESS], CANcer Distributed Learning Environment [CANDLE]) and to other software libraries that rely on underlying dense linear algebra services (e.g., Factorization Based Sparse Solvers and Preconditioners [FBSS]). SLATE will also fill the void left by ScaLAPACK’s inability to utilize hardware accelerators, and it will ease the difficulties associated with ScaLAPACK’s legacy matrix layout and Fortran API.

https://icl.utk.edu/slate/[+https://icl.utk.edu/slate/+]

https://bitbucket.org/icl/slate[+https://bitbucket.org/icl/slate+]

https://www.exascaleproject.org/project/slate-software-linear-algebra-targeting-exascale/[+https://www.exascaleproject.org/project/slate-software-linear-algebra-targeting-exascale/+]

SLICOT
~~~~~~

The subroutine library SLICOT provides Fortran 77 implementations of numerical algorithms for computations in systems and control theory. Based on numerical linear algebra routines from BLAS and LAPACK libraries, SLICOT provides methods for the design and analysis of control systems.

The current version of SLICOT consists of over 570 user-callable and computational routines in various domains of systems and control. Almost all of these routines have associated on-line documentation. Over 240 routines have associated example programs, data and results. New routines are still in preparation. Due to the use of Fortran 77, reusability of the software is obtained, so SLICOT can serve as the core for various existing and future CACSD platforms and production quality software. SLICOT routines can be linked to MATLAB through a gateway compiler, e.g., the NAG Gateway Generator. Recently, MATLAB or Scilab interfaces have been developed for many routines.

The use of Fortran 77 allows to exploit the structural features of the underlying computational problem and the use of appropriate data structures. This is advantageous for speed of computation and required memory. As the complexity of systems and related control solutions is ever increasing, the issue of speed and memory remains a valid one. The performance of the library has been assessed with respect to numerical quality, computational speed, and memory requirements for a variety of examples. Comparisons indicate that SLICOT routines usually outperform equivalent MATLAB functions, often by orders of magnitude.

http://slicot.org/[+http://slicot.org/+]

http://slicot.org/objects/software/reports/nic97-3.ps.gz[+http://slicot.org/objects/software/reports/nic97-3.ps.gz+]

https://github.com/KTH-AC/slicot[+https://github.com/KTH-AC/slicot+]

https://github.com/henjo/slicot[+https://github.com/henjo/slicot+]

https://github.com/python-control/Slycot[+https://github.com/python-control/Slycot+]

SLinGen
~~~~~~~

We introduce SLinGen and LGen, program generators for small scale linear algebra computations. The input is a mathematical description (as in a book) of the algorithm. The output is optimized C code usning the available vector ISA. The project was executed in 3 steps with increasing functionality:

* LGen: Basic linear algebra compiler: A compiler that translates basic operations (BLACs) on scalars, vectors, and matrices into vectorized C code

* LGen extension: Basic linear algebra compiler for structured matrices: Extensible framework inside LGen to support structured matrices

* SLinGen: Program generator for linear algebra: Building on 1. and 2. a generator for entire algorithms that are composed from linear algebra operations. This includes Cholesky factorization, solving Sylvester equations, and higher-level algorithms such as a Kalman filter

By “basic linear algebra” we mean computations on matrices, vectors, and scalars that are composed from matrix multiplication, matrix addition, transposition, and scalar multiplication.

The input to LGen is a BLAC including the (fixed) size of its operands. The output is an optimized C function, optionally using intrinsics for vectorization, that computes the BLAC.

https://acl.inf.ethz.ch/research/LGen/[+https://acl.inf.ethz.ch/research/LGen/+]

https://github.com/danielesgit/slingen[+https://github.com/danielesgit/slingen+]

Slurm
~~~~~

The Slurm Workload Manager (formerly known as Simple Linux Utility for Resource Management or SLURM), or Slurm, is a free and open-source job scheduler for Linux and Unix-like kernels, used by many of the world's supercomputers and computer clusters. 
It provides three key functions:

* allocating exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work,
* providing a framework for starting, executing, and monitoring work (typically a parallel job such as MPI) on a set of allocated nodes, and
* arbitrating contention for resources by managing a queue of pending jobs.

Slurm's design is very modular with about 100 optional plugins. In its simplest configuration, it can be installed and configured in a couple of minutes. More sophisticated configurations provide database integration for accounting, management of resource limits and workload prioritization. 

https://slurm.schedmd.com/[+https://slurm.schedmd.com/+]

https://en.wikipedia.org/wiki/Slurm_Workload_Manager[+https://en.wikipedia.org/wiki/Slurm_Workload_Manager+]

Slycat
~~~~~~

Slycat™ is a web-based system for analysis of large, high-dimensional data, developed to provide a collaborative platform for remote analysis of data ensembles. An ensemble is a collection of data sets, typically produced through a series of related simulation runs. More generally, an ensemble is a set of samples, each consisting of the same set of variables, over a shared high-dimensional space describing a particular problem domain. Ensemble analysis is a form of meta-analysis that looks at the combined behaviors and features of a group of simulations in an effort to understand and describe the underlying domain space. For instance, sensitivity analysis uses ensembles to examine how simulation input parameters and simulation results are correlated. By looking at groups of runs as a whole, higher level patterns can be seen despite variations in the individual runs.

The Slycat™ system integrates data management, scalable analysis, and visualization via commodity web clients using a multi-tiered hierarchy of computation and data storage. Analysis models are computed local or on the Slycat™ server, and model artifacts are stored in a project database. These artifacts are the basis for visualizations that are delivered to users’ desktops through ordinary web browsers. Slycat™ currently provides two types of analysis: canonical correlation analysis (CCA) to model relationships between inputs and output metrics, and time series analysis featuring clustering and comparative visualization of waveforms.

https://slycat.readthedocs.io/en/latest/[+https://slycat.readthedocs.io/en/latest/+]

https://github.com/sandialabs/slycat[+https://github.com/sandialabs/slycat+]

Snakemake
~~~~~~~~~

The Snakemake workflow management system is a tool to create reproducible and scalable data analyses. Workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment.

https://snakemake.readthedocs.io/en/stable/[+https://snakemake.readthedocs.io/en/stable/+]

https://bitbucket.org/snakemake/snakemake[+https://bitbucket.org/snakemake/snakemake+]

https://lhcb.github.io/starterkit-lessons/second-analysis-steps/analysis-automation-snakemake.html[+https://lhcb.github.io/starterkit-lessons/second-analysis-steps/analysis-automation-snakemake.html+]

https://www.biostars.org/p/335903/[+https://www.biostars.org/p/335903/+]

https://news.ycombinator.com/item?id=13865400[+https://news.ycombinator.com/item?id=13865400+]

Snap
~~~~

Snap! is a free, blocks- and browser-based educational graphical programming language that allows students to create interactive animations, games, stories, and more, while learning about mathematical and computational ideas. Snap! was inspired by Scratch, but also targets both novice and more advanced students by including and expanding Scratch's features.

Since version 4.0, it is entirely browser-based, with no software that needs to be installed on the local device, much like Scratch. 

The most important features that Snap! offers, but Scratch does not, include:

* expressions using "nested functions", consisting of one or more "anonymous functions", each of which is represented by a block having one or more empty(ed) slot(s)/parameter(s) that are waiting for a "higher order function" (the one that is calling the anonymous one) to be filled by. (Their computer-science theorethical basis being First class functions, which in turn have "Lambda calculus" as their even more abstract, mathematical, foundation),
* lists that are first class (including lists of lists),
* First class sprites (in other words prototype-oriented instance-based classless programming),
* nestable sprites
* codification of Snap! programs to mainstream languages such as Python, JavaScript, C, etc.

https://snap.berkeley.edu/[+https://snap.berkeley.edu/+]

https://github.com/jmoenig/Snap[+https://github.com/jmoenig/Snap+]

https://en.wikipedia.org/wiki/Snap!_(programming_language)[+https://en.wikipedia.org/wiki/Snap!_(programming_language)+]

snappy
~~~~~~

Snappy is a software deployment and package management system originally designed and built by Canonical for the Ubuntu phone operating system. The packages, called snaps and the tool for using them, snapd, work across a range of Linux distributions allowing distro-agnostic upstream software packaging. The system is designed to work for internet of things, cloud and desktop computing.  Snapcraft is a tool for developing snap packages. 

Snaps themselves have no dependency on any "app store", can be obtained from any source and can be therefore used for upstream software deployment. When snaps are deployed on Ubuntu and other versions of Linux, the Ubuntu app store is used as default back-end, but other stores can be enabled as well. 

The snap file format is a single compressed filesystem (based on squashfs format) that is mounted dynamically by the host operating system, together with declarative metadata that is interpreted by the snap system to set up an appropriately shaped secure sandbox or container for that application. The file format extension is .snap.

The snapd daemon manages the snap environment on the local system. Its installation will include the snap tool for interacting with snaps.

With snapd installed, snaps can be discovered, searched for, and installed from the Snap Store and directly from the command line.

https://snapcraft.io/[+https://snapcraft.io/+]

https://en.wikipedia.org/wiki/Snappy_(package_manager)[+https://en.wikipedia.org/wiki/Snappy_(package_manager)+]

*Snap Store* - https://snapcraft.io/store[+https://snapcraft.io/store+]

Snorkel
~~~~~~~

Snorkel is a system for rapidly creating, modeling, and managing training data.

Today's state-of-the-art machine learning models require massive labeled training sets--which usually do not exist for real-world applications. Instead, Snorkel is based around the new data programming paradigm, in which the developer focuses on writing a set of labeling functions, which are just scripts that programmatically label data. The resulting labels are noisy, but Snorkel automatically models this process—learning, essentially, which labeling functions are more accurate than others—and then uses this to train an end model (for example, a deep neural network in TensorFlow). 

Surprisingly, by modeling a noisy training set creation process in this way, we can take potentially low-quality labeling functions from the user, and use these to train high-quality end models. We see Snorkel as providing a general framework for many weak supervision techniques, and as defining a new programming model for weakly-supervised machine learning systems. 

https://hazyresearch.github.io/snorkel/[+https://hazyresearch.github.io/snorkel/+]

https://github.com/HazyResearch/snorkel[+https://github.com/HazyResearch/snorkel+]

socat
~~~~~

socat is a relay for bidirectional data transfer between two independent data
channels. Each of these data channels may be a file, pipe, device (serial line
etc. or a pseudo terminal), a socket (UNIX, IP4, IP6 - raw, UDP, TCP), an
SSL socket, proxy CONNECT connection, a file descriptor (stdin etc.), the GNU
line editor (readline), a program, or a combination of two of these. 
These modes include generation of "listening" sockets, named pipes, and pseudo
terminals.

socat can be used, e.g., as TCP port forwarder (one-shot or daemon), as an
external socksifier, for attacking weak firewalls, as a shell interface to UNIX
sockets, IP6 relay, for redirecting TCP oriented programs to a serial line, to
logically connect serial lines on different computers, or to establish a
relatively secure environment (su and  chroot) for running client or server
shell scripts with network connections. 

Many options are available to refine socats behaviour:
terminal parameters, open() options, file permissions, file and process owners,
basic socket options like bind address, advanced socket options like IP source
routing, linger, TTL, TOS (type of service), or TCP performance tuning.

More capabilities, like daemon mode with forking, client address check,
"tail -f" mode, some stream data processing (line terminator conversion),
choosing sockets, pipes, or ptys for interprocess communication, debug and
trace options, logging to syslog, stderr or file, and last but not least
precise error messages make it a versatile tool for many different purposes.

In fact, many of these features already exist in specialized tools; but until
now, there does not seem to exists another tool that provides such a generic,
flexible, simple and almost comprehensive (UNIX) byte stream connector.

http://www.dest-unreach.org/socat/[+http://www.dest-unreach.org/socat/+]

https://repo.or.cz/socat.git[+https://repo.or.cz/socat.git+]

SOFA
~~~~

SOFA operates under the auspices of the International Astronomical Union (IAU) to provide algorithms and software for use in astronomical computing. The initiative is managed by an international panel, the SOFA Board, appointed through IAU Division A. The Board obtains the latest IAU-approved models and theories from the fundamental-astronomy community, implements them as computer code and checks them for accuracy. SOFA works closely with all the Commissions of the Division and with the International Earth Rotation and Reference Systems Service (IERS).

The SOFA Collection consists of two libraries of routines, one coded in Fortran 77 the other in ANSI C. There is a suite of vector/matrix routines and various utilities that underpin the astronomy algorithms, which include routines for the following:

* Astrometry
* Calendars
* Time Scales
* Ecliptic Coordinates
* Earth Rotation and Sidereal Time
* Ephemerides (medium precision)
* Fundamental Arguments
* Galactic Coordinates
* Geocentric/Geodetic Transformations
* Precession, Nutation and Polar Motion
* Star Catalog Conversion

http://www.iausofa.org/[+http://www.iausofa.org/+]

http://www.iausofa.org/current.html[+http://www.iausofa.org/current.html+]

http://www.iausofa.org/cookbooks.html[+http://www.iausofa.org/cookbooks.html+]

https://github.com/Starlink/pal[+https://github.com/Starlink/pal+]

ERFA
^^^^

This is the source code repository for ERFA (Essential Routines for Fundamental Astronomy). ERFA is a C library containing key algorithms for astronomy, and is based on the SOFA library published by the International Astronomical Union (IAU).

ERFA is intended to replicate the functionality of SOFA (aside from possible bugfixes in ERFA that have not yet been included in SOFA), but is licensed under a three-clause BSD license to enable its compatibility with a wide range of open source licenses. Permission for this release has been obtained from the SOFA board, and is avilable in the LICENSE file included in this source distribution.

https://github.com/liberfa/erfa[+https://github.com/liberfa/erfa+]

SoftFloat
~~~~~~~~~

Berkeley SoftFloat is a free, high-quality software implementation of binary floating-point that conforms to the IEEE Standard for Floating-Point Arithmetic. SoftFloat is completely faithful to the IEEE Standard, while at the same time being relatively fast. All functions dictated by the original 1985 version of the standard are supported except for conversions to and from decimal. The latest release of SoftFloat implements five floating-point formats: 16-bit half-precision, 32-bit single-precision, 64-bit double-precision, 80-bit double-extended-precision, and 128-bit quadruple-precision. All required rounding modes, exception flags, and special values are supported. Fused multiply-add is also implemented for all formats except 80-bit double-extended-precision.

SoftFloat is distributed in the form of ISO/ANSI C source code and should be compilable with almost any ISO-compliant C compiler. Using the GNU C Compiler (gcc), the package has been compiled and tested for several platforms. Target-specific code is provided for various Intel x86 and ARM processors. Other machines can be targeted using these as examples. 

http://www.jhauser.us/arithmetic/SoftFloat.html[+http://www.jhauser.us/arithmetic/SoftFloat.html+]

http://www.math.utah.edu/\~beebe/software/ieee/[+http://www.math.utah.edu/~beebe/software/ieee/+]

TestFloat
^^^^^^^^^

Berkeley TestFloat is a small collection of programs for testing whether an implementation of binary floating-point conforms to the IEEE Standard for Floating-Point Arithmetic. All operations required by the original 1985 version of the IEEE Standard can be tested, except for conversions to and from decimal. With the current release, the following binary formats can be tested: 16-bit half-precision, 32-bit single-precision, 64-bit double-precision, 80-bit double-extended-precision, and/or 128-bit quadruple-precision. TestFloat cannot test decimal floating-point.

TestFloat performs relatively simple tests designed to check the fundamental soundness of a floating-point implementation. All of the modes and special cases of the IEEE Standard are tested. However, TestFloat is not especially good at testing difficult rounding cases for divisions and square roots. It also makes no attempt to find bugs specific to SRT division and the like (such as the infamous Pentium division bug).

TestFloat works by comparing the behavior of the floating-point under test with that of the Berkeley SoftFloat software implementation of floating-point. Any differences found are reported as probable errors in the floating-point being tested. It is the responsibility of the user to verify that the discrepencies TestFloat finds actually represent faults in the floating-point being tested. Unfortunately, TestFloat’s output is not easily interpreted. Detailed knowledge of the IEEE Standard is required to use TestFloat responsibly.

TestFloat is distributed in the form of ISO/ANSI C source code and should be compilable with almost any ISO-compliant C compiler. Using the GNU C Compiler (gcc), the TestFloat programs have been compiled and run on several platforms. To test a new floating-point implementation, some additional code will likely be needed for invoking the new floating-point. 

http://www.jhauser.us/arithmetic/TestFloat.html[+http://www.jhauser.us/arithmetic/TestFloat.html+]

SOLLVE
~~~~~~

In the SOLLVE project, we will enhance OpenMP to cover the major requirements of ECP application codes. In addtion, this project will deliver a high-quality, robust implementation of OpenMP and project extensions in LLVM, an open source compiler infrastructure with an active developer community that impacts the DOE pre-exascale systems (CORAL). It will further develop the LLVM BOLT runtime system to exploit light-weight threading for scalability and facilitate interoperability with MPI. We propose to help drive work toward a common solution for lightweight threading/tasking support in the ECP software stack. Based upon OpenMP needs and project experiences. We also propose to create a validation suite to assess our progress and that of vendors to ensure that quality implementations of OpenMP are being delivered to Exascale systems. The project will also encourage the accelerated development of similarly high-quality, complete vendor implementations and facilitate extensive interactions between the applications developers and OpenMP developers in industry.

SOLLVE proposes an application-driven approach that requires us to interact extensively with ECP applications, to systematically obtain their input and feedback and ensure that we respond effectively to their needs. We will engage the key vendors, relevant ECP co-design centers and ST teams, and the broader OpenMP community as broadly as possible in order to obtain the best-possible solutions to ECP applications problems, to secure their adoption in new versions of the standard, and to address scalability requirements in the implementation. We will interact with ST teams to develop common solutions to system-wide problems, facilitate integration of our results into the ECP software stack, and help develop the OpenMP requirements needed for the procurement of future Exascale systems. The project’s work on extensions to the OpenMP specification will shape future versions (the near one being OpenMP 5.0). SOLLVE meets ECP’s critical OpenMP requirements.

https://www.bnl.gov/compsci/projects/SOLLVE/current-openMP-LLVM-clang.php[+https://www.bnl.gov/compsci/projects/SOLLVE/current-openMP-LLVM-clang.php+]

https://www.bnl.gov/compsci/projects/SOLLVE/index.php[+https://www.bnl.gov/compsci/projects/SOLLVE/index.php+]

https://www.bnl.gov/compsci/projects/SOLLVE/software.php[+https://www.bnl.gov/compsci/projects/SOLLVE/software.php+]

https://github.com/SOLLVE[+https://github.com/SOLLVE+]

SOMAR
~~~~~

SOMAR is the The Stratified Ocean Model with Adaptive Refinement.
SOMAR is free software provided jointly by the Marine Sciences department of the University of North Carolina at Chapel Hill and Jefferson University.
The features include:

* Nonhydrostatic - Our model solves the Boussinesq Navier-Stokes equations without the hydrostatic approximation in order to properly model the internal waves and tides that are ubiquitous in the ocean.
* Complex topography - We have maintained general covariance so that the domain can be described in curvilinear coordinates. This allows us to model irregular boundaries with stretched grids and a logically rectangular coordinate system.
* Separation of background density and its deviation - By splitting the density field into a vertical background stratification and a deviation, we relieve the Poisson solver of computing the associated hydrostatic component of the pressure. This treatment, already implemented in some regional models including MITgcm, also prevents diffusion of oceanic features that are maintained by unmodeled phenomena.
* Stable integration of stiff forcing terms - The forcing terms that lead to buoyancy oscillations often impose instabilities in the form of fast, high-frequency waves. To quell these unphysical modes, we developed an integration method that updates all of the state variables semi-implicitly without requiring additional costly Poisson solves, as is the case with implicit Runge-Kutta schemes.
* Anisotropic grid refinement. A coarse underlying grid along with dynamic local refinement over transient features eliminates unnecessary computation in large portions of the domain. Since the background stratification requires some level of vertical resolution, it is often the case that we only need further resolution in the horizontal. Our anisotropic refinement methods are capable of providing additional cells only in those directions that are under-resolved. Furthermore, our coarse grids operate on larger timesteps than the finer grids. This refinement in both time and space provides a drastic speedup of computation, minimizes the number of required Poisson solves, and ensures all levels are evolving at a Courant number close to one, reducing numerical dissipation.
* Anisotropic Poisson solvers - In the absence of the hydrostatic approximation, we are faced with an ill-conditioned and exceedingly expensive Poisson problem for the pressure. To that end, we provide a leptic Poisson solver as well as a sophisticated semicoarsening multigrid solver to efficiently enforce the incompressibility condition. The leptic iterative method is a well-behaved, perturbative solution to highly anisotropic elliptic problems that has been demonstrated to work over several different geometries and degrees of anisotropy (See: Scotti & Mitran 2008, Santilli & Scotti, 2011).

SOMAR is built upon Chombo 3.1, an AMR framework that has been developed and is being distributed by the Applied Numerical Algorithms Group of Lawrence Berkeley National Lab. Before you can compile SOMAR, you must first download the Chombo software from https://commons.lbl.gov/display/chombo and build its libraries. We strongly advise that you look at the Chombo Design Document for compilation instructions and prerequisites.

https://github.com/UNC-CFD/somar[+https://github.com/UNC-CFD/somar+]

SOSIE
~~~~~

SOSIE is a tool that allows fast and high-quality 2D and 3D interpolation of geophysical fields from a gridded domain to another. "Sosie" is the french word for "doppelganger". SOSIE is developed and distributed under the GNU General Public License (GPL). 

SOSIE was originally intended to interpolate geophysical fields onto the ORCA family of tri-polar grids on which the NEMO global ocean general circulation model is run (so-called ORCA grid). It now supports a wide range of source/target grid configuration for scalar fields interpolation but vector rotation in distorted target grid regions is only supported for the ORCA grids so far. 

The main interpolation algorithm of SOSIE is based on the method of Akima (1970): "A New Method of Interpolation and Smooth Surface Fitting Based On Local Procedures, J.of Applied Comput. Math., 17, 589-602." In SOSIE, this method has been coded from scratch in plain Fortran-90. The algorithm is made highly efficient by skipping solving a 16x16 linear system for each treated point of the target grid. Instead, a particularly efficient way to calculate the general solution of the 16x16 system was found (J.M. Brankart, personal communication). 

Compared to more widely used interpolation methods such as bilinear or bicubic splines, the Akima method allows, at an extremely low numerical cost, continuous and smooth interpolated fields without errors related to overshoots (as for polynomial functions, see 1D illustrations below). 

SOSIE can perform 3D interpolation. This is fake 3D interpolation though: each level is independently interpolated on the target horizontal domain and vertical interpolation (using Akima 1D algorithm) is then performed. SOSIE uses Netcdf as IO file format. 

https://brodeau.github.io/sosie/[+https://brodeau.github.io/sosie/+]

https://github.com/brodeau/sosie[+https://github.com/brodeau/sosie+]

Spack
~~~~~

Spack is now a flexible, configurable, Python-based HPC package manager, automating the installation and fine-tuning of simulations and libraries. It operates on a wide variety of HPC platforms and enables users to build many code configurations. Software installed by Spack runs correctly regardless of environment, and file management is streamlined. Unlike typical package managers, Spack can install many variants of the same build using different compilers, options, and Message Passing Interface (MPI) implementations.

Moreover, writing package recipes for Spack is very simple. A single file contains a templated recipe for different builds of the same package, and recipe authors can differentiate between versions using a custom specification language developed by Gamblin. Behind the scenes, Spack handles the complexity of connecting each software package with a consistent set of dependencies. Each software’s dependency graph—showing the packages it relies on to function—is a unique configuration, and Spack installs each of those configurations in a unique directory, enabling configurations of the same package to run on the same machine. Spack uses RPATH linking so that each package knows where to find its dependencies.

By ensuring one configuration of each library per dependency graph, Spack guarantees that the interface between program modules (the application binary interface, or ABI) remains consistent. Users do not need to know dependency graph structure—only the dependency names. In addition, dependencies may be optional; concretization, a generalization technique, fills in missing configuration details when the user is not specific enough, based on user/site preferences.

https://spack.io/[+https://spack.io/+]

https://github.com/spack/spack[+https://github.com/spack/spack+]

https://computation.llnl.gov/projects/spack-hpc-package-manager[+https://computation.llnl.gov/projects/spack-hpc-package-manager+]

https://spack.readthedocs.io/en/latest/tutorial.html[+https://spack.readthedocs.io/en/latest/tutorial.html+]

SPAMS
~~~~~

SPAMS (SPArse Modeling Software) is an optimization toolbox for solving various sparse estimation problems.

* Dictionary learning and matrix factorization (NMF, sparse PCA, ...)
* Solving sparse decomposition problems with LARS, coordinate descent, OMP, SOMP, proximal methods
* Solving structured sparse decomposition problems (l1/l2, l1/linf, sparse group lasso, tree-structured regularization, structured sparsity with overlapping groups,...). 

It is coded in C++ with a Matlab interface. Interfaces for R and Python have been developed.

http://spams-devel.gforge.inria.fr/[+http://spams-devel.gforge.inria.fr/+]

http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf[+http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf+]

Spectra
~~~~~~~

Spectra stands for Sparse Eigenvalue Computation Toolkit as a Redesigned ARPACK. It is a Cxx library for large scale eigenvalue problems, built on top of Eigen, an open source linear algebra library.

Spectra is implemented as a header-only Cxx library, whose only dependency, Eigen, is also header-only. Hence Spectra can be easily embedded in Cxx projects that require calculating eigenvalues of large matrices.

ARPACK is a software written in FORTRAN for solving large scale eigenvalue problems. The development of Spectra is much inspired by ARPACK, and as the whole name indicates, Spectra is a redesign of the ARPACK library using Cxx language.

In fact, Spectra is based on the algorithms described in the ARPACK Users’ Guide, but it does not use the ARPACK code, and it is NOT a clone of ARPACK for Cxx. In short, Spectra implements the major algorithms in ARPACK, but Spectra provides a completely different interface, and it does not depend on ARPACK.

https://spectralib.org/[+https://spectralib.org/+]

https://github.com/yixuan/spectra/[+https://github.com/yixuan/spectra/+]

spectralDNS
~~~~~~~~~~~

spectralDNS contains a classical high-performance pseudo-spectral Navier-Stokes DNS solver for triply periodic domains. The most notable feature of this solver is that it's written entirely in Python using NumPy, MPI for Python (mpi4py) and pyFFTW. MPI decomposition is performed using either the "slab" or the "pencil" approach and, stripping away unnecessary pre- and post-processing steps, the slab solver is no more than 100 lines long, including the MPI. The code has been found to scale very well in tests on the Shaheen Blue Gene/P supercomputer at KAUST Supercomputing Laboratory. Results of both weak and strong scaling tests are shown below. In addition to incompressible Navier-Stokes there are also solvers for MHD and Navier-Stokes or MHD with variable density through a Boussinesq approximation. 

The efficiency of the pure NumPy/mpi4py solver has been enhanced using Cython for certain routines. The strong scaling results on Shaheen shown below have used the optimized Python/Cython solver, which is found to be faster than a pure Cxx implementation of the same solver.

https://github.com/spectralDNS/spectralDNS[+https://github.com/spectralDNS/spectralDNS+]

https://arxiv.org/abs/1602.03638[+https://arxiv.org/abs/1602.03638+]

https://arxiv.org/abs/1701.03787[+https://arxiv.org/abs/1701.03787+]

https://arxiv.org/abs/1607.00850[+https://arxiv.org/abs/1607.00850+]

Spectral Workbench
~~~~~~~~~~~~~~~~~~

Use a homemade spectrometer to scan different materials, and contribute to an open source database.

https://spectralworkbench.org/[+https://spectralworkbench.org/+]

https://github.com/publiclab/spectral-workbench[+https://github.com/publiclab/spectral-workbench+]

SPHASE
~~~~~~

Smoothed Particle Hydrodynamics Activated Sludge Engine is a 2D SPH solver for CPU and CUDA capable devices.

https://iut-ibk.github.io/sphase/[+https://iut-ibk.github.io/sphase/+]

https://www.sciencedirect.com/science/article/pii/S0010465518303126[+https://www.sciencedirect.com/science/article/pii/S0010465518303126+]

https://www.uibk.ac.at/umwelttechnik/research/projects/sphase.html[+https://www.uibk.ac.at/umwelttechnik/research/projects/sphase.html+]

Sphinx
~~~~~~

Sphinx is a tool that makes it easy to create intelligent and beautiful documentation, written by Georg Brandl and licensed under the BSD license.

It was originally created for the Python documentation, and it has excellent facilities for the documentation of software projects in a range of languages. The following features should be highlighted:

* Output formats: HTML (including Windows HTML Help), LaTeX (for printable PDF versions), ePub, Texinfo, manual pages, plain text
* Extensive cross-references: semantic markup and automatic links for functions, classes, citations, glossary terms and similar pieces of information
* Hierarchical structure: easy definition of a document tree, with automatic links to siblings, parents and children
* Automatic indices: general index as well as a language-specific module indices
* Code handling: automatic highlighting using the Pygments highlighter
* Extensions: automatic testing of code snippets, inclusion of docstrings from Python modules (API docs), and more
* Contributed extensions: more than 50 extensions contributed by users in a second repository; most of them installable from PyPI

Sphinx uses reStructuredText as its markup language, and many of its strengths come from the power and straightforwardness of reStructuredText and its parsing and translating suite, the Docutils. 

http://www.sphinx-doc.org/en/master/[+http://www.sphinx-doc.org/en/master/+]

https://fosdem.org/2019/schedule/event/sphinx/[+https://fosdem.org/2019/schedule/event/sphinx/+]

Spindle
~~~~~~~

Using dynamically-linked libraries is common in most computational
environments, but they can cause serious problem when used on large
clusters and supercomputers.  Shared libraries are frequently stored
on shared file systems, such as NFS.  When thousands of processes
simultaneously start and attempt to search for and load libraries, it
resembles a denial-of-service attack against the shared file system.
This "attack" doesn't just slow down the application, but impacts
every user on the system.  We encountered cases where it took over ten
hours for a dynamically-linked MPI application running on 16K
processes to reach main.

Spindle presents a novel solution to this problem.  It transparently
runs alongside your distributed application and takes over its library
loading mechanism.  When processes start to load a new library,
Spindle intercepts the operation, designates one process to read the
file from the shared file system, then distributes the library's
contents to every process with a scalable broadcast operation.

Spindle is very scalable.  On a cluster at LLNL the Pynamic benchmark
(which measures library loading performance) was unable to scale much
past 100 nodes.  Even at that small scale it was causing significant
performance problems that were impacting everyone on the cluster.
When running Pynamic under Spindle, we were able to scale up to the
max job size at 1,280 nodes without showing any signs of file-system
stress or library-related slowdowns.

Unlike competing solutions, Spindle does not require any special
hardware, and libraries do not have to be staged into any special
locations.  Applications can work out-of-the-box do not need any
special compile or link flags.  Spindle is completely userspace and
does not require kernel patches or root privileges.

Spindle can trigger scalable loading of dlopened libraries, dependent
library, executables, python modules and specified application data
files.

https://github.com/hpc/Spindle[+https://github.com/hpc/Spindle+]

https://computation.llnl.gov/projects/spindle[+https://computation.llnl.gov/projects/spindle+]

Spire
~~~~~

Spire is a numeric library for Scala which is intended to be generic, fast, and precise.

Using features such as specialization, macros, type classes, and implicits, Spire works hard to defy conventional wisdom around performance and precision trade-offs. A major goal is to allow developers to write efficient numeric code without having to "bake in" particular numeric representations. In most cases, generic implementations using Spire's specialized type classes perform identically to corresponding direct implementations.

https://github.com/non/spire[+https://github.com/non/spire+]

https://typelevel.org/blog/2013/07/07/generic-numeric-programming.html[+https://typelevel.org/blog/2013/07/07/generic-numeric-programming.html+]

SPLAT
~~~~~

SPLAT! is an RF Signal Propagation, Loss, And Terrain analysis tool for the electromagnetic spectrum between 20 MHz and 20 GHz.

Applications of SPLAT! include site engineering, wireless network design, amateur radio communications, frequency coordination, communication system design, and terrestrial analog and digital television and radio broadcasting.

SPLAT! provides site engineering data such as the great circle distances and bearings between sites, antenna elevation angles (uptilt), depression angles (downtilt), antenna height above mean sea level, antenna height above average terrain, bearings and distances to known obstructions based on U.S. Geological Survey and Space Shuttle Radar Topography Mission elevation data, path loss and field strength based on the Longley-Rice Irregular Terrain as well as the new Irregular Terrain With Obstructions (ITWOM v3.0) model, and minimum antenna height requirements needed to establish line-of-sight communication paths and Fresnel Zone clearances absent of obstructions due to terrain. 

SPLAT! produces reports, graphs, and highly detailed and carefully annotated topographic maps depicting line-of-sight paths, path loss, field strength, and expected coverage areas of transmitters and repeater systems. When performing line-of-sight analysis in situations where multiple transmitter or repeater sites are employed, SPLAT! determines individual and mutual areas of coverage within the network specified. SPLAT! also produces .geo Georeference Information Files for interoperability with Xastir software, and .kml Keyhole Markup Language files for interoperability with Google Earth. 

https://www.qsl.net/kd2bd/splat.html[+https://www.qsl.net/kd2bd/splat.html+]

https://github.com/pointhi/PySplat[+https://github.com/pointhi/PySplat+]

http://xastir.org/index.php/Main_Page[+http://xastir.org/index.php/Main_Page+]

spmpython
~~~~~~~~~

Spectral Methods in MATLAB rewritten in Python.

https://github.com/mikaem/spmpython[+https://github.com/mikaem/spmpython+]

https://people.maths.ox.ac.uk/trefethen/spectral.html[+https://people.maths.ox.ac.uk/trefethen/spectral.html+]

SQLAlchemy
~~~~~~~~~~

SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.

It provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.

SQL databases behave less like object collections the more size and performance start to matter; object collections behave less like tables and rows the more abstraction starts to matter. SQLAlchemy aims to accommodate both of these principles.

SQLAlchemy considers the database to be a relational algebra engine, not just a collection of tables. Rows can be selected from not only tables but also joins and other select statements; any of these units can be composed into a larger structure. SQLAlchemy's expression language builds on this concept from its core.

SQLAlchemy is most famous for its object-relational mapper (ORM), an optional component that provides the data mapper pattern, where classes can be mapped to the database in open ended, multiple ways - allowing the object model and database schema to develop in a cleanly decoupled way from the beginning.

SQLAlchemy's overall approach to these problems is entirely different from that of most other SQL / ORM tools, rooted in a so-called complimentarity- oriented approach; instead of hiding away SQL and object relational details behind a wall of automation, all processes are fully exposed within a series of composable, transparent tools. The library takes on the job of automating redundant tasks while the developer remains in control of how the database is organized and how SQL is constructed.

https://docs.sqlalchemy.org/en/latest/intro.html[+https://docs.sqlalchemy.org/en/latest/intro.html+]

https://www.sqlalchemy.org/[+https://www.sqlalchemy.org/+]

GeoAlchemy
^^^^^^^^^^

GeoAlchemy 2 provides extensions to SQLAlchemy for working with spatial databases.

GeoAlchemy 2 focuses on PostGIS. PostGIS 1.5 and PostGIS 2 are supported.

SpatiaLite is also supported, but using GeoAlchemy 2 with SpatiaLite requires some specific configuration on the application side. GeoAlchemy 2 works with SpatiaLite 4.3.0 and higher.

GeoAlchemy 2 aims to be simpler than its predecessor, GeoAlchemy. Simpler to use, and simpler to maintain.

https://geoalchemy-2.readthedocs.io/en/latest/[+https://geoalchemy-2.readthedocs.io/en/latest/+]

SPICE (JPL)
~~~~~~~~~~~

NASA's Navigation and Ancillary Information Facility (NAIF) offers NASA flight projects and NASA funded researchers an observation geometry information system named "SPICE" to assist scientists in planning and interpreting scientific observations from space-based instruments aboard robotic planetary spacecraft. SPICE is also widely used in engineering tasks associated with these missions. 

 SPICE is focused on solar system geometry (pdf). The SPICE system includes a suite of software known as the SPICE Toolkit consisting of application program interfaces (APIs) that customers incorporate in their own application programs to read SPICE data files and, using those data, compute derived observation geometry such as altitude, latitude/longitude and lighting angles. The Toolkit also contains a number of utility programs used to make and manage SPICE data files.

SPICE data and software may be used within many popular computing environments. The SPICE Toolkit software is offered in FORTRAN 77, ANSI C, IDL® and MATLAB®. A version for Java Native Interface is available upon request. Third party versions for Python, Ruby and other languages exist as well. 

https://naif.jpl.nasa.gov/naif/[+https://naif.jpl.nasa.gov/naif/+]

https://naif.jpl.nasa.gov/naif/toolkit.html[+https://naif.jpl.nasa.gov/naif/toolkit.html+]

https://naif.jpl.nasa.gov/naif/utilities.html[+https://naif.jpl.nasa.gov/naif/utilities.html+]

SpiceyPy
^^^^^^^^

SpiceyPy is a Python wrapper for the NAIF C SPICE Toolkit (N66), compatible with Python 2 and 3, written using ctypes.

https://github.com/AndrewAnnex/SpiceyPy[+https://github.com/AndrewAnnex/SpiceyPy+]

SPICE
~~~~~

SPICE ("Simulation Program with Integrated Circuit Emphasis")[1][2] is a general-purpose, open-source analog electronic circuit simulator. It is a program used in integrated circuit and board-level design to check the integrity of circuit designs and to predict circuit behavior. 

Circuit simulation programs, of which SPICE and derivatives are the most prominent, take a text netlist describing the circuit elements (transistors, resistors, capacitors, etc.) and their connections, and translate[3] this description into equations to be solved. The general equations produced are nonlinear differential algebraic equations which are solved using implicit integration methods, Newton's method and sparse matrix techniques. 

https://en.wikipedia.org/wiki/SPICE[+https://en.wikipedia.org/wiki/SPICE+]

https://bwrcs.eecs.berkeley.edu/Classes/IcBook/SPICE/[+https://bwrcs.eecs.berkeley.edu/Classes/IcBook/SPICE/+]

ahkab
^^^^^

A SPICE-like electronic circuit simulator written in Python.

https://github.com/ahkab/ahkab[+https://github.com/ahkab/ahkab+]

https://ahkab.readthedocs.io/en/latest/[+https://ahkab.readthedocs.io/en/latest/+]

IRSIM
^^^^^

IRSIM is a tool for simulating digital circuits. It is a "switch-level" simulator; that is, it treats transistors as ideal switches. Extracted capacitance and lumped resistance values are used to make the switch a little bit more realistic than the ideal, using the RC time constants to predict the relative timing of events.

IRSIM shares a history with magic, although it is an independent program. Magic was designed to produce, and IRSIM to read, the ".sim" file format, which is largely unused outside of these two programs. IRSIM was developed at Stanford, while Magic was developed at Berkeley. Parts of Magic were developed especially for use with IRSIM, allowing IRSIM to run a simulation in the "background" (i.e., a forked process communicating through a pipe), while displaying information about the values of signals directly on the VLSI layout. 

http://opencircuitdesign.com/irsim/[+http://opencircuitdesign.com/irsim/+]

ngspice
^^^^^^^

ngspice is the open source spice simulator for electric and electronic circuits.

Such a circuit may comprise of JFETs, bipolar and MOS transistors, passive elements like R, L, or C, diodes, transmission lines and other devices, all interconnected in a netlist. Digital circuits are simulated as well, event driven and fast, from single gates to complex circuits. And you may enter the combination of both analog and digital as a mixed-signal circuit.

ngspice offers a wealth of device models for active, passive, analog, and digital elements. Model parameters are provided by the semiconductor manufacturers. The user add her circuits as a netlist, and the output is one or more graphs of currents, voltages and other electrical quantities or is saved in a data file.

ngspice does not provide schematic entry. Its input is command line or file based. There are however third party interfaces available.

http://ngspice.sourceforge.net/[+http://ngspice.sourceforge.net/+]

https://en.wikipedia.org/wiki/Ngspice[+https://en.wikipedia.org/wiki/Ngspice+]

https://fosdem.org/2019/schedule/event/ngspice/[+https://fosdem.org/2019/schedule/event/ngspice/+]

PySpice
^^^^^^^

PySpice is a Python module which interface Python to the Ngspice and Xyce circuit simulators.

https://github.com/FabriceSalvaire/PySpice[+https://github.com/FabriceSalvaire/PySpice+]

https://pyspice.fabrice-salvaire.fr/[+https://pyspice.fabrice-salvaire.fr/+]

Xyce
^^^^

Xyce (zīs, rhymes with “spice”) is an open source, SPICE-compatible, high-performance analog circuit simulator, capable of solving extremely large circuit problems by supporting large-scale parallel computing platforms. It also supports serial execution on all common desktop platforms, and small-scale parallel runs on Unix-like systems. In addition to analog electronic simulation, Xyce has also been used to investigate more general network systems, such as neural networks and power grids.

The Xyce Parallel Electronic Simulator is a SPICE-compatible circuit simulator, developed internally at Sandia National Laboratories and funded by the National Nuclear Security Administration's Advanced Simulation and Computing (ASC) Campaign. In continuous development since 1999, Xyce is designed to run on large-scale parallel computing platforms, though it also executes efficiently on a variety of architectures, including single processor workstations. As a mature platform for large-scale parallel circuit simulation, Xyce supports standard capabilities available from commercial simulators, in addition to a variety of devices and models specific to Sandia's needs.

https://xyce.sandia.gov/about_xyce/index.html[+https://xyce.sandia.gov/about_xyce/index.html+]

SPIR
~~~~

SPIR (Standard Portable Intermediate Representation) was initially developed for use by OpenCL and SPIR versions 1.2 and 2.0 were based on LLVM. SPIR has now evolved into a cross-API intermediate language that is fully defined by Khronos with native support for shader and kernel features used by APIs such as Vulkan – called SPIR-V.

SPIR-V is the first open standard, cross-API intermediate language for natively representing parallel compute and graphics and is part of the core specifications of OpenCL 2.1, OpenCL 2.2, and the Vulkan GPU API. SPIR-V is also supported in an OpenGL 4.6 extension. SPIR–V does not use LLVM, and so is isolated from LLVM roadmap changes. Khronos has open sourced SPIR-V/LLVM conversion tools to enable construction of flexible toolchains that use both intermediate languages.

SPIR-V exposes the machine model for OpenCL 1.2, 2.0, 2.1, 2.2 and Vulkan - including full flow control, and graphics and parallel constructs not supported in LLVM. SPIR-V also supports OpenCL C and OpenCL Cxx kernel languages as well as the GLSL shader language for Vulkan and OpenGL.

SPIR-V is catalyzing a revolution in the language compiler ecosystem - it can split the compiler chain across multiple vendors’ products, enabling high-level language front-ends to emit programs in a standardized intermediate form to be ingested by Vulkan, OpenGL or OpenCL drivers.   For hardware vendors, ingesting SPIR-V eliminate the need to build a high-level language source compiler into device drivers, significantly reducing driver complexity, and will enable a broad range of language and framework front-ends to run on diverse hardware architectures.

https://www.khronos.org/spir/[+https://www.khronos.org/spir/+]

SPIRV-Cross
^^^^^^^^^^^

SPIRV-Cross is a tool designed for parsing and converting SPIR-V to other shader languages.
The features include:

* Convert SPIR-V to readable, usable and efficient GLSL
* Convert SPIR-V to readable, usable and efficient Metal Shading Language (MSL)
* Convert SPIR-V to readable, usable and efficient HLSL
* Convert SPIR-V to debuggable Cxx [EXPERIMENTAL]
* Convert SPIR-V to a JSON reflection format [EXPERIMENTAL]
* Reflection API to simplify the creation of Vulkan pipeline layouts
* Reflection API to modify and tweak OpDecorations
* Supports "all" of vertex, fragment, tessellation, geometry and compute shaders.

SPIRV-Cross tries hard to emit readable and clean output from the SPIR-V. The goal is to emit GLSL or MSL that looks like it was written by a human and not awkward IR/assembly-like code.

https://github.com/KhronosGroup/SPIRV-Cross[+https://github.com/KhronosGroup/SPIRV-Cross+]

SPIRV-Tools
^^^^^^^^^^^

The SPIR-V Tools project provides an API and commands for processing SPIR-V modules.

The project includes an assembler, binary module parser, disassembler, validator, and optimizer for SPIR-V. Except for the optimizer, all are based on a common static library. The library contains all of the implementation details, and is used in the standalone tools whilst also enabling integration into other code bases directly. The optimizer implementation resides in its own library, which depends on the core library.

https://github.com/KhronosGroup/SPIRV-Tools[+https://github.com/KhronosGroup/SPIRV-Tools+]

SQLite
~~~~~

A relational database management system contained in a C programming library. In contrast to many other database management systems, SQLite is not a client–server database engine. Rather, it is embedded into the end program.

SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity. This means that one can, for example, insert a string into an column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string "123" into an integer in this case, but does not guarantee such conversions, and will store the data as-is if such a conversion is not possible.

SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. SQLite has bindings to many programming languages. 

Unlike client–server database management systems, the SQLite engine has no standalone processes with which the application program communicates. Instead, the SQLite library is linked in and thus becomes an integral part of the application program. Linking may be static or dynamic. The application program uses SQLite's functionality through simple function calls, which reduce latency in database access: function calls within a single process are more efficient than inter-process communication.

SQLite stores the entire database (definitions, tables, indices, and the data itself) as a single cross-platform file on a host machine. It implements this simple design by locking the entire database file during writing. SQLite read operations can be multitasked, though writes can only be performed sequentially.

Due to the server-less design, SQLite applications require less configuration than client-server databases. SQLite is called zero-conf because it does not require service management (such as startup scripts) or access control based on GRANT and passwords. Access control is handled by means of file system permissions given to the database file itself. Databases in client-server systems use file system permissions which give access to the database files only to the daemon process.

Another implication of the serverless design is that several processes may not be able to write to the database file. In server-based databases, several writers will all connect to the same daemon, which is able to handle its locks internally. SQLite on the other hand has to rely on file-system locks. It has less knowledge of the other processes that are accessing the database at the same time. Therefore, SQLite is not the preferred choice for write-intensive deployments. However, for simple queries with little concurrency, SQLite performance profits from avoiding the overhead of passing its data to another process.

SQLite uses PostgreSQL as a reference platform. "What would PostgreSQL do" is used to make sense of the SQL standard. One major deviation is that, with the exception of primary keys, SQLite does not enforce type checking; the type of a value is dynamic and not strictly constrained by the schema (although the schema will trigger a conversion when storing, if such a conversion is potentially reversible).

https://sqlite.org/index.html[+https://sqlite.org/index.html+]

http://www.sqlitetutorial.net/[+http://www.sqlitetutorial.net/+]

SpatiaLite
^^^^^^^^^^

SpatiaLite is a spatial extension to SQLite, providing vector geodatabase functionality. It is similar to PostGIS, Oracle Spatial, and SQL Server with spatial extensions, although SQLite/SpatiaLite aren't based on client-server architecture: they adopt a simpler personal architecture. i.e. the whole SQL engine is directly embedded within the application itself: a complete database simply is an ordinary file which can be freely copied (or even deleted) and transferred from one computer/OS to a different one without any special precaution.

SpatiaLite extends SQLite's existing spatial support to cover the OGC's SFS specification.[2] It isn't necessary to use SpatiaLite to manage spatial data in SQLite, which has its own implementation of R-tree indexes and geometry types. But SpatiaLite is needed for advanced spatial queries and to support multiple map projections. SpatiaLite is provided natively for Linux and Windows as a software library as well several utilities that incorporate the SpatiaLite library. These utilities include command line tools that extend SQLite's own with spatial macros, a graphical GUI for manipulating Spatialite databases and their data, and a simple desktop GIS tool for browsing data.

Being a single binary file, SpatiaLite is also being used as a GIS vector format to exchange geospatial data.

https://www.gaia-gis.it/fossil/libspatialite/index[+https://www.gaia-gis.it/fossil/libspatialite/index+]

https://en.wikipedia.org/wiki/SpatiaLite[+https://en.wikipedia.org/wiki/SpatiaLite+]

https://github.com/lokkju/pyspatialite[+https://github.com/lokkju/pyspatialite+]

SSH
~~~

Blah.

assh
^^^^

A transparent wrapper that adds regex, aliases, gateways, dynamic hostnames, graphviz, json output, yaml configuration to SSH.  Usage examples:

* Connect to +hosta+ using +hostb+ as a gateway: +ssh hosta/hostb+
* Connect to +hosta+ using +hostb+ as a gateway using +hostc+ as a gateway: +ssh hosta/hostb/hostc+

More advanced workflows can be degined via a configuration file.

https://github.com/moul/assh[+https://github.com/moul/assh+]

AsyncSSH
^^^^^^^^

AsyncSSH is a Python package which provides an asynchronous client and server implementation of the SSHv2 protocol on top of the Python 3.4+ asyncio framework.
The features include:

* Full support for SSHv2, SFTP, and SCP client and server functions
* Multiple simultaneous sessions on a single SSH connection
* Multiple SSH connections in a single event loop
* Byte and string based I/O with settable encoding
* A variety of key exchange, encryption, and MAC algorithms


* Support for gzip compression
* User and host-based public key, password, and keyboard-interactive authentication methods
* Many types and formats of public keys and certificates
* Support for accessing keys managed by ssh-agent on UNIX systems
* Support for accessing keys managed by PuTTY’s Pageant agent on Windows
* Support for accessing host keys via OpenSSH’s ssh-keysign
* OpenSSH-style known_hosts file support
* OpenSSH-style authorized_keys file support
* Compatibility with OpenSSH “Encrypt then MAC” option for better security

https://asyncssh.readthedocs.io/en/latest/[+https://asyncssh.readthedocs.io/en/latest/+]

https://github.com/ronf/asyncssh[+https://github.com/ronf/asyncssh+]

mosh
^^^^

Remote terminal application that allows roaming, supports intermittent connectivity, and provides intelligent local echo and line editing of user keystrokes.

Mosh is a replacement for interactive SSH terminals. It's more robust and responsive, especially over Wi-Fi, cellular, and long-distance links.

The features include:

* Mosh automatically roams as you move between Internet connections.
* With Mosh, you can put your laptop to sleep and wake it up later, keeping your connection intact. If your Internet connection drops, Mosh will warn you — but the connection resumes when network service comes back.
* SSH waits for the server's reply before showing you your own typing. That can make for a lousy user interface. Mosh is different: it gives an instant response to typing, deleting, and line editing.
* You don't need to be the superuser to install or run Mosh. The client and server are executables run by an ordinary user and last only for the life of the connection.
* Mosh doesn't listen on network ports or authenticate users. The mosh client logs in to the server via SSH, and users present the same credentials (e.g., password, public key) as before. Then Mosh runs the mosh-server remotely and connects to it over UDP.
* Mosh is a command-line program, like ssh. You can use it inside xterm, gnome-terminal, urxvt, Terminal.app, iTerm, emacs, screen, or tmux. 
* Unlike SSH, mosh's UDP-based protocol handles packet loss gracefully, and sets the frame rate based on network conditions. Mosh doesn't fill up network buffers, so Control-C always works to halt a runaway process.

https://mosh.org/[+https://mosh.org/+]

sshproxy
^^^^^^^^

sshproxy is used on a gateway to transparently proxy a user SSH connection on the gateway to an internal host via SSH. scp, sftp, rsync, etc. are supported.

sshproxy defines routes which bind an SSH listening address to a pool of destination hosts. It can then choose the first available destination or load-balance the connections with a simple round-robin algorithm.

Because of its design sshproxy is stateless (it is launched as a ForceCommand by sshd). It can be made stateful by using the sshproxy-managerd daemon.

sshproxy can be configured to monitor SSH sessions and either to save them in files or to send them to the very simple sshproxy-dumpd daemon. Sessions can be replayed with the sshproxy-replay command.

https://github.com/cea-hpc/sshproxy[+https://github.com/cea-hpc/sshproxy+]

sshttp
^^^^^^

In case your FW policy forbids SSH access to the DMZ or internal network from outside, but you still want to use ssh on machines which only have one open port, e.g. HTTP, you can use sshttpd.

https://github.com/stealth/sshttp[+https://github.com/stealth/sshttp+]

sshuttle
^^^^^^^^

The only program that solves the following common case:

* Your client machine (or router) is Linux, MacOS, FreeBSD, OpenBSD or pfSense.
* You have access to a remote network via ssh.
* You don’t necessarily have admin access on the remote network.
* The remote network has no VPN, or only stupid/complex VPN protocols (IPsec, PPTP, etc). Or maybe you are the admin and you just got frustrated with the awful state of VPN tools.
* You don’t want to create an ssh port forward for every single host/port on the remote network.
* You hate openssh’s port forwarding because it’s randomly slow and/or stupid.
* You can’t use openssh’s PermitTunnel feature because it’s disabled by default on openssh servers; plus it does TCP-over-TCP, which has terrible performance

https://sshuttle.readthedocs.io/en/stable/[+https://sshuttle.readthedocs.io/en/stable/+]

https://github.com/sshuttle/sshuttle[+https://github.com/sshuttle/sshuttle+]

wssh
^^^^

wssh is a SSH to WebSockets Bridge that lets you invoke a remote shell using nothing but HTTP.
The client connecting to wssh doesn't need to speak the SSH protocol - rather, the SSH connection is terminated at the bridge level and the pty is wrapper through a thin layer of JSON and sent back to the client.
This means you can implement a WSSH client in just a few lines of code, even for a web browser.

wsshd provides a web interface giving you access to a Javascript client.

https://github.com/aluzzardi/wssh[+https://github.com/aluzzardi/wssh+]

SSL/TLS
~~~~~~~

Transport Layer Security (TLS), and its now-deprecated predecessor, Secure Sockets Layer (SSL),[1] are cryptographic protocols designed to provide communications security over a computer network.[2] Several versions of the protocols find widespread use in applications such as web browsing, email, instant messaging, and voice over IP (VoIP). Websites can use TLS to secure all communications between their servers and web browsers. 

https://en.wikipedia.org/wiki/Transport_Layer_Security[+https://en.wikipedia.org/wiki/Transport_Layer_Security+]

BoringSSL
^^^^^^^^^

BoringSSL is a fork of OpenSSL that is designed to meet Google's needs.

Although BoringSSL is an open source project, it is not intended for general use, as OpenSSL is. We don't recommend that third parties depend upon it. Doing so is likely to be frustrating because there are no guarantees of API or ABI stability.

Programs ship their own copies of BoringSSL when they use it and we update everything as needed when deciding to make API changes. This allows us to mostly avoid compromises in the name of compatibility. It works for us, but it may not work for you.

BoringSSL arose because Google used OpenSSL for many years in various ways and, over time, built up a large number of patches that were maintained while tracking upstream OpenSSL. As Google's product portfolio became more complex, more copies of OpenSSL sprung up and the effort involved in maintaining all these patches in multiple places was growing steadily.

Currently BoringSSL is the SSL library in Chrome/Chromium, Android (but it's not part of the NDK) and a number of other apps/programs.

https://boringssl.googlesource.com/boringssl/[+https://boringssl.googlesource.com/boringssl/+]

LibreSSL
^^^^^^^^

LibreSSL is an open-source implementation of the Transport Layer Security (TLS) protocol. The implementation is named after Secure Sockets Layer (SSL), the deprecated predecessor of TLS, for which support was removed in release 2.3.0. The OpenBSD project forked LibreSSL from OpenSSL 1.0.1g in April 2014 as a response to the Heartbleed security vulnerability,[7][8][9][10] with the goals of modernizing the codebase, improving security, and applying development best practices.

LibreSSL releases contain several parts:

* libcrypto: a library of cryptography fundamentals
* libssl: a TLS library
* libtls: a new TLS library, designed to make it easier to write foolproof applications
* Various utilities such as openssl(1), nc(1), and ocspcheck(8).

https://www.libressl.org/[+https://www.libressl.org/+]

https://github.com/libressl-portable/portable[+https://github.com/libressl-portable/portable+]

OpenSSL
^^^^^^^

OpenSSL is a software library for applications that secure communications over computer networks against eavesdropping or need to identify the party at the other end. It is widely used in Internet web servers, serving a majority of all web sites.

OpenSSL contains an open-source implementation of the SSL and TLS protocols. The core library, written in the C programming language, implements basic cryptographic functions and provides various utility functions. Wrappers allowing the use of the OpenSSL library in a variety of computer languages are available. 

Version 1.1.1 of OpenSSL contains support for TSL 1.3.

https://www.openssl.org/[+https://www.openssl.org/+]

wolfSSL
^^^^^^^

The wolfSSL embedded SSL library is a lightweight, portable, C-language-based SSL/TLS library targeted at IoT, embedded, and RTOS environments primarily because of its size, speed, and feature set. It works seamlessly in desktop, enterprise, and cloud environments as well. wolfSSL supports industry standards up to the current TLS 1.3 and DTLS 1.2, is up to 20 times smaller than OpenSSL, offers a simple API, an OpenSSL compatibility layer, OCSP and CRL support, and is backed by the robust wolfCrypt cryptography library.

The wolfCrypt cryptography engine is a lightweight crypto library written in ANSI C and targeted for embedded, RTOS, and resource-constrained environments - primarily because of its small size, speed, and feature set.  It is commonly used in standard operating environments as well because of its royalty-free pricing and excellent cross platform support.  wolfCrypt supports the most popular algorithms and ciphers as well as progressive ones such as HC-128, RABBIT, and NTRU.  wolfCrypt is stable, production-ready, and backed by our excellent team of security experts.  It is used in millions of application and devices worldwide.

https://www.wolfssl.com/[+https://www.wolfssl.com/+]

https://www.wolfssl.com/products/wolfcrypt/[+https://www.wolfssl.com/products/wolfcrypt/+]

SST
~~~

The Structural Simulation Toolkit uses the supercomputers of today to build the supercomputers of tomorrow.

Exploring novel computer system designs involves modification of both programming models and hardware organization. This exploration is frequently made difficult by the lack of compiler tools, well defined ISA features, or complete microarchitectural definitions. Premature definition of hardware or software limits the flexibility of the other. However, without such assumptions, neither software or hardware can progress. A key goal is to allow programming models and the underlying hardware to evolve separately, while allowing feedback between them.

The Structural Simulation Toolkit (SST) was developed to explore innovations in highly concurrent systems where the ISA, microarchitecture, and memory interact with the programming model and communications system. The package provides two novel capabilities. The first is a fully modular design that enables extensive exploration of an individual system parameter without the need for intrusive changes to the simulator. The second is a parallel simulation environment based on MPI. This provides a high level of performance and the ability to look at large systems. The framework has been successfully used to model concepts ranging from processing in memory to conventional processors connected by conventional network interfaces and running MPI.

http://sst-simulator.org/[+http://sst-simulator.org/+]

https://github.com/sstsimulator[+https://github.com/sstsimulator+]

S-Store
~~~~~~~

Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. In the past, these two modes of operation were found only in separate, stove-piped systems. However, with the creation of NewSQL OLTP systems, it becomes possible to perform scalable real-time operations without sacrificing transactional support. Enter S-Store, the world’s first transactional streaming database system. 

S-Store is built on H-Store, a distributed main-memory OLTP system, taking advantage of its ability to process transactions with low latency and high throughput. We add support for push-based processing, including two layers of triggering mechanisms. We also introduce native data structures for streams and windows.

S-Store models its streaming workflow as DAGs of streaming transactions. A streaming transaction is considered to be a stored procedure (SP) operating on an input batch of tuples. Due to data and processing dependencies, transaction executions (TEs) cannot be performed in an arbitrary order, and additional isolation rules are necessary to preserve ACID guarantees.
Transaction Model

http://sstore.cs.brown.edu/[+http://sstore.cs.brown.edu/+]

https://github.com/s-store/s-store[+https://github.com/s-store/s-store+]

StarPU
~~~~~~

StarPU is a task programming library for hybrid architectures.

StarPU is a software tool aiming to allow programmers to exploit the computing power of the available CPUs and GPUs, while relieving them from the need to specially adapt their programs to the target machine and processing units.

At the core of StarPU is its runtime support library, which is responsible for scheduling application-provided tasks on heterogeneous CPU/GPU machines. In addition, StarPU comes with programming language support, in the form of extensions to languages of the C family (C Extensions), as well as an OpenCL front-end (SOCL OpenCL Extensions).

StarPU's runtime and programming language extensions support a task-based programming model. Applications submit computational tasks, with CPU and/or GPU implementations, and StarPU schedules these tasks and associated data transfers on available CPUs and GPUs. The data that a task manipulates are automatically transferred among accelerators and the main memory, so that programmers are freed from the scheduling issues and technical details associated with these transfers.

The supported architectures are:

* SMP/Multicore Processors (x86, PPC, ARM, ... all Debian architecture have been tested)
* NVIDIA GPUs (e.g. heterogeneous multi-GPU), with pipelined and concurrent kernel execution support (new in v1.2) and GPU-GPU direct transfers (new in v1.1)
* OpenCL devices
* Cell Processors (experimental)
* Intel SCC (experimental, new in v1.2)
* Intel MIC / Xeon Phi (new in v1.2)

http://starpu.gforge.inria.fr/[+http://starpu.gforge.inria.fr/+]

KSTAR
^^^^^

KStar provides an OpenMP 4 -compatible interface on top of StarPU. This allows to just rebuild OpenMP applications with the K'Star source-to-source compiler, then build it with the usual compiler, and the result will use the StarPU runtime. 

KSTAR (also nicknamed 'Klang-Omp') is a C and Cxx source-to-source OpenMP compiler based on LLVM framework and its C/Cxx front-end CLang. It translates OpenMP directives into calls to task-based runtime system APIs. KSTAR currently targets the StarPU runtime and optionnally the Kaapi runtime as well.
The compiler supports independent tasks as defined by the OpenMP specification revision 3.1 as well as dependent tasks introduced with OpenMP 4.
The two runtime systems currently targeted by KSTAR, StarPU and Kaapi, both natively implement dependent tasks. KSTAR also supports long established OpenMP constructs such as parallel loops and sections.

http://kstar.gforge.inria.fr/[+http://kstar.gforge.inria.fr/+]

STEDY
~~~~~

STEDY is a software package based on MATLAB to enable researchers to simulate dynamics of tensegrity structures. We have developed a Lagrangian formulation for deriving the algebraic differential equations governing the dynamics of classical tensegrity systems. The framework is general enough to allow modeling of general multi-body systems with actuated joints. Furthermore, with the help of the direct correction method that minimizes both geometric and energy constraint violations, errors that arise from numerical integration are corrected on the position and velocity levels, thereby improving simulation accuracy.

The software is aimed at researchers familiar with tensegrity structures, but can be adopted by users from other backgrounds as well. However, users are required to have a preliminary understanding of MATLAB's interface in order to make the best use of this package.

https://github.com/isrlab/stedy[+https://github.com/isrlab/stedy+]

http://joss.theoj.org/papers/932cce456c4dd1514e6c4e46d2088ad7[+http://joss.theoj.org/papers/932cce456c4dd1514e6c4e46d2088ad7+]

StereoPipeline
~~~~~~~~~~~~~~

The NASA Ames Stereo Pipeline (ASP) is a suite of free and open source
automated geodesy and stereogrammetry tools designed for processing
stereo imagery captured from satellites (around Earth and other
planets), robotic rovers, aerial cameras, and historical imagery, with
and without accurate camera pose information. It produces cartographic
products, including digital elevation models (DEMs), ortho-projected
imagery, 3D models, and bundle-adjusted networks of cameras. ASP's
data products are suitable for science analysis, mission planning, and
public outreach.

Please install USGS ISIS version 3.5.2 if you would like to process
NASA non-terrestrial imagery. Users wishing to process Earth images,
such as Digital Globe, satellites with RPC cameras, or various
frame/pinhole cameras do not need to download anything else.

https://github.com/NeoGeographyToolkit/StereoPipeline[+https://github.com/NeoGeographyToolkit/StereoPipeline+]

STNG
~~~~

A compiler for Fortran stencils using verified lifting.
STNG consists of the following components:

* a frontend parser that parses the input Fortran code, identifies potential stencils, and generates input for the synthesizer.
* a synthesizer that attempts to lift each of the identified stencil to a high-level summary that can be re-targeted to execute on GPUs using Halide.
* a backend that generates Halide code after a summary has been found.

High-performance codes are prevalent in many domains (image processing, physical simulations, etc). These codes are often manually tuned over long periods of time for performance. Unfortunately, newly emerged architectures and software frameworks render such tuning obsolete, and rewriting legacy codes, either manually for each application or by implementing a new compiler, to leverage the new features offered by the new architectures is a big pain.

Verified lifting is a new technique that automatically rewrites input codes from source to target language. Rather than manually writing rules to translate source language constructs to the target language, verified lifting uses program synthesis to automatically search for rewrites in the target language given a piece of input code. Any found candidate is formally verified using theorem provers to guarantee that it preserves the same program semantics as the original.

Stng is a compiler built using verified lifting. It takes in input codes in Fortran and automatically rewrites them to Halide, a new high-performance domain-specific language that leverages GPUs for computation. We have used Stng to compile real-world high-performance applications consisting of stencil computations and the resulting applications achieved median performance speedups of 4.1x and up to 24x as compared to the original. 

https://github.com/uwplse/stng[+https://github.com/uwplse/stng+]

http://halide-lang.org/[+http://halide-lang.org/+]

http://stng.uwplse.org/[+http://stng.uwplse.org/+]

StochOpt.jl
~~~~~~~~~~~

A suite of stochastic optimization methods for solving the empirical risk minimization problem. 
The goal is to provide en enviroment where competing stochastic methods can be compared on equal footing. This is why all methods are called by the same wrapper function "minimizeFunc" (or it's extension minimizeFunc_grid_stepsize). All performance measures such as time taken, test error or epochs are calculated by these wrapper functions. Each new method need only supply a stepmethod and a bootmethod. The stepmethod returns an update vector d which is then added to x_k to give the next iterate x_{k+1}. The bootmethod is called once to initialize the method.

The methods implemented are:

* SVRG, the original SVRG algorithm;
* SVRG2, which tracks the gradients using the full Hessian.
* 2D, which tracks the gradients using the diagonal of the Hessian.
* 2Dsec, which tracks the gradients using the robust secant equation.
* SVRG2emb, which tracks the gradients using a low-rank approximation of the Hessians.
* CM, which tracks the gradients using the low-rank curvature matching approximation of the Hessian
* AM, which uses the low-rank action matching approximation of the Hessian.
* BFGS, the standard, full memory BFGS method.
* BFGS_accel, an accelerated BFGS method.
* SAGA, stochastic average gradient descent, with several options of samplings (including optimal probabilities)

https://github.com/gowerrobert/StochOpt.jl[+https://github.com/gowerrobert/StochOpt.jl+]

https://arxiv.org/abs/1805.02632[+https://arxiv.org/abs/1805.02632+]

https://arxiv.org/abs/1802.04079[+https://arxiv.org/abs/1802.04079+]

STOQS
~~~~~

STOQS (Spatial Temporal Oceanographic Query System) is a geospatial database software package designed for providing efficient access to in situ oceanographic measurement data. This efficient data access capability enables development of advanced visualization and analysis tools. One of them in wide use is a web-based user interface that follows Ben Shneiderman’s mantra (Overview first, zoom and filter, then details-on-demand) for exploring collections of observational data.

STOQS complements other data management technologies such as NetCDF and OPeNDAP by providing an ability to index data retrieval across parameter and spatial dimensions in addition to the a priori indexed coordinate dimensions of CF-NetCDF. It also provides a functional bridge between NetCDF and Geographic Information Systems technologies.

After installation, data is loaded into a STOQS database from a variety of data sources, including OPeNDAP data sets, other relational databases, and flat files. Products are delivered in numerous formats, including KML, X3D, WMS, CSV, and HTML via REST-style web requests. Programmatic access is possible through direct SQL queries. STOQS is used at the Monterey Bay Aquarium Research Institute for data management, visualization, and analysis of a wide assortment of in situ measurement data. 

https://www.stoqs.org/[+https://www.stoqs.org/+]

https://github.com/stoqs/stoqs[+https://github.com/stoqs/stoqs+]

https://github.com/stoqs/stoqs/wiki[+https://github.com/stoqs/stoqs/wiki+]

http://stoqs.mbari.org:8000/[+http://stoqs.mbari.org:8000/+]

https://hub.docker.com/r/mbarimike/stoqs[+https://hub.docker.com/r/mbarimike/stoqs+]

Storm
~~~~~

Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!

Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate.

Storm integrates with the queueing and database technologies you already use. A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.

http://storm.apache.org/[+http://storm.apache.org/+]

Streamz
~~~~~~~

Streamz helps you build pipelines to manage continuous streams of data. It is simple to use in simple cases, but also supports complex pipelines that involve branching, joining, flow control, feedback, back pressure, and so on.

Optionally, Streamz can also work with Pandas dataframes to provide sensible streaming operations on continuous tabular data.

https://streamz.readthedocs.io/en/latest/[+https://streamz.readthedocs.io/en/latest/+]

https://github.com/mrocklin/streamz[+https://github.com/mrocklin/streamz+]

STRUMPACK
~~~~~~~~~

STRUMPACK – STRUctured Matrix PACKage – is a Cxx library for computations with dense and sparse matrices. It uses so-called structured matrices, i.e., matrices that exhibit some kind of low-rank property, in this case with Hierarchically Semi-Separable matrices (HSS), to speed up linear algebra operations. This version of STRUMPACK unifies two main components that were separate in previous versions: a package for dense matrix computations (STRUMPACK-dense) and a package (STRUMPACK-sparse) for sparse linear systems. The algorithms for solving dense linear systems are described in [7] while the algorithms for solving sparse linear systems are described in [5,4].

STRUMPACK can be used as a general algebraic sparse direct solver (based on the multifrontal factorization method), or as an efficient preconditioner for sparse matrices obtained by discretization of partial differential equations. Included in STRUMPACK are also the GMRES and BiCGStab iterative Krylov solvers, that use the approximate, HSS-accelerated, sparse solver as a preconditioner for the efficient solution of sparse linear systems.

http://portal.nersc.gov/project/sparse/strumpack/master/[+http://portal.nersc.gov/project/sparse/strumpack/master/+]

https://github.com/pghysels/STRUMPACK[+https://github.com/pghysels/STRUMPACK+]

Substance
~~~~~~~~~

Substance is a library for creating web-based WYSIWYG editors with all the features you would expect.

As opposed to other existing editors, such as TinyMCE, Aloha etc. Substance is not just a widget you include into your web app. It is a library. Widgets are often not good enough. They lead to a bad UX. They are like alien isles within the web-app. And those are very limited regarding customization.

The unique point of Substance is Customizability. You can customize everything. And we make this as simple as possible for you.

https://medium.com/@_mql/build-your-own-editor-with-substance-7790eb600109[+https://medium.com/@_mql/build-your-own-editor-with-substance-7790eb600109+]

archivist
^^^^^^^^^

Archivist is a full-stack publishing solution involving different technologies to power digital archives.  They are published as full transcriptions
complete with multimedia sources (audio, video). Editors are able to tag and link subjects, locations, persons and definitions in the text, so that the archive can be queried later in interesting ways. Researchers are able to perform a full-text search, but also filter interviews by related subjects and external entities. 

https://github.com/archivist/archivist[+https://github.com/archivist/archivist+]

https://medium.com/@_daniel/publish-interactive-historical-documents-with-archivist-7019f6408ee6[+https://medium.com/@_daniel/publish-interactive-historical-documents-with-archivist-7019f6408ee6+]

Lens
^^^^

eLife Lens makes using scientific articles easier by making it possible to explore figures, figure descriptions, references and more – without losing your place in the article text. While most online research articles simply replicate print, eLife Lens takes full advantage of the Internet’s flexibility. You can absorb key elements in an important paper more readily, more quickly, and more effectively.

What is truly awesome about this tool is that, with it, eLife and our partners have broken out of the mould of replicating the print version of a research article online. Instead we have treated the document as data, and used the tools the web provides to make the links that exist within a research article really easy to navigate. It helps solve the problem of only being able to see a small part of an article at any one moment – one reason why many people still print out articles to read on paper.

https://github.com/elifesciences/lens[+https://github.com/elifesciences/lens+]

http://lens.elifesciences.org/[+http://lens.elifesciences.org/+]

https://elifesciences.org/inside-elife/0414db99/seeing-through-the-elife-lens-a-new-way-to-view-research[+https://elifesciences.org/inside-elife/0414db99/seeing-through-the-elife-lens-a-new-way-to-view-research+]

Stencila
^^^^^^^^

Stencila provides a set of open-source software components enabling reproducible research within the tool of your choice. Stencila allows you to write reproducible documents containing interactive source code using the interfaces you are most familiar with. Stencila components can be combined in various ways and plugged into existing reproducible infrastructure.

Stencila execution engine is its heart. The engine enables a spreadsheet-like, reactive programming model. It maintains a graph of the dependency between code cells. This means that as you change your code or data, all other parts of your document depending on them get updated as well. Stencila execution engine does not itself execute the code embedded in the documents but instead sends it to the execution contexts.

The execution contexts compile and execute code from the code cells that the execution engine passes on. This means that Stencila can be used for practically any programming language as the execution contexts can be developed using Stencila API. Currently we provide execution contexts for R, Python, JavaScript, SQL and Mini (Stencila's own simple language).

The execution contexts determine the inputs and outputs of cells which enables automatic conversion of data between languages. In other words, you can combine of multiple languages in one document.

Stencila comes with its own simple interface, Stencila Sheet, which can be used similarly to a typical spreadsheet application.
In other words, Stencila Sheets (and in the future, the plugins) allow for execution of R, Python, SQL (and more languages) code via special “code-snippet” functions e.g. r, py, sql. This feature will help researchers working in spreadsheet make their work more robust, easier to test and collaborate on.

Stencila includes a set of import/export converters which allow you to convert between a range of formats commonly used for among researchers (and not only). Converters support lossless conversion of interactive source code sections and (most of the time) formatting.

https://stenci.la/[+https://stenci.la/+]

https://github.com/stencila/[+https://github.com/stencila/+]

https://stenci.la/use/install.html[+https://stenci.la/use/install.html+]

SuiteSparse
~~~~~~~~~~~

SuiteSparse is a suite of sparse matrix algorithms, including:

* GraphBLAS: graph algorithms in the language of linear algebra
* Mongoose: graph partitioning
* ssget: MATLAB and Java interface to the SuiteSparse Matrix Collection
* UMFPACK: multifrontal LU factorization.  Appears as LU and x=A\b in MATLAB.
* CHOLMOD: supernodal Cholesky.  Appears as CHOL and x=A\b in MATLAB.  Now with CUDA acceleration, in collaboration with NVIDIA.
* SPQR: multifrontal QR.  Appears as QR and x=A\b in MATLAB, with CUDA acceleration.
* KLU and BTF:  sparse LU factorization, well-suited for circuit simulation. 
* Ordering methods (AMD, CAMD, COLAMD, and CCOLAMD).  AMD and COLAMD appear in MATLAB.
* CSparse and CXSparse: a concise sparse Cholesky factorization package for my SIAM book.
* spqr_rank: a MATLAB package for reliable sparse rank detection, null set bases, pseudoinverse solutions, and basic solutions.
* Factorize: an object-oriented solver for MATLAB (a reusable backslash).
* SSMULT and SFMULT: sparse matrix multiplication.  Appears as the built-in C=A*B operator in MATLAB.

http://faculty.cse.tamu.edu/davis/suitesparse.html[+http://faculty.cse.tamu.edu/davis/suitesparse.html+]

SUMMA
~~~~~

A hydrologic modeling framework that can be used for the systematic analysis of alternative model conceptualizations with respect to flux parameterizations, spatial configurations, and numerical solution techniques. It can be used to configure a wide range of hydrological model alternatives and we anticipate that systematic model analysis will help researchers and practitioners understand reasons for inter-model differences in model behavior. When applied across a large sample of catchments, SUMMA may provide insights in the dominance of different physical processes and regional variability in the suitability of different modeling approaches. An important application of SUMMA is selecting specific physics options to reproduce the behavior of existing models – these applications of "model mimicry" can be used to define reference (benchmark) cases in structured model comparison experiments, and can help diagnose weaknesses of individual models in different hydroclimatic regimes.

SUMMA is built on a common set of conservation equations and a common numerical solver, which together constitute the “structural core” of the model. Different modeling approaches can then be implemented within the structural core, enabling a controlled and systematic analysis of alternative modeling options, and providing insight for future model development.

The important modeling features are:

* The formulation of the conservation model equations is cleanly separated from their numerical solution;
* Different model representations of physical processes (in particular, different flux parameterizations) can be used within a common set of conservation equations; and
* The physical processes can be organized in different spatial configurations, including model elements of different shape and connectivity (e.g., nested multi-scale grids and HRUs).

https://github.com/NCAR/summa[+https://github.com/NCAR/summa+]

https://summa.readthedocs.io/en/latest/[+https://summa.readthedocs.io/en/latest/+]

https://ral.ucar.edu/projects/summa[+https://ral.ucar.edu/projects/summa+]

http://opensky.ucar.edu/islandora/object/technotes:526[+http://opensky.ucar.edu/islandora/object/technotes:526+]

SUNDIALS
~~~~~~~~

SUNDIALS is a SUite of Nonlinear and DIfferential/ALgebraic equation Solvers.  It consists of the following six solvers: CVODE, solves initial value problems for ordinary differential equation (ODE) systems; CVODES, solves ODE systems and includes sensitivity analysis capabilities (forward and adjoint); ARKODE, solves initial value ODE problems with additive Runge-Kutta methods, include support for IMEX methods; IDA, solves initial value problems for differential-algebraic equation (DAE) systems; IDAS, solves DAE systems and includes sensitivity analysis capabilities (forward and adjoint); KINSOL, solves nonlinear algebraic systems.

SUNDIALS is implemented with the goal of providing robust time integrators and nonlinear solvers that can easily be incorporated into existing simulation codes. The primary design goals are to require minimal information from the user, allow users to easily supply their own data structures underneath the packages, and allow for easy incorporation of user-supplied linear solvers and preconditioners.

The main numerical operations performed in these codes are operations on data vectors, and the codes have been written in terms of interfaces to these vector operations. The result of this design is that users can relatively easily provide their own data structures to the solvers by telling the solver about their structures and providing the required operations on them. The codes also come with default vector structures with pre-defined operation implementations for serial, shared-memory parallel (openMP and PThreads), and distributed memory parallel (MPI) environments in case a user prefers not to supply their own structures. Wrappers for the hypre ParVector and a PETSc vector are also provided.  In addition, all parallelism is contained within specific vector operations (norms, dot products, etc.). No other operations within the solvers require knowledge of parallelism. Thus, using a solver in parallel consists of using a parallel vector implementation, either one provided with SUNDIALS or the user’s own parallel vector structure, underneath the solver. Hence, we do not make a distinction between parallel and serial versions of the codes.

https://computation.llnl.gov/projects/sundials[+https://computation.llnl.gov/projects/sundials+]

CVODE
^^^^^

CVODE is a solver for stiff and nonstiff ordinary differential equation (ODE) systems (initial value problem) given in explicit form y’ = f(t,y). The methods used in CVODE are variable-order, variable-step multistep methods. For nonstiff problems, CVODE includes the Adams-Moulton formulas, with the order varying between 1 and 12. For stiff problems, CVODE includes the Backward Differentiation Formulas (BDFs) in so-called fixed-leading coefficient form, with order varying between 1 and 5. For either choice of formula, the resulting nonlinear system is solved (approximately) at each integration step. For this, CVODE offers the choice of either functional iteration, suitable only for nonstiff systems, and various versions of Newton iteration. 

In the cases of a direct linear solver (dense or banded), the Newton iteration is a Modified Newton iteration, in that the Jacobian is fixed (and usually out of date). When using a Krylov method as the linear solver, the iteration is an Inexact Newton iteration, using the current Jacobian (through matrix-free products), in which the linear residual is nonzero but controlled. The implicit nonlinear systems within implicit integrators are solved approximately at each integration step using a modified Newton method, an Inexact Newton method, or fixed-point solver (functional iteration). 

For the Newton-based methods and the serial or threaded NVECTOR modules in SUNDIALS, CVODE provides both direct (dense, band, or sparse) and preconditioned Krylov iterative (GMRES, BiCGStab, TFQMR) linear solvers. When used with one of the distributed parallel NVECTOR modules, including PETSc and hypre vectors, or a user-provided vector data structure, only the Krylov solvers are available, although a user may supply their own linear solver for any data structures if desired.  For the serial vector structure, there is a banded preconditioner module called CVBANDPRE for use with the Krylov solvers, while for the distributed memory parallel structure there is a preconditioner module called CVBBDPRE which provides a band-block-diagonal preconditioner.

For use with Fortran applications, a set of Fortran/C interface routines, called FCVODE, is also supplied. These are written in C, but assume that the user calling program and all user-supplied routines are in Fortran.

https://computation.llnl.gov/projects/sundials/cvode[+https://computation.llnl.gov/projects/sundials/cvode+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

CVODES
^^^^^^

CVODES is a solver for stiff and nonstiff ODE systems (initial value problem) given in explicit form y’ = f(t,y,p) with sensitivity analysis capabilities (both forward and adjoint modes). CVODES is a superset of CVODE and hence all options available to CVODE (with the exception of the FCVODE interface module) are also available for CVODES. Both integration methods (Adams-Moulton and BDF) and the corresponding nonlinear iteration methods, as well as all linear solver and preconditioner modules, are available for the integration of the original ODEs, the sensitivity systems, or the adjoint system. 

Depending on the number of model parameters and the number of functional outputs, one of two sensitivity methods is more appropriate. The forward sensitivity analysis (FSA) method is mostly suitable when the gradients of many outputs (for example the entire solution vector) with respect to relatively few parameters are needed. In this approach, the model is differentiated with respect to each parameter in turn to yield an additional system of the same size as the original one, the result of which is the solution sensitivity. The gradient of any output function depending on the solution can then be directly obtained from these sensitivities by applying the chain rule of differentiation. 

The adjoint sensitivity analysis (ASA) method is more practical than the forward approach when the number of parameters is large and the gradients of only few output functionals are needed. In this approach, the solution sensitivities need not be computed explicitly. Instead, for each output functional of interest, an additional system, adjoint to the original one, is formed and solved. The solution of the adjoint system can then be used to evaluate the gradient of the output functional with respect to any set of model parameters. 

The FSA module in CVODES implements a simultaneous corrector method as well as two flavors of staggered corrector methods–for the case when sensitivity right hand sides are generated all at once or separated for each model parameter. The ASA module provides the infrastructure required for the backward integration in time of systems of differential equations dependent on the solution of the original ODEs. It employs a checkpointing scheme for efficient interpolation of forward solutions during the backward integration.

https://computation.llnl.gov/projects/sundials/cvodes[+https://computation.llnl.gov/projects/sundials/cvodes+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

ARKode
^^^^^^

ARKode is a solver library that provides adaptive-step time integration of the initial value problem for systems of stiff, nonstiff, and multi-rate systems of ordinary differential equations (ODEs) given in linearly implicit form M y’ = fE(t,y) + fI(t,y), where M is a given nonsingular matrix (possibly time dependent). The right-hand side function is partitioned into two components–fE(t,y), containing the “slow” time scale components to be integrated explicitly, and fI(t,y), containing the “fast” time scale components to be integrated implicitly.

The methods used in ARKode are adaptive-step additive Runge Kutta methods, defined by combining two complementary Runge-Kutta methods–one explicit (ERK) and the other diagonally implicit (DIRK). Only the components in fI(t,y) must be solved implicitly, allowing for splittings tuned for use with optimal implicit solvers. ARKode is packaged with a wide array of built-in methods, including adaptive explicit methods of orders 2-6, adaptive implicit methods of orders 2-5, and adaptive implicit-explicit (IMEX) methods of orders 3-5. 

The implicit nonlinear systems within implicit integrators are solved approximately at each integration step using a modified Newton method, an Inexact Newton method, or an accelerated fixed-point solver. For the Newton-based methods and the serial or threaded NVECTOR modules in SUNDIALS, ARKode provides both direct (dense, band, or sparse) and preconditioned Krylov iterative (GMRES, BiCGStab, TFQMR, FGMRES, PCG) linear solvers.

When used with one of the distributed parallel NVECTOR modules, including PETSc and hypre vectors, or a user-provided vector data structure, only the Krylov solvers are available, although a user may supply their own linear solver for any data structures if desired.  For the serial vector structure, there is a banded preconditioner module called ARKBANDPRE for use with the Krylov solvers, while for the distributed memory parallel structure there is a preconditioner module called ARKBBDPRE which provides a band-block-diagonal preconditioner.

For use with Fortran applications, a set of Fortran/C interface routines, called FARKode, is also supplied. These are written in C, but assume that the user calling program and all user-supplied routines are in Fortran.

https://computation.llnl.gov/projects/sundials/arkode[+https://computation.llnl.gov/projects/sundials/arkode+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

IDA
^^^

IDA is a package for the solution of differential-algebraic equation (DAE) systems in the form F(t,y,y’)=0. It is written in C, but derived from the package DASPK which is written in Fortran. The integration method in IDA is variable-order, variable-coefficient BDF in fixed-leading-coefficient form. The method order varies between 1 and 5. The solution of the resulting nonlinear system is accomplished with some form of Newton iteration. In the cases of a direct linear solver (dense, banded, or sparse), the nonlinear iteration is a Modified Newton iteration, in that the Jacobian is fixed (and usually out of date). When using any of the Krylov methods as the linear solver, the iteration is an Inexact Newton iteration, using the current Jacobian (through matrix-free products), in which the linear residual is nonzero but controlled.

The implicit nonlinear systems within implicit integrators are solved approximately at each integration step using a modified Newton method, an Inexact Newton method, or a fixed-point solver (functional iteration). For the Newton-based methods and the serial or threaded NVECTOR modules in SUNDIALS, IDA provides both direct (dense, band, or sparse) and preconditioned Krylov iterative (GMRES, BiCGStab, TFQMR) linear solvers. When used with one of the distributed parallel NVECTOR modules, including PETSc and hypre vectors, or a user-provided vector data structure, only the Krylov solvers are available, although a user may supply their own linear solver for any data structures if desired.  In addition to the basic Krylov method modules, the IDA package also contains a preconditioner module called IDABBDPRE, which provides a band-block-diagonal preconditioner for use with the distributed memory parallel vector.

For use with Fortran applications, a set of Fortran/C interface routines, called FIDA, is also supplied. These are written in C, but assume that the user calling program and all user-supplied routines are in Fortran.

https://computation.llnl.gov/projects/sundials/ida[+https://computation.llnl.gov/projects/sundials/ida+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

IDAS
^^^^

IDAS is a package for the solution of differential-algebraic equation (DAE) systems in the form F(t,y,y’,p)=0 with sensitivity analysis capabilities (both forward and adjoint modes).

IDAS is a superset of IDA and hence all options available to IDA (with the exception of the FIDA interface module) are also available for IDAS.

Depending on the number of model parameters and the number of functional outputs, one of two sensitivity methods is more appropriate. The forward sensitivity analysis (FSA) method is mostly suitable when the gradients of many outputs (for example the entire solution vector) with respect to relatively few parameters are needed. In this approach, the model is differentiated with respect to each parameter in turn to yield an additional system of the same size as the original one, the result of which is the solution sensitivity. The gradient of any output function depending on the solution can then be directly obtained from these sensitivities by applying the chain rule of differentiation.

The adjoint sensitivity analysis (ASA) method is more practical than the forward approach when the number of parameters is large and the gradients of only few output functionals are needed. In this approach, the solution sensitivities need not be computed explicitly. Instead, for each output functional of interest, an additional system, adjoint to the original one, is formed and solved. The solution of the adjoint system can then be used to evaluate the gradient of the output functional with respect to any set of model parameters.

The FSA module in IDAS offers the choice between a simultaneous corrector method and a staggered corrector method. The ASA module provides the infrastructure required for the backward integration in time of systems of differential-algebraic equations dependent on the solution of the original DAEs. It employs a checkpointing scheme for efficient interpolation of forward solutions during the backward integration.

https://computation.llnl.gov/projects/sundials/idas[+https://computation.llnl.gov/projects/sundials/idas+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

KINSOL
^^^^^^

KINSOL is a solver for nonlinear algebraic systems. It includes a Newton-Krylov solver as well as Picard and fixed point solvers, both of which can be accelerated with Anderson acceleration. KINSOL is based on the previous Fortran package NKSOL of Brown and Saad.

KINSOL’s Newton solver employs the Inexact Newton method. As this solver is intended mainly for large systems, four iterative methods are provided to solve the resulting linear systems–GMRES, Bi-CGStab, TFQMR, and FGMRES. These are Krylov methods, implemented with scaling and preconditioning, and can be used with all versions of the NVECTOR module.

For the sake of convenience to users with smaller systems, KINSOL (used with the serial NVECTOR module) also includes direct (dense, banded, and sparse) linear solvers for the linear systems. In this case the nonlinear iteration is a Modified Newton method.

In addition, KINSOL (used with the serial NVECTOR module) also includes interfaces to the sparse direct solvers, KLU, and the the multi-threaded sparse solver, SuperLU_MT.

In addition to the basic Krylov method modules, the KINSOL package includes a module called KINBBDPRE, which provides a band-block-diagonal preconditioner for the native MPI parallel vector.

For use with Fortran applications, a set of Fortran/C interface routines, called FKINSOL, is also supplied. These are written in C, but assume that the user calling program and all user-supplied routines are in Fortran.

https://computation.llnl.gov/projects/sundials/kinsol[+https://computation.llnl.gov/projects/sundials/kinsol+]

https://computation.llnl.gov/projects/sundials/sundials-software[+https://computation.llnl.gov/projects/sundials/sundials-software+]

SuperCollider
~~~~~~~~~~~~~

SuperCollider is a platform for audio synthesis and algorithmic composition, used by musicians, artists, and researchers working with sound. It is free and open source software available for Windows, macOS, and Linux.

SuperCollider features three major components:

* *scsynth*, a real-time audio server, forms the core of the platform. It features 400+ unit generators (“UGens”) for analysis, synthesis, and processing. Its granularity allows the fluid combination of many known and unknown audio techniques, moving between additive and subtractive synthesis, FM, granular synthesis, FFT, and physical modeling. You can write your own UGens in Cxx, and users have already contributed several hundred more to the sc3-plugins repository.
* *sclang*, an interpreted programming language. It is focused on sound, but not limited to any specific domain. sclang controls scsynth via Open Sound Control. You can use it for algorithmic composition and sequencing, finding new sound synthesis methods, connecting your app to external hardware including MIDI controllers, network music, writing GUIs and visual displays, or for your daily programming experiments. It has a stock of user-contributed extensions called Quarks.
* *scide* is an editor for sclang with an integrated help system.

SuperCollider was developed by James McCartney and originally released in 1996. In 2002, he generously released it as free software under the GNU General Public License. It is now maintained and developed by an active and enthusiastic community.

https://supercollider.github.io/[+https://supercollider.github.io/+]

https://en.wikipedia.org/wiki/SuperCollider[+https://en.wikipedia.org/wiki/SuperCollider+]

[[SuperLU]]
SuperLU
~~~~~~

A  general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations on high performance machines. The library is written in C and is callable from either C or Fortran. The library routines will perform an LU decomposition with partial pivoting and triangular system solves through forward and back substitution. The LU factorization routines can handle non-square matrices but the triangular solves are performed only for square matrices. The matrix columns may be preordered (before factorization) either through library or user supplied routines. This preordering for sparsity is completely separate from the factorization. Working precision iterative refinement subroutines are provided for improved backward stability. Routines are also provided to equilibrate the system, estimate the condition number, calculate the relative backward error, and estimate error bounds for the refined solutions.

SuperLU is available in three different versions:

* *SuperLU* for sequential machines;
* *SuperLU_MT* for shared memory parallel machines; and
* *SuperLU_DIST* for distributed memory parallel machines.

http://crd-legacy.lbl.gov/\~xiaoye/SuperLU/[+http://crd-legacy.lbl.gov/~xiaoye/SuperLU/+]

https://github.com/xiaoyeli/superlu[+https://github.com/xiaoyeli/superlu+]

https://github.com/xiaoyeli/superlu_dist[+https://github.com/xiaoyeli/superlu_dist+]

https://crd-legacy.lbl.gov/~xiaoye/SuperLU/ug.pdf[+https://crd-legacy.lbl.gov/~xiaoye/SuperLU/ug.pdf+]

SWMM
~~~~

EPA's Stormwater Management Model (SWMM) is used for single event or long-term simulations of water runoff quantity and quality in primarily urban areas–although there are also many applications that can be used for drainage systems in non-urban areas. It is used throughout the world for planning, analysis, and design related to stormwater runoff, combined and sanitary sewers, and other drainage systems. SWMM was developed to help support local, state, and national stormwater management objectives to reduce runoff through infiltration and retention, and help to reduce discharges that cause impairment of our Nation’s waterbodies.

SWMM has undergone several major upgrades since it was first developed in 1971, including the addition green infrastructure practices as low impact development (LID) controls. It is widely used to evaluate gray infrastructure stormwater control strategies, such as pipes and storm drains, and is a useful tool for creating cost effective green/gray hybrid stormwater control solutions.

SWMM contains a flexible set of hydraulic modeling capabilities used to route runoff and external inflows through the drainage system network of pipes, channels, storage/treatment units and diversion structures. These include the ability to:

* Handle drainage networks of unlimited size.
* Use a wide variety of standard closed and open conduit shapes as well as natural channels.
* Model special elements, such as storage/treatment units, flow dividers, pumps, weirs, and orifices.
* Apply external flows and water quality inputs from surface runoff, groundwater interflow, rainfall-dependent infiltration/inflow, dry weather sanitary flow, and user-defined inflows.
* Utilize either kinematic wave or full dynamic wave flow routing methods.
* Model various flow regimes, such as backwater, surcharging, reverse flow, and surface ponding. apply user-defined dynamic control rules to simulate the operation of pumps, orifice openings, and weir crest levels.
* Percolation of infiltrated water into groundwater layers.
* Interflow between groundwater and the drainage system.
* Nonlinear reservoir routing of overland flow. Runoff reduction via LID controls.

SWMM accounts for various hydrologic processes that produce runoff from urban areas, which include:

* Runoff reduction via green infrastructure practices.
* Time-varying rainfall (precipitation) and evaporation of standing surface water.
* Snow accumulation and melting.
* Rainfall interception from depression storage.
* Infiltration of rainfall into unsaturated soil layers.
* Percolation of infiltrated water into groundwater layers Interflow between groundwater and the drainage system.
* Nonlinear reservoir routing of overland flow.

Spatial variability in all of these processes is achieved by dividing a study area into a collection of smaller, homogeneous sub-catchment areas. Each of the areas contains its own fraction of pervious and impervious sub-areas. Overland flow can be routed between sub-areas, between sub-catchments, or between entry points of a drainage system.

The source codes for the SWWM computational engine and GUI are freely available.

https://www.epa.gov/water-research/storm-water-management-model-swmm[+https://www.epa.gov/water-research/storm-water-management-model-swmm+]

GisToSWMM5
^^^^^^^^^^

GisToSWMM5 is a tool for automatically constructing SWMM5 model descriptions.

The input files for the tool can be prepared using GIS software and the resulting SWMM model can be studied in GIS. The tool takes elevation, land-use, and flow direction information from the user-prepared input files, creates subcatchments for the studied area, and routes water between subcatchments and into the stormwater network.

https://github.com/AaltoUrbanWater/GisToSWMM5/[+https://github.com/AaltoUrbanWater/GisToSWMM5/+]

SYCL
~~~~

SYCL (pronounced ‘sickle’) is a royalty-free, cross-platform abstraction layer that builds on the underlying concepts, portability and efficiency of OpenCL that enables code for heterogeneous processors to be written in a “single-source” style using completely standard Cxx. SYCL single-source programming enables the host and kernel code for an application to be contained in the same source file, in a type-safe way and with the simplicity of a cross-platform asynchronous task graph. SYCL includes templates and generic lambda functions to enable higher-level application software to be cleanly coded with optimized acceleration of kernel code across the extensive range of shipping OpenCL 1.2 implementations. Developers program at a higher level than OpenCL C or Cxx, but always have access to lower-level code through seamless integration with OpenCL, C/Cxx libraries, and frameworks such as OpenCV™ or OpenMP™.

https://www.khronos.org/sycl/[+https://www.khronos.org/sycl/+]

ComputeCpp
^^^^^^^^^^

ComputeCpp, Codeplay's implementation of the open standard SYCL, enables you to integrate parallel computing into your application and accelerate your code using OpenCL devices such as GPUs. Applications that require a large number of common operations can make huge performance improvements by running the operations in parallel on OpenCL devices. For example, the neural networks used in machine learning perform huge numbers of matrix calculations and ComputeCpp can be used to run these operations in parallel, vastly increasing performance and reducing the power used by the application.

With ComputeCpp and SYCL you can write code once and execute on a range of OpenCL enabled devices reducing your development effort. Develop with standard Cxx and the SYCL open standard, re-using your existing Cxx libraries. ComputeCpp is also building support for Cxx17 Parallel STL enabling parallelized library functions to run on accelerated processors. ComputeCpp also works with a number of frameworks including ParallelSTL and VisionCpp.

https://www.codeplay.com/products/computesuite/computecpp[+https://www.codeplay.com/products/computesuite/computecpp+]

triSYCL
^^^^^^^

triSYCL is an open source implementation to experiment with the specification of the SYCL 1.2.1 Cxx layer and to give feedback to the Khronos Group SYCL and OpenCL Cxx 2.2 kernel language committees and also to the ISO Cxx committee.

This SYCL implementation is mainly based on Cxx17 and OpenMP or TBB for execution on the CPU, with Boost.Compute for the non single-source OpenCL interoperability layer and with LLVM/Clang for the device compiler providing full single-source SYCL experience, typically targeting a SPIR device. Since in SYCL there is a host fall-back, this CPU implementation can be seen as an implementation of this fall-back too.

https://github.com/triSYCL/triSYCL[+https://github.com/triSYCL/triSYCL+]

symPACK
~~~~~~~

symPACK is a package for the direct solution of sparse symmetric positive definite systems of linear equations. The routines perform an LL^T (or LDL^T without pivoting) decomposition and triangular solutions through forward and back substitutions. Capabilities include asynchronous task-based computation, matrix column reordering and relative backward error calculation.

symPACK is the default solver used in the preprocessing step of the Parallel Selected Inversion implemented in Pole EXpansion and Selected Inversion (PEXSI) package, which is included in multiple electronic structure calculation packages, such as SIESTA and CP2K. symPACK and PEXSI have recently been incorporated into the ELectronic Structure Infrastructure (ELSI) library, which is an NSF effort providing a common interface to multiple software libraries used in electronic structure calculations in materials science. It has been used by several DOE application codes, including DGDFT, and is used in the current BES SciDAC application partnership projects.

http://www.sympack.org/[+http://www.sympack.org/+]

https://github.com/symPACK/symPACK[+https://github.com/symPACK/symPACK+]

sympl
~~~~~

sympl is an open source project aims to enable researchers and other users to write understandable, modular, accessible Earth system and planetary models in Python. It is meant to be used in combination with other packages that provide model components in order to write model scripts. 

Traditional atmospheric and Earth system models can be hard to read and change for many reasons. Sympl tries to learn from the past experience of these models to speed up research and improve accessibility.

At its core, Sympl defines a model in terms of a state that gets changed in sequence by components of a model (like the radiation scheme, or dynamical core). Each of those components as well-defined and documented inputs and outputs, and code in Sympl will automatically handle unit and dimensionality conversions (such as dimension orderings) to give components the inputs they need.

Sympl defines a framework of Python object interfaces (APIs) that can be combined to create a model. This has many benefits:

* Objects can use code written in any language that can be called from Python, including Fortran, C, Cxx, Julia, Matlab, and others.
* Each object, such as a radiation parameterization, has a clearly documented interface and can be understood without looking at any other part of a model’s code. Certain interfaces have been designed to force model code to self-document, such as having inputs and outputs as properties of a scheme.
* Objects can be swapped out with other compatible objects. For example, Sympl makes it trivial to change the type of time stepping used on a prognostic scheme.
* Code can be re-used between different types of models. For instance, an atmospheric general circulation model, numerical weather prediction model, and large-eddy simulation could all use the same RRTM radiation object.
* Already-existing documentation for Sympl can tell your users how to configure and run your model. You will likely spend less time writing documentation, but end up with a better documented model. As long as you write docstrings, you’re good to go!

Sympl also contains a number of commonly used objects, such as time steppers and NetCDF output objects.

ympl is not a model itself. In particular, physical parameterizations and dynamical cores are not present in Sympl. This code instead can be found in other projects that make use of Sympl.

Sympl is meant to be a community ecosystem that allows researchers and other users to use and combine components from a number of different sources. By keeping model physics/dynamics code outside of Sympl itself, researchers can own and maintain their own models. The framework API ensures that models using Sympl are clear and accessible, and allows components from different models and packages to be used alongside one another.

https://github.com/mcgibbon/sympl[+https://github.com/mcgibbon/sympl+]

https://sympl.readthedocs.io/en/latest/[+https://sympl.readthedocs.io/en/latest/+]

https://sympl.readthedocs.io/en/latest/overview.html[+https://sympl.readthedocs.io/en/latest/overview.html+]

https://github.com/climt/climt[+https://github.com/climt/climt+]

https://www.geosci-model-dev.net/11/3781/2018/[+https://www.geosci-model-dev.net/11/3781/2018/+]

SymPy
~~~~~

SymPy is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python. 

SymPy is trivial to install and to inspect because it is written entirely in Python with few dependencies.[2][3][4] This ease of access combined with a simple and extensible code base in a well known language make SymPy a computer algebra system with a relatively low barrier to entry.

https://en.wikipedia.org/wiki/SymPy[+https://en.wikipedia.org/wiki/SymPy+]

https://www.sympy.org/en/index.html[+https://www.sympy.org/en/index.html+]

#TTTT

Tablib
~~~~~~

Tablib is an MIT Licensed format-agnostic tabular dataset library, written in Python. It allows you to import, export, and manipulate tabular data sets. Advanced features include, segregation, dynamic columns, tags & filtering, and seamless format import & export.

http://docs.python-tablib.org/en/latest/[+http://docs.python-tablib.org/en/latest/+]

https://github.com/kennethreitz/records[+https://github.com/kennethreitz/records

taco
~~~~

The Tensor Algebra Compiler (taco) is a Cxx library that computes tensor algebra expressions on sparse and dense tensors. It uses novel compiler techniques to get performance competitive with hand-optimized kernels in widely used libraries for both sparse tensor algebra and sparse linear algebra.

You can use taco as a Cxx library that lets you load tensors, read tensors from files, and compute tensor expressions. You can also use taco as a code generator that generates C functions that compute tensor expressions.

Under the hood, the taco library employs a novel compiler-based technique to generate kernels that are optimized for the computations you want to perform. This enables taco to achieve performance that exceeds the MATLAB Tensor Toolbox by up to several orders of magnitude and that is competitive with other high-performance sparse linear and tensor algebra libraries like Eigen, Intel MKL, and SPLATT.

With taco, you can define even complex tensor algebra computations on dense and sparse tensors in just a couple of lines of Cxx code using tensor index notation. The taco library takes care of generating the potentially very complicated kernels that are needed to perform your desired computations.

https://github.com/tensor-compiler/taco[+https://github.com/tensor-compiler/taco+]

https://tensor-compiler.org/[+https://tensor-compiler.org/+]

Tails
~~~~~

Tails or The Amnesic Incognito Live System is a security-focused Debian-based Linux distribution aimed at preserving privacy and anonymity.[5] All its incoming and outgoing connections are forced to go through Tor,[6] and any and all non-anonymous connections are blocked. The system is designed to be booted as a live DVD or live USB, and will leave no digital footprint on the machine unless explicitly told to do so. The Tor Project has provided financial support for its development.

https://tails.boum.org/[+https://tails.boum.org/+]

https://en.wikipedia.org/wiki/Tails_(operating_system)[+https://en.wikipedia.org/wiki/Tails_(operating_system)+]

TAL_SH
~~~~~~

Tensor algebra library routines for shared memory systems.  The TAL_SH library provides API for performing basic tensor algebra operations on multicore CPU, NVidia GPU, Intel Xeon Phi, and other accelerators.  Basic tensor algebra operations include tensor contraction, tensor product, tensor addition, tensor transpose, multiplication by a scalar, etc., which operate on locally stored tensors.  The execution of tensor operations on accelerators is asynchronous with respect to the CPU host, if the underlying node is heterogeneous.  Both Fortran and C/Cxx API interfaces are provided.  The library has a simplified object-oriented design, although without explicit object-oriented syntax.

https://github.com/DmitryLyakh/TAL_SH[+https://github.com/DmitryLyakh/TAL_SH+]

Tarantool
~~~~~~~~~

Tarantool is an in-memory database and application server.
The application server features include:

* 100% compatible drop-in replacement for Lua 5.1, based on LuaJIT 2.1. Simply use #!/usr/bin/tarantool instead of #!/usr/bin/lua in your script
* full support for Lua modules and a rich set of own modules, including cooperative multitasking, non-blocking I/O, access to external databases, etc

The features of the database include:

* MsgPack data format and MsgPack based client-server protocol
* two data engines: 100% in-memory with optional persistence and a 2-level disk-based B-tree, to use with large data sets
* multiple index types: HASH, TREE, RTREE, BITSET
* asynchronous master-master replication
* authentication and access control
* the database is just a C extension to the application server and can be turned off

Tarantool is ideal for data-enriched components of scalable Web architecture: queue servers, caches, stateful Web applications.

https://github.com/tarantool/tarantool[+https://github.com/tarantool/tarantool+]

https://tarantool.io/en/developers/[+https://tarantool.io/en/developers/+]

targetDP
~~~~~~~~

The targetDP programming model [1], used internally by (and developed in co-design with) Ludwig, is a simple performance portable framework that allows grid-based codes to perform well on modern multi/many-core CPUs as well as GPUs.

A lightweight abstraction layer which allows grid-based applications to target data parallel hardware in a platform agnostic manner. We demonstrate the effectiveness of our pragmatic approach by presenting performance results for a complex fluid application (with which the model was co-designed), plus separate lattice quantum chromodynamics particle physics code. For each application, a single source code base is seen to achieve portable performance, as assessed within the context of the Roofline model. TargetDP can be combined with Message Passing Interface (MPI) to allow use on systems containing multiple nodes: we demonstrate this through provision of scaling results on traditional and graphics processing unit-accelerated large scale supercomputers.

http://ludwig.epcc.ed.ac.uk/targetdp[+http://ludwig.epcc.ed.ac.uk/targetdp+]

https://journals-sagepub-com/doi/full/10.1177/1094342016682071[+https://journals-sagepub-com/doi/full/10.1177/1094342016682071+]

Tasmanian
~~~~~~~~~

The Toolkit for Adaptive Stochastic Modeling and Non-Intrusive ApproximatioN is a collection of robust libraries for high dimensional integration and interpolation as well as parameter calibration. The code consists of several modules that can be used individually or conjointly.

Sparse Grids is a family of algorithms for constructing multidimensional quadrature and interpolation rules using multiple tensor products of one dimensional rules with varying degree of precision. The Tasmanian Sparse Grids Module implements a variety of grids that fall into five major categories:

* Global grids use polynomials supported over the entire domain of integration or interpolation. Such grids are suitable for approximating functions with smooth response.
* Sequence grids work much like Global grids, but use optimized internal data-structures for rules that are based on sequences of points formed from solving a greedy optimization problem
* Local polynomial grids use hierarchical piece-wise polynomials with compact support. Such grids are suitable for approximating functions with localized sharp behavior.
* Wavelet grids use special functions that form a Riesz basis which allows for more accurate local error estimations. Compared to Local polynomial grids, the wavelet basis can provide similar accuracy with significantly fewer points, but the advantage of the Riesz basis could also be negated from the higher Lebesgue constant near the domain boundary.
* Fourier grids use trigonometric functions with varying frequency and rely on Discrete (complex) Fourier Transforms. Such grids are suitable for approximating periodic functions, since periodicity if preserved in the approximation.

The DiffeRential Evolution Adaptive Metropolis (DREAM) is a method to draw samples from an arbitrary probability distribution defined by an arbitrary non-negative function (not necessarily normalized to integrate to 1). The DREAM approach is similar to the classical Markov Chain Monte Carlo, but it evolves a large number of chains simultaneously which leads to better parallelization and (potentially) faster convergence. In addition, multiple chains allow for better exploration of the probability domain, which is often advantageous when working with multi-modal distributions.

One of the main applications of DREAM is in the field of Bayesian inference, where samples are drawn from a posterior distribution comprised from a data-informed likelihood and an arbitrary model. The DREAM module of Tasmanian can use Tasmanian Sparse Grids approximation to either the model or the likelihood.

https://github.com/ORNL/TASMANIAN[+https://github.com/ORNL/TASMANIAN+]

https://tasmanian.ornl.gov/[+https://tasmanian.ornl.gov/+]

TAU
~~~

TAU Performance System® is a portable profiling and tracing toolkit for performance analysis of parallel programs written in Fortran, C, Cxx, UPC, Java, Python.

TAU (Tuning and Analysis Utilities) is capable of gathering performance information through instrumentation of functions, methods, basic blocks, and statements as well as event-based sampling. All Cxx language features are supported including templates and namespaces. The API also provides selection of profiling groups for organizing and controlling instrumentation. The instrumentation can be inserted in the source code using an automatic instrumentor tool based on the Program Database Toolkit (PDT), dynamically using DyninstAPI, at runtime in the Java Virtual Machine, or manually using the instrumentation API.

TAU's profile visualization tool, paraprof, provides graphical displays of all the performance analysis results, in aggregate and single node/context/thread forms. The user can quickly identify sources of performance bottlenecks in the application using the graphical interface. In addition, TAU can generate event traces that can be displayed with the Vampir, Paraver or JumpShot trace visualization tools. 

http://www.cs.uoregon.edu/research/tau/home.php[+http://www.cs.uoregon.edu/research/tau/home.php+]

Tea
~~~

Though statistical analyses are centered on research questions and hypotheses, current statistical analysis tools are not. Users must first translate their hypotheses into specific statistical tests and then perform API calls with functions and parameters. To do so accurately requires that users have statistical expertise. To lower this barrier to valid, replicable statistical analysis, we introduce Tea, a high-level declarative language and runtime system. In Tea, users express their study design, any parametric assumptions, and their hypotheses. Tea compiles these high-level specifications into a constraint satisfaction problem that determines the set of valid statistical tests, and then executes them to test the hypothesis. We evaluate Tea using a suite of statistical analyses drawn from popular tutorials. We show that Tea generally matches the choices of experts while automatically switching to non-parametric tests when parametric assumptions are not met. We simulate the effect of mistakes made by non-expert users and show that Tea automatically avoids both false negatives and false positives that could be produced by the application of incorrect statistical tests. 

Tea is a domain specific language for expressing the assertions and intentions/goals for statistical analyses (e.g., to compare two groups on an outcome). The user provides a dataset (currently only CSV) and a experimental design specification. The Tea runtime system then translates these high-level expressions, calculates properties about the data, and translates these properties into constarints to find a set of valid statistical tests. Tea uses Z3 as its constraint solver.

https://github.com/emjun/tea-lang[+https://github.com/emjun/tea-lang+]

https://arxiv.org/abs/1904.05387[+https://arxiv.org/abs/1904.05387+]

https://github.com/Z3Prover/z3[+https://github.com/Z3Prover/z3+]

TECA
~~~~

TECA (Toolkit for Extreme Climate Analysis) is a collection of climate analysis algorithms geared toward extreme event detection and tracking implemented in a scalable parallel framework. The core is written in modern c++ and uses MPI+thread for parallelism. The framework supports a number of parallel design patterns including distributed data parallelism and map-reduce. Python bindings make the high performance c++ code easy to use. TECA has been used up to 750k cores.

https://github.com/LBL-EESA/TECA[+https://github.com/LBL-EESA/TECA+]

https://github.com/LBL-EESA/TECA/blob/master/doc/teca_users_guide.pdf[+https://github.com/LBL-EESA/TECA/blob/master/doc/teca_users_guide.pdf+]

https://github.com/LBL-EESA/TECA_superbuild[+https://github.com/LBL-EESA/TECA_superbuild+]


TELEMAC-MASCARET
~~~~~~~~~~~~~~~~

The TELEMAC-MASCARET system is a powerful integrated modeling tool for use in the field of free-surface flows. Having been used in the context of very many studies throughout the world (several hundred to date), it has become one of the major standards in its field.

The various simulation modules use high-capacity algorithms based on the finite-element method. Space is discretised in the form of an unstructured grid of triangular elements, which means that it can be refined particularly in areas of special interest. This avoids the need for systematic use of embedded models, as is the case with the finite-difference method.

All the numerical algorithms are gathered into a single library (BIEF) that is shared by all the simulation modules. This makes for consistency throughout the TELEMAC-MASCARET system.

The pre- and post-processing tools are particularly powerful and user-friendly. Most of them are based on the use of Ilog/Views libraries and offer a range of extremely sophisticated functions. The grid can be generated with the generator embedded in the TELEMAC-MASCARET system.

TELEMAC-MASCARET has numerous applications in both river and maritime hydraulics.
The hydrodynamics components are:

* ARTEMIS: Wave agitation in harbours
* MASCARET: One-dimensional flows
* TELEMAC-2D: Two-dimensional flows - Saint-Venant equations (including transport of a diluted tracer)
* TELEMAC-3D: Three-dimensional flows - Navier-Stokes equations (including transport of active or passive tracers)
* TOMAWAC: Wave propagation in the coastal zone

The transport/dispersion components are:

* SISYPHE: 2D sediment transport
* SEDI-3D: 3D suspended sediment transport
* NESTOR: Simulation of dredging operations

The pre- and post-processors are:

* STBTEL: Grid interface
* POSTEL-3D: 2D sections through the results of a 3D simulation

http://www.opentelemac.org/[+http://www.opentelemac.org/+]

http://www.opentelemac.org/index.php/installation[+http://www.opentelemac.org/index.php/installation+]

http://wiki.opentelemac.org/doku.php?id=installation_on_linux[+http://wiki.opentelemac.org/doku.php?id=installation_on_linux+]

TempestExtremes
~~~~~~~~~~~~~~~

A new open-source software framework for automated pointwise feature tracking that is applicable to a wide array of climate datasets using either structured or unstructured grids. Common climatological pointwise features include tropical cyclones, extratropical cyclones and tropical easterly waves. To enable support for a wide array of detection schemes, a suite of algorithmic kernels have been developed that capture the core functionality of algorithmic tracking routines throughout the literature.

Automated pointwise feature tracking is used for objective identification and tracking of meteorological features, such as extratropical cyclones, tropical cyclones and tropical easterly waves, and has emerged as an important and desirable data-processing capability in climate science. In the interest of exploring tracking functionality, this paper introduces a framework for the development of robust tracking algorithms that is useful for intercomparison and optimization of tracking schemes.

https://github.com/ClimateGlobalChange/tempestextremes[+https://github.com/ClimateGlobalChange/tempestextremes+]

https://www.geosci-model-dev.net/10/1069/2017/[+https://www.geosci-model-dev.net/10/1069/2017/+]

TempestRemap
~~~~~~~~~~~~

This software constitutes the beta release of TempestRemap, a conservative, consistent and monotone remapping package for arbitrary grid geometry with support for finite volumes and finite elements.

The remapping process requires multiple stages. First you will need an Exodus file (file extension .g) for your input mesh and your output mesh. This can either be done via the SQuadGen mesh utility, or via the three GenerateMesh executables that come with TempestRemap.

Once your input and output meshes are generated, you will need to generate the overlap mesh (that is, the mesh obtained by placing the input and output mesh overtop one another and recalculating intersections). 

Once the overlap mesh is generated, you can now generate the weight file, which the contains information on remapping from one mesh to the other.

https://github.com/ClimateGlobalChange/tempestremap[+https://github.com/ClimateGlobalChange/tempestremap+]

Temporal.jl
~~~~~~~~~~~

This package provides a flexible & efficient time series class, TS, for the Julia programming language. While still early in development, the overarching goal is for the class to be able to slice & dice data with the rapid prototyping speed of R's xts and Python's pandas packages, while retaining the performance one expects from Julia.

https://github.com/dysonance/Temporal.jl[+https://github.com/dysonance/Temporal.jl+]

tensor
~~~~~~

Blah.

*Tensor Decompositions and Applications* (46 pp.) - https://epubs.siam.org/doi/abs/10.1137/07070111X[+https://epubs.siam.org/doi/abs/10.1137/07070111X+]

*How to Conquer Tensorphobia* - https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/[+https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/+]

*A Gentle Introduction to Tensors for Machine Learning with NumPy* - https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/[+https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/+]

*A literature survey of low-rank tensor approximation techniques* (20 pp.) - https://arxiv.org/abs/1302.7121[+https://arxiv.org/abs/1302.7121+]

*Introduction to Tensor Decompositions and their Applications in Machine Learning* (13 pp.) - https://arxiv.org/abs/1711.10781[+https://arxiv.org/abs/1711.10781+]

*Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions* - https://arxiv.org/abs/1403.2048[+https://arxiv.org/abs/1403.2048+]

*Fundamental Tensor Operations for Large-Scale Data Analysis in Tensor Train Formats* (36 pp.) - https://arxiv.org/abs/1405.7786[+https://arxiv.org/abs/1405.7786+]

*Tensor Decompositions for Signal Processing Applications* (34 pp.) - https://arxiv.org/abs/1403.4462[+https://arxiv.org/abs/1403.4462+]

*Review of Tensor Network Contraction Approaches* - https://arxiv.org/abs/1708.09213[+https://arxiv.org/abs/1708.09213+]

*Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems*

* *Part 1 - Perspectives and Challenges* (176 pp.) - https://arxiv.org/abs/1609.00893[+https://arxiv.org/abs/1609.00893+]
* *Part 2 - Applications and Future Perspectives* (232 pp.) - https://arxiv.org/abs/1708.09165[+https://arxiv.org/abs/1708.09165+]

*Blind Source Separation: A Tutorial* - https://arxiv.org/abs/1603.03089[+https://arxiv.org/abs/1603.03089+]

*High dimensionality: The latest challenge to data analysis* - https://arxiv.org/abs/1902.04679[+https://arxiv.org/abs/1902.04679+]

*High-Dimensional Data Analysis : The Curses and Blessings of Dimensionality* - https://www.semanticscholar.org/paper/High-Dimensional-Data-Analysis-%3A-The-Curses-and-of-Piatetsky-Shapiro-Bosch/63c68278418b69f60b4814fae8dd15b1b1854295[+https://www.semanticscholar.org/paper/High-Dimensional-Data-Analysis-%3A-The-Curses-and-of-Piatetsky-Shapiro-Bosch/63c68278418b69f60b4814fae8dd15b1b1854295+]

*Hubs in space: Popular nearest neighbors in high-dimensional data* (45 pp.) - http://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf[+http://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf+]

*The Curse of Dimensionality* - https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335[+https://medium.freecodecamp.org/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335+]

*Blessing of dimensionality: mathematical foundations of the statistical physics of data* - https://arxiv.org/abs/1801.03421[+https://arxiv.org/abs/1801.03421+]

einops
^^^^^^

Deep learning operations reinvented (supports tf, pytorch, chainer, gluon and others).
Flexible and powerful tensor operations for readable and reliable code.

https://github.com/arogozhnikov/einops[+https://github.com/arogozhnikov/einops+]

NamedTensor
^^^^^^^^^^^

Despite its ubiquity in deep learning, Tensor is broken. It forces bad habits such as exposing private dimensions, broadcasting based on absolute position, and keeping type information in documentation. This post presents a proof-of-concept of an alternative approach, named tensors, with named dimensions. This change eliminates the need for indexing, dim arguments, einsum- style unpacking, and documentation-based coding.

NamedTensor is an thin-wrapper on Torch tensor that makes three changes to the API:

* Naming: Dimension access and reduction use a named dim argument instead of an index. Constructing and adding dimensions use a name argument. Axis-based indexing [ ] is replaced by named indexing.
* Broadcasting: All functions broadcast based on set-operations not through heuristic ordering rules, e.g. if z = x + y then z has the union of the dimension in x and y.
* Lifting: Order-based functions can be lifted by providing name annotations through .spec methods. For instance, convolution requires the user to name the channel and kernel dims, e.g .conv2d.spec("channel", ("x", "y")). This provides dynamic checks, better error messages, and consistent documentation.

https://github.com/harvardnlp/NamedTensor[+https://github.com/harvardnlp/NamedTensor+]

http://nlp.seas.harvard.edu/NamedTensor[+http://nlp.seas.harvard.edu/NamedTensor+]

PlaidML
^^^^^^^

PlaidML is an advanced and portable tensor compiler for enabling deep learning on laptops, embedded devices, or other devices where the available computing hardware is not well supported or the available software stack contains unpalatable license restrictions.

PlaidML sits underneath common machine learning frameworks, enabling users to access any hardware supported by PlaidML. PlaidML supports Keras, ONNX, and nGraph.

As a component within the nGraph Compiler stack, PlaidML further extends the capabilities of specialized deep-learning hardware (especially GPUs,) and makes it both easier and faster to access or make use of subgraph-level optimizations that would otherwise be bounded by the compute limitations of the device.

As a component under Keras, PlaidML can accelerate training workloads with customized or automatically-generated Tile code. It works especially well on GPUs, and it doesn't require use of CUDA/cuDNN on Nvidia* hardware, while achieving comparable performance.

https://github.com/plaidml/plaidml[+https://github.com/plaidml/plaidml+]

https://github.com/NervanaSystems/ngraph[+https://github.com/NervanaSystems/ngraph+]

https://keras.io/[+https://keras.io/+]

Redberry
^^^^^^^^

Redberry is an open source computer algebra system designed for algebraic manipulations with tensors.

http://redberry.cc/[+http://redberry.cc/+]

https://arxiv.org/abs/1302.1219[+https://arxiv.org/abs/1302.1219+]

TensorLy
^^^^^^^^

Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a high-level API for tensor methods and deep tensorized neural networks in Python. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and seamlessly integrates with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows users to easily design and train deep tensorized neural networks. 

https://github.com/tensorly/tensorly[+https://github.com/tensorly/tensorly+]

http://tensorly.org/[+http://tensorly.org/+]

https://github.com/JeanKossaifi/tensorly-notebooks[+https://github.com/JeanKossaifi/tensorly-notebooks+]

http://jeankossaifi.com/blog.html[+http://jeankossaifi.com/blog.html+]

https://arxiv.org/abs/1610.09555[+https://arxiv.org/abs/1610.09555+]

tsalib
^^^^^^

Writing deep learning programs which manipulate multi-dim tensors (numpy, pytorch, keras, tensorflow, ...) requires you to carefully keep track of shapes of tensors. Carrying around the tensor shapes in your head gets increasingly hard as programs become more complex. For example, reshaping before a matmult, figuring out RNN output shapes, examining/modifying deep pre-trained architectures (resnet, densenet, elmo), designing new kinds of attention mechanisms (multi-head attention), and so on.

In absence of a principled way to name tensor dimensions and track shapes, most developers resort to writing adhoc shape comments embedded in code.

The tsalib library enables you to write

* first-class, library-independent, shape annotations (TSAs) over named dimension variables (x: (B,T,D)),
* defensive shape assertions using these named shapes (assert x.shape == (B,T,D)),
* more fluent shape transformations and tensor operations using tensor shorthand notation (TSN). ('b,d,t').
* avoid memorizing a laundry list of APIs (reshape,permute,stack, concat) -- use the one-stop warp operator for shape transformations. warp(x, '(btd)* -> btdl -> bdtl -> b,d//2,t*2,l', 'jpv')

TSAs expose the typically invisible tensor dimension names, which enhances code clarity, accelerates debugging and leads to improved productivity across the board.

https://github.com/ofnote/tsalib[+https://github.com/ofnote/tsalib+]

https://towardsdatascience.com/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b[+https://towardsdatascience.com/introducing-tensor-shape-annotation-library-tsalib-963b5b13c35b+]

http://nlp.seas.harvard.edu/NamedTensor[+http://nlp.seas.harvard.edu/NamedTensor+]

TensorFlow Lite
~~~~~~~~~~~~~~~

TensorFlow Lite is TensorFlow’s lightweight solution for mobile and embedded devices. It enables on-device machine learning inference with low latency and a small binary size. TensorFlow Lite also supports hardware acceleration with the Android Neural Networks API.

TensorFlow Lite uses many techniques for achieving low latency such as optimizing the kernels for mobile apps, pre-fused activations, and quantized kernels that allow smaller and faster (fixed-point math) models.

TensorFlow Lite supports a set of core operators, both quantized and float, which have been tuned for mobile platforms. They incorporate pre-fused activations and biases to further enhance performance and quantized accuracy. Additionally, TensorFlow Lite also supports using custom operations in models.

TensorFlow Lite defines a new model file format, based on FlatBuffers. FlatBuffers is an efficient open-source cross-platform serialization library. It is similar to protocol buffers, but the primary difference is that FlatBuffers does not need a parsing/unpacking step to a secondary representation before you can access data, often coupled with per-object memory allocation. Also, the code footprint of FlatBuffers is an order of magnitude smaller than protocol buffers.

TensorFlow Lite has a new mobile-optimized interpreter, which has the key goals of keeping apps lean and fast. The interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.

https://www.tensorflow.org/lite/[+https://www.tensorflow.org/lite/+]

https://developer.android.com/ndk/guides/neuralnetworks/[+https://developer.android.com/ndk/guides/neuralnetworks/+]

https://google.github.io/flatbuffers/[+https://google.github.io/flatbuffers/+]

TensorFlow Probability
~~~~~~~~~~~~~~~~~~~~~~

TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow Probability provides integration of probabilistic methods with deep networks, gradient-based inference via automatic differentiation, and scalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation.

The layered structure of TensorFlow Probability is:

* Layer 1: Statistical Building Blocks

** Distributions (tfp.distributions, tf.distributions): A large collection of probability distributions and related statistics with batch and broadcasting semantics. See the Distributions Tutorial.
** Bijectors (tfp.bijectors): Reversible and composable transformations of random variables. Bijectors provide a rich class of transformed distributions, from classical examples like the log-normal distribution to sophisticated deep learning models such as masked autoregressive flows.

* Layer 2: Model Building

** Edward2 (tfp.edward2): A probabilistic programming language for specifying flexible probabilistic models as programs. See the Edward2 README.md.
** Probabilistic Layers (tfp.layers): Neural network layers with uncertainty over the functions they represent, extending TensorFlow Layers.
** Trainable Distributions (tfp.trainable_distributions): Probability distributions parameterized by a single Tensor, making it easy to build neural nets that output probability distributions.

* Layer 3: Probabilistic Inference

** Markov chain Monte Carlo (tfp.mcmc): Algorithms for approximating integrals via sampling. Includes Hamiltonian Monte Carlo, random-walk Metropolis-Hastings, and the ability to build custom transition kernels.
** Variational Inference (tfp.vi): Algorithms for approximating integrals via optimization.
** Optimizers (tfp.optimizer): Stochastic optimization methods, extending TensorFlow Optimizers. Includes Stochastic Gradient Langevin Dynamics.
** Monte Carlo (tfp.monte_carlo): Tools for computing Monte Carlo expectations.

https://github.com/tensorflow/probability/[+https://github.com/tensorflow/probability/+]

https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245[+https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245+]

TensorLy
~~~~~~~~

TensorLy is a Python library that aims at making tensor learning simple and accessible. It allows to easily perform tensor decomposition, tensor learning and tensor algebra. Its backend system allows to seamlessly perform computation with NumPy, MXNet, PyTorch, TensorFlow or CuPy, and run methods at scale on CPU or GPU.

https://github.com/tensorly/tensorly[+https://github.com/tensorly/tensorly+]

http://tensorly.org/stable/home.html[+http://tensorly.org/stable/home.html+]

terminfo
~~~~~~~~

Terminfo is a library and database that enables programs to use display terminals in a device-independent manner. Mark Horton implemented the first terminfo library in 1981–1982 as an improvement over termcap.

Terminfo (formerly Termcap) is a database of terminal capabilities and more. For every (well almost) model of terminal it tells application programs what the terminal is capable of doing. It tells what escape sequences (or control characters) to send to the terminal in order to do things such as move the cursor to a new location, erase part of the screen, scroll the screen, change modes, change appearance (colors, brightness, blinking, underlining, reverse video etc.). After about 1980, many terminals supported over a hundred different commands (some of which take numeric parameters).

One way in which terminfo gives the its information to an application program is via the "ncurses" functions that a programmer may put into a C program. For example, if a program wants to move the cursor to row 3, col 6 it simply calls: move(3,6). The move() function (part of ncurses) knows how to do this for your terminal (it has read terminfo). So it sends the appropriate escape sequence to the terminal to make this particular move for a certain terminal. Some programs get info directly from a terminfo files without using ncurses. Thus a Linux package that doesn't require ncurses may still need a terminfo file for your terminal.

The terminfo abbreviations are usually longer than those of termcap and thus it's easier to guess what they mean. The manual pages for terminfo are more detailed (and include the old termcap abbreviations). Also, the termcap entries had a size limitation which is not present for terminfo. Thus, unless you are already committed to working with termcap, it's suggested you use terminfo.

https://en.wikipedia.org/wiki/Terminfo[+https://en.wikipedia.org/wiki/Terminfo+]

https://www.tldp.org/HOWTO/Text-Terminal-HOWTO-16.html[+https://www.tldp.org/HOWTO/Text-Terminal-HOWTO-16.html+]

The most reliable terminfo database is that distributed with ncurses 5.0, or via followup development patches. The original process of incorporating terminal descriptions from various sources corrects some errors in the originals, but introduces others (either translation errors, or misconceptions). Besides working to resolve these, from time to time we incorporate new sources.

As noted in 29 February 2004, the terminfo database at http://www.catb.org/~esr/terminfo/ does not appear to be actively maintained. Since the release of ncurses 5.0 in late 1999, there are numerous fixes which are not in that database.

https://invisible-island.net/ncurses/ncurses.faq.html#which_terminfo[+https://invisible-island.net/ncurses/ncurses.faq.html#which_terminfo+]

Terra
~~~~

Terra is a new low-level system programming language that is designed to interoperate seamlessly with the Lua programming language. It is also backwards compatible with (and embeddable in) existing C code. Like C, Terra is a monomorphic, statically-typed, compiled language with manual memory management. But unlike C, it is designed to make interaction with Lua easy. Terra code shares Lua's syntax and control-flow constructs. It is easy to call Lua functions from Terra (or Terra functions from Lua).

Furthermore, you can use Lua to meta-program Terra code. The Lua meta-program handles details like conditional compilation, namespaces, and templating in Terra code that are normally special constructs in low-level languages. This coupling additionally enables more powerful features like function specialization, lisp-style macros, and manually controlled JIT compilation. Since Terra's compiler is also available at runtime, it makes it easy for libraries or embedded languages to generate low-level code dynamically.

https://github.com/zdevito/terra[+https://github.com/zdevito/terra+]

http://terralang.org/[+http://terralang.org/+]

Ebb
^^^

A programming language for writing physical simulations. Ebb programs are performance portable: they can be efficiently executed on both CPUs and GPUs. Ebb is embedded in the Lua programming language using Terra, which can itself be embedded in C/Cxx programs as a library.

Ebb was designed with a flexible data model that allows for encoding a range of different domains. As a non-exhaustive list, Ebb supports triangle meshes, grids, tetrahedral meshes, and particles.

Furthermore, domain libraries are user-authorable and can be coupled together in user code. For example, Ebb seamlessly supports coupling particles to a grid, or coupling the vertices of a mesh to a grid.

http://ebblang.org/[+http://ebblang.org/+]

TerriaJS
~~~~~~~~

TerriaJS is a library for building rich, web-based geospatial data explorers, used to drive National Map, AREMI and NEII Viewer. It uses Cesium and WebGL for a full 3D globe in the browser with no plugins. It gracefully falls back to 2D with Leaflet on systems that can't run Cesium. It can handle catalogs of thousands of layers, with dozens of geospatial file and web service types supported. It is almost entirely JavaScript in the browser, meaning it can even be deployed as a static website, making it simple and cheap to host.

The features include:

* Nested catalog of layers which can be independently enabled to create mashups of many layers.
* Supports GeoJSON, KML, CSV (point and region-mapped), GPX and CZML file types natively, and others including zipped shapefiles with an optional server-side conversion service.
* Supports WMS, WFS, Esri MapServer, ABS ITT, Bing Maps, OpenStreetMap-style raster tiles, Mapbox, Urthecast, and WMTS item types.
* Supports querying WMS, WFS, Esri MapServer, CSW, CKAN and Socrata services for groups of items.
* 3D globe (Cesium) or 2D mode (Leaflet). 3D objects supported in CZML format.
* Time dimensions supported for CSV, CZML, WMS. Automatically animate layers, or slide the time control forward and backward.
* Drag-and-drop files from your desktop to the browser, for instant visualisation (no file upload to server required).
* Wider range of file types supported through server-side OGR2OGR service (requires upload).
* All ASGS (Australian Statistical Geographic Standard) region types (LGA, SA2, commonwealth electoral district etc) supported for CSV region mapping, plus several others: Primary Health Networks, Statistical Local Areas, ISO 3 letter country codes, etc.
* Users can generate a reusable URL link of their current map view, to quickly share mashups of web-hosted data.

https://github.com/TerriaJS/terriajs[+https://github.com/TerriaJS/terriajs+]

https://terria.io/[+https://terria.io/+]

tess2
~~~~~

An open-source package for parallelizing Delaunay and Voronoi tessellation over distributed-memory HPC architecture.

Computing a Voronoi or Delaunay tessellation from a set of points is a core part of the analysis of many simulated and measured datasets: N-body simulations, molecular dynamics codes, and LIDAR point clouds are just a few examples. Such computational geometry methods are common in data analysis and visualization; but as the scale of simulations and observations surpasses billions of particles, the existing serial and shared-memory algorithms no longer suffice. A distributed-memory scalable parallel algorithm is the only feasible approach. Tess is a parallel Delaunay and Voronoi tessellation algorithm that automatically determines which neighbor points need to be exchanged among the subdomains of a spatial decomposition. Computing tessellations at scale performs poorly when the input data is unbalanced, which is why Tess uses k-d trees to evenly distribute points among processes. The running times are up to 100 times faster using k-d tree compared with regular grid decomposition. Moreover, in unbalanced data sets, decomposing the domain into a k-d tree is up to five times faster than decomposing it into a regular grid. 

https://github.com/diatomic/tess2[+https://github.com/diatomic/tess2+]

https://www.mcs.anl.gov/\~tpeterka/software.html[+https://www.mcs.anl.gov/~tpeterka/software.html+]

Tesseract
~~~~~~~~~

Tesseract is an optical character recognition engine for various operating systems. It is free software, released under the Apache License, Version 2.0, and development has been sponsored by Google since 2006.

Tesseract up to and including version 2 could only accept TIFF images of simple one-column text as inputs. These early versions did not include layout analysis, and so inputting multi-columned text, images, or equations produced garbled output. Since version 3.00 Tesseract has supported output text formatting, hOCR positional information and page-layout analysis. Support for a number of new image formats was added using the Leptonica library. Tesseract can detect whether text is monospaced or proportionally spaced.

Tesseract is suitable for use as a backend and can be used for more complicated OCR tasks including layout analysis by using a frontend such as OCRopus.

Tesseract's output will have very poor quality if the input images are not preprocessed to suit it: Images (especially screenshots) must be scaled up such that the text x-height is at least 20 pixels,[13] any rotation or skew must be corrected or no text will be recognized, low-frequency changes in brightness must be high-pass filtered, or Tesseract's binarization stage will destroy much of the page, and dark borders must be manually removed, or they will be misinterpreted as characters.

The presently available Tesseract package contains an OCR engine - libtesseract and a command line program - tesseract. Tesseract 4 adds a new neural net (LSTM) based OCR engine which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0). It also needs traineddata files which support the legacy engine, for example those from the tessdata repository.

Tesseract has unicode (UTF-8) support, and can recognize more than 100 languages "out of the box".
It supports various output formats: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV. The master branch also has experimental support for ALTO (XML) output.

https://en.wikipedia.org/wiki/Tesseract_(software)[+https://en.wikipedia.org/wiki/Tesseract_(software)+]

https://github.com/tesseract-ocr/tesseract[+https://github.com/tesseract-ocr/tesseract+]

https://github.com/tesseract-ocr/tesseract/wiki/User-Projects-%E2%80%93-3rdParty[+https://github.com/tesseract-ocr/tesseract/wiki/User-Projects-%E2%80%93-3rdParty+]

https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality[+https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality+]

http://leptonica.com/[+http://leptonica.com/+]

TetGen
~~~~~~

TetGen is a program to generate tetrahedral meshes of any 3D polyhedral domains. TetGen generates exact constrained Delaunay tetrahedralizations, boundary conforming Delaunay meshes, and Voronoi partitions. The following pictures respectively illustrate a 3D polyhedral domain, a boundary conforming Delaunay tetrahedral mesh, and its dual - a Voronoi partition.

TetGen provides various features to generate good quality and adaptive tetrahedral meshes suitable for numerical methods, such as finite element or finite volume methods. 
The features include:

* TetGen is written in ANSI Cxx. The code is highly portable - it should be very easy to compile and run on all major computer systems
* For a set of 3D points, TetGen computes its exact Delaunay tetrahedralization and its dual Voronoi diagram.
* For any piecewise linear boundary (such as a surface mesh) of a 3D domain. TetGen generates a tetrahedral mesh which covers the interior of the domain and preserves the domain boundary.
* If the input boundary contains no acute angle (in practice, this condition can be relaxed to no input angle smaller than 60 degree), TetGen will generate a boundary conforming Delaunay mesh.
* TetGen performs efficient mesh refinement (inserting new vertices) to improve the overall mesh quality. The resulting tetrahedral mesh is a finite element mesh which consists of good-shaped tetrahedra and the mesh size well conforms to the boundary size or a user-defined smoothing size function.
* TetGen provides flexible options for mesh refinement.
* You can assign a boundary marker (an integer) for each facet of a PLC. In the final mesh, all boundary faces on that facet will have the same boundary marker. A common use of the boundary marker is to determine where boundary conditions should be applied to a finite element or finite volume mesh.
* There are a number of ways to perform adaptive mesh generation in TetGen
* TetGen provides two options for coarsening tetrahedral meshes:
* TetGen is able to remesh a surface triangulation into its constrained Delaunay triangulation or conforming Delaunay triangulation.
* TetGen can detect self-intersecting facets
* In stead of reading and writting meshes in TetGen's own file formats, TetGen also supports other popular polyhedral and mesh file formats, such as OFF (Object File Format), PLY (Polygon File Format), and so on

http://wias-berlin.de/software/index.jsp?id=TetGen&lang=1[+http://wias-berlin.de/software/index.jsp?id=TetGen&lang=1+]

http://wias-berlin.de/software/tetgen/[+http://wias-berlin.de/software/tetgen/+]

TeX
~~~

Blah.

Engrafo
^^^^^^^

Engrafo converts LaTeX documents into beautiful responsive web pages using LaTeXML.

It is a set of stylesheets and scripts for LaTeXML output. It makes the design responsive so you can read it on phones, and adds various interactive bits like footnote tooltips.

https://github.com/arxiv-vanity/engrafo[+https://github.com/arxiv-vanity/engrafo+]

LaTeX3
^^^^^^

A project to develop the next generation of LaTeX.

https://github.com/latex3/latex3[+https://github.com/latex3/latex3+]

https://www.latex-project.org/latex3/[+https://www.latex-project.org/latex3/+]

LaTeXML
^^^^^^^

In brief, latexml is a program, written in Perl, that attempts to faithfully mimic TeX’s behavior, but produces XML instead of dvi. The document model of the target XML makes explicit the model implied by LaTeX. The processing and model are both extensible; you can define the mapping between TeX constructs and the XML fragments to be created. A postprocessor, latexmlpost converts this XML into other formats such as HTML or XHTML, with options to convert the math into MathML (currently only presentation) or images.

https://github.com/brucemiller/LaTeXML[+https://github.com/brucemiller/LaTeXML+]

https://dlmf.nist.gov/LaTeXML/[+https://dlmf.nist.gov/LaTeXML/+]

LuaTeX
^^^^^^

LuaTeX is an extended version of pdfTeX using Lua as an embedded scripting language. The LuaTeX project's main objective is to provide an open and configurable variant of TeX while at the same time offering downward compatibility. From the user perspective we have pdfTeX as stable and more or less frozen 8 bit engine, XeTeX as unicode input and font aware engine using libraries for font handling, and LuaTeX as engine that is programmable and delegates as much as possible to Lua, with the objective to keep the core engine lean and mean. Each engine has its benefits and drawbacks.

The main objective of the project is to provide a version of TeX where all internals are accessible from Lua. In the process of opening up TeX much of the internal code is rewritten. Instead of hard coding new features in TeX itself, users (or macro package writers) can write their own extensions. LuaTeX offers native support for OpenType fonts. In contrast to XeTeX, the fonts are not accessed through the operating system libraries, but through a library based on FontForge. 

https://en.wikipedia.org/wiki/LuaTeX[+https://en.wikipedia.org/wiki/LuaTeX+]

http://www.luatex.org/[+http://www.luatex.org/+]

https://www.overleaf.com/learn/latex/Articles/An_Introduction_to_LuaTeX_(Part_1):_What_is_it%E2%80%94and_what_makes_it_so_different%3F[+https://www.overleaf.com/learn/latex/Articles/An_Introduction_to_LuaTeX_(Part_1):_What_is_it%E2%80%94and_what_makes_it_so_different%3F+]

T-Flows
~~~~~~~

T-Flows (stands for Turbulent Flows) is a Computational Fluid Dynamics (CFD) program, originally developed at Delft University of Technology, the Netherlands. It features second order accurate, unstructured, cell-centered, finite volume discretization of incompressible Navier-Stokes equations with heat transfer and species transport. It is written in Fortran 90 and uses Message Passing Interface (MPI) for parallel execution.

The basic features are:

* Unstructured (cell-centered) grid, arbitrary cell shapes, second order accuracy in space
* Linear solvers: preconditioned conjugate gradient, bi-conjugate-gradient and conjugate gradient squared
* Eddy-viscosity and second moment closure Reynolds-Averaged Navier-Stokes (RANS) models
* Large Eddy Simulation (LES) with Smagorinsky, Dynamic and Wall-Adapting Local Eddy viscosity (WALE) models for sub-grid scales (SGS)
* Hybrid RANS/LES methods
* Conventional and advanced treatment of wall boundary conditions (wall integration, wall functions with roughness, compound wall treatment, blending wall function and wall integration approach)
* Compatibility with several open-source and commercial grid generators

The turbulence models implemented are:

* Linear eddy-viscosity k-ε models
** Standard high-Re
** Low-Re version (Abe, Kondoh and Nagano) with compound wall treatment
* Elliptic-relaxation eddy-viscosity model
** Linear ζ − f with compound wall treatment
* Second-moment closures full Reynolds-stress models
** Elliptic blending model (Manceau-Hanjalic)
** Hanjalic-Jakirlic model
** Hybrid linear eddy-viscosity and second-moment closure model (Basara-Jakirlic)
* LES
** Smagorinsky SGS model
** Dynamic Smagorinsky SGS model
** WALE SGS model
* Hybrid LES/RANS
** Detached eddy simulation (Spalart-Allmaras)
** Hybrid LES/RANS ζ − f model

All RANS models can be ran in unsteady mode, thus effectively becoming Unsteady RANS (URANS) model.

https://github.com/DelNov/T-Flows[+https://github.com/DelNov/T-Flows+]

Theano
~~~~~~

Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Theano features:

* tight integration with NumPy – Use numpy.ndarray in Theano-compiled functions.
* transparent use of a GPU – Perform data-intensive computations much faster than on a CPU.
* efficient symbolic differentiation – Theano does your derivatives for functions with one or many inputs.
* speed and stability optimizations – Get the right answer for log(1+x) even when x is really tiny.
* dynamic C code generation – Evaluate expressions faster.
* extensive unit-testing and self-verification – Detect and diagnose many types of errors.

Theano has been powering large-scale computationally intensive scientific investigations since 2007. But it is also approachable enough to be used in the classroom.
Using Theano it is possible to attain speeds rivaling hand-crafted C implementations for problems involving large amounts of data. It can also surpass C on a CPU by many orders of magnitude by taking advantage of recent GPUs.

Theano combines aspects of a computer algebra system (CAS) with aspects of an optimizing compiler. It can also generate customized C code for many mathematical operations. This combination of CAS with optimizing compilation is particularly useful for tasks in which complicated mathematical expressions are evaluated repeatedly and evaluation speed is critical. For situations where many different expressions are each evaluated once Theano can minimize the amount of compilation/analysis overhead, but still provide symbolic features such as automatic differentiation.

http://deeplearning.net/software/theano/[+http://deeplearning.net/software/theano/+]

https://en.wikipedia.org/wiki/Theano_(software)[+https://en.wikipedia.org/wiki/Theano_(software)+]

Thetis
~~~~~~

Thetis is an unstructured grid coastal ocean model built using the Firedrake finite element framework. Currently Thetis consists of 2D depth averaged and full 3D baroclinic models.

Unstructured grid ocean models are advantageous for simulating the coastal ocean and river–estuary–plume systems. However, unstructured grid models tend to be diffusive and/or computationally expensive, which limits their applicability to real-life problems. In this paper, we describe a novel discontinuous Galerkin (DG) finite element discretization for the hydrostatic equations. The formulation is fully conservative and second-order accurate in space and time. Monotonicity of the advection scheme is ensured by using a strong stability-preserving time integration method and slope limiters. Compared to previous DG models, advantages include a more accurate mode splitting method, revised viscosity formulation, and new second-order time integration scheme. We demonstrate that the model is capable of simulating baroclinic flows in the eddying regime with a suite of test cases. Numerical dissipation is well-controlled, being comparable or lower than in existing state-of-the-art structured grid models.

https://thetisproject.org/[+https://thetisproject.org/+]

https://github.com/thetisproject/thetis[+https://github.com/thetisproject/thetis+]

https://www.geosci-model-dev.net/11/4359/2018/[+https://www.geosci-model-dev.net/11/4359/2018/+]

THOTH
~~~~~

A Python package for the efficient estimation of information-theoretic quantities from empirical data.
Information is an essential part of the natural and social worlds. From Shannon’s pioneering work in the 1940s, we now know how it can be quantified. But estimating these quantities from limited data is more difficult than it might seem.

THOTH allows for the efficient and consistent estimation of entropy, mutual information, Jensen-Shannon divergence, and other quantities essential to the understanding of uncertainty, prediction and information-processing in real-world systems.

The core is written in ANSI C, and wrapped in Python. THOTH is a factor of twenty times faster than a Python-only implementation. We believe it will prove useful to workers in the biological and social sciences.

http://tuvalu.santafe.edu/\~simon/page7/page7.html[+http://tuvalu.santafe.edu/~simon/page7/page7.html+]

THREDDS
~~~~~~~

The THREDDS Data Server (TDS) is a web server that provides metadata and data access for scientific datasets, using OPeNDAP, OGC WMS and WCS, HTTP, and other remote data access protocols. The TDS is developed and supported by Unidata, a division of the University Corporation for Atmospheric Research (UCAR), and is sponsored by the National Science Foundation.

Some of the technology in the TDS:

* THREDDS Dataset Inventory Catalogs are used to provide virtual directories of available data and their associated metadata. These catalogs can be generated dynamically or statically.
* The Netcdf-Java/CDM library reads NetCDF, OpenDAP, and HDF5 datasets, as well as other binary formats such as GRIB and NEXRAD into a Common Data Model (CDM), essentially an (extended) netCDF view of the data. Datasets that can be read through the Netcdf-Java library are called CDM datasets.
 TDS can use the NetCDF Markup Language (NcML) to modify and create virtual aggregations of CDM datasets.
* An integrated server provides OPeNDAP access to any CDM dataset. OPeNDAP is a widely used, subsetting data access method extending the HTTP protocol.
* An integrated server provides bulk file access through the HTTP protocol.
* An integrated server provides data access through the OpenGIS Consortium (OGC) Web Coverage Service (WCS) protocol, for any "gridded" dataset whose coordinate system information is complete.
* An integrated server provides data access through the OpenGIS Consortium (OGC) Web Map Service (WMS) protocol, for any "gridded" dataset whose coordinate system information is complete. THREDDS integrates a version of ncWMS and its companion "Godiva" web application for WMS access and online visualization, developed at the University of Reading.
* The integrated ncISO server provides automated metadata analysis and ISO metadata generation.
* The integrated NetCDF Subset Service allows subsetting certain CDM datasets in coordinate space, using a REST API. Gridded data subsets can be returned in CF-compliant netCDF-3 or netCDF-4. Point data subsets can be returned in CSV, XML, WaterML (with assistance from ERDDAP [NOAA / Robert Simons/CoHort Software], license information), or CF-DSG netCDF files.

The THREDDS Data Server is implemented in 100% Java*, and is contained in a single war file, which allows very easy installation into a servlet container such as the open-source Tomcat web server. Configuration is made as simple and as automatic as possible, and we have made the server as secure as possible. The library is freely available and the source code is released under the (MIT-style) netCDF library license. 

https://www.unidata.ucar.edu/software/thredds/current/tds/[+https://www.unidata.ucar.edu/software/thredds/current/tds/+]

https://github.com/Unidata/thredds-docker[+https://github.com/Unidata/thredds-docker+]

NcSOS
^^^^^

NcSOS adds an OGC SOS service to datasets in your existing THREDDS server. It
complies with the IOOS SWE Milestone 1.0 templates and requires your datasets
be in any of the CF 1.6 Discrete Sampling Geometries.

NcSOS acts like other THREDDS services (such an OPeNDAP and WMS) where as
there are individual service endpoints for each dataset. It is best to
aggregate your files and enable the NcSOS service on top of the aggregation.
i.e. The NcML aggregate of hourly files from an individual station would be a
good candidate to serve with NcSOS. Serving the individual hourly files with
NcSOS would not be as beneficial.

https://github.com/asascience-open/ncSOS[+https://github.com/asascience-open/ncSOS+]

Siphon
^^^^^^

A collection of Python utilities for retrieving atmospheric and oceanic data from remote sources, focusing on being able to retrieve data from Unidata data technologies, such as the THREDDS data server. 

https://github.com/Unidata/siphon[+https://github.com/Unidata/siphon+]

https://unidata.github.io/siphon/latest/[+https://unidata.github.io/siphon/latest/+]

https://unidata.github.io/python-gallery/[+https://unidata.github.io/python-gallery/+]

tds-utils
^^^^^^^^^

A collection of python scripts to do various tasks relating to THREDDS data server. 
The scripts are:

* *aggregate* - Read filenames of datasets from standard input and print an NcML aggregation to standard output
* *cache_remote_aggregations* - Send HTTP requests to OPeNDAP/WMS aggregation endpoints based on dataset IDs found in the input JSON. This makes sure THREDDS caches aggregations before any end-user tries to access them.
* *find_ncml* - Parse a THREDDS catalog and print paths of all referenced NcML aggregations to stdout.
* *partition_files* - Read file paths from stdin and partition into sets such that paths in each set only differ by having a different date in the directory components of the path.
* *find_netcdf* - Parse a THREDDS catalog and list the references NetCDF files.
* *create_catalog* - Create a THREDDS catalog from a list of NetCDF files. Can create either dataset catalogs or root-level catalogs with links to other catalogs.

https://github.com/cedadev/tds-utils[+https://github.com/cedadev/tds-utils+]

thredds_crawler
^^^^^^^^^^^^^^^

A simple crawler/parser for THREDDS catalogs.

https://github.com/ioos/thredds_crawler[+https://github.com/ioos/thredds_crawler+]

Thrift
~~~~~~

Thrift is an interface definition language and binary communication protocol[1] used for defining and creating services for numerous languages.[2] It forms a remote procedure call (RPC) framework and was developed at Facebook for "scalable cross-language services development". It combines a software stack with a code generation engine to build cross-platform services which can connect applications written in a variety of languages and frameworks.

Thrift includes a complete stack for creating clients and servers.[8] The top part is generated code from the Thrift definition. From this file, the services generate client and processor code. In contrast to built-in types, created data structures are sent as result in generated code. The protocol and transport layer are part of the runtime library. With Thrift, it is possible to define a service and change the protocol and transport without recompiling the code. Besides the client part, Thrift includes server infrastructure to tie protocols and transports together, like blocking, non-blocking, and multi-threaded servers. The underlying I/O part of the stack is implemented differently for different languages. 

Apache Thrift allows you to define data types and service interfaces in a simple definition file. Taking that file as input, the compiler generates code to be used to easily build RPC clients and servers that communicate seamlessly across programming languages. Instead of writing a load of boilerplate code to serialize and transport your objects and invoke remote methods, you can get right down to business.

https://thrift.apache.org/[+https://thrift.apache.org/+]

Thrust
~~~~~~

Thrust is a powerful library of parallel algorithms and data structures. Thrust provides a flexible, high-level interface for GPU programming that greatly enhances developer productivity. Using Thrust, Cxx developers can write just a few lines of code to perform GPU-accelerated sort, scan, transform, and reduction operations orders of magnitude faster than the latest multi-core CPUs. For example, the thrust::sort algorithm delivers 5x to 100x faster sorting performance than STL and TBB. 

https://developer.nvidia.com/thrust[+https://developer.nvidia.com/thrust+]

https://github.com/thrust/thrust[+https://github.com/thrust/thrust+]

http://thrust.github.io/[+http://thrust.github.io/+]

http://tinycorelinux.net/corebook.pdf[+http://tinycorelinux.net/corebook.pdf+]

Tide
~~~~

Tide is BlueBrain's Tiled Interactive Display Environment. It provides multi-window, multi-user touch interaction on large surfaces - think of a giant collaborative wall-mounted tablet. 

Tide is a distributed application that can run on multiple machines to power display walls or projection systems of any size.

Its user interface is designed to offer an intuitive experience on touch walls. It works just as well on non touch-capable installations by using its web interface from any web browser.

Tide helps users with:

* Presenting and collaborating on a variety of media such as high-resolution images, movies and pdfs.
* Sharing multiple desktop or laptop screens using the DesktopStreamer application.
* Sketching new ideas by drawing on a whiteboard and browsing websites.
* Interacting with content streamed from remote sources such as high-performance visualisation machines through the Deflect protocol. In particular all Equalizer-based applications as well as Brayns ray-tracing engine have built-in support.
* Viewing high-resolution, immersive stereo 3D streams on compatible hardware.

https://github.com/BlueBrain/Tide[+https://github.com/BlueBrain/Tide+]

http://bluebrain.github.io/Tide-1.5/user_guide.html[+http://bluebrain.github.io/Tide-1.5/user_guide.html+]

Collage
^^^^^^^

Collage is a cross-platform Cxx library for building heterogenous, distributed applications. Among others, it is the cluster backend for the Equalizer parallel rendering framework. Collage provides an abstraction of different network connections, peer-to-peer messaging, node discovery, synchronization and high-performance, object-oriented, versioned data distribution. Collage is designed for low-overhead multi-threaded execution which allows applications to easily exploit multi-core architectures.

https://github.com/Eyescale/Collage[+https://github.com/Eyescale/Collage+]

Deflect
^^^^^^^

A Cxx library for streaming pixels to other Deflect-based applications, for example Tide.

https://github.com/BlueBrain/Deflect[+https://github.com/BlueBrain/Deflect+]

Equalizer
^^^^^^^^^

Tstandard middleware to create and deploy parallel, scalable OpenGL applications. It enables applications to benefit from multiple graphics cards, processors and computers to scale the rendering performance, visual quality and display size. An Equalizer application runs unmodified on any visualization system, from a simple workstation to large scale graphics clusters, multi-GPU workstations and Virtual Reality installations.

Equalizer provides the following major features to facilitate the development and deployment of scalable OpenGL applications:

* Runtime Configurability: An Equalizer application is configured automatically or manually at runtime and can be deployed on laptops, multi-GPU workstations and large-scale visualization clusters without recompilation.
* Runtime Scalability: An Equalizer application can benefit from multiple graphics cards, processors and computers to scale rendering performance, visual quality and display size.
* Distributed Execution: Equalizer applications can be written to support cluster-based execution. Equalizer uses the Collage network library, a cross-platform Cxx library for building heterogenous, distributed applications.
* Support for Stereo and Immersive Environments: Equalizer supports stereo rendering head tracking, head-mounted displays and other advanced features for immersive Virtual Reality installations.

https://github.com/Eyescale/Equalizer[+https://github.com/Eyescale/Equalizer+]

Rockets
^^^^^^^

A REST and websockets Cxx library.
Rockets provides the following features:

* HTTP server with integrated websockets support
* HTTP client for making simple asynchronous requests (with or without payload)
* Websockets client for sending, broadcasting and receiving text and binary messages
* Support for JSON-RPC as a communication protocol over HTTP and websockets. Rockets extends the 2.0 specification by providing support for Cancellation and Progress notifications of pending requests.
* Event loop integration for Qt and libuv through the appropriate Server constructor

https://github.com/BlueBrain/Rockets[+https://github.com/BlueBrain/Rockets+]

TIFF
~~~~

Tagged Image File Format, abbreviated TIFF or TIF, is a computer file format for storing raster graphics images, popular among graphic artists, the publishing industry,[1] and photographers. TIFF is widely supported by scanning, faxing, word processing, optical character recognition, image manipulation, desktop publishing, and page-layout applications.

TIFF is a flexible, adaptable file format for handling images and data within a single file, by including the header tags (size, definition, image-data arrangement, applied image compression) defining the image's geometry. A TIFF file, for example, can be a container holding JPEG (lossy) and PackBits (lossless) compressed images. A TIFF file also can include a vector-based clipping path (outlines, croppings, image frames). The ability to store image data in a lossless format makes a TIFF file a useful image archive, because, unlike standard JPEG files, a TIFF file using lossless compression (or none) may be edited and re-saved without losing image quality. This is not the case when using the TIFF as a container holding compressed JPEG. Other TIFF options are layers and pages. 

https://en.wikipedia.org/wiki/TIFF[+https://en.wikipedia.org/wiki/TIFF+]

BigTIFF
^^^^^^^

Until recently TIFF files were limited in size to 4GB.  A new version of libtiff has been created which supports BigTIFF files - TIFF files which are larger than 4GB.  The new version is backward-compatible with previous versions and in many cases applications will not have to change at all in order to read or write BigTIFF files.

There are more technical details below, but at the highest level the BigTIFF changes made to libtiff were quite simple.  The TIFF file format internally uses 32-bit byte offsets.  The largest offset which can be represented is 232 = 4GB, making that the upper limit of the file size which could be supported by the design.  The BigTIFF modifications to libtiff consisted primarily of changing all internal byte offsets to 64-bits.  A key goal was to maintain backward compatibility with existing applications and files to the largest extent possible.

bigtiff.org/[+bigtiff.org/+]

COG
^^^

A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing ​HTTP GET range requests to ask for just the parts of a file they need.

The two main organization techniques that Cloud Optimized GeoTIFF’s use are Tiling and Overviews. And the data is also compressed for more efficient passage online.

Tiling creates a number of internal ‘tiles’ inside the actual image, instead of using simple ‘stripes’ of data. With a stripe of data then the whole file needs to be read to get the key piece. With tiles much quicker access to a certain area is possible, so that just the portion of the file that needs to be read is accessed.

Overviews create downsampled versions of the same image. This means it’s ‘zoomed out’ from the original image - it has much less detail (1 pixel where the original might have 100 or 1000 pixels), but is also much smaller. Often a single GeoTIFF will have many overviews, to match different zoom levels. These add size to the overall file, but are able to be served much faster, since the renderer just has to return the values in the overview instead of figuring out how to represent 1000 different pixels as one.

http://www.cogeo.org/[+http://www.cogeo.org/+]

http://matthewrocklin.com/blog/work/2018/02/06/hdf-in-the-cloud[+http://matthewrocklin.com/blog/work/2018/02/06/hdf-in-the-cloud+]

*COG Explorer* - https://geotiffjs.github.io/cog-explorer/[+https://geotiffjs.github.io/cog-explorer/+]

GeoTIFF
^^^^^^^

GeoTIFF is a public domain metadata standard which allows georeferencing information to be embedded within a TIFF file. The potential additional information includes map projection, coordinate systems, ellipsoids, datums, and everything else necessary to establish the exact spatial reference for the file. The GeoTIFF format is fully compliant with TIFF 6.0, so software incapable of reading and interpreting the specialized metadata will still be able to open a GeoTIFF format file.

http://trac.osgeo.org/geotiff/[+http://trac.osgeo.org/geotiff/+]

https://github.com/OSGeo/libgeotiff[+https://github.com/OSGeo/libgeotiff+]

https://www.geospatialworld.net/article/geotiff-a-standard-image-file-format-for-gis-applications/[+https://www.geospatialworld.net/article/geotiff-a-standard-image-file-format-for-gis-applications/+]

LibTIFF
^^^^^^^

This software provides support for the Tag Image File Format (TIFF), a widely used format for storing image data.

Included in this software distribution is a library, libtiff, for reading and writing TIFF, a small collection of tools for doing simple manipulations of TIFF images, and documentation on the library and tools.

libtiff provides interfaces to image data at several layers of abstraction (and cost). At the highest level image data can be read into an 8-bit/sample, ABGR pixel raster format without regard for the underlying data organization, colorspace, or compression scheme. Below this high-level interface the library provides scanline-, strip-, and tile-oriented interfaces that return data decompressed but otherwise untransformed. These interfaces require that the application first identify the organization of stored data and select either a strip-based or tile-based API for manipulating data. At the lowest level the library provides access to the raw uncompressed strips or tiles, returning the data exactly as it appears in the file. 

This software distribution comes with a small collection of programs for converting non-TIFF format images to TIFF and for manipulating and interrogating the contents of TIFF images. Several of these tools are useful in their own right. Many of them however are more intended to serve as programming examples for using the TIFF library.

http://simplesystems.org/libtiff/[+http://simplesystems.org/libtiff/+]

TileDB
~~~~~~

TileDB is an efficient multi-dimensional array management system which introduces a novel format that can effectively store dense and sparse array data with support for fast updates and reads. It features excellent compression, an efficient parallel I/O system for high scalability, and high-level APIs including Python, R, Golang and more.

TileDB stores your array data on persistent storage locally or in the cloud, with built-in support for S3 and HDFS storage backends.

The features include:

* Novel Format. TileDB introduces a novel multi-dimensional array format that effectively handles both dense and sparse data with fast updates. Contrary to other popular systems like HDF5 that are optimized mostly for dense arrays, TileDB is optimized for both dense and sparse arrays, exposing a unified API. TileDB enables efficient updates through its concept of immutable, append-only "fragments."
* Multiple Backends. Transparently store and access your arrays on AWS S3 (or other S3 compatiable store) or HDFS with a single API.
* Compression. Achieve high compression ratios with TileDB's tile-based compression approach. TileDB can compress array data with a growing number of compressors, such as GZIP, BZIP2, LZ4, ZStandard, double-delta and run-length encoding.
* Parallelism. Use every core with TileDB's parallelized I/O and compression systems (using Intel TBB), and build powerful parallel analytics on top of the TileDB array storage manager (e.g., using OpenMP or MPI) leveraging TileDB's thread-/process-safety.
* Portability. TileDB works on Linux, macOS and Windows, offering easy installation packages, binaries and Docker containerization.
* Language Bindings. Enable your NumPy data science applications to work with immense amounts of data using TileDB's Python API. Other APIs include Golang, R and Java.
* Key-value Store. Store any persistent metadata with TileDB's key-value storage functionality. A TileDB key-value store inherits all the benefits of TileDB arrays such as compression, parallelism, and multiple backend support.
* Virtual Filesystem. Add general file management and I/O to your applications for any supported storage backend using TileDB's unified "virtual filesystem" (VFS) API.

https://github.com/TileDB-Inc/TileDB[+https://github.com/TileDB-Inc/TileDB+]

https://docs.tiledb.io/en/latest/[+https://docs.tiledb.io/en/latest/+]

time series
~~~~~~~~~~~

Blah.

https://github.com/ikding/pycon_time_series[+https://github.com/ikding/pycon_time_series+]

TIMESAT
^^^^^^^

TIMESAT is a software package for analysing time-series of satellite sensor data. We have developed TIMESAT to be able to investigate the seasonality of satellite time-series data and their relationship with dynamic properties of vegetation, such as phenology and temporal development. The temporal domain holds important information about short- and long-term vegetation changes.

TIMESAT was originally intended for handling noisy time-series of AVHRR NDVI data and to extract seasonality information from the data. The program now has the capability to handle different types of remotely sensed time-series , e.g. data from Terra/MODIS at different time resolutions. It has also been tested with eddy covariance data and moisture data, although these applications are not the main target.

http://web.nateko.lu.se/timesat/timesat.asp[+http://web.nateko.lu.se/timesat/timesat.asp+]

Tiny Core Linux
~~~~~~~~~~~~~~~

Tiny Core Linux (TCL) is a minimal Linux operating system focusing on providing a base system using BusyBox and FLTK, developed by Robert Shingledecker.[2] The distribution is notable for its small size (11 to 16 MB) and minimalism; additional functions are provided by extensions.

he Core Project, as suggested by our name, is not a turnkey desktop distribution. Instead we deliver just the core Linux from which it is quite easy to add what you want. We offer 3 different x86 "cores" to get you started:

* Core is the base system which provides only a command line interface and is therefore recommended for experienced users only. Command line tools are provided so that extensions can be added to create a system with a graphical desktop environment. Ideal for servers, appliances, and custom desktops. 

* TinyCore is the recommended option for new users who have a wired network connection. It includes the base Core system plus X/GUI extensions for a dynamic FLTK/FLWM graphical desktop environment.

* CorePlus is an installation image and not the distribution. It is recommended for new users who only have access to a wireless network or who use a non-US keyboard layout. It includes the base Core System and installation tools to provide for the setup with the following options: Choice of 7 Window Managers, Wireless support via many firmware files and ndiswrapper, non-US keyboard support, and a remastering tool.

http://tinycorelinux.net/[+http://tinycorelinux.net/+]

https://en.wikipedia.org/wiki/Tiny_Core_Linux[+https://en.wikipedia.org/wiki/Tiny_Core_Linux+]

TMSU
~~~~

TMSU was born out of frustration with the hierarchical nature of filesystems.
It is a tool for tagging your files. It provides a simple command-line tool for applying tags and a virtual filesystem so that you can get a tag-based view of your files from within any other program.

TMSU does not alter your files in any way: they remain unchanged on disk, or on the network, wherever you put them. TMSU maintains its own database and you simply gain an additional view, which you can mount, based upon the tags you set up. The only commitment required is your time and there's absolutely no lock-in. 

TMSU also has a virtual filesystem that can be mounted.
Files in the virtual file-system are actually just symbolic links back to the tagged file's real location elsewhere on the file-system. This means they can be used just like regular files from any application.

https://tmsu.org/[+https://tmsu.org/+]

https://github.com/oniony/TMSU[+https://github.com/oniony/TMSU+]

tmux
~~~~

tmux is a terminal multiplexer for Unix-like operating systems. It allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time. It can also be used to detach processes from their controlling terminals, allowing SSH sessions to remain active without being visible.

tmux includes most features of GNU Screen. It allows users to start a terminal session with clients that are not bound to a specific physical or virtual console; multiple terminal sessions can be created within a single terminal session and then freely rebound from one virtual console to another, and each session can have several connected clients. Some Tmux notable features are[3]

* Menus for interactive selection of running sessions, windows or clients
* Window can be linked to an arbitrary number of sessions[4]
* vi-like or Emacs command mode (with auto completion) for managing tmux[5]
* Lack of built-in serial and telnet clients (which some consider bloat for the terminal multiplexer)[3]
* Easier configuration[6][7][better source needed]
* Different command keys—it is not a drop-in replacement for screen, but can be configured to use compatible keybindings
* Vertical and horizontal Window split support

https://github.com/tmux/tmux[+https://github.com/tmux/tmux+]

https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340[+https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340+]

https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/[+https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/+]

TNSPackage
~~~~~~~~~~

Recently, the tensor network states (TNS) methods have proven to be very powerful tools to investigate the strongly correlated many-particle physics in one and two dimensions. The implementation of TNS methods depends heavily on the operations of tensors, including contraction, permutation, reshaping tensors, SVD and so on. Unfortunately, the most popular computer languages for scientific computation, such as Fortran and C/Cxx do not have a standard library for such operations, and therefore make the coding of TNS very tedious. We develop a Fortran2003 package that includes all kinds of basic tensor operations designed for TNS. It is user-friendly and flexible for different forms of TNS, and therefore greatly simplifies the coding work for the TNS methods.

The TNSpackage is written in Fortran2003. We use an object oriented programming (OOP) style to improve the readability and re-usability. So far it has about 42000 lines, and more than 200 subroutines and functions which are grouped into 8 modules: Tensor.f90, function.f90, Dimension.f90, print_in_TData.f90, element_in_TData.f90, modify_in_TData.f90, permutation_in_TData.f90, TData.f90, among which Tensor.f90 is the main module. To use the package, one need to include the Tensor.f90 module in the codes. The package have been tested to be a stable version. With high reusability, the users are able to write their own children objects through inheritance. The procedure polymorphism and data polymorphism of the package make it suitable for coding different algorithms for TNS or for other purposes involving high dimension arrays.

In the package, we define a new data type Tensor, whose elements are stored in a one-dimensional array. The new data type Tensor supports all data types of Fortran, including integer, single/double precision real number, single/double precision complex number, logical and character. We overwrite most functions of Fortran2003 so they can be directly applied to Tensor, as they are applied to other fundamental data types.

https://www.sciencedirect.com/science/article/pii/S001046551830078X[+https://www.sciencedirect.com/science/article/pii/S001046551830078X+]

ToMaTo
~~~~~~

The Topology Management Tool (ToMaTo) is a topology-centric network testbed, giving researchers the possibility to run their software in specifically designed virtual networking topologies. ToMaTo utilizes Proxmox virtualization technology (OpenVZ and KVM), Tinc VPN and Dummynet ink emulation to organize virtual machines in virtual topologies.
The features include:

* Create your virtual network topology in an intuitive web-based editor
* Have a look at what goes over the wire. Just activate packet capturing on a link and analyze the traffic in Wireshark
* Use virtual switches or external networks to connect your topology.
* Even very complex topologies can be configured and started in a few seconds
* Configure different properties like latency, packet loss or bandwidth on each link
* Select from full virtualization (KVM) and container virtualization (OpenVZ) for each topology component to save resources
* Create topologies that go around the globe, select freely from our various sites.
* Control your virtual machines directly from a web-based console viewer. Watch your VM boot or control graphical user interfaces
* Select from a wide range of pre-configures VM images spanning from Debian, Ubuntu and other Linux systems to Windows XP

https://github.com/GLab/ToMaTo[+https://github.com/GLab/ToMaTo+]

http://tomato-lab.org/[+http://tomato-lab.org/+]

https://openvz.org/[+https://openvz.org/+]

TOMS
~~~~

Software associated with papers published in the Transactions on Mathematical Software, as well as other ACM journals are incorporated in CALGO. This software is refereed for originality, accuracy, robustness, completeness, portability, and lasting value.

All algorithms numbered 493 and above, as well as a few earlier ones, may be downloaded from this server.

http://netlib.org/toms/index.html[+http://netlib.org/toms/index.html+]

https://toms.acm.org/[+https://toms.acm.org/+]

TooN
~~~~

The TooN library is a set of Cxx14 header files which provide basic numerics facilities:

* Vectors, matrices and etc
* Matrix decompositions
* Function optimization
* Parameterized matrices (eg transformations)
* linear equations
* Functions (eg automatic differentiation and numerical derivatives)

It provides classes for statically- (known at compile time) and dynamically- (unknown at compile time) sized vectors and matrices and it can delegate advanced functions (like large SVD or multiplication of large matrices) to LAPACK and BLAS (this means you will need libblas and liblapack).

The library makes substantial internal use of templates to achieve run-time speed efficiency whilst retaining a clear programming syntax.
The compelling reasons to use this library are:

* Because it supports statically sized vectors and matrices very efficiently.
* Because it provides extensive type safety for statically sized vectors and matrices (you can't attempt to multiply a 3x4 matrix and a 2-vector).
* Because it supports transposition, subscripting and slicing of matrices (to obtain a vector) very efficiently.
* Because it interfaces well to other libraries.
* Because it exploits LAPACK and BLAS (for which optimised versions exist on many platforms).
* Because it is fast, but not at the expense of numerical stability.

https://codedocs.xyz/edrosten/TooN/[+https://codedocs.xyz/edrosten/TooN/+]

https://github.com/edrosten/TooN[+https://github.com/edrosten/TooN+]

https://www.edwardrosten.com/cvd/toon.html[+https://www.edwardrosten.com/cvd/toon.html+]

libCVD
^^^^^^

libCVD is a very portable and high performance Cxx library for computer vision, image, and video processing. The emphasis is on providing simple and efficient image and video handling and high quality implementations of common low-level image processing function. The library is designed in a loosely-coupled manner, so that parts can be used easily in isolation if the whole library is not required. The video grabbing module provides a simple, uniform interface for videos from a variety of sources (live and recorded) and allows easy access to the raw pixel data. Likewise, the image loading/saving module provides simple, uniform interfaces for loading and saving images from bitmaps to 64 bit per channel RGBA images. The image processing routines can be applied easily to images and video, and accelerated versions exist for platforms supporting SSE. 

https://github.com/edrosten/libcvd[+https://github.com/edrosten/libcvd+]

https://www.edwardrosten.com/cvd/index.html[+https://www.edwardrosten.com/cvd/index.html+]

tag
^^^

tag is a companion library to TooN, a Cxx header file collection for numerics computation. It provides a set of algorithms typically found in Computer Vision applications.

https://www.edwardrosten.com/cvd/tag/html/index.html[+https://www.edwardrosten.com/cvd/tag/html/index.html+]

https://github.com/edrosten/tag[+https://github.com/edrosten/tag+]

TOPCAT
~~~~~~

TOPCAT is an interactive graphical viewer and editor for tabular data. Its aim is to provide most of the facilities that astronomers need for analysis and manipulation of source catalogues and other tables, though it can be used for non-astronomical data as well. It understands a number of different astronomically important formats (including FITS, VOTable and CDF) and more formats can be added.

It offers a variety of ways to view and analyse tables, including a browser for the cell data themselves, viewers for information about table and column metadata, and facilities for sophisticated interactive 1-, 2-, 3- and higher-dimensional visualisation, calculating statistics and joining tables using flexible matching algorithms. Using a powerful and extensible Java-based expression language new columns can be defined and row subsets selected for separate analysis. Table data and metadata can be edited and the resulting modified table can be written out in a wide range of output formats.

It is a stand-alone application which works quite happily with no network connection. However, because it uses Virtual Observatory (VO) standards, it can cooperate smoothly with other tools, services and datasets in the VO world and beyond. 

The capabilities of TOPCAT include:

* Fast access to large datasets (millions of rows/hundreds of columns)
* View/edit table data in a scrollable browser
* View/edit table and column metadata
* Re-order and hide/reveal columns
* Insert 'synthetic' columns defined by algebraic expression
* Sort rows on the values in a given column
* Define row subsets in various ways
* View interactive and configurable plots of column-based quantities against each other distinguishing different data sets:
** Plot types are histogram, plane, sky, cube, sphere, time
** Features include variable transparency, error bars, point labelling, colour axes, all-sky plots, configurable density shading, vectors, ellipses, contours, density maps, KDEs, analytic functions, plain text/LaTeX axis annotation, ...
** Plots can be exported in bitmapped or vector formats, and a command to script the same plot is displayed
* Calculate statistics on each column for some or all rows
* Perform flexible and fast matching of rows in the same or different tables
* Concatenate the rows of existing tables to create new ones
* Cause various things to happen when a row or plotted point is selected.
* Communicate with other applications using SAMP
* Acquire tables from a file, URL, or SQL query
* Communicate with external VO and non-VO data services, including TAP, VizieR, CDS X-Match, cone search, SIA, SSA or DataLink
* Perform multiple (per-row) cone search, SIA or SSA queries, to join a local to a remote catalogue or image/spectrum archive.
* Write modified tables out in original or different format to file or an SQL database
* Comprehensive documentation supplied within the application or off-line

http://www.star.bristol.ac.uk/\~mbt/topcat/[+http://www.star.bristol.ac.uk/~mbt/topcat/+]

TopoJSON
~~~~~~~~

TopoJSON is an extension of GeoJSON that encodes topology. Rather than representing geometries discretely, geometries in TopoJSON files are stitched together from shared line segments called arcs. This technique is similar to Matt Bloch’s MapShaper and the Arc/Info Export format, .e00. TopoJSON eliminates redundancy, allowing related geometries to be stored efficiently in the same file. For example, the shared boundary between California and Nevada is represented only once, rather than being duplicated for both states. A single TopoJSON file can contain multiple feature collections without duplication, such as states and counties. Or, a TopoJSON file can efficiently represent both polygons (for fill) and boundaries (for stroke) as two feature collections that share the same arc mesh.

As a result, TopoJSON is substantially more compact than GeoJSON. The above shapefile of U.S. counties is 2.2M as a GeoJSON file, but only 436K as a boundary mesh, a reduction of 80.4% even without simplification. TopoJSON can also be more efficient to render since shared control points need only be projected once. To further reduce file size, TopoJSON uses fixed-precision delta-encoding for integer coordinates rather than floats. This is similar to rounding coordinate values (e.g., LilJSON), but with greater precision. Like GeoJSON, TopoJSON files are easily modified in a text editor and amenable to gzip compression.

Lastly, encoding topology has numerous useful applications for maps and visualization. It facilitates geometry simplification that preserves the connectedness of adjacent features; this applies even across feature collections, such as simultaneous consistent simplification of state and county boundaries. Topology can also be used for Dorling cartograms and other techniques that need shared boundary information.

https://github.com/topojson/topojson/wiki[+https://github.com/topojson/topojson/wiki+]

https://medium.com/@mbostock/command-line-cartography-part-1-897aa8f8ca2c[+https://medium.com/@mbostock/command-line-cartography-part-1-897aa8f8ca2c+]

Torch
~~~~~

Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. It is easy to use and efficient, thanks to an easy and fast scripting language, LuaJIT, and an underlying C/CUDA implementation.

The goal of Torch is to have maximum flexibility and speed in building your scientific algorithms while making the process extremely simple. Torch comes with a large ecosystem of community-driven packages in machine learning, computer vision, signal processing, parallel processing, image, video, audio and networking among others, and builds on top of the Lua community.

At the heart of Torch are the popular neural network and optimization libraries which are simple to use, while having maximum flexibility in implementing complex neural network topologies. You can build arbitrary graphs of neural networks, and parallelize them over CPUs and GPUs in an efficient manner.

http://torch.ch/[+http://torch.ch/+]

https://github.com/torch/torch7[+https://github.com/torch/torch7+]

Tornado
~~~~~~~

Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed. By using non-blocking network I/O, Tornado can scale to tens of thousands of open connections, making it ideal for long polling, WebSockets, and other applications that require a long-lived connection to each user.

Tornado can be roughly divided into four major components:

* A web framework (including RequestHandler which is subclassed to create web applications, and various supporting classes).
* Client- and server-side implementions of HTTP (HTTPServer and AsyncHTTPClient).
* An asynchronous networking library including the classes IOLoop and IOStream, which serve as the building blocks for the HTTP components and can also be used to implement other protocols.
* A coroutine library (tornado.gen) which allows asynchronous code to be written in a more straightforward way than chaining callbacks. This is similar to the native coroutine feature introduced in Python 3.5 (async def). Native coroutines are recommended in place of the tornado.gen module when available.

The Tornado web framework and HTTP server together offer a full-stack alternative to WSGI. While it is possible to use the Tornado web framework in a WSGI container (WSGIAdapter), or use the Tornado HTTP server as a container for other WSGI frameworks (WSGIContainer), each of these combinations has limitations and to take full advantage of Tornado you will need to use the Tornado’s web framework and HTTP server together.

http://www.tornadoweb.org/en/stable/[+http://www.tornadoweb.org/en/stable/+]

https://github.com/tornadoweb/tornado[+https://github.com/tornadoweb/tornado+]

TPM
~~~

Trusted Platform Module (TPM, also known as ISO/IEC 11889) is an international standard for a secure cryptoprocessor, a dedicated microcontroller designed to secure hardware through integrated cryptographic keys. 

Trusted Platform Module provides

* A random number generator
* Facilities for the secure generation of cryptographic keys for limited uses.
* Remote attestation: Creates a nearly unforgeable hash key summary of the hardware and software configuration. The software in charge of hashing the configuration data determines the extent of the summary. This allows a third party to verify that the software has not been changed.
* Binding: Encrypts data using the TPM bind key, a unique RSA key descended from a storage keyclarification needed].
* Sealing: Similar to binding, but in addition, specifies the TPM state for the data to be decrypted (unsealed).

Computer programs can use a TPM to authenticate hardware devices, since each TPM chip has a unique and secret RSA key burned in as it is produced. Pushing the security down to the hardware level provides more protection than a software-only solution.

https://en.wikipedia.org/wiki/Trusted_Platform_Module[+https://en.wikipedia.org/wiki/Trusted_Platform_Module+]

https://trustedcomputinggroup.org/work-groups/trusted-platform-module/[+https://trustedcomputinggroup.org/work-groups/trusted-platform-module/+]

https://paolozaino.wordpress.com/2017/03/18/configure-and-use-your-tpm-module-on-linux/[+https://paolozaino.wordpress.com/2017/03/18/configure-and-use-your-tpm-module-on-linux/+]

https://blog.hansenpartnership.com/tpm2-and-linux/[+https://blog.hansenpartnership.com/tpm2-and-linux/+]

https://github.com/fox-it/linux-luks-tpm-boot[+https://github.com/fox-it/linux-luks-tpm-boot+]

https://archive.fosdem.org/2018/schedule/event/tpm/[+https://archive.fosdem.org/2018/schedule/event/tpm/+]

tpm2-tss
^^^^^^^^

This repository hosts source code implementing the Trusted Computing Group's (TCG) TPM2 Software Stack (TSS). This stack consists of the following layers from top to bottom:

* Enhanced System API (ESAPI) as described in the TSS 2.0 Enhanced System API (ESAPI) Specification. This API is a 1-to-1 mapping of the TPM2 commands documented in Part 3 of the TPM2 specification. Additionally there are asynchronous versions of each command. In addition to SAPI, the ESAPI performs tracking of meta data for TPM object and automatic calculation of session based authorization and encryption values. Both the synchronous and asynchronous API are exposed through a single library: libtss2-esys.
* System API (SAPI) as described in the system level API and TPM command transmission interface specification. This API is a 1-to-1 mapping of the TPM2 commands documented in Part 3 of the TPM2 specification. Additionally there are asynchronous versions of each command. These asynchronous variants may be useful for integration into event-driven programming environments. Both the synchronous and asynchronous API are exposed through a single library: libtss2-sys.
* Marshaling/Unmarshaling (MU) as described in the TCG TSS 2.0 Marshaling/Unmarshaling API Specification. This API provides a set of marshaling and unmarshaling functions for all data types define by the TPM library specification. The Marshaling/Unmarshaling API is exposed through a library called libtss2-mu.
* TPM Command Transmission Interface (TCTI) that is described in the same specification. This API provides a standard interface to transmit / receive TPM command / response buffers. It is expected that any number of libraries implementing the TCTI API will be implemented as a way to abstract various platform specific IPC mechanisms. Currently this repository provides two TCTI implementations: libtss2-tcti-device and libtss2-tcti-mssim. The former should be used for direct access to the TPM through the Linux kernel driver. The latter implements the protocol exposed by the Microsoft software TPM2 simulator.
* The TCG TSS 2.0 Overview and Common Structures Specification forms the basis for all implementations in this project. NOTE: We deviate from this draft of the specification by increasing the value of TPM2_NUM_PCR_BANKS from 3 to 16 to ensure compatibility with TPM2 implementations that have enabled a larger than typical number of PCR banks. This larger value for TPM2_NUM_PCR_BANKS is expected to be included in a future revision of the specification.

https://github.com/tpm2-software/tpm2-tss[+https://github.com/tpm2-software/tpm2-tss+]

TrustedGRUB2
^^^^^^^^^^^^

This file describes the extensions made to transform a standard GRUB2 into a version that offers TCG (TPM) support for granting the integrity of the boot process (trusted boot). 

https://github.com/Rohde-Schwarz-Cybersecurity/TrustedGRUB2[+https://github.com/Rohde-Schwarz-Cybersecurity/TrustedGRUB2+]

Traffic Server
~~~~~~~~~~~~~~

Apache Traffic Server™ is a high-performance web proxy cache that improves network efficiency and performance by caching frequently-accessed information at the edge of the network. This brings content physically closer to end users, while enabling faster delivery and reduced bandwidth use. Traffic Server is designed to improve content delivery for enterprises, Internet service providers (ISPs), backbone providers, and large intranets by maximizing existing and available bandwidth.

http://trafficserver.apache.org/[+http://trafficserver.apache.org/+]

https://github.com/apache/trafficserver[+https://github.com/apache/trafficserver+]

tree-sitter
~~~~~~~~~~~

Tree-sitter is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be:

* General enough to parse any programming language
* Fast enough to parse on every keystroke in a text editor
* Robust enough to provide useful results even in the presence of syntax errors
* Dependency-free so that the runtime library (which is written in pure C) can be embedded in any application

http://tree-sitter.github.io/tree-sitter/[+http://tree-sitter.github.io/tree-sitter/+]

https://github.com/tree-sitter/tree-sitter[+https://github.com/tree-sitter/tree-sitter+]

https://www.thestrangeloop.com/2018/tree-sitter---a-new-parsing-system-for-programming-tools.html[+https://www.thestrangeloop.com/2018/tree-sitter---a-new-parsing-system-for-programming-tools.html+]

triNNity
~~~~~~~~

triNNity is a header-only Cxx17 template library with over 80 DNN convolution algorithms. It’s a collaborative effort with several other people in our research group to collect as many DNN convolution algorithms as possible in one place, and give them clean, simple, and performant implementations. It is also a testbed for algorithm design for DNN convolution.

The library implements normal dense convolution (both direct and GEMM-based), strided convolution, dilated convolution, group convolution, sparse convolution, Winograd convolution, FFT convolution, and more, including super high performance specialized algorithms for cases like 1x1 convolution.

Many libraries and frameworks present algorithms like im2col, fft, and others, as monolithic operations, but there are in fact dozens of algorithmic variants of these approaches, all of which are better suited to some kinds of convolutions than others. Our paper in ASAP 2017 details many of these algorithms.

Under the hood, the library uses BLAS, OpenMP multithreading, SIMD vectorization, and more, without any programmer intervention required. It can also run completely standalone, without any, or with only a subset, of these components enabled. We currently support x86_64 and aarch64, but support for more platforms is planned. Since the library is released as header-only Cxx, all that’s really required to bring up a new platform is a working compiler supporting the Cxx17 standard.

https://www.scss.tcd.ie/\~andersan/projects/live/triNNity.html[+https://www.scss.tcd.ie/~andersan/projects/live/triNNity.html+]

https://bitbucket.org/STG-TCD/trinnity[+https://bitbucket.org/STG-TCD/trinnity+]

Trio
~~~~

The Trio project’s goal is to produce a production-quality, permissively licensed, async/await-native I/O library for Python. Like all async libraries, its main purpose is to help you write programs that do multiple things at the same time with parallelized I/O. A web spider that wants to fetch lots of pages in parallel, a web server that needs to juggle lots of downloads and websocket connections at the same time, a process supervisor monitoring multiple subprocesses… that sort of thing. Compared to other libraries, Trio attempts to distinguish itself with an obsessive focus on usability and correctness. Concurrency is complicated; we try to make it easy to get things right.

Trio was built from the ground up to take advantage of the latest Python features, and draws inspiration from many sources, in particular Dave Beazley’s Curio. The resulting design is radically simpler than older competitors like asyncio and Twisted, yet just as capable. Trio is the Python I/O library I always wanted; I find it makes building I/O-oriented programs easier, less error-prone, and just plain more fun. Perhaps you’ll find the same.

https://trio.readthedocs.io/en/latest/[+https://trio.readthedocs.io/en/latest/+]

https://github.com/python-trio/trio[+https://github.com/python-trio/trio+]

TSAD
~~~~

The Time Series Anomaly Detector (TSAD) software implements a customizable evaluation model for
time series anomaly detection.

Given a set of real anomaly ranges and a set of predicted anomaly ranges, TSAD iterates over the set of all real anomaly ranges computing a Recall score for each range and adds them up into a total Recall score. This total score is then divided by the total number of real anomalies to obtain an average Recall score for the whole time series.
Likewise, for range-based Precision, it iterates over the set of predicted anomaly ranges to compute a Precision score for each predicted anomaly range and then divide the total by the number of ranges to arrive at the average Precision score.

TSAD  also provides the user with customizable parameters (e.g., positional bias) to match the specific needs of various domains.

https://github.com/IntelLabs/TSAD-Evaluator[+https://github.com/IntelLabs/TSAD-Evaluator+]

https://arxiv.org/abs/1803.03639[+https://arxiv.org/abs/1803.03639+]

https://www.intel.ai/precision-and-recall-for-time-series/[+https://www.intel.ai/precision-and-recall-for-time-series/+]

[[Trilinos]]
Trilinos
~~~~~~~~

An effort to develop algorithms and enabling technologies within an object-oriented software framework for the solution of large-scale, complex multi-physics engineering and scientific problems. A unique design feature of Trilinos is its focus on packages.

Each Trilinos package is a self-contained, independent piece of software with its own set of requirements, its own development team and group of users. Because of this, Trilinos itself is designed to respect the autonomy of packages. Trilinos offers a variety of ways for a particular package to interact with other Trilinos packages. It also offers a set of tools that can assist package developers with builds across multiple platforms, generating documentation and regression testing across a set of target platforms. At the same time, what a package must do to be called a Trilinos package is minimal, and varies with each package.

https://trilinos.github.io/[+https://trilinos.github.io/+]

https://trilinos.org/[+https://trilinos.org/+]

https://arxiv.org/abs/1307.6638[+https://arxiv.org/abs/1307.6638+]

*Trilinos Hands-On Tutorial* - https://code.google.com/p/trilinos/wiki/TrilinosHandsOnTutorial[+https://code.google.com/p/trilinos/wiki/TrilinosHandsOnTutorial+]

*ETUG Meeting 2015* - https://trilinos.org/community/events/european-trilinos-user-group-meeting-2015/[+https://trilinos.org/community/events/european-trilinos-user-group-meeting-2015/+]

ForTrilinos
^^^^^^^^^^^

ForTrilinos is a part of the Trilinos project and provides object-oriented Fortran interfaces to Trilinos Cxx packages.
This is the new effort to provide Fortran interfaces to Trilinos through automatic code generation using SWIG.

https://github.com/trilinos/ForTrilinos[+https://github.com/trilinos/ForTrilinos+]

[[Amesos]]
Amesos
^^^^^^

Amesos provides an object-oriented interface to several direct sparse solvers, both sequential and parallel. 
It supports the following classes:

* Amesos_Lapack - Interface to LAPACK's serial dense solver DGETRF.
* Amesos_Scalapack - Interface to ScaLAPACK's parallel dense solver PDGETRF.
* Amesos_Klu - Interface to Tim Davis serial solver KLU (distributed within Amesos).
* Amesos_Umfpack - Interface to Tim Davis's UMFPACK (version 4.3 or later)
* Amesos_Pardiso - Interface to PARDISO (prototype)
* Amesos_Taucs - Interface to TAUCS
* Amesos_Superlu - Interface to Xiaoye Li's SuperLU serial memory code with serial input interface (version 3.0 or later).
* Amesos_Superludist - Interface to Xiaoye Li's SuperLU Distributed memory code with serial input interface (version 2.0 or later).
* Amesos_Dscpack - Interface to Padma Raghavan's DSCPACK
* Amesos_Mumps - Interface to CERFACS' MUMPS (version 4.3.1 or later)

https://trilinos.org/docs/dev/packages/amesos/doc/html/index.html[+https://trilinos.org/docs/dev/packages/amesos/doc/html/index.html+]

[[Anasazi]]
Anasazi
^^^^^^^

Anasazi is a powerful, extensible and interoperable Cxx framework for the numerical solution of large-scale eigenvalue problems. The motivation for this framework is to provide a generic interface to a collection of algorithms for solving large-scale eigenvalue problems. Anasazi's interoperability results from its understanding of operators and vectors as opaque objects—these objects are accessed only via elementary operations. Hence the user has the flexibility to specify the data representation for operators and vectors and may so leverage any existing software investment. This mechanism is accomplished via the Anasazi Operator/Vector abstract interface. Current interfaces available include Thyra, Epetra and Tpetra. As a result, any existing code employing Thyra, Epetra or Tpetra operators and vectors (such as other packages in Trilinos) may also be used in conjunction with Anasazi.

The Anasazi eigensolver framework describes eigensolver and eigensolver managers that provide efficient, convenient and powerful computational methods. Anasazi's power is a result of the distribution of functionality across different computational entities in the framework: orthogonalization, sorting/selection, eigenvalue iterations, restarting methodologies, etc. Anasazi's extensibility comes via the abstract interfaces describing these entities. Anasazi currently provides a robust set of options, and users are able to expand this set to suit specific research and/or application needs. The ability to interact directly with these objects provides great flexibility in computation, while the existence of solver managers allow convenient programming for standard use cases.

Unlike ARPACK, which provides a single eigensolver, Anasazi provides a framework capable of describing a wide variety of eigenproblems and algorithms for solving them. Anasazi can currently solve complex and real, Hermitian and non-Hermitian, eigenvalue problems, via the following included methods:

* Block Krylov-Schur method
* Block Davidson method 
* LOBPCG, or locally optimal block preconditioned conjugate gradient method
* IRTR, an implicit version of the Riemannian Trust-Region Eigensolver
* Generalized Davidson method, a non-symmetric variant of the block Davidson solver
* TraceMin method
* TraceMin-Davidson method 

https://trilinos.org/packages/anasazi/[+https://trilinos.org/packages/anasazi/+]

https://trilinos.org/docs/dev/packages/anasazi/doc/html/index.html[+https://trilinos.org/docs/dev/packages/anasazi/doc/html/index.html+]

[[AztecOO]]
AztecOO
^^^^^^^

Provides an object-oriented interface the the well-known xref:Aztec[Aztec] solver library. Furthermore, it allows flexible construction of matrix and vector arguments via Epetra matrix and vector classes. Finally, AztecOO provide additional functionality not found in Aztec and any future enhancements to the Aztec package will be available only through the AztecOO interfaces.

The AztecOO classes are:

* AztecOO - Primary solver class. An AztecOO object is instantiated using and Epetra_LinearProblem object. The solver options and parameters can be set using SetAztecOption() and SetAztecParam() methods on an AztecOO object.
* Aztec2Petra() - Utility function to convert from Aztec data structures to Epetra objects. This function can be useful when migrating from Aztec to AztecOO. It is used internally by the AZOO_iterate function.
* AZOO_iterate() - Utility function that mimics the behavior and calling sequence of AZ_iterate, the primary solver call in Aztec. AZOO_iterate converts Aztec matrix, vectors, options and params into Epetra and AztecOO objects. It then calls AztecOO to solve the problem. For current Aztec users, this function should provide identical functionality to AZ_iterate, except for the extra memory used by having Epetra versions of the matrix and vectors.

https://trilinos.org/packages/aztecoo/[+https://trilinos.org/packages/aztecoo/+]

https://trilinos.org/docs/dev/packages/aztecoo/doc/html/index.html[+https://trilinos.org/docs/dev/packages/aztecoo/doc/html/index.html+]

[[Belos]]
Belos
^^^^^

Belos is an extensible and interoperable Cxx framework for the numerical solution of large-scale linear systems of equations. It provides a collection of iterative linear solver algorithms, which work for any combination of matrix, preconditioner, and vector types which make sense together.

Belos is interoperable because it accesses matrices, preconditioners, and vectors as opaque objects with simple interfaces. These interfaces do not depend on the internal representation of these objects. Belos' solvers only use matrices and preconditioners as black-box "operators" that take one or more vectors as input, and return the same number of vectors as output. They only access vectors via a simple interface for adding vectors together or computing their inner products or norms.

Implementations of operator and vector interfaces for Epetra, Tpetra, and Thyra objects are provided. As a result, any existing code employing Epetra, Tpetra, or Thyra operators and vectors (such as other packages in Trilinos) may also be used in conjunction with Belos. Belos is extensible because users may define this interface for their own operators and vectors. This allows them to use any Belos solver with their objects without changing a line of Belos code. Defining this interface requires only a modest effort, and Belos provides plenty of examples. For more details, please refer to the Belos Operator/Vectorabstract interface".

The Belos linear solver framework describes solver managers that provide efficient, convenient, and powerful computational methods for solving large-scale linear systems of equations. Belos' power is a result of the distribution of functionality across different computational entities in the framework, including orthogonalization, iterations, stopping criteria, and restarting methodologies. Belos' extensibility comes via the abstract interfaces describing these entities. Belos currently provides a robust set of options, and users are able to expand this set to suit specific research and/or application needs. The ability to interact directly with these objects provides great flexibility in computation, while the existence of solver managers allow convenient programming for standard use cases.

Belos provides a framework capable of solving a wide variety of linear systems of equations. Belos can currently solve real-valued (often complex-valued), Hermitian and non-Hermitian, linear problems, via the following included solvers:

* Single-vector and block GMRES
* Single-vector and block CG
* Pseudo-block variants (perform single-vector algorithms simultaneously): pseudo-block CG, pseudo-block GMRES
* Recycling solvers: GCRO-DR and RCG
* Flexible variants: Flexible GMRES

https://trilinos.org/docs/dev/packages/belos/doc/html/index.html[+https://trilinos.org/docs/dev/packages/belos/doc/html/index.html+]

https://trilinos.org/packages/belos/[+https://trilinos.org/packages/belos/+]

[[Epetra]]
Epetra
^^^^^^

Epetra implements serial and parallel linear algebra. This includes both construction and use of sparse graphs, sparse matrices, and dense vectors. Epetra also provides data distribution and redistribution facilities. This package provides the underlying foundation for a whole stack of Trilinos solvers.

Epetra contains parallel, serial and utility classes.  The parallel classes are:

* a communicator class that is an abstraction of a subset of the parallel machine, and supports serial and
MPI programming models
* a map class containing  information used to distribute vectors, matrices and other objects on a parallel (or serial) machine
* a vector class that supports construction and use of vectors on a parallel machine
* a multi-vector class that supports construction and use of multi-vectors on a parallel machine
* a sparse row graph class that supports construction of a serial or parallel graph that determines the communication pattern for subsequent matrix objects
* a pure virtual row matrix class that  specifies interfaces needed to do most of the common operations required by a row matrix
* a row matrix class that has many pure virtual functions that must be implemented by an adaptor
* a sparse row matrix class that supports construction and use of row-wise sparse matrices
* a sparse block row matrix class that supports construction and use of row-wise block sparse matrices
* a jagged diagonal sparse matrix class that constructs and updates a jagged-diagonal format sparse matrix from an existing row matrix
* import/export classes

The serial classes are:

* General dense matrix/vector classes
* General dense solver class
* Symmetric dense matrix class
* Symmetric definite dense solver

The utility classes are:

* Timing class providing timing functions for the purposes of performance analysis
* Floating point operation class providing floating point operations (FLOPS) counting and reporting functions for the purposes of performance analysis. 
* Distributed directory class that allows construction of a distributed directory
* BLAS wrapper class
* LAPACK wrapper class

Epetra provides a small set of user-extendable Fortran and C interfaces.  These wrappers are hand-generated, so adding to the list requires hand-coding. The Trilinos team is looking at automated techniques to support Fortran and C.

Epetra can be used as a stand-alone package. However, it also provides the foundation for Trilinos.

https://trilinos.org/docs/dev/packages/epetra/doc/html/index.html[+https://trilinos.org/docs/dev/packages/epetra/doc/html/index.html+]

https://trilinos.org/packages/epetra/[+https://trilinos.org/packages/epetra/+]

[[Fei]]
Fei
^^^

A general interface for assembling finite-element data into a linear system of equations. It is an abstraction layer that insulates finite-element application codes from linear-algebra issues such as sparse matrix storage formats and mappings from nodes and solution fields to distributed equation spaces. It puts a common face on various linear solvers, allowing finite-element applications to switch from one solver library to another with minimal changes to application code. FEI provides natural mechanisms for assembling finite-element data such as element-wise stiffness arrays and load vectors, boundary-condition specifications and constraint relations. It accepts data from multi-physics problems, allowing arbitrarily complicated elements with multiple solution fields per node, and cell centered fields. It is designed for use in distributed-memory parallel finite-element applications, to assemble and solve distributed linear systems using scalable underlying solver libraries.

https://trilinos.org/packages/fei/[+https://trilinos.org/packages/fei/+]

https://trilinos.org/docs/dev/packages/fei/doc/html/index.html[+https://trilinos.org/docs/dev/packages/fei/doc/html/index.html+]

[[Galeri]]
Galeri
^^^^^^

The Galeri package is a small Trilinos package, based on Epetra and Teuchos, whose aim is to easily generate Epetra_Map, Epetra_CrsMatrix and Epetra_VbrMatrix objects. Galeri offers a set of functionalities that are very close to that of the MATLAB's gallery() function. Several well know finite difference matrices can be created. Also, Galeri contains a simple finite element code that can be used to discretize scalar second order elliptic problems using Galerkin and SUPG techniques, on both 2D and 3D unstructured grids.

The main functions of this package are:

* Generation of Epetra_Map objects
* Generation of Epetra_CrsMatrix objects
* Discretization of scalar, linear second-order PDE problems using finite element
* Creation of a VBR matrix and other utilities
* providing a Python interface

https://trilinos.org/packages/galeri/[+https://trilinos.org/packages/galeri/+]

https://trilinos.org/docs/dev/packages/galeri/doc/html/index.html[+https://trilinos.org/docs/dev/packages/galeri/doc/html/index.html+]

[[IFPACK]]
IFPACK
^^^^^^

This provides a suite of object-oriented algebraic preconditioners for the solution of preconditioned iterative solvers. IFPACK constructors expect an Epetra_RowMatrix object for construction. IFPACK is part of the Trilinos Solver Project and IFPACK object interacts well with other Trilinos classes. In particular, IFPACK can be used as a preconditioner for AztecOO.

Most IFPACK preconditioners can be rewritten as additive Schwarz methods, of overlapping domain decomposition type. The user can adopt a minimal-overlap (that is, zero-row overlap), or ask IFPACK to extend the overlap.
This results in preconditioner whose operator can be factored via:

* Point relaxation preconditioners
* Block relaxation preconditioners
* Point incomplete factorizations
* Exact factorizations
* Chebyshev polynomials

IFPACK preconditioners can be used as smoothers for multilevel solvers, like ML.

xref:Amesos[Amesos] to enable direct solvers. xref:ML[ML] can take advantage of IFPACK to define smoothers.

https://trilinos.org/packages/ifpack/[+https://trilinos.org/packages/ifpack/+]

https://trilinos.org/docs/dev/packages/ifpack/doc/html/index.html[+https://trilinos.org/docs/dev/packages/ifpack/doc/html/index.html+]

[[Intrepid]]
Intrepid
^^^^^^^^

Intrepid is a library of interoperable tools for compatible discretizations of Partial Differential Equations (PDEs). Included with the Trilinos 10.0 release is the "expert version" of Intrepid. This version is intended primarily for application developers who want to reuse large parts of their existing code frameworks such as I/O, data structures, assembly routines, etc. while gaining access to advanced discretization capabilities provided by Intrepid. In such cases the bulk of the data is owned and managed by the user rather than by Intrepid. To avoid unnecessary and possibly expensive copying of data to and from Intrepid, the expert version of the package comprises of mostly stateless classes operating on user-owned data. Virtually all numerical data required by PDE codes can be represented as a multi-dimensional array of scalar values. For this reason, and to enhance interoprability, Intrepid classes are templated on generic multi-dimensional arrays. The Shards package provides an implementation of a multi-dimensional array that can be used for that purpose, or users can write their own multi-dimensional arrays as long as a minimal interface is supported.

Intrepid includes the following features:

* Default finite element basis functions for H(grad), H(curl), H(div) and L2 spaces of orders up to 2 on standard cell topologies in 1D, 2D and 3D
* High-order (up to 10) basis functions for H(grad), H(curl), H(div) and L2 spaces on select cell topologies
* Pullbacks (transformations) from reference coordinate frame of H(grad), H(curl), H(div) and L2 fields
* Pullbacks of gradient, curl and divergence of H(grad), H(curl), H(div) fields
* Cubature rules of orders up to 20 on most standard 1D, 2D and 3D cell topologies
* Implementation of multi-diumensional arrays and algebraic operations on them
* Examples showing solution of basic 2nd order elliptic boundary value problems (Poisson, div-curl, and curl-curl systems) using Intrepid

https://trilinos.org/packages/intrepid/[+https://trilinos.org/packages/intrepid/+]

https://trilinos.org/docs/dev/packages/intrepid/doc/html/index.html[+https://trilinos.org/docs/dev/packages/intrepid/doc/html/index.html+]

[[Mesquite]]
Mesquite
^^^^^^^^

A linkable software library that applies a variety of node-movement algorithms to improve the quality and/or adapt a given mesh. 
Mesquite improves surface or volume meshes which are structured, unstructured, hybrid, or non-comformal. A variety of element types are permitted. Mesquite is designed to be as efficient as possible so that large meshes can be improved.

The Mesquite mesh quality improvement toolkit is a software library that can be run stand-alone using provided drivers or called directly from an application code to improve meshes via node-movement techniques.
Mesquite architecture is derived directly from the underlying mathematical mesh optimization theory.
For example, at the highest level Mesquite consists of objects and classes such as mesh quality metrics, objective function templates, mesh quality improvement solvers), mesh quality assessors, and Target-matrix calculators. The latter class provides the crucial link between geometric mesh properties such as length, area, angles, and orientation and solution properties such as gradients, curvature, and local error estimates.

https://trilinos.org/packages/mesquite/[+https://trilinos.org/packages/mesquite/+]

https://github.com/trilinos/mesquite[+https://github.com/trilinos/mesquite+]

[[ML]]
ML
^^

ML is Sandia’s main multigrid preconditioning package. ML is designed to solve large sparse linear systems of equations arising primarily from elliptic PDE discretizations.

ML contains a variety of parallel multigrid schemes:

* smoothed aggregation
* FAS nonlinear multigrid
* a special algebraic multigrid for the eddy current approximations to Maxwell’s equations. This eddy current solver is a unique capability of ML and utilizes the discrete nullspace of the operator in building the smoothers and grid hierarchy.

Within each of these methods there are several different algorithms to guide the type of coarsening and the inter-grid transfers (including the ability to drop weak coupling within the operator during inter-transfer construction).

ML can also be used as a framework to generate new multigrid methods. Using ML’s internal aggregation routines and Galerkin products, it is possible to focus on new types of inter-grid transfer operators without having to address the cumbersome aspects of generating an entirely new parallel algebraic multigrid code. We have used this flexibility to produce special multilevel methods using coarse grid finite element functions to serve as inter-grid transfers.

Our primary goal has been to provide state-of-the-art iterative methods that perform well on parallel computers (applications on over 3000 processors have been run) and that at the same time are easy to use for application engineers. In addition to providing algebraic multilevel methods to engineers, the ML library is also used in our research on preconditioners.

https://trilinos.org/packages/ml/[+https://trilinos.org/packages/ml/+]

[[Moertel]]
Moertel
^^^^^^^

Moertel provides fundamental construction routines and services that are required for serial and parallel mesh-tying and contact formulations using Mortar methods.

This package supplies capabilities for nonconforming mesh tying and contact formulations in 2 and 3 dimensions using Mortar methods. Mortar methods are a type of Lagrange Multiplier constraints that can be used in contact formulations and in non-conforming or conforming mesh tying as well as in domain decomposition techniques. Originally introduced as a domain decomposition method for spectral elements, Mortar methods are used in a large class of nonconforming situations such as the surface coupling of different physical models, discretization schemes or non-matching triangulations along interior interfaces of a domain.

Suitable conditions at interfaces are formulated as weak continuity conditions in dual variables introducing Lagrange multipliers on the interface. By an approbiate choice of the function space of the Lagrange multipliers on an interface it is possible to locally condense Lagrange multipliers such that a sparse and definite system of equations can be formed which is equivalent to the saddle point system resulting from the Lagrange multiplier formulation.
Moreover if the problems on subdomains are symmetric positive definite (spd) or symmetric positive indefinite and the complete problem is known to be spd, then a spd system of equations can be recovered that leads to the same solution as the saddle point system of equations. This way, unmodfied standard solution techniques such as multigrid can be applied and the reduction in the solution costs can be expected to over-compensate costs for the Mortar formulation of the problem.

This package currently has the following capabilities and features:

* Construction of one or several non-conforming interfaces in a 2D/3D problem, where each side of an interface is discretized independently with low order finite elements.
* Handling of 'corner nodes' and 'edge nodes' in the case of several interfaces sharing common nodes.
* Construction of a discrete Lagrange multiplier space on interfaces, were the space is determined by the choice of the user and the trace spaces of the underlying finite element functions on the interfaces.
* Construction of corresponding coupling matrices and construction of a saddle point system of equations if the user provides the stiffness matrices of the subdomains.
* Construction of an equivalent sparse symmetric or unsymmetric definite system of equations if the user provides the stiffness matrices of the subdomains and depending on the user's choice of the Lagrange multiplier space.
* Full support interfaces to Trilinos packages Amesos (direct solvers), AztecOO (preconditioned iterative methods) and ML (algebraic multigrid preconditioners) for the solution of resulting systems of equations.
* The user for example might supply subdomain stiffness matrices, left and right hand side vectors, choose solver options, define interfaces among subdomains and then use the Moertel package and its solver interfaces to obtain the solution of the tied problem.
* Access to internal data to let the user build his/her own contact formulation or domain decomposition method based on Mortar formulations without the neccessity to actually implement core Mortar functionality.
* Easy extensibility due to a modular and object orientated implementation.
* Support for parallel and serial applications, where in the parallel case the interfaces are not expected to meet any partitioning requirements among processors, as most application will focuse on the partitioning of subdomains and not on interfaces. Partitioning of one or more interfaces is completely arbitrary.
* 'Grey box' mesh tying for low order finite element discretizations

Moertel severely depends on the other Trilinos packages Epetra, EpetraExt, Teuchos, Amesos, AztecOO and ML.

https://trilinos.org/packages/moertel/[+https://trilinos.org/packages/moertel/+]

https://trilinos.org/docs/dev/packages/moertel/doc/html/index.html[+https://trilinos.org/docs/dev/packages/moertel/doc/html/index.html+]

[[MOOCHO]]
MOOCHO
^^^^^^

MOOCHO (Multifunctional Object-Oriented arCHitecture for Optimization) is a Trilinos package written in Cxx designed to solve large-scale, equality and inequality nonlinearly constrained, non-convex optimization problems (i.e. nonlinear programs) using reduced-space successive quadratic programming (SQP) methods.

The current algorithms in MOOCHO are well suited to solving optimization problems with massive numbers of unknown variables and equations but few so-called degrees of optimization freedom (i.e. the degrees of freedom = the number of variables minus the number of equality constraints = $n-m$). Various line-search based globalization methods are available, including exact penalty functions and a form of the filter method. Many of the algorithms in MOOCHO are provably locally and globally convergent for a wide class of problems in theory but in practice the behavior and the performance of the algorithms varies greatly from problem to problem.

MOOCHO was initially developed to solve general sparse optimization problems where there is no clear distinction between state variables and optimization parameters. For these types of problems a serial sparse direct solver must be used (i.e. MA28) to find a square basis that is needed for the variable reduction decompositions that are current supported.
More recently, MOOCHO has been interfaced through Thyra and the Thyra::ModelEvaluator interface to address very large-scale, massively parallel, simulation-constrained optimization problems.

For simulation-constrained optimization problems, MOOCHO can utilize the full power of the massively parallel iterative linear solvers and preconditioners available in Trilinos through Thyra through the Stratimikos package by just flipping a few switches in a parameter list. These include all of the direct solves in Amesos, the preconditioners in Ifpack and ML, and the iterative Krylov solvers in AztecOO and Belos (Belos is not being released but is available in the development version of Trilinos). For small to moderate numbers of optimization parameters, the only bottleneck to parallel scalability is the linear solver used to solve linear systems involving the state Jacobian. The reduced-space SQP algorithms in MOOCHO itself achieve extremely good parallel scalability. The parallel scalability of the linear solvers is controlled by the simulation application and the Trilinos linear solvers and preconditioners themselves. Typically, the parallel scalability of the linear solve is limited by the preconditioner as the problem is partitioned to more and more processes.

MOOCHO also includes a minimally invasive mode for reduced-space SQP where the simulator application only needs to compute the objective and constraint functions and solve only forward linear systems.
All other derivatives can be approximated with directional finite differences but any exact derivatives that can be computed by the application are happily accepted and fully utilized by MOOCHO through the Thyra interface.

https://trilinos.org/packages/moocho/[+https://trilinos.org/packages/moocho/+]

https://trilinos.org/docs/dev/packages/moocho/doc/html/index.html[+https://trilinos.org/docs/dev/packages/moocho/doc/html/index.html+]

[[PAMGEN]]
PAMGEN
^^^^^^

A xref:Trilinos[Trilinos] package that creates hexahedral or quadrilateral (in 2D) finite element meshes of simple shapes (cubes and cylinders) in parallel. When linked to an application as a library, it allows each process of a parallel simulation to generate its finite element domain representation at execution time.

Serial generation of large finite-element meshes is a serious bottleneck for large parallel simulations. PAMGEN, a parallel mesh generation library, surmounts this barrier by allowing on-the-fly scalable generation of simple finite element meshes.  While each processor is provided with a complete specification of the mesh, it only creates a full representation of the elements that will be local to that processor. Since each processor has a complete specification of the mesh, no inter-processor communication is performed. The mesh generation proceeds through steps of decomposition, local element creation, and communication information generation. The final product of the library is a data structure that may be queried to determine local mesh geometry and topology as well as inter-processor connections. 

https://trilinos.org/packages/pamgen/[+https://trilinos.org/packages/pamgen/+]

[[Phalanx]]
Phalanx
^^^^^^^

Phalanx is a local graph-based field evaluation toolkit. While the intended use case is for solving general partial differential equations (PDEs), there is NO specific code implemented for PDEs in Phalanx. It can be applied to any system that requires function evaluation. In terms of PDE discretization schemes it can be used for finite element, finite difference and finite volume.

Phalanx is a local node evaluation tool. Phalanx relies on the Kokkos package for performance portability and provides a simple performant interface to multicore and manycore architectures. While its main use is for large scale parallel high performance computing, the MPI communication (possibly required for ghosting) must be handled by other packages in the toolchain (separation of concerns). Users can handle this manually as Phalanx places no requirements on this but we recomend the Tpetra package in Trilinos be leveraged.

The main goal of Phalanx is to decompose a complex problem into a number of simpler problems with managed dependencies to support rapid development and extensibility. Through the use of template metaprogramming concepts, Phalanx supports arbitrary user defined data types and evaluation types. This allows for extreme flexibility in integration with user applications and provides extensive support for embedded technology such as automatic differentiation for sensitivity analysis, optimization, and uncertainty quantification. This approach, coupled with the template capabilities of Cxx offers a number of unique and powerful capabilities:

* Fast integration with flexible and extensible models, including support for arbitrary
datatypes via template metaprogramming concepts
* Support for advanced embedding technology, e.g. replacing the default scalar type in a
code with an object that overloads the typical math operators
* Consistent field evaluations via dependency chain management: When users switch models, the dependencies for fields evaluated by the model can change. This can force field to be evaluated in a different order based on the new dependencies. Phalanx will automatically perform the sorting and ordering of the evaluator routines
* Efficient evaluation of field data: Phalanx was designed to support the concept of worksets, where a
workset is an arbitrarily sized block of cells (or egdesor faces or nodes) to be evaluated on the local processor.

Phalanx is a hammer. It's use should be carefully considered. We recommend its use when writing a general PDE framework where one needs support for flexibility in equation sets, discretizations, and material models. It should not be used for a fixed set of equations and a single discretization that never changes.

https://trilinos.org/packages/phalanx/[+https://trilinos.org/packages/phalanx/+]

https://trilinos.org/docs/dev/packages/phalanx/doc/html/index.html[+https://trilinos.org/docs/dev/packages/phalanx/doc/html/index.html+]

[[Piro]]
Piro
^^^^

Piro is the top-level, unifying package of the Embedded Nonlinear Analysis Capability area. The main purpose of the package is to provide driver classes for the common uses of Trilinos nonlinear analysis tools. These so-called Solvers all can be constructed similarly, with a Model Evaluator and a parameter list, to make it simple to switch between different types of analysis. As Reponse-Only Model Evaluators, they also all derive from the same base classes so that the resulting analysis can in turn be driven by non-intrusive analysis routines.

The name Piro is an acronym for "Parameters In -- Responses Out," which is a Tarzan-grammar description of the abstraction for a solved forward problem. It also is the name of a Native American Pueblo originally located near present day Socorro, New Mexico.

Piro attempts to unify the usage of the nonlinear analysis capabilities in Trilinos.
The nonlinear analysis tools that are (or will be) wrapped by Piro include:

* NOX: Nonlinear Solver
* LOCA: Continuation and Bifurcation Analysis Solver
* Rythmos: Transient DAE solver
* Stokhos: Embedded UQ solver for Stochastic-Galerkin over random variables
* MOOCHO: Emebedded Optimization algorithms with rSQP algorithms
* LIME: Algorithms for multi-physics coupling (under development)

Any Piro solver will satisfy the interface for non-intrusive optimization and UQ algorithms. In Trilinos, these include: 

* OptiPack: Distributed memory optimization algorithms with line searches
* MOOCHO: Distributed memory SQP methods with constraints
* Dakota via TriKota: The entire Dakota framework (developed at Sandia independently from Trilinos) is available through the adapters and build system of the TriKota package.

Piro provides convenient driver routines that perform the analysis using any of these packages.

https://trilinos.org/packages/piro/[+https://trilinos.org/packages/piro/+]

https://trilinos.org/docs/dev/packages/piro/doc/html/index.html[+https://trilinos.org/docs/dev/packages/piro/doc/html/index.html+]

[[Pliris]]
Pliris
^^^^^^

Pliris is an object-oriented interface to a LU solver for dense matrices on parallel platforms. These matrices are double precision real matrices distributed on a parallel machine.

The matrix is torus-wrap mapped onto the processors(transparent to the user) and uses partial pivoting during the factorization of the matrix. Each processor contains a portion of the matrix and the right hand sides determined by a distribution function to optimally load balance the computation and communication during the factorization of the matrix. The general prescription is that no processor can have no more(or less) than one row or column of the matrix than any other processor. Since the input matrix is not torus-wrapped permutation of the results is performed to "unwrap the results" which is transparent to the user.

https://trilinos.org/packages/pliris/[+https://trilinos.org/packages/pliris/+]

https://trilinos.org/docs/dev/packages/pliris/doc/html/index.html[+https://trilinos.org/docs/dev/packages/pliris/doc/html/index.html+]

[[PyTrilinos]]
PyTrilinos
^^^^^^^^^^

A set of python wrappers for selected Trilinos packages. This allows a python programmer to dynamically import Trilinos packages into a python script or the python command-line interpreter, allowing the creation and manipulation of Trilinos objects and the execution of Trilinos algorithms, without the need to constantly recompile.

https://trilinos.org/packages/pytrilinos/[+https://trilinos.org/packages/pytrilinos/+]

*Distributed Sparse Linear Algebra with PyTrilinos* (PDF) - http://trilinos.org/oldsite/packages/pytrilinos/UsersGuide.pdf[+http://trilinos.org/oldsite/packages/pytrilinos/UsersGuide.pdf+]

[[ROL]]
ROL
^^^

Rapid Optimization Library (ROL) is a Cxx package for large-scale optimization. It is used for the solution of optimal design, optimal control and inverse problems in large-scale engineering applications. Other uses include mesh optimization and image processing.

ROL aims to combine flexibility, efficiency and robustness. Key features:

* Matrix-free application programming interfaces (APIs) —enable direct use of application data structures and memory spaces, linear solvers, nonlinear solvers and preconditioners.
* State-of-the-art algorithms for unconstrained optimization, constrained optimization and optimization under uncertainty —enable inexact and adaptive function evaluations and iterative linear system solves.
* Special APIs for simulation-based optimization —enable a streamlined embedding into engineering applications, rigorous implementation verification and efficient use.
* Modular interfaces throughout the optimization process —enable custom and user-defined algorithms, stopping criteria, hierarchies of algorithms, and selective use of a variety of tools and components.

https://trilinos.org/packages/rol/[+https://trilinos.org/packages/rol/+]

https://trilinos.org/docs/dev/packages/rol/doc/html/index.html[+https://trilinos.org/docs/dev/packages/rol/doc/html/index.html+]

[[RTOp]]
RTOp
^^^^

The xref:Trilinos[Trilinos] Reduction/Transformation Operators provide the basic mechanism for implementing vector operations in a flexible and efficient manner.
The RTOp class categories are:

* classes that define  the basic interoperability mechanism for RTOp
* classes that make it easier to provide concrete RTOp implementations and to perform global reduction operations using MPI
* a collection of concrete reduction and/or transformation operator classes

https://trilinos.org/packages/rtop/[+https://trilinos.org/packages/rtop/+]

https://trilinos.org/docs/dev/packages/rtop/doc/html/index.html[+https://trilinos.org/docs/dev/packages/rtop/doc/html/index.html+]

[[Rythmos]]
Rythmos
^^^^^^^

Rythmos is a transient integrator for ordinary differential equations and differential-algebraic equations with support for explicit, implicit, one-step and multi-step algorithms. The fundamental design of Rythmos is aimed at supporting operator-split algorithms, multi-physics applications, block linear algebra, and adjoint integration.

The time integration software classes are:
* a re-implementation of the algorithms in the LLNL Sundials code IDA
* a four stage fourth order explicit RK stepper with fixed step-sizes provided by the application
* the explicit forward Euler algorithm with fixed step-sizes provided by the application
* the implicit backward Euler algorithm with fixed step-sizes provided by the application

https://trilinos.org/docs/dev/packages/rythmos/doc/html/index.html[+https://trilinos.org/docs/dev/packages/rythmos/doc/html/index.html+]

https://trilinos.org/packages/rythmos/[+https://trilinos.org/packages/rythmos/+]

[[Sacado]]
Sacado
^^^^^^

A set of automatic differentiation tools for Cxx applications. It provides templated classes for forward, reverse and Taylor mode automatic differentiation.
The basic AD classes in Sacado are:

* Forward mode AD with the number of derivative components chosen at run-time
* Forward mode AD with the number of derivative components chosen at compile-time
* Forward mode AD with the maximum number of derivative components chosen at compile-time but actual number used chosen at run-time
* Reverse mode
* High-order univariate Taylor polynomials

Experimental and/or advanced classes are:

* Forward mode AD using caching expression templates
* Forward mode AD with custom memory manager
* Taylor polynomial AD using caching expression templates

https://trilinos.org/packages/sacado/[+https://trilinos.org/packages/sacado/+]

https://trilinos.org/docs/dev/packages/sacado/doc/html/index.html[+https://trilinos.org/docs/dev/packages/sacado/doc/html/index.html+]

[[Shards]]
Shards
^^^^^^

This xref:Trilinos[Trilinos] package provides a suite of common tools for numerical and topological data that facilitate interoperability between typical software modules used to solve Partial Differential Equations (PDEs) by finite element, finite volume and finite difference methods. Shards comprises of two categories of tools: methods to manage and access information about cell topologies used in mesh-based methods for PDEs, and methods to work with multi-dimensional arrays used to store numerical data in corresponding computer codes. The basic cell topology functionality of Shards includes methods to query adjacencies of subcells, find subcell permutation with respect to a global cell and create user-defined custom cell topologies. Multi-dimensional array part of the package provides specialized compile-time dimension tags, multi-index access methods, rank and dimension queries.

Shards design is based on a domain model for cell topology data motivated by algebraic topology. In this approach the mesh is viewed as a chain complex consisting of 0,1,2 and 3-dimensional cells representing the nodes, edges, faces and elements in the mesh. Cell topologies are explicitly defined by a composition of subcells of dimension less or equal to that of the parent cell.

https://trilinos.org/packages/shards/[+https://trilinos.org/packages/shards/+]

https://github.com/trilinos/Trilinos/tree/master/packages/shards[+https://github.com/trilinos/Trilinos/tree/master/packages/shards+]

[[ShyLU]]
ShyLU
^^^^^

ShyLU is a package for solving sparse linear systems. It can be used either as a preconditioner or a full solver. Currently, we recommand using it as an Ifpack preconditioner.

ShyLU uses a hybrid direct/iterative approach based on Schur complements. The goal is to provide robustness similar to sparse direct solvers, but memory usage more similar to preconditioned iterative solvers.

ShyLU was designed as a node-level solver. It can use both MPI and threads in several ways. ShyLU was designed to: (a) solve difficult but medium-size problems; and (b) be used as a subdomain solver or smoother for very large problems with an iterative scheme. It is a purely algebraic method so can be used as a black-box solver. ShyLU has been particularly successful in electrical circuit simulation (Xyce).

ShyLU is highly configurable and supports a number of parameters to control how Schur complement is approximated (dropping and probing) and how the inner-outer iteratiion works. These features are not fully documented yet.

https://trilinos.org/packages/shylu/[+https://trilinos.org/packages/shylu/+]

https://github.com/trilinos/Trilinos/tree/master/packages/shylu[+https://github.com/trilinos/Trilinos/tree/master/packages/shylu+]

https://trilinos.org/docs/dev/packages/shylu/doc/html/index.html[+https://trilinos.org/docs/dev/packages/shylu/doc/html/index.html+]

[[STK]]
STK
^^^

The Sierra Toolkit Mesh product provides a unstructured mesh in-memory, parallel-distributed database. Mesh capabilities include a mesh topology data structure, mesh subsetting, coefficient data, mesh field data, support for changing the mesh topology, and support for parallel operations on the mesh.
The STK contains:

* support for threaded and CUDA algorithms;
* I/O routines;
* a linear assembly system supporting  assembly of matrix/vector contributions from mesh data in FEI
interfaces
* support for parallel distributed, heterogeneous, and dynamically modifiable unstructured meshes with computational field data
* a rebalance package supporting computing a new mesh partition to reduce computational load imbalance and the corresponding migration of mesh entities to the new partition.
* utility functions that provide many low level functions and classes which support the development of parallel applications

https://trilinos.org/packages/stk/[+https://trilinos.org/packages/stk/+]

https://github.com/trilinos/Trilinos/tree/master/packages/stk[+https://github.com/trilinos/Trilinos/tree/master/packages/stk+]

https://trilinos.org/docs/r11.6/packages/stk/doc/html/index.html[+https://trilinos.org/docs/r11.6/packages/stk/doc/html/index.html+]

[[Stokhos]]
Stokhos
^^^^^^^

Stokhos is a Trilinos package for applying intrusive stochastic Galerkin uncertainty quantification methods to nonlinear dynamical systems, primarily stochastic partial differential equations.

Stokhos provides several types of capabilities:

* Classes defining families of univariate and multivariate orthogonal polynomials: Stokhos::OneDOrthogPolyBasis and Stokhos::OrthogPolyBasis.
* Classes defining quadrature methods for orthogonal polynomials: Stokhos::Quadrature.
* Classes for generating stochastic residual coefficients using automatic differentiation with Sacado: Stokhos::OrthogPolyExpansion
* Classes for forming and solving stochastic Galerkin linear systems: Stokhos::MatrixFreeEpetraOp and Stokhos::MeanEpetraOp.
* Classes for providing a nonlinear interface to nonlinear stochastic Galerkin problems: Stokhos::SGModelEvaluator.

In addition to the standard Trilinos third-party libraries of BLAS, LAPACK, and MPI, Stokhos can make use of two more: the Fortran UQ Toolkit and Dakota.

https://trilinos.org/packages/stokhos/[+https://trilinos.org/packages/stokhos/+]

https://github.com/trilinos/Trilinos/tree/master/packages/stokhos[+https://github.com/trilinos/Trilinos/tree/master/packages/stokhos+]

https://trilinos.org/docs/dev/packages/stokhos/doc/html/index.html[+https://trilinos.org/docs/dev/packages/stokhos/doc/html/index.html+]

[[Teuchos]]
Teuchos
^^^^^^^

This provides a suite of common tools for Trilinos for developers to use.  These tools include memory management classes such as "smart" pointers and arrays, "parameter lists" for communicating hierarchical lists of parameters between library or application layers, templated wrappers for the BLAS and xref:LAPACK[LAPACK], XML parsers, and other utilities. They provide a unified "look and feel" across Trilinos packages, and help avoid common programming mistakes.

Teuchos contains several different types of software. These different collections are organized into different subpackages.  These are:

* *Core* subpackage contains basic, general-purpose utilities for memory management, low-level
language support, output support, exception handling, containers and testing unit support
* *ParameterList* subpackage contains the Teuchos::ParameterList class and related utilities. The Teuchos::ParameterList class is a serializable hierarchical object database. Many Trilinos packages let users specify run-time options and flags as a Teuchos::ParameterList. They also use it internally to pass information between different software layers.
* *Comm* subpackage contains support for SPMD parallel programs and useful utilities that depend on these.
* *Numerics* subpackage contains BLAS and LAPACK Cxx wrappers and Cxx classes that use these for dense serial matrices and vectors.
* *Parser* subpackage contains classes to parse text-based file formats described as formal grammars. It can be used to parse XML, YAML, and other complex textual input formats. It can be thought of as a version of Flex and Bison which is a set of Cxx functions acting on Cxx objects as opposed to a set of programs acting on source files.
* *Remainder* subpackage contains left over stuff that does not fit will into the above subpackages or is slated to be deprecated and removed.

https://trilinos.org/packages/teuchos/[+https://trilinos.org/packages/teuchos/+]

[[Thyra]]
Thyra
^^^^^

A set of xref:Trilinos[Trilinos] interfaces and supporting code that defines basic interoperability mechanisms between different types of numerical software. The foundation of all of these interfaces are the mathematical concepts of vectors, vector spaces, and linear operators. All other interfaces and support software is built on the basic operator/vector interfaces.

The Thyra modules contain:

* fundamental and extended operator/vector interoperability interfaces
* operator solve interfaces  that provide a high-level interface to preconditioners, linear solvers, and factories for these
* model evaluator and nonlinear solver interfaces

https://trilinos.org/docs/dev/packages/thyra/doc/html/index.html[+https://trilinos.org/docs/dev/packages/thyra/doc/html/index.html+]

https://trilinos.org/packages/thyra/[+https://trilinos.org/packages/thyra/+]

[[Tpetra]]
Tpetra
^^^^^^

Tpetra implements linear algebra objects, such as sparse matrices and dense vectors. Tpetra is "hybrid parallel," meaning that it uses at least two levels of parallelism:

* MPI (the Message Passing Interface) for distributed-memory parallelism, and
* any of various shared-memory parallel programming models within an MPI process.

We say "distributed linear algebra" because Tpetra objects may be distributed over one or more parallel MPI processes. The shared-memory programming models that Tpetra may use within a process include

* OpenMP
* POSIX Threads (Pthreads)
* Nvidia's CUDA programming model for graphics processing units (GPUs)

Tpetra differs from Epetra, Trilinos' previous distributed linear algebra package, in the following ways:

* Tpetra has native support for solving very large problems (with over 2 billion unknowns).
* Tpetra lets you construct matrices and vectors with different kinds of data, such as floating-point types of different precision, or complex-valued types. Our goal is for Tpetra objects to be able to contain any type of data that implements a minimal compile-time interface. Epetra objects only support double-precision floating-point data (of type double).
* Tpetra can exploit many different kinds of hybrid parallelism, and most of its kernels do so natively. Epetra only supports OpenMP shared-memory parallelism for a few kernels. Tpetra also has optimizations for shared-memory parallel systems with nonuniform memory access (NUMA). All effort in supporting future node-level computer architectures will go into Tpetra.

https://trilinos.org/packages/tpetra/[+https://trilinos.org/packages/tpetra/+]

https://trilinos.org/docs/dev/packages/tpetra/doc/html/index.html[+https://trilinos.org/docs/dev/packages/tpetra/doc/html/index.html+]

[[TriKota]]
TriKota
^^^^^^^

TriKota is a package that wraps the library-mode use of Dakota, which is Sandia's Toolkit for Large-Scale Engineering Optimization and Uncertainty Analysis. For information on Dakota's capabilities, see http://www.cs.sandia.gov/dakota . Note that TriKota only wraps the library-mode usage of Dakota, and not the more common black-box mode where Dakota is run as an executable.

The purposes of TriKota are:

* Simplify the build/install of Dakota libraries for Trilinos users so that Dakota capabilities can be accessed as simply as those in native Trilinos packages.
* Provide a simple and continuously maintained example for the library-mode usage of Dakota from a Trilinos code.
* Supply adapters between the Trilinos and Dakota interfaces, so that users can easily select between using Dakota or Trilinos analysis algorithms.

The Dakota project delivers both state-of-the-art research and robust, usable software for optimization and UQ. Broadly, the Dakota software's advanced parametric analyses enable design exploration, model calibration, risk analysis, and quantification of margins and uncertainty with computational models. The Dakota toolkit provides a flexible, extensible interface between such simulation codes and its iterative systems analysis methods, which include:

* optimization with gradient and nongradient-based methods;
* uncertainty quantification with sampling, reliability, stochastic expansion, and epistemic methods;
* parameter estimation using nonlinear least squares (deterministic) or Bayesian inference (stochastic); and
* sensitivity/variance analysis with design of experiments and parameter study methods.

https://trilinos.org/packages/trikota/[+https://trilinos.org/packages/trikota/+]

https://trilinos.org/docs/dev/packages/TriKota/doc/html/index.html[+https://trilinos.org/docs/dev/packages/TriKota/doc/html/index.html+]

TuckerMPI
~~~~~~~~~

As parallel computing trends towards the exascale, scientific data produced by high-fidelity simulations are growing increasingly massive. For instance, a simulation on a three-dimensional spatial grid with 512 points per dimension that tracks 64 variables per grid point for 128 time steps yields 8~TB of data, assuming double precision. By viewing the data as a dense five-way tensor, we can compute a Tucker decomposition to find inherent low-dimensional multilinear structure, achieving compression ratios of up to 5000 on real-world data sets with negligible loss in accuracy. So that we can operate on such massive data, we present the first-ever distributed-memory parallel implementation for the Tucker decomposition, whose key computations correspond to parallel linear algebra operations, albeit with nonstandard data layouts. Our approach specifies a data distribution for tensors that avoids any tensor data redistribution, either locally or in parallel. We provide accompanying analysis of the computation and communication costs of the algorithms. To demonstrate the compression and accuracy of the method, we apply our approach to real-world data sets from combustion science simulations. We also provide detailed performance results, including parallel performance in both weak and strong scaling experiments.

https://gitlab.com/tensors/TuckerMPI[+https://gitlab.com/tensors/TuckerMPI+]

https://arxiv.org/abs/1510.06689[+https://arxiv.org/abs/1510.06689+]

turbulucid
~~~~~~~~~~

A Python package for post-processing of plane two-dimensional data from computational fluid dynamics simulations is presented. The package, called turbulucid, provides means for scripted, reproducible analysis of large simulation campaigns and includes routines for both data extraction and visualization. For the former, the Visualization Toolkit (VTK) is used, allowing for post-processing of simulations performed on unstructured meshes. For visualization, several matplotlib-based functions for creating highly customizable, publication-quality plots are provided.

VTK has become a popular format for storing unstructured datasets (both 3d and 2d). The API of VTK and software such as Paraview already provide the means for working with VTK data, so why the need for a new package? The answer is that while the above-mentioned tools are excellent for general inspection of large 3d datasets, they do not provide the means for producing publication-quality plots of 2d data.
Also, while VTK provides an abundance of filters for extracting specific parts of a dataset, the object-oriented API is hard to learn and quite verbose. Turbulucid provides easier access to the data and a set of functions for performing simple extractions.

Under the hood turbulucid uses VTK to handle the data, but exposes everything to the user in terms of numpy arrays. The plotting functions use matplotlib, and return associated matplotlib objects. This allows the user to harness the full customization power of matplotlib to make the plots look exactly as desired.

https://github.com/timofeymukha/turbulucid[+https://github.com/timofeymukha/turbulucid+]

https://timofeymukha.github.io/turbulucid/[+https://timofeymukha.github.io/turbulucid/+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.213/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.213/+]

#UUUU

U-Boot
~~~~~~

Das U-Boot (subtitled "the Universal Boot Loader" and often shortened to U-Boot) is an open-source, primary boot loader used in embedded devices to package the instructions to boot the device's operating system kernel. It is available for a number of computer architectures, including 68k, ARM, Blackfin, MicroBlaze, MIPS, Nios, SuperH, PPC, RISC-V and x86.

U-Boot is both a first-stage and second-stage bootloader. It is loaded by the system's ROM or BIOS from a supported boot device, such as an SD card, SATA drive, NOR flash (e.g. using SPI or I²C), or NAND flash. If there are size constraints, U-Boot may be split into stages: the platform would load a small SPL (Secondary Program Loader), which is a stripped-down version of U-Boot, and the SPL would do initial hardware configuration and load the larger, fully featured version of U-Boot.[4][5][6] Regardless of whether the SPL is used, U-Boot performs both first-stage (e.g., configuring memory controllers and SDRAM) and second-stage booting (performing multiple steps to load a modern operating system from a variety of devices that must be configured, presenting a menu for users to interact with and control the boot process, etc.). 

https://en.wikipedia.org/wiki/Das_U-Boot[+https://en.wikipedia.org/wiki/Das_U-Boot+]

https://github.com/u-boot/u-boot[+https://github.com/u-boot/u-boot+]

https://fosdem.org/2019/schedule/event/hw_uboot/[+https://fosdem.org/2019/schedule/event/hw_uboot/+]

UBoxed
~~~~~~

Science Box is a container-based version of EOS, CERNBox, and SWAN services.
It is available in a single machine demo version UBoxed and a Kubernetes
scalable deployment version KUBoxed.

UBoxed A self-contained, containerized demo for next-generation cloud storage and computing services for scientific and general-purpose use.
This contains local versions of:

* https://cernbox.web.cern.ch/[CERNBox] - A cloud data storage service.
* https://eos.web.cern.ch/[EOS] - A software solution for central data recording, user analysis and data processing. 
* https://swan.web.cern.ch/[SWAN] - A platform to perform interactive data analysis in the cloud. This has  Jupyter notebook interface as well as shell access.
* https://cernvm.cern.ch/portal/filesystem[CVMFS] = A scalable, reliable and low-maintenance
software distribution service.

http://sciencebox.web.cern.ch/sciencebox/[+http://sciencebox.web.cern.ch/sciencebox/+]

https://github.com/cernbox/uboxed[+https://github.com/cernbox/uboxed+]

https://www.exascaleproject.org/event/jupyter/[+https://www.exascaleproject.org/event/jupyter/+]

UDR
~~~

UDR is a wrapper around rsync that enables rsync to use UDT.

UDR must be on the client and server machines that data will be transferred between. UDR uses ssh to do authentication and automatically start the server-side UDR process. At least one UDP port needs to be open between the machines, by default UDR starts with port 9000 and looks for an open port up to 9100, changing this is an option. Encryption is off by default. When turned on encryption uses OpenSSL with aes-128 by default.

https://github.com/LabAdvComp/UDR[+https://github.com/LabAdvComp/UDR+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

UGRID
~~~~~

A proposal for storing unstructured (or flexible mesh) model data in the Unidata Network Common Data Form (NetCDF) file.

Our focus is on data for environmental applications and hence we start from the Climate & Forecast (CF) Metadata Conventions. The CF Conventions have been the standard in climate research for many years, and are being adopted by others as the metadata standard (e.g. NASA,Open Geospatial Consortium). The CF conventions allow you to provide the geospatial and temporal coordinates for scientific data, but currently assumes that the horizontal topology may be inferred from the i,j indices of structured grids. This proposal adds conventions for specifying the topology for unstructured (e.g. triangular) grids.

In its most basic form unstructured data may be stored as data defined at a series of points, the CF-conventions are then sufficient. However, it is often useful or even necessary to also know the topology of the underlying unstructured mesh: is it a one dimensional (1D) network, a two dimensional (2D) triangular mesh or more flexible mixed triangle/quadrilateral mesh, a 2D mesh with vertical layers, or a fully unstructured three dimensional (3D) mesh. This document describes the attribute conventions for storing the mesh topology and for associating variables with (specific locations on) the mesh topology. The conventions have been designed to store the output data of a combined 1D-2D-3D flow model with staggered data, but the metadata for a simple 1D network or 2D triangular mesh doesn’t suffer from the genericity needed for the most complex models.

http://ugrid-conventions.github.io/ugrid-conventions/[+http://ugrid-conventions.github.io/ugrid-conventions/+]

pyugrid
^^^^^^^

A Python API to utilize data written using the netCDF unstructured grid conventions.

This is being continued at https://github.com/NOAA-ORR-ERD/gridded[+https://github.com/NOAA-ORR-ERD/gridded+].

https://github.com/pyugrid/pyugrid[+https://github.com/pyugrid/pyugrid+]

ugtm
~~~~

Generative topographic mapping (GTM) is a probabilistic dimensionality reduction algorithm
which can also be used for classification and regression using class maps or activity landscapes,
It is a probabilistic counterpart of Kohonen maps.

ugtm is a python package implementing GTM and GTM prediction algorithms. ugtm contains the core functions and runGTM.py (in bin directory) is an easy-to-use program. The kernel version of the algorithm (kGTM) is also implemented. You can also generate regression or classification maps, or evaluate the predictive accuracy (classification) or RMSE/R2 (regression) in repeated cross-validation experiments.

https://github.com/hagax8/ugtm[+https://github.com/hagax8/ugtm+]

https://ugtm.readthedocs.io/en/latest/[+https://ugtm.readthedocs.io/en/latest/+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.235/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.235/+]

ulmo
~~~~

Ulmo provides a clean, simple and fast access to public hydrology and climatology data. It retrieves and parses datasets from the web, returns simple python data structures that can be easily pulled into more sophisticated tools for analysis, and caches datasets locally and harvests updates as needed.

https://github.com/ulmo-dev/ulmo[+https://github.com/ulmo-dev/ulmo+]

UMAP
~~~~

Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data

* The data is uniformly distributed on a Riemannian manifold;
* The Riemannian metric is locally constant (or can be approximated as such);
* The manifold is locally connected.

From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.

https://github.com/lmcinnes/umap[+https://github.com/lmcinnes/umap+]

Umbrella
~~~~~~~~

Umbrella is a tool for specifying and materializing comprehensive execution environments, from the hardware all the way up to software and data. A user simply invokes Umbrella with the desired task, and Umbrella parses the specification, determines the minimum mechanism necessary to run the task, downloads missing dependencies, and executes the application through the available minimal mechanism, which may be direct execution, a system container (Docker, chroot, Parrot), a local virtual machine (i.e., VMware), or submission to a cloud environment (i.e., Amazon EC2) or grid environment (i.e., Condor).

Umbrella contains five parts: user inputs, Umbrella, underlying execution engines, remote archive and metadata database. User inputs include the specification, the task command, the input files, and the output directory. Umbrella connects the user's execution environment specification with the underlying execution engines, which includes local resources, clusters, cloud resources, and grid resources. The remote archive stores the OS images, software dependencies and data dependencies. The metadata database maintains the mapping relationship between the dependency name referred in the specification and the actual storage location within the remote archive.

Currently, Umbrella supports the following execution engines: parrot, destructive, docker, ec2, local, and condor. Parrot execution engine can be used without any special authority on the host machine; Docker execution engine requires Docker is installed on the host machine and the user is given the right authority to use Docker; destructive execution engine requires the user to be the root user. Local execution engine first check whether Docker exists. If yes, use docker execution engine; if not, use parrot execution engine. 

http://ccl.cse.nd.edu/software/umbrella/[+http://ccl.cse.nd.edu/software/umbrella/+]

UMWM
~~~~

A third-generation spectral ocean wave model.

This is the reference implementation of UMWM, described by Donelan et al. (2012). UMWM solves the wave energy balance equation on a curvilinear grid. It has been used to simulate:

* Global swell and windsea
* Waves in coastal and hurricane conditions
* Wave-induced material transport (Stokes drift)
* Ancient Martian seas and methane lakes on Titan
* Waves in laboratory settings such as wave tanks

he University of Miami Wave Model (UMWM) is a prediction model for wave energy and wind stress on the interface between a liquid and a gas.
It proceeds through a numerical solution of the wave energy balance equation on a horizontal 2-dimensional grid. 
The wave energy is a positive definite quantity in logarithmically spaced frequency bins and uniformly spaced directional bins. 
The energy spectrum is carried as the wavenumber-directional surface elevation variance spectrum and every frequency at each location is identified with the theoretical wavenumber. 
Thus wave energy is a 5-dimensional quantity

The source functions are parametric descriptions of the various phenomena that increase, decrease or interchange among wavenumbers the energy in the wavenumber spectrum. 
The wavenumber spectrum is evaluated in separable magnitude and direction bins. 
The phenomena relevant to the prediction of storm waves in water of arbitrary depth are:

* Input of energy and momentum from the wind and export of wave energy and momentum to the wind when the waves overrun or run against the wind;
* Dissipation of wave energy at and near the surface due to viscosity, ambient turbulence and breaking;
* Enhanced dissipation at and near the top and bottom interfaces due to shoaling;
* Movement of energy to lower wavenumbers (down-shifting) due to nonlinear interactions including breaking;
* Enhanced dissipation due to straining by longer waves.

https://github.com/umwm/umwm[+https://github.com/umwm/umwm+]

https://umwm.org/[+https://umwm.org/+]

unfold
~~~~~~

unfold reads a binary STL file on standard input and generates a SVG that contains the triangles "folded flat" so that they can be laser cut. It will output multiple groups in the SVG file that will need to be re-arranged to fit on the laser cutter bed.

https://github.com/osresearch/papercraft[+https://github.com/osresearch/papercraft+]

UnionFS
~~~~~~~

Unionfs is a filesystem service for Linux, FreeBSD and NetBSD which implements a union mount for other file systems. It allows files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system. Contents of directories which have the same path within the merged branches will be seen together in a single merged directory, within the new, virtual filesystem.

When mounting branches, the priority of one branch over the other is specified. So when both branches contain a file with the same name, one gets priority over the other.

The different branches may be either read-only or read-write file systems, so that writes to the virtual, merged copy are directed to a specific real file system. This allows a file system to appear as writable, but without actually allowing writes to change the file system, also known as copy-on-write. This may be desirable when the media is physically read-only, such as in the case of Live CDs. 

https://en.wikipedia.org/wiki/UnionFS[+https://en.wikipedia.org/wiki/UnionFS+]

http://unionfs.filesystems.org/[+http://unionfs.filesystems.org/+]

http://wrapfs.filesystems.org/[+http://wrapfs.filesystems.org/+]

http://www.filesystems.org/[+http://www.filesystems.org/+]

Unison
~~~~~~

Unison is a file-synchronization tool for OSX, Unix, and Windows. It allows two replicas of a collection of files and directories to be stored on different hosts (or different disks on the same host), modified separately, and then brought up to date by propagating the changes in each replica to the other.

Unison shares a number of features with tools such as configuration management packages (CVS, PRCS, Subversion, BitKeeper, etc.), distributed filesystems (Coda, etc.), uni-directional mirroring utilities (rsync, etc.), and other synchronizers (Intellisync, Reconcile, etc). However, there are several points where it differs:

* Unison runs on both Windows and many flavors of Unix (Solaris, Linux, OS X, etc.) systems. Moreover, Unison works across platforms, allowing you to synchronize a Windows laptop with a Unix server, for example.
* Unlike simple mirroring or backup utilities, Unison can deal with updates to both replicas of a distributed directory structure. Updates that do not conflict are propagated automatically. Conflicting updates are detected and displayed.
* Unlike a distributed filesystem, Unison is a user-level program: there is no need to modify the kernel or to have superuser privileges on either host.
* Unison works between any pair of machines connected to the internet, communicating over either a direct socket link or tunneling over an encrypted ssh connection. It is careful with network bandwidth, and runs well over slow links such as PPP connections. Transfers of small updates to large files are optimized using a compression protocol similar to rsync.
* Unison is resilient to failure. It is careful to leave the replicas and its own private structures in a sensible state at all times, even in case of abnormal termination or communication failures.
* Unison has a clear and precise specification.
* Unison is free; full source code is available under the GNU Public License. 

http://www.cis.upenn.edu/\~bcpierce/unison/[+http://www.cis.upenn.edu/~bcpierce/unison/+]

https://github.com/bcpierce00/unison[+https://github.com/bcpierce00/unison+]

https://www.tecmint.com/file-synchronization-in-linux-using-unison/[+https://www.tecmint.com/file-synchronization-in-linux-using-unison/+]

https://www.linode.com/docs/tools-reference/tools/synchronize-files-with-unison/[+https://www.linode.com/docs/tools-reference/tools/synchronize-files-with-unison/+]

UPC
~~~

Unified Parallel C (UPC) is an extension of the C programming language designed for high performance computing on large-scale parallel machines.The language provides a uniform programming model for both shared and distributed memory hardware. The programmer is presented with a single shared, partitioned address space, where variables may be directly read and written by any processor, but each variable is physically associated with a single processor. UPC uses a Single Program Multiple Data (SPMD) model of computation in which the amount of parallelism is fixed at program startup time, typically with a single thread of execution per processor.

The goal of the Berkeley UPC compiler group is to develop a portable, high performance implementation of UPC for large-scale multiprocessors, PC clusters, and clusters of shared memory multiprocessors.  We are actively developing an open-source UPC compiler suite whose goals are portability and high-performance. 

* Lightweight Runtime and Networking Layers: On distributed memory hardware, references to remote shared variables usually translate into calls to a communication library. Because of the shared memory abstraction that it offers, UPC encourages a programming style where remote data is accessed with a low granularity (i.e. the granularity of an access is often the size of the primitive C types - int, float, double).  In order to be able to obtain good performance from an implementation, it is therefore important that the overhead of accessing the underlying communication hardware is minimized and the implementation exploits the most efficient hardware mechanisms available.  Our group has thus developed a lightweight communication and run-time layer for global address space programming languages. 

* Compilation techniques for explicitly parallel languages: The group is working on developing communication optimizations to mask the latency of network communication, aggregate communication into more efficient bulk operations, and cache data locally. UPC allows programmers to specify memory accesses with "relaxed " consistency semantics, which can be exploited by the compiler to hide communication latency by overlapping communications with computation and/or other communication. 

* Application benchmarks: The group is working on benchmarks and applications to demonstrate the features of the UPC language and compilers, especially targeting problems with irregular computation and communication patterns. This effort will also allow us to determine the potential for optimizations in UPC programs. In general, applications with fine-grained data sharing benefit from the lightweight communication that underlies UPC implementations, and the shared address space model is especially appropriate when the communication is asynchronous. 

* Active Testing: UPC programs can have classes of bugs not possible in a programming model such as MPI. In order to help find and correct data races, deadlocks and other programming errors, we are working on UPC Thrille. 

* Dynamic Tasking: UPC Task Library is a simple and effective way of adding task parallelism to SPMD programs. It provides a high-level API that abstracts concurrent task management details and a dynamic load balancing mechanism. 

There are multiple compiler infrastructures available for use with the Berkeley UPC runtime and compiler driver. The LLVM-based (Clang-UPC) and GCC-based (GUPC) compilers are developed by INTREPID Technology Inc.. An Open64-based (BUPC) translator is developed at LBNL.
The compilers are:

* Clang-UPC source-to-source UPC-to-C (CUPC2C) translator

* Clang-UPC source-to-binary (CUPC) compiler

* Open64-based source-to-source UPC-to-C (BUPC) translator

* GNU UPC source-to-binary (GUPC) compiler

http://upc.lbl.gov/[+http://upc.lbl.gov/+]

http://clangupc.github.io/download[+http://clangupc.github.io/download+]

http://upc.lbl.gov/download/[+http://upc.lbl.gov/download/+]

http://gccupc.org/download[+http://gccupc.org/download+]

https://upc-lang.org/[+https://upc-lang.org/+]

Upspin
~~~~~~

Upspin provides a global name space to name all your files. Given an Upspin name, a file can be shared securely, copied efficiently without “download” and “upload”, and accessed from anywhere that has a network connection.

Its target audience is personal users, families or groups of friends. Although it might have application in corporate environments, that is not its motivation.

Upspin provides a uniform naming mechanism for all data, along with easy-to-understand and easy-to-use secure sharing, as well as end-to-end encryption that guarantees privacy.

Upspin is not an “app” or a web service, but rather a suite of software components, intended to run in the network and on devices connected to it, that together provide a secure information storage and sharing network. Upspin is a layer of infrastructure that other software and services can build on to facilitate secure access and sharing.

Upspin looks a bit like a global file system, but its real contribution is a set of interfaces, protocols and components from which an information management system can be built, with properties such as security and access control suited to a modern, networked world.

https://upspin.io/[+https://upspin.io/+]

https://github.com/upspin/upspin[+https://github.com/upspin/upspin+]

usql
~~~~

A universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases.

usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support.

Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases.

https://github.com/xo/usql[+https://github.com/xo/usql+]

#VVVV

vaex
~~~~

Vaex is a python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It can calculate statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid for more than a billion (10^9) objects/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).

https://github.com/vaexio/vaex[+https://github.com/vaexio/vaex+]

https://vaex.io/[+https://vaex.io/+]

Vagrant
~~~~~~~

Vagrant is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the "works on my machine" excuse a relic of the past.

Vagrant provides easy to configure, reproducible, and portable work environments built on top of industry-standard technology and controlled by a single consistent workflow to help maximize the productivity and flexibility of you and your team.

To achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can automatically install and configure software on the virtual machine.

If you are a developer, Vagrant will isolate dependencies and their configuration within a single disposable, consistent environment, without sacrificing any of the tools you are used to working with (editors, browsers, debuggers, etc.). Once you or someone else creates a single Vagrantfile, you just need to vagrant up and everything is installed and configured for you to work.

https://www.vagrantup.com/[+https://www.vagrantup.com/+]

VAPOR
~~~~~

VAPOR is the Visualization and Analysis Platform for Ocean, Atmosphere, and Solar Researchers.  VAPOR provides an interactive 3D visualization environment that can also produce animations and still frame images.  VAPOR runs on most UNIX and Windows systems equipped with modern 3D graphics cards.

The VAPOR Data Collection (VDC) data model allows users progressively access the fidelity of their data, allowing for the visualization of terascale data sets on commodity hardware.  VAPOR can also directly import data formats including WRF, MOM, POP, ROMS, and some GRIB and NetCDF files.

Users can perform ad-hoc analysis with VAPOR's interactive Python interpreter; which allows for the creation, modification, and visualization of new variables based on input model data.

VAPOR3 supports data sampled on both structured and unstructured grids. The addition of support for unstructured grids, and greatly improved support for structured grids, is one of the most signficant changes between VAPOR3 and VAPOR2.
 
VAPOR3's beta release supports outputs from WRF, MPAS-A, POP atmosphere and ocean models, and, in general, supports NetCDF Climate and Forecast (NetCDF CF) compliant data sets.

One of VAPOR3's most significant features is its ability to create renderings of datasets that come from different models, simultaneously.  For example, multiple nested WRF domains may be loaded and rendered alongside each other, to show how the effects of coarsened grids on simulated phenomena.  Ocean and atmosphere models may be rendered together to demonstrate the interactions that occur at their interface.

https://www.vapor.ucar.edu/[+https://www.vapor.ucar.edu/+]

https://github.com/NCAR/VAPOR[+https://github.com/NCAR/VAPOR+]

VCL
~~~

The Visual Compute Library was designed to provide an interface through which users can interact with visual data. We currently support images, though we are in the process of developing support for feature vectors and videos.

The VCL was developed to support the use of an array-based image format, the TDB format that is built on the array data manager TileDB. This results in a new lossless image format that is well suited for visual analytics. In addition, the VCL provides the same interface for PNG and JPEG formats by implementing an abstraction layer over OpenCV.

https://github.com/IntelLabs/vcl[+https://github.com/IntelLabs/vcl+]

VDMS
~~~~

VDMS is a storage solution for efficient access of big-”visual”-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. We use an in-persistent-memory graph database developed in our team called Persistent Memory Graph Database (PMGD) as the metadata tier and we are exploring the use of an array data manager, TileDB and other formats for images, visual descriptors, and videos as part of our Visual Compute Library (VCL). VDMS is run as a server listening for client requests and we provide client side bindings to enable communication between ( python, Cxx) applications and the server. Hence, it also has a Request Server component defined to implement the VDMS API, handle concurrent client requests, and coordinate the request execution across its metadata and data components to return unified responses. This project aims to research the use of a scalable multi-node graph based metadata store as part of a hierarchical storage framework specifically aimed at processing visual data, and also it includes an investigation into the right hardware and software optimizations to store and efficiently access large scale (pre-processed) visual data.

Data access is swiftly becoming a bottleneck in visual data processing, providing an opportunity to influence the way visual data is treated in the storage system. To foster this discussion, we identify two key areas where storage research can strongly influence visual processing run-times: efficient metadata storage and new storage formats for visual data. We propose a storage architecture designed for efficient visual data access that exploits next generation hardware and give preliminary results showing how it enables efficient vision analytics.

https://github.com/IntelLabs/vdms[+https://github.com/IntelLabs/vdms+]

https://github.com/IntelLabs/vdms/wiki[+https://github.com/IntelLabs/vdms/wiki+]

Verde
~~~~~

Verde is a Python library for processing spatial data (bathymetry, geophysics surveys, etc) and interpolating it on regular grids (i.e., gridding).

Most gridding methods in Verde use a Green's functions approach. A linear model is estimated based on the input data and then used to predict data on a regular grid (or in a scatter, a profile, as derivatives). The models are Green's functions from (mostly) elastic deformation theory. This approach is very similar to machine learning so we implement gridder classes that are similar to scikit-learn regression classes. The API is not 100% compatible but it should look familiar to those with some scikit-learn experience.

https://github.com/fatiando/verde[+https://github.com/fatiando/verde+]

http://www.fatiando.org/verde/latest/[+http://www.fatiando.org/verde/latest/+]

http://joss.theoj.org/papers/610a04bf43ba7913a1ad3b504aa02577[+http://joss.theoj.org/papers/610a04bf43ba7913a1ad3b504aa02577+]

Verilog
~~~~~~~

Verilog, standardized as IEEE 1364, is a hardware description language (HDL) used to model electronic systems. It is most commonly used in the design and verification of digital circuits at the register-transfer level of abstraction. It is also used in the verification of analog circuits and mixed-signal circuits, as well as in the design of genetic circuits.

https://en.wikipedia.org/wiki/Verilog[+https://en.wikipedia.org/wiki/Verilog+]

Icarus Verilog
^^^^^^^^^^^^^^

Icarus Verilog is a Verilog simulation and synthesis tool. It operates as a compiler, compiling source code written in Verilog (IEEE-1364) into some target format. For batch simulation, the compiler can generate an intermediate form called vvp assembly. This intermediate form is executed by the ``vvp'' command. For synthesis, the compiler generates netlists in the desired format.

The compiler proper is intended to parse and elaborate design descriptions written to the IEEE standard IEEE Std 1364-2005. This is a fairly large and complex standard, so it will take some time to fill all the dark alleys of the standard, but that's the goal. 

http://iverilog.icarus.com/[+http://iverilog.icarus.com/+]

ViennaCL
~~~~~~~

The Vienna Computing Library (ViennaCL) is a scientific computing library written in C++. It provides simple, high-level access to the vast computing resources available on parallel architectures such as GPUs and multi-core CPUs by using either a host based computing backend, an OpenCL computing backend, or CUDA. The primary focus is on common linear algebra operations (BLAS levels 1, 2 and 3) and the solution of large sparse systems of equations by means of iterative methods. The following iterative solvers are currently implemented:

* Conjugate Gradient (CG)
* Stabilized BiConjugate Gradient (BiCGStab)
* Generalized Minimum Residual (GMRES)

A number of preconditioners is also provided in order to improve convergence of these solvers.

The solvers and some preconditioners can also be used with different libraries due to their generic implementation. Currently it is possible to use the solvers and some preconditioners directly with types from the uBLAS library, which is part of Boost.
The iterative solvers can directly be used with Armadillo, Eigen, and MTL 4.

Under the hood, ViennaCL uses a unified layer to access CUDA, OpenCL, and/or OpenMP for accessing and executing code on compute devices. Therefore, ViennaCL is not tailored to products from a particular vendor and can be used on many different platforms. At present, ViennaCL is known to work on all current CPUs and modern GPUs from NVIDIA and AMD (see table below), CPUs using either the AMD Accelerated Parallel Processing (APP) SDK (formerly ATI Stream SDK) or the Intel OpenCL SDK, and Intels MIC platform (Xeon Phi). Double precision arithmetic on GPUs is only possible if provided in hardware by the respective device. There is no double precision emulation in software in ViennaCL.

http://viennacl.sourceforge.net/[+http://viennacl.sourceforge.net/+]

http://viennacl.sourceforge.net/doc/usermanual.html[+http://viennacl.sourceforge.net/doc/usermanual.html+]

https://github.com/viennacl[+https://github.com/viennacl+]

PyViennaCL
^^^^^^^^^^

PyViennaCL provides the Python bindings for the ViennaCL linear algebra and numerical computation library for GPGPU and heterogeneous systems. These bindings make available to Python programmers ViennaCL’s fast OpenCL and CUDA algorithms, in a way that is idiomatic and compatible with the Python community’s most popular scientific packages, NumPy and SciPy.

http://viennacl.sourceforge.net/pyviennacl.html[+http://viennacl.sourceforge.net/pyviennacl.html+]

https://github.com/viennacl/pyviennacl-dev[+https://github.com/viennacl/pyviennacl-dev+]

Virgil 3D
~~~~~~~~~

Virgil is a research project to investigate the possibility of creating a virtual 3D GPU for use inside qemu virtual machines, that allows the guest operating system to use the capabilities of the host GPU to accelerate 3D rendering. The plan is to have a guest GPU that is fully independent of the host GPU.

The project entails creating a virtual 3D capable graphics card for virtual machines running inside qemu. The design of this card is based around the concepts of Gallium3D to make writing Mesa and (eventually) Direct3D drivers for it easy. The card natively uses the Gallium TGSI intermediate representation for its shaders. The implementation of rendering for the card is done in the host system as part of qemu and is implemented purely on OpenGL so you can accelerated rendering on any sufficiently capable card/driver combination.

The project also consists of a complete Linux guest stack, composed of a Linux kernel KMS driver, X.org 2D DDX driver and Mesa 3D driver.

https://virgil3d.github.io/[+https://virgil3d.github.io/+]

https://cgit.freedesktop.org/virglrenderer[+https://cgit.freedesktop.org/virglrenderer+]

https://fosdem.org/2019/schedule/event/virtual_gpu/[+https://fosdem.org/2019/schedule/event/virtual_gpu/+]

virtualization
~~~~~~~~~~~~~~

In computing, virtualization refers to the act of creating a virtual (rather than actual) version of something, including virtual computer hardware platforms, storage devices, and computer network resources

Hardware virtualization or platform virtualization refers to the creation of a virtual machine that acts like a real computer with an operating system. Software executed on these virtual machines is separated from the underlying hardware resources. For example, a computer that is running Microsoft Windows may host a virtual machine that looks like a computer with the Ubuntu Linux operating system; Ubuntu-based software can be run on the virtual machine.[2][3]

In hardware virtualization, the host machine is the machine which is used by the virtualization and the guest machine is the virtual machine. The words host and guest are used to distinguish the software that runs on the physical machine from the software that runs on the virtual machine. The software or firmware that creates a virtual machine on the host hardware is called a hypervisor or virtual machine monitor.

Different types of hardware virtualization include:

* Full virtualization – almost complete simulation of the actual hardware to allow a software environments, including a guest operating system and its apps, to run unmodified.
* Paravirtualization – the guest apps are executed in their own isolated domains, as if they are running on a separate system, but a hardware environment is not simulated. Guest programs need to be specifically modified to run in this environment.

Hardware virtualization is not the same as hardware emulation. In hardware emulation, a piece of hardware imitates another, while in hardware virtualization, a hypervisor (a piece of software) imitates a particular piece of computer hardware or the entire computer. Furthermore, a hypervisor is not the same as an emulator; both are computer programs that imitate hardware, but their domain of use in language differs. 

*Hypervisor Virtualization*

A hypervisor or virtual machine monitor (VMM) is computer software, firmware or hardware that creates and runs virtual machines. A computer on which a hypervisor runs one or more virtual machines is called a host machine, and each virtual machine is called a guest machine. The hypervisor presents the guest operating systems with a virtual operating platform and manages the execution of the guest operating systems. Multiple instances of a variety of operating systems may share the virtualized hardware resources: for example, Linux, Windows, and macOS instances can all run on a single physical x86 machine. This contrasts with operating-system-level virtualization, where all instances (usually called containers) must share a single kernel, though the guest operating systems can differ in user space, such as different Linux distributions with the same kernel. 

*Containerization*

Operating-system-level virtualization, also known as containerization, refers to an operating system feature in which the kernel allows the existence of multiple isolated user-space instances. Such instances, called containers,[1] partitions, virtual environments (VEs) or jails (FreeBSD jail or chroot jail), may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside a container can only see the container's contents and devices assigned to the container. 

https://en.wikipedia.org/wiki/Virtualization[+https://en.wikipedia.org/wiki/Virtualization+]

Firecracker
^^^^^^^^^^^

Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services that provide serverless operational models. Firecracker runs workloads in lightweight virtual machines, called microVMs, which combine the security and isolation properties provided by hardware virtualization technology with the speed and flexibility of containers.

The main component of Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel Virtual Machine (KVM) to create and run microVMs. Firecracker has a minimalist design. It excludes unnecessary devices and guest-facing functionality to reduce the memory footprint and attack surface area of each microVM. This improves security, decreases the startup time, and increases hardware utilization. Firecracker currently supports Intel CPUs, with planned AMD and Arm support. Firecracker will also be integrated with popular container runtimes.

Firecracker was developed at Amazon Web Services to accelerate the speed and efficiency of services like AWS Lambda and AWS Fargate. Firecracker is open sourced under Apache version 2.0.

https://github.com/firecracker-microvm/firecracker[+https://github.com/firecracker-microvm/firecracker+]

https://firecracker-microvm.github.io/[+https://firecracker-microvm.github.io/+]

KVM
^^^

Kernel-based Virtual Machine (KVM) is a virtualization module in the Linux kernel that allows the kernel to function as a hypervisor. It was merged into the Linux kernel mainline in kernel version 2.6.20, which was released on February 5, 2007. KVM requires a processor with hardware virtualization extensions, such as Intel VT or AMD-V.

KVM provides hardware-assisted virtualization for a wide variety of guest operating systems including Linux, BSD, Solaris, Windows, Haiku, ReactOS, Plan 9, AROS Research Operating System and OS X.
Additionally, KVM provides paravirtualization support for Linux, OpenBSD, FreeBSD, NetBSD, Plan 9 and Windows guests using the VirtIO API. This includes a paravirtual Ethernet card, disk I/O controller, balloon device, and a VGA graphics interface using SPICE or VMware drivers. 

https://www.linux-kvm.org/page/Main_Page[+https://www.linux-kvm.org/page/Main_Page+]

https://planet.virt-tools.org/[+https://planet.virt-tools.org/+]

libvirt
^^^^^^^

libvirt is an open-source API, daemon and management tool for managing platform virtualization. It can be used to manage KVM, Xen, VMware ESXi, QEMU and other virtualization technologies. These APIs are widely used in the orchestration layer of hypervisors in the development of a cloud-based solution. 

libvirt is a C library with bindings in other languages, notably in Python, Perl, OCaml, Ruby, Java, JavaScript (via Node.js) and PHP. libvirt for these programming languages is composed of wrappers around another class/package called libvirtmod. libvirtmod's implementation is closely associated with its counterpart in C/Cxx in syntax and functionality.

Various virtualization programs and platforms use libvirt. Virtual Machine Manager and others provide graphical interfaces. The most popular command line interface is virsh, and higher level tools such as oVirt.

https://libvirt.org/[+https://libvirt.org/+]

https://en.wikipedia.org/wiki/Libvirt[+https://en.wikipedia.org/wiki/Libvirt+]

LXC
^^^

LXC (Linux Containers) is an operating-system-level virtualization method for running multiple isolated Linux systems (containers) on a control host using a single Linux kernel.

The Linux kernel provides the cgroups functionality that allows limitation and prioritization of resources (CPU, memory, block I/O, network, etc.) without the need for starting any virtual machines, and also namespace isolation functionality that allows complete isolation of an applications' view of the operating environment, including process trees, networking, user IDs and mounted file systems.

LXC provides operating system-level virtualization through a virtual environment that has its own process and network space, instead of creating a full-fledged virtual machine. LXC relies on the Linux kernel cgroups functionality that was released in version 2.6.24. It also relies on other kinds of namespace isolation functionality, which were developed and integrated into the mainline Linux kernel. 

LXC is similar to other OS-level virtualization technologies on Linux such as OpenVZ and Linux-VServer, as well as those on other operating systems such as FreeBSD jails, AIX Workload Partitions and Solaris Containers. In contrast to OpenVZ, LXC works in the vanilla Linux kernel requiring no additional patches to be applied to the kernel sources. 

https://linuxcontainers.org/[+https://linuxcontainers.org/+]

https://github.com/lxc[+https://github.com/lxc+]

https://en.wikipedia.org/wiki/LXC[+https://en.wikipedia.org/wiki/LXC+]

NEMU
^^^^

NEMU is an open source hypervisor specifically built and designed to run modern cloud workloads on modern 64-bit Intel and ARM CPUs.

There currently is no open source hypervisor solutions with a clear and narrow focus on running cloud specific workloads on modern CPUs. All available solutions have evolved over time and try to be fairly generic. They attempt to support a wide range of virtual hardware architectures and run on hardware that has varying degree of hardware virtualization support. This results in a need to provide a large set of legacy platforms and device models requiring CPU, device and platform emulation. As a consequence they are built on top of large and complex code bases.

NEMU on the other hand aims to leverage KVM, be narrow focused on exclusively running modern, cloud native workloads, on top of a limited set of hardware architectures and platforms. It assumes fairly recent CPUs and KVM allowing for the the elimination of most emulation logic.

This will allow for smaller code base, lower complexity and a reduced attack surface compared to existing solutions. It also gives more space for providing cloud specific optimizations and building a more performant hypervisor for the cloud. Reducing the size and complexity of the code allows for easier review, fuzz testing, modularization and future innovation.

NEMU is based off QEMU and leverage its rich feature set, but with a much narrower focus. It leverages the performant, robust and stable QEMU codebase without the need to supporting the myriad of features, platforms and harware that are not relevant for the cloud.

The goal of NEMU is to retain the absolute minimal subset of the QEMU codebase that is required for the feature set described below. The QEMU code base will also be simplified to reduce the number of generic abstractions.

https://github.com/intel/nemu[+https://github.com/intel/nemu+]

qboot
^^^^^

A minimal x86 firmware that runs on QEMU and, together with
a slimmed-down QEMU configuration, boots a virtual machine in 40
milliseconds[2] on an Ivy Bridge Core i7 processor.
qboot combines various existing open source components:

* a minimal (really minimal) 16-bit BIOS runtime based on kvmtool's own BIOS
* a couple hardware initialization routines written mostly from scratch
but with good help from SeaBIOS source code
* a minimal 32-bit libc based on kvm-unit-tests
* the Linux loader from QEMU itself

https://github.com/bonzini/qboot[+https://github.com/bonzini/qboot+]

QEMU
^^^^

QEMU is a generic and open source machine emulator and virtualizer.

When used as a machine emulator, QEMU can run OSes and programs made for one machine (e.g. an ARM board) on a different machine (e.g. your own PC). By using dynamic translation, it achieves very good performance.

When used as a virtualizer, QEMU achieves near native performance by executing the guest code directly on the host CPU. QEMU supports virtualization when executing under the Xen hypervisor or using the KVM kernel module in Linux. When using KVM, QEMU can virtualize x86, server and embedded PowerPC, 64-bit POWER, S390, 32-bit and 64-bit ARM, and MIPS guests.

https://wiki.qemu.org/Main_Page[+https://wiki.qemu.org/Main_Page+]

https://memcpy.io/building-chromiumos-for-qemu.html[+https://memcpy.io/building-chromiumos-for-qemu.html+]

VirtualBox
^^^^^^^^^^

VirtualBox is a powerful x86 and AMD64/Intel64 virtualization product for enterprise as well as home use. Not only is VirtualBox an extremely feature rich, high performance product for enterprise customers, it is also the only professional solution that is freely available as Open Source Software.

Presently, VirtualBox runs on Windows, Linux, Macintosh, and Solaris hosts and supports a large number of guest operating systems including but not limited to Windows (NT 4.0, 2000, XP, Server 2003, Vista, Windows 7, Windows 8, Windows 10), DOS/Windows 3.x, Linux (2.4, 2.6, 3.x and 4.x), Solaris and OpenSolaris, OS/2, and OpenBSD. 

Users of VirtualBox can load multiple guest OSs under a single host operating-system (host OS). Each guest can be started, paused and stopped independently within its own virtual machine (VM). The user can independently configure each VM and run it under a choice of software-based virtualization or hardware assisted virtualization if the underlying host hardware supports this. The host OS and guest OSs and applications can communicate with each other through a number of mechanisms including a common clipboard and a virtualized network facility. Guest VMs can also directly communicate with each other if configured to do so.

https://www.virtualbox.org/[+https://www.virtualbox.org/+]

Xen
^^^

The Xen Project hypervisor is an open-source type-1 or baremetal hypervisor, which makes it possible to run many instances of an operating system or indeed different operating systems in parallel on a single machine (or host). The Xen Project hypervisor is the only type-1 hypervisor that is available as open source. It is used as the basis for a number of different commercial and open source applications, such as: server virtualization, Infrastructure as a Service (IaaS), desktop virtualization, security applications, embedded and hardware appliances. The Xen Project hypervisor is powering the largest clouds in production today.

Here are some of the Xen Project hypervisor's key features:

* Small footprint and interface (is around 1MB in size). Because it uses a microkernel design, with a small memory footprint and limited interface to the guest, it is more robust and secure than other hypervisors.
* Operating system agnostic: Most installations run with Linux as the main control stack (aka "domain 0"). But a number of other operating systems can be used instead, including NetBSD and OpenSolaris.
* Driver Isolation: The Xen Project hypervisor has the capability to allow the main device driver for a system to run inside of a virtual machine. If the driver crashes, or is compromised, the VM containing the driver can be rebooted and the driver restarted without affecting the rest of the system.
* Paravirtualization: Fully paravirtualized guests have been optimized to run as a virtual machine. This allows the guests to run much faster than with hardware extensions (HVM). Additionally, the hypervisor can run on hardware that doesn't support virtualization extensions.

The Xen Project hypervisor runs directly on the hardware and is responsible for handling CPU, Memory, timers and interrupts. It is the first program running after exiting the bootloader. On top of the hypervisor run a number of virtual machines. A running instance of a virtual machine is called a domain or guest. A special domain, called domain 0 contains the drivers for all the devices in the system. Domain 0 also contains a control stack and other system services to manage a Xen based system.

https://www.xenproject.org/[+https://www.xenproject.org/+]

https://xenserver.org/[+https://xenserver.org/+]

visdom
~~~~~~

A flexible tool for creating, organizing, and sharing visualizations of live, rich data.
Visdom aims to facilitate visualization of (remote) data with an emphasis on supporting scientific experimentation.
You  can broadcast visualizations of plots, images, and text for yourself and your collaborators.
You can also organize your visualization space programmatically or through the UI to create dashboards for live data, inspect results of experiments, or debug experimental code.

https://github.com/facebookresearch/visdom[+https://github.com/facebookresearch/visdom+]

Visit
~~~~~

VisIt is an open source, turnkey application for large scale simulated and experimental data sets. Its charter goes beyond pretty pictures; the application is an infrastructure for parallelized, general post-processing of extremely massive data sets. Target use cases include data exploration, comparative analysis, visual debugging, quantitative analysis, and presentation graphics.

The VisIt product delivers the efforts of many software developers in a single package. First, VisIt leverages several third party libraries: the Qt widget library for its user interface, the Python programming language for a command line interpreter, and the Visualization ToolKit (VTK) library for its data model and many of its visualization algorithms. On top of that, an additional fifty man-years worth of effort have been devoted to the development of VisIt itself. The VisIt-specific effort has largely been focused on parallelization for large data sets, user interface, implementing custom data analysis routines, addressing non-standard data models (such as adaptive refinement meshes (AMR) and mixed materials zones), and creating a robust overall product. VisIt consists over one and a half million lines of code, and its third party libraries have an additional million lines of code. It has been ported to Windows, Mac, and many UNIX variants, including AIX, IRIX, Solaris, Tru64, and, of course, Linux, including ports for SGI's Altix, Cray's XT4, and many commodity clusters.

The basic design is a client-server model, where the server is parallelized. The client-server aspect allows for effective visualization in a remote setting, while the parallelization of the server allows for the largest data sets to be processed reasonably interactively. The tool has been used to visualize many large data sets, including a two hundred and sixteen billion data point structured grid, a one billion point particle simulation, and curvilinear, unstructured, and AMR meshes with hundreds of millions to billions of elements. The most common form of the server is as a stand alone process that reads in data from files. However, an alternate form exists where a simulation code can link in "lib-VisIt" and become itself the server, allowing for in situ visualization and analysis.

VisIt follows a data flow network paradigm where interoperable modules are connected to perform custom analysis. The modules come from VisIt's five primary user interface abstractions and there are many examples of each. There are twenty one ``plots" (ways to render data), forty-two ``operators" (ways to manipulate data), eighty-five file format readers, over fifty ``queries" (ways to extract quantitative information), and over one hundred ``expressions" (ways to create derived quantities). Further, a plugin capability allows for dynamic incorporation of new plot, operator, and database modules. These plugins can be partially code generated, even including automatic generation of Qt and Python user interfaces. 

https://www.visitusers.org/[+https://www.visitusers.org/+]

netCDF
^^^^^^

VisIt can read data from several codes that write their data files to the NETCDF data file format. NETCDF is mainly a smart array storage file format from VisIt's point of view. Complex data structures such as unstructured meshes or AMR meshes are often stored in several arrays (coordinates, connectivity, etc.) and the meaning of each array is known to the code that wrote the file. For VisIt to read that file back in, VisIt's plugins must also understand the conventions used to write the NETCDF file so when certain arrays are seen, they can be reassembled into the more complex data structure that they collectively represent. VisIt's NETCDF database reader plugin understands the conventions of a few different codes that stored their data using NETCDF. 

https://www.visitusers.org/index.php?title=Reading_NETCDF[+https://www.visitusers.org/index.php?title=Reading_NETCDF+]

Vistle
~~~~~~

Vistle, the VISualization Testing Laboratory for Exascale computing, is an extensible software environment that integrates simulations on supercomputers, post-processing and parallel interactive visualization.

A Vistle work flow consists of several processing modules, each of which is a parallel MPI program that uses OpenMP within nodes. These can be configured graphically or from Python. Shared memory is used for transfering data between modules on a single node. Work flows can be distributed across several clusters.

For rendering in immersive projection systems, Vistle uses OpenCOVER. Visualization parameters can be manipulated from within the virtual environment. Large data sets can be displayed with OpenGL sort-last parallel rendering and depth compositing. For scaling with the simulation on remote HPC resources, a CPU based hybrid sort-last/sort first parallel ray casting renderer is available. "Remote hybrid rendering" allows to combine its output with local rendering, while ensuring smooth interactivity by decoupling it from remote rendering.

The Vistle system is modular and can be extended easily with additional visualization algorithms. Source code is available on GitHub and licensed under the LPGL.

http://www.hlrs.de/vistle/[+http://www.hlrs.de/vistle/+]

https://github.com/vistle/vistle[+https://github.com/vistle/vistle+]

VisualSFM
~~~~~~~~~

VisualSFM is a GUI application for 3D reconstruction using structure from motion (SFM). The reconstruction system integrates several of my previous projects: SIFT on GPU(SiftGPU), Multicore Bundle Adjustment, and Towards Linear-time Incremental Structure from Motion. VisualSFM runs fast by exploiting multicore parallelism for feature detection, feature matching, and bundle adjustment.

For dense reconstruction, this program integrates the execution of Yasutaka Furukawa's PMVS/CMVS tool chain. The SfM output of VisualSFM works with several additional tools, including CMP-MVS by Michal Jancosek, MVE by Michael Goesele's research group, SURE by Mathias Rothermel and Konrad Wenzel, and MeshRecon by Zhuoliang Kang.

http://ccwu.me/vsfm/[+http://ccwu.me/vsfm/+]

http://grail.cs.washington.edu/projects/mcba/[+http://grail.cs.washington.edu/projects/mcba/+]

https://github.com/simonfuhrmann/mve[+https://github.com/simonfuhrmann/mve+]

Vitess
~~~~~~

Vitess is a database clustering system for horizontal scaling of MySQL through generalized sharding.

By encapsulating shard-routing logic, Vitess allows application code and database queries to remain agnostic to the distribution of data onto multiple shards. With Vitess, you can even split and merge shards as your needs grow, with an atomic cutover step that takes only a few seconds.

Vitess has been a core component of YouTube's database infrastructure since 2011, and has grown to encompass tens of thousands of MySQL nodes.

https://github.com/vitessio/vitess[+https://github.com/vitessio/vitess+]

https://vitess.io/[+https://vitess.io/+]

VIVO
~~~~

VIVO is member-supported, open source software and an ontology for representing scholarship.  VIVO supports recording, editing, searching, browsing, and visualizing scholarly activity. VIVO encourages showcasing the scholarly record, research discovery, expert finding, network analysis, and assessment of research impact.  VIVO is easily extended to support additional domains of scholarly activity.

When installed and populated with researcher interests, activities, and accomplishments by an institution, VIVO enables the discovery of research and scholarship across disciplines at that institution and beyond. VIVO supports browsing and a search function which returns faceted results for rapid retrieval of desired information. Content in a VIVO installation may be maintained manually,  brought into VIVO in automated ways from local systems of record, such as HR, grants, course, and faculty activity databases, or from database providers such as publication aggregators and funding agencies.

https://duraspace.org/vivo/[+https://duraspace.org/vivo/+]

Vitro
~~~~~

Vitro is a general-purpose web-based ontology and instance editor with customizable public browsing.

Vitro is an integrated ontology editor and semantic web application.

Vitro is a Java web application that runs in a Tomcat servlet container.

With Vitro, you can:

* Create or load ontologies in OWL format
* Edit instances and relationships
* Build a public web site to display your data
* Search your data with Apache Solr

Vitro was originally developed at Cornell University, and is used as the core of the popular research and scholarship portal, VIVO.

https://github.com/vivo-project/Vitro[+https://github.com/vivo-project/Vitro+]

VOFtools
~~~~~~~~

The VOFTools library includes efficient analytical and geometrical routines for (1) area/volume computation, (2) truncation operations that typically arise in VOF (volume of fluid) methods, (3) area/volume conservation enforcement (VCE) in PLIC (piecewise linear interface calculation) reconstruction and(4) computation of the distance from a given point to the reconstructed interface. The computation of a polyhedron volume uses an efficient formula based on a quadrilateral decomposition and a 2D projection of each polyhedron face. The analytical VCE method is based on coupling an interpolation procedure to bracket the solution with an improved final calculation step based on the above volume computation formula. Although the library was originally created to help develop highly accurate advection and reconstruction schemes in the context of VOF methods, it may have more general applications. To assess the performance of the supplied routines, different tests, which are provided in FORTRAN and C, were implemented for several 2D and 3D geometries.

http://www.dimf.upct.es/personal/lrj/voftools.html[+http://www.dimf.upct.es/personal/lrj/voftools.html+]

https://www.sciencedirect.com/science/article/pii/S0010465517303430[+https://www.sciencedirect.com/science/article/pii/S0010465517303430+]

VoltDB
~~~~~~

VoltDB is a horizontally-scalable, in-memory SQL RDBMS designed for applications that benefit from strong consistency, high throughput and low, predictable latency.

https://github.com/VoltDB/voltdb[+https://github.com/VoltDB/voltdb+]

http://hstore.cs.brown.edu/[+http://hstore.cs.brown.edu/+]

https://www.voltdb.com/[+https://www.voltdb.com/+]

VOLTTRON
~~~~~~~~

VOLTTRON is an open source platform for distributed sensing and control. The platform provides services for collecting and storing data from buildings and devices and provides an environment for developing applications which interact with that data.
The features include:

* Message Bus allows agents to subcribe to data sources and publish results and messages
* Driver framework for collecting data from and sending control actions to buildings and devices
* Historian framework for storing data
* Agent lifecycle managment in the platform
* Web UI for managing deployed instances from a single central instance.

Inexpensive, small-scale computers—such as the
Raspberry Pi—can have VOLTTRON™ installed as
a controller.

https://github.com/VOLTTRON/volttron[+https://github.com/VOLTTRON/volttron+]

https://volttron.readthedocs.io/en/develop/index.html[+https://volttron.readthedocs.io/en/develop/index.html+]

https://bgintegration.pnnl.gov/volttron.asp[+https://bgintegration.pnnl.gov/volttron.asp+]

https://volttron.org/[+https://volttron.org/+]

Voreen
~~~~~~

Voreen is an open source rapid application development framework for the interactive visualization and analysis of multi-modal volumetric data sets. It provides GPU-based volume rendering and data analysis techniques and offers high flexibility when developing new analysis workflows in collaboration with domain experts. The Voreen framework consists of a multi-platform Cxx library, which can be easily integrated into existing applications, and a Qt-based stand-alone application. It is licensed under the terms of the GNU General Public License.

The design of the Voreen framework revolves around the concept of data-flow networks.
These networks consist of modular units, called processors, which encapsulate rendering and data processing algorithms. A processor operates on input data it receives from its inports and outputs the processed results via its outports.

The data-flow is established by unidirectional connections from outports to inports. Processor ports are typed, i.e., each port transmits a certain type of data, such as a 2D image, a 3D volume, geometric data, or a collection of objects of these basic types. Processors furthermore have properties for the parameterization of the encapsulated algorithms. Voreen offers various types of properties ranging from primitive numeric types to complex rendering parameters, such as camera position and transfer function. Properties can be linked within and across processors, in order to synchronize their values.

The Voreen framework provides a graphical network editor that enables the user to interactively design data-flow networks from existing processors without writing a single line of code. In particular, the user can manipulate the network topology by adding or removing processors, establishing or deleting port connections, and editing property links. Furthermore, processors can be aggregated to higher-level groupings.

https://www.uni-muenster.de/Voreen/[+https://www.uni-muenster.de/Voreen/+]

VOVR
~~~~

The virtual observatory (VO) and its standards have become a success story in providing uniform access to a huge amount of data sets. Those data sets contain correlations, distributions, and relations that have to be unveiled. Visualization has always been a key tool to understand complex structures. Typically high-dimensional information is projected to a two dimensional plane to create a diagnostic plot. Besides expensive stereoscopic visualization cubes, only stereoscopic displays provided an affordable tool to peek into a three dimensional data space.

We present a low-cost immersive visualization environment that makes use of a smart-phone, a game controllers and Google cardboard. This simple equipment allows you to explore your data more natively by flying through your data space. The presented software consists of a central server application running on a computer and a client implementation performing the rendering on multiple smart-phones, enabling users to inspect the data jointly. As the server application uses the VO simple application messaging protocol (SAMP), it is seamlessly integrated with other VO tools, like topcat or aladin. Access the data in the usual way and employ Virtual Observatory Virtual Reality (VOVR) to explore it.

https://www.h-its.org/ain-software-en/self-organising-map-under-construction/[+https://www.h-its.org/ain-software-en/self-organising-map-under-construction/+]

Vowpal Wabbit
~~~~~~~~~~~~~

Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.

VW is the essence of speed in machine learning, able to learn from terafeature datasets with ease. Via parallel learning, it can exceed the throughput of any single machine network interface when doing linear learning, a first amongst learning algorithms. 

https://github.com/VowpalWabbit/vowpal_wabbit[+https://github.com/VowpalWabbit/vowpal_wabbit+]

http://hunch.net/\~vw/[+http://hunch.net/~vw/+]

VPN
~~~

A virtual private network (VPN) extends a private network across a public network, and enables users to send and receive data across shared or public networks as if their computing devices were directly connected to the private network. Applications running across a VPN may therefore benefit from the functionality, security, and management of the private network.

A VPN is created by establishing a virtual point-to-point connection through the use of dedicated connections, virtual tunneling protocols, or traffic encryption. A VPN available from the public Internet can provide some of the benefits of a wide area network (WAN). From a user perspective, the resources available within the private network can be accessed remotely.

http://www.vpnlabs.com/[+http://www.vpnlabs.com/+]

https://en.wikipedia.org/wiki/Virtual_private_network[+https://en.wikipedia.org/wiki/Virtual_private_network+]

ChaosVPN
^^^^^^^^

ChaosVPN is a system to connect Hackers.

Design principals include that it should be without Single Point of Failure, make usage of full encryption, use RFC1918 ip ranges, scales well on >100 connected networks and is being able to run on a embedded hardware you will find in our todays router.

It should be designed that noone sees other peoples traffic.

It should be mainly autoconfig as in that besides the joining node no administrator of the network should be in the need to acutally do something when a node joins or leaves.

If you want to find a solution for a Network without Single Point of failure, has - due to Voice over IP - low latency and that noone will see other peoples traffic you end up pretty quick with a full mesh based network.

Therefore we came up with the tinc solution. tinc does a fully meshed peer to peer network and it defines endpoints and not tunnels.

ChaosVPN connects hacker wherever they are. We connect roadwarriors with their notebook. Servers, even virtual ones in Datacenters, Hackerhouses and hackerspaces. To sum it up we connect networks - maybe down to a small /32. 

https://wiki.hamburg.ccc.de/ChaosVPN[+https://wiki.hamburg.ccc.de/ChaosVPN+]

dn42
^^^^

dn42 is a big dynamic VPN, which employs Internet technologies (BGP, whois database, DNS, etc). Participants connect to each other using network tunnels (GRE, OpenVPN, Tinc, IPsec) and exchange routes thanks to the Border Gateway Protocol. Network addresses are assigned in the 172.20.0.0/14 range and private AS numbers are used (see registry) as well as IPv6 addresses from the ULA-Range (fd00::/8).

dn42 can be used to learn networking and to connect private networks, such as hackerspaces or community networks. But above all, experimenting with routing in dn42 is fun.

Participating in dn42 is primarily useful for learning routing technologies such as BGP, using a reasonably large network (~200 AS, ~400 prefixes).

Since dn42 is very similar to the Internet, it can be used as a hands-on testing ground for new ideas, or simply to learn real networking stuff that you probably can't do on the Internet (BGP multihoming, transit). The biggest advantage when compared to the Internet: if you break something in the network, you won't have any big network operator yelling angrily at you.

https://dn42.net/Home[+https://dn42.net/Home+]

http://freerouter.nop.hu/online.html[+http://freerouter.nop.hu/online.html+]

https://fahrplan.events.ccc.de/congress/2009/Fahrplan/events/3504.en.html[+https://fahrplan.events.ccc.de/congress/2009/Fahrplan/events/3504.en.html+]

https://en.wikipedia.org/wiki/Decentralized_network_42[+https://en.wikipedia.org/wiki/Decentralized_network_42+]

SoftEther
^^^^^^^^^

SoftEther VPN is free open-source, cross-platform, multi-protocol VPN client and VPN server software, developed as part of Daiyuu Nobori's master's thesis research at the University of Tsukuba. VPN protocols such as SSL VPN, L2TP/IPsec, OpenVPN, and Microsoft Secure Socket Tunneling Protocol are provided in a single VPN server. It was released using the GPLv2 license on January 4, 2014. The license was switched to Apache License 2.0 on January 21, 2019.

SoftEther VPN supports NAT traversal, making it useful to run VPN servers on computers that are behind residential gateways, facility routers, and firewalls. Firewalls performing deep packet inspection are unable to detect SoftEther's VPN transport packets as a VPN tunnel because HTTPS is used to camouflage the connection.

SoftEther VPN optimizes performance by using full Ethernet frame utilization, reducing memory copy operations, parallel transmission, and clustering. Together, these reduce latency normally associated with VPN connections while increasing throughput. 

The SoftEther architecture features include:

* *Virtual Hub* - A Virtual Hub is the software-emulated virtual Ethernet switch. It learns and maintains its own forwarding-database table inside. While traditional physical Ethernet switches implement this function by hardware, SoftEther VPN implements the same function by software.
* *Virtual Network Adapter* - A Virtual Network Adapter is the software-emulated virtual Ethernet adapter. A VPN Client can create several Virtual Network Adapters on the client computer. A VPN user can establish a VPN session between the Virtual Network Adapter and the destination Virtual Hub of the remote VPN Server.
* *Virtual Layer-3 Switch* - A Virtual Layer-3 Switch is the software-emulated virtual IP router. Several Virtual Layer-3 Switches can be created on a single VPN Server instance. A Virtual Layer-3 Switch has virtual IP interfaces connected to Virtual Hubs. It also has several static routing table entries. 
* *Cascade Connection* - The administrator can define a cascade connection between local or remote Virtual Hubs. After the cascade connection has been established, the originally-isolated two Ethernet segments are combined to the single Ethernet segment. Therefore, the cascade connection function is used to build the site-to-site layer-2 Ethernet bridging. 
* *Local Bridge* - Since Virtual Hubs and Virtual Network Adapters are only software-emulated virtual Ethernet devices, the Ethernet packets through these virtual devices cannot communicate with physical Ethernet devices. Therefore, a bridge between the virtual and the physical is necessary to build a remote-access VPN or site-to-site VPN. To make a bridge, the Local Bridge function exchanges the Ethernet packets between a Virtual Hub and a physical Ethernet network adapter to combine both isolated Ethernet segments into a single Ethernet segment. 
* *Transparency* - One of the key features of SoftEther VPN is the transparency for firewalls, proxy servers, and NATs (Network Address Translators). To do this, SoftEther VPN supports SSL-VPN and NAT Traversal. SoftEther VPN uses HTTPS protocol in order to establish a VPN tunnel. HTTPS (HTTP over SSL) protocol uses the TCP/IP port 443 (may vary) as destination. 
* *Parallel Transmission* - When the user chooses SSL-VPN protocol between the VPN Client and VPN Server, SoftEther VPN Server and VPN Client use a parallel transmission mechanism to improve the throughput of the SSL-VPN tunnel. A user can set up the number of concurrent parallel transmission channels from 1 to 32. In an environment such as a slow and delaying network, this performance tuning will result in a faster throughput.
* *NAT Traversal* - Traditional VPN systems require the user to ask the firewall's administrator of the company to open an endpoint (TCP or UDP port) on the firewall or NAT on the border between the company and the Internet. In order to reduce the necessity to open an endpoint on the firewall, SoftEther VPN Server has the NAT Traversal function. NAT Traversal is enabled by default. As long as it is enabled, SoftEther VPN Client computers can connect to your VPN Server behind the firewall or NAT. No special settings on the firewall or NAT are necessary.
* *VPN Tunneling* - A few very-restricted networks only permit to pass ICMP or DNS packets. On such a network, TCP or UDP are filtered. Only ICMP and DNS are permitted. In order to make it possible to establish a SoftEther VPN client-server session via such a very-restricted network, SoftEther VPN has the "VPN over ICMP" and the "VPN over DNS" function. 
* *VPN Gate* - VPN Gate is a plugin for SoftEther VPN, which allows users to connect to free VPN servers, run by volunteers who use SoftEther to host their VPN servers. Volunteers use personal computers as "servers".

https://www.softether.org/[+https://www.softether.org/+]

https://github.com/SoftEtherVPN/SoftEtherVPN/[+https://github.com/SoftEtherVPN/SoftEtherVPN/+]

https://en.wikipedia.org/wiki/SoftEther_VPN[+https://en.wikipedia.org/wiki/SoftEther_VPN+]

tinc
^^^^

tinc is a Virtual Private Network (VPN) daemon that uses tunnelling and encryption to create a secure private network between hosts on the Internet. tinc is Free Software and licensed under the GNU General Public License version 2 or later. Because the VPN appears to the IP level network code as a normal network device, there is no need to adapt any existing software. This allows VPN sites to share information with each other over the Internet without exposing any information to others.

http://tinc-vpn.org/[+http://tinc-vpn.org/+]

VpnCloud
^^^^^^^^

VpnCloud is a simple VPN over UDP. It creates a virtual network interface on the host and forwards all received data via UDP to the destination. VpnCloud establishes a fully-meshed VPN network in a peer-to-peer manner. It can work on TUN devices (IP based) and TAP devices (Ethernet based).  VpnCloud features the following functionality:

* Setting up tunnels between two networks via Ethernet (TAP) and IP (TUN)
* Connecting multiple networks with multiple forwarding behaviors (Hub, Switch, Router)
* Encrypted connections using libsodium
* Automatic peer-to-peer meshing, no central servers
* NAT and (limited) firewall traversal using hole punching
* Automatic reconnecting when connections are lost
* Non-native forwarding modes, e.g. IP based learning switch and prefix routed Ethernet networks.
* High throughput and low additional latency (see performance page)
* Support for tunneled VLans (TAP device)
* Option to hide protocol header
* Automatic port forwarding via UPnP

https://vpncloud.ddswd.de/[+https://vpncloud.ddswd.de/+]

https://github.com/dswd/vpncloud[+https://github.com/dswd/vpncloud+]

VPP
~~~

The VPP platform is an extensible framework that provides out-of-the-box production quality switch/router functionality. It is the open source version of Cisco's Vector Packet Processing (VPP) technology: a high performance, packet-processing stack that can run on commodity CPUs.

The benefits of this implementation of VPP are its high performance, proven technology, its modularity and flexibility, and rich feature set.

The VPP technology is based on proven technology that has helped ship over $1 Billion of Cisco products. It is a modular design. The framework allows anyone to "plug in" new graph nodes without the need to change core/kernel code. 

https://wiki.fd.io/view/VPP[+https://wiki.fd.io/view/VPP+]

https://wiki.fd.io/view/VPP/Installing_VPP_binaries_from_packages[+https://wiki.fd.io/view/VPP/Installing_VPP_binaries_from_packages+]

VPython
~~~~~~~

VPython makes it unusually easy to write programs that generate navigable real-time 3D animations. It is based on the Python programming language which is widely used in introductory programming courses thanks to its clean design, and it is also widely used in science and business. At glowscript.org, click "Example programs" to see how VPython is used.

https://vpython.org/[+https://vpython.org/+]

https://www.glowscript.org/docs/VPythonDocs/index.html[+https://www.glowscript.org/docs/VPythonDocs/index.html+]

https://github.com/BruceSherwood/vpython-jupyter[+https://github.com/BruceSherwood/vpython-jupyter+]

V-REP
~~~~~

The robot simulator V-REP, with integrated development environment, is based on a distributed control architecture: each object/model can be individually controlled via an embedded script, a plugin, ROS nodes, BlueZero nodes, remote API clients, or a custom solution. This makes V-REP very versatile and ideal for multi-robot applications. Controllers can be written in C/Cxx, Python, Java, Lua, Matlab, Octave or Urbi.

Following are just a few of V-REP's applications:

* Simulation of factory automation systems
* Remote monitoring
* Hardware control
* Fast prototyping and verification
* Safety monitoring
* Fast algorithm development
* Robotics related education
* Product presentation

V-REP can be used as a stand-alone application or can easily be embedded into a main client application: its small footprint and elaborate API makes V-REP an ideal candidate to embed into higher-level applications. An integrated Lua script interpreter makes V-REP an extremely versatile application, leaving the freedom to the user to combine the low/high-level functionalities to obtain new high-level functionalities. 

http://www.coppeliarobotics.com/helpFiles/[+http://www.coppeliarobotics.com/helpFiles/+]

http://coppeliarobotics.com/[+http://coppeliarobotics.com/+]

VTK
~~~

Blah.

vtk.js
^^^^^^

vtk.js is a rendering library made for Scientific Visualization on the Web. It adapts the VTK structure and expertise to bring high performance rendering into your browser.

https://kitware.github.io/vtk-js/[+https://kitware.github.io/vtk-js/+]

VTK-m
~~~~~

VTK-m is a toolkit of scientific visualization algorithms for emerging processor architectures. VTK-m supports the fine-grained concurrency for data analysis and visualization algorithms required to drive extreme scale computing by providing abstract models for data and execution that can be applied to a variety of algorithms across many different processor architectures. 

http://m.vtk.org/index.php/Main_Page[+http://m.vtk.org/index.php/Main_Page+]

https://gitlab.kitware.com/vtk/vtk-m[+https://gitlab.kitware.com/vtk/vtk-m+]

https://ieeexplore.ieee.org/document/7466740[+https://ieeexplore.ieee.org/document/7466740+]

Vulkan
~~~~~~

Vulkan is a low-overhead, cross-platform 3D graphics and computing API. Vulkan targets high-performance realtime 3D graphics applications such as video games and interactive media across all platforms. Compared to OpenGL and Direct3D 11, and like Direct3D 12 and Metal, Vulkan is intended to offer higher performance and more balanced CPU/GPU usage. Other major differences from Direct3D 11 (and prior) and OpenGL are Vulkan being a considerably lower level API and offering parallel tasking. Vulkan also has the ability to render 2D graphics applications.[10] In addition to its lower CPU usage, Vulkan is also able to better distribute work among multiple CPU cores.

https://en.wikipedia.org/wiki/Vulkan_(API)[+https://en.wikipedia.org/wiki/Vulkan_(API)+]

https://developer.nvidia.com/Vulkan[+https://developer.nvidia.com/Vulkan+]

https://vulkan-tutorial.com/[+https://vulkan-tutorial.com/+]

https://github.com/KhronosGroup/Vulkan-Docs[+https://github.com/KhronosGroup/Vulkan-Docs+]

https://vulkan.lunarg.com/[+https://vulkan.lunarg.com/+]

https://github.com/vinjn/awesome-vulkan[+https://github.com/vinjn/awesome-vulkan+]

bs::framework
~~~~~~~~~~~~~

bs::f provides everything you need for development of graphical applications, ranging from audio, animation, GUI, input, physics, rendering to scripting systems. It also comes with built-in support for over 30 image, mesh and audio formats, as well as a wide range of utility systems, from an extensive math library, run-time type information, CPU/GPU profiling, SIMD instruction API and a lot more. Cross-platform on Windows, Linux and macOS.

All of the framework's functionality is provided through a neatly designed, modern Cxx14 API. It is simple to understand and use, while offering a lot of advanced options for those that require them. Every non-trivial method is documented in an extensive API reference. Over a hundred manuals are provided guiding you through all major systems from start to finish, with extensive amount of code snippets, as well as fully functional examples.

The framework comes with a low-level rendering API based on Vulkan. It offers extensive support for all modern rendering features, including compute and tesselation shaders, instanced rendering, texture arrays, buffers, unordered access reads and writes, as well as full API support for pipeline state objects, command lists, explicit multi-GPU and async compute/upload queues. The framework also comes with DirectX 11 and OpenGL 4.5 render backends, and is easily extensible to others. Full support for both GLSL and HLSL is provided, including reflection as well as bytecode caching. 

https://www.bsframework.io/[+https://www.bsframework.io/+]

https://github.com/GameFoundry/bsf[+https://github.com/GameFoundry/bsf+]

Zink
^^^^

Zink is an OpenGL implementation on top of Vulkan. Or to be a bit more specific, Zink is a Mesa Gallium driver that leverage the existing OpenGL implementation in Mesa to provide hardware accelerated OpenGL when only a Vulkan driver is available.

https://www.kusma.xyz/blog/2018/10/31/introducing-zink.html[+https://www.kusma.xyz/blog/2018/10/31/introducing-zink.html+]

https://gitlab.freedesktop.org/kusma/mesa/tree/zink[+https://gitlab.freedesktop.org/kusma/mesa/tree/zink+]



#WWWW

WABBIT
~~~~~~

The (W)avelet (A)daptive (B)lock-(B)ased solver for (I)nteractions with (T)urbulence
package makes it possible to solve partial differential equations on block-based adaptive grids. Calculations in 2D and 3D are possible and is performed fully parallel. As in WABBIT the set of PDE is encapsulated from the rest of the code the PDE implementation is similar to calculatoins with single domain code.

https://github.com/adaptive-cfd/WABBIT[+https://github.com/adaptive-cfd/WABBIT+]

https://github.com/adaptive-cfd/python-tools[+https://github.com/adaptive-cfd/python-tools+]

https://www.cfd.tu-berlin.de/[+https://www.cfd.tu-berlin.de/+]

waf
~~~

Waf is a Python-based framework for configuring, compiling and installing applications. Here are perhaps the most important features of Waf:

* Automatic build order: the build order is computed from input and output files, among others
* Automatic dependencies: tasks to execute are detected by hashing files and commands
* Performance: tasks are executed in parallel automatically, the startup time is meant to be fast (separation between configuration and build)
* Flexibility: new commands and tasks can be added very easily through subclassing, bottlenecks for specific builds can be eliminated through dynamic method replacement
* Extensibility: though many programming languages and compilers are already supported by default, many others are available as extensions
* IDE support: Eclipse, Visual Studio and Xcode project generators (waflib/extras/)
* Documentation: the application is based on a robust model documented in The Waf Book and in the API docs
* Python compatibility: cPython 2.5 to 3.4, Jython 2.5, IronPython, and Pypy

For researchers and build system writers, Waf also provides a framework for creating custom build systems and package distribution systems.

https://gitlab.com/ita1024/waf/[+https://gitlab.com/ita1024/waf/+]

https://waf.io/[+https://waf.io/+]

WareWulf
~~~~~~~~

Warewulf is a computer cluster implementation toolkit that facilitates the process of installing a cluster and long term administration. It does this by changing the administration paradigm to make all of the slave node file systems manageable from one point, and automate the distribution of the node file system during node boot. It allows a central administration model for all slave nodes and includes the tools needed to build configuration files, monitor, and control the nodes. It is totally customizable and can be adapted to just about any type of cluster. From the software administration perspective it does not make much difference if you are running 2 nodes or 500 nodes. The procedure is still the same, which is why Warewulf is scalable from the admins perspective. Also, because it uses a standard chroot'able file system for every node, it is extremely configurable and lends itself to custom environments very easily.

While Warewulf was designed to be a HPC system, it is not an HPC system in itself. Warewulf is more along the lines of a distributed Linux distribution, or more specifically a system for replicating and managing small, lightweight Linux systems from one master. Using Warewulf, HPC packages such as LAM/MPI/MPICH, Sun Grid Engine, PVM, etc. can be easily deployed throughout the cluster.

Warewulf solves the problem of slave node management rather than being a strict HPC specific system (even though it was designed with HPC in mind). Because of this it is as flexible as a home grown cluster, but administratively scales very well. As a result of this flexibility and ease of customization, Warewulf has been used not only on production HPC implementations, but also development systems like KASY0 (the first system to break the one hundred dollar per GFLOPS barrier), and non HPC systems such as web server cluster farms, intrusion detection clusters, and high-availability clusters. 

http://warewulf.lbl.gov/[+http://warewulf.lbl.gov/+]

https://github.com/warewulf[+https://github.com/warewulf+]

http://www.admin-magazine.com/HPC/Articles/Warewulf-Cluster-Manager-Master-and-Compute-Nodes/(language)/eng-US[+http://www.admin-magazine.com/HPC/Articles/Warewulf-Cluster-Manager-Master-and-Compute-Nodes/(language)/eng-US+]

warpcore
~~~~~~~~

Warpcore is a minimal userspace UDP/IPv4/Ethernet stack for the netmap packet I/O framework. Due to its dependency on netmap, warpcore supports Linux (with a netmap-patched kernel) and FreeBSD (which has netmap support since release 11). However, warpcore has a backend implementation using the Socket API that should compile on generic POSIX platforms, such as Darwin.

Warpcore prioritizes performance over features, and likely over full standards compliance. It supports zero-copy transmit and receive, and uses neither threads, timers nor signals. It exposes the underlying file descriptors to an application, for easy integration with different event loops (e.g., libev).

https://github.com/NTAP/warpcore[+https://github.com/NTAP/warpcore+]

https://github.com/luigirizzo/netmap[+https://github.com/luigirizzo/netmap+]

https://github.com/NTAP/quant[+https://github.com/NTAP/quant+]

Wayback
~~~~~~~

Wayback is an implementation of a versioning file system for Linux. This means that when you use a Wayback file system, old versions of files are never lost. No matter how much you change a file or directory, everything is always kept in a versioning file so that you never lose important data. Wayback provides the ability to remount any already mounted file system with versioning support under a different directory. Because of this, you can use Wayback on any block device with any base file system (ext3, ReiserFS, FAT, etc).

Wayback uses the FUSE userspace file system module to implement the file system.

http://wayback.sourceforge.net/[+http://wayback.sourceforge.net/+]

http://virtuoso.cs.northwestern.edu/[+http://virtuoso.cs.northwestern.edu/+]

WebSocket
~~~~~~~~~

WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C. 

WebSocket is a different protocol from HTTP. Both protocols are located at layer 7 in the OSI model and, as such, depend on TCP at layer 4. Although they are different, RFC 6455 states that WebSocket "is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries" thus making it compatible with the HTTP protocol. To achieve compatibility, the WebSocket handshake uses the HTTP Upgrade header[1] to change from the HTTP protocol to the WebSocket protocol.

The WebSocket protocol enables interaction between a web browser (or other client application) and a web server with lower overheads, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, a two-way ongoing conversation can take place between the client and the server. The communications are done over TCP port number 80 (or 443 in the case of TLS-encrypted connections), which is of benefit for those environments which block non-web Internet connections using a firewall. Similar two-way browser-server communications have been achieved in non-standardized ways using stopgap technologies such as Comet. 

https://en.wikipedia.org/wiki/WebSocket[+https://en.wikipedia.org/wiki/WebSocket+]

channelstream
^^^^^^^^^^^^^

This is a websocket-based communication server for python applications, your applications communicate with it via simple JSON REST API.

https://github.com/Channelstream/channelstream[+https://github.com/Channelstream/channelstream+]

Hypercorn
^^^^^^^^^

Hypercorn is an ASGI web server based on the sans-io hyper, h11, h2, and wsproto libraries and inspired by Gunicorn. Hypercorn supports HTTP/1, HTTP/2, and WebSockets (over HTTP/1 and HTTP/2) and the ASGI 2 specification. Hypercorn can utilise asyncio, uvloop, or trio worker types.

https://gitlab.com/pgjones/hypercorn[+https://gitlab.com/pgjones/hypercorn+]

libwebsockets
^^^^^^^^^^^^^

Libwebsockets is a simple-to-use, pure C library providing client and server for http/1, http/2, websockets and other protocols in a security-minded, lightweight, configurable, scalable and flexible way. It's easy to build and cross-build via cmake and is suitable for tasks from embedded RTOS through mass cloud serving.

https://github.com/warmcat/libwebsockets[+https://github.com/warmcat/libwebsockets+]

https://libwebsockets.org/[+https://libwebsockets.org/+]

noPoll
^^^^^^

noPoll is a OpenSource WebSocket implementation (RFC 6455), written in ansi C, that allows building pure WebSocket solutions or to provide WebSocket support to existing TCP oriented applications.

noPoll provides support for WebSocket (ws://) and TLS (secure) WebSocket (wss://), allowing message based (handler notified) programming or stream oriented access.

noPoll was written to have a clean and easy to use library. It is released under the terms of LGPL 2.1 (so you can build OpenSource or commercial applications) and currently is being used, among others, by Vortex Library, Turbulence and Core-Admin to provide with WebSocket support to those projects (BEEP over WebSocket). 

http://www.aspl.es/nopoll/[+http://www.aspl.es/nopoll/+]

https://github.com/ASPLes/nopoll[+https://github.com/ASPLes/nopoll+]

noVNC
^^^^^

VNC client using HTML5, WebSockets and Canvas.

https://github.com/novnc/noVNC[+https://github.com/novnc/noVNC+]

responder
^^^^^^^^^

Responder is a web service framework, written for human beings. 
The features include:

* A pleasant API, with a single import statement.
* Class-based views without inheritance.
* ASGI framework, the future of Python web services.
* WebSocket support!
* The ability to mount any ASGI / WSGI app at a subroute.
* f-string syntax route declaration.
* Mutable response object, passed into each view. No need to return anything.
* Background tasks, spawned off in a ThreadPoolExecutor.
* GraphQL (with GraphiQL) support!
* OpenAPI schema generation, with interactive documentation!
* Single-page webapp support

https://python-responder.org/en/latest/[+https://python-responder.org/en/latest/+]

https://github.com/kennethreitz/responder[+https://github.com/kennethreitz/responder+]

uvicorn
^^^^^^^

Uvicorn is a lightning-fast ASGI server, built on uvloop and httptools.

Until recently Python has lacked a minimal low-level server/application interface for asyncio frameworks. The ASGI specification fills this gap, and means we're now able to start building a common set of tooling usable across all asyncio frameworks.

ASGI should help enable an ecosystem of Python web frameworks that are highly competitive against Node and Go in terms of achieving high throughput in IO-bound contexts. It also provides support for HTTP/2 and WebSockets, which cannot be handled by WSGI.

https://www.uvicorn.org/[+https://www.uvicorn.org/+]

websocat
~~~~~~~~

Netcat, curl and socat for WebSockets.
The Swiss Army Knife of network debugging tools.

https://github.com/vi/websocat[+https://github.com/vi/websocat+]

http://www.dest-unreach.org/socat/[+http://www.dest-unreach.org/socat/+]

https://medium.com/@copyconstruct/socat-29453e9fc8a6[+https://medium.com/@copyconstruct/socat-29453e9fc8a6+]

websocketd
^^^^^^^^^^

websocketd is a small command-line tool that will wrap an existing command-line interface program, and allow it to be accessed via a WebSocket.

WebSocket-capable applications can now be built very easily. As long as you can write an executable program that reads STDIN and writes to STDOUT, you can build a WebSocket server. Do it in Python, Ruby, Perl, Bash, .NET, C, Go, PHP, Java, Clojure, Scala, Groovy, Expect, Awk, VBScript, Haskell, Lua, R, whatever! No networking libraries necessary.

The features include:

* Very simple install. Just download the single executable for Linux, Mac or Windows and run it. Minimal dependencies, no installers, no package managers, no external libraries. Suitable for development and production servers.
* Server side scripts can access details about the WebSocket HTTP request (e.g. remote host, query parameters, cookies, path, etc) via standard CGI environment variables.
* As well as serving websocket daemons it also includes a static file server and classic CGI server for convenience.
* Command line help available via websocketd --help.
* Includes WebSocket developer console to make it easy to test your scripts before you've built a JavaScript frontend.
* Examples in many programming languages are available to help you getting started.

https://github.com/joewalnes/websocketd[+https://github.com/joewalnes/websocketd+]

http://websocketd.com/[+http://websocketd.com/+]

wsproto
^^^^^^^

A pure-Python implementation of a WebSocket protocol stack. It's written from the ground up to be embeddable in whatever program you choose to use, ensuring that you can communicate via WebSockets, as defined in RFC6455, regardless of your programming paradigm.

This repository does not provide a parsing layer, a network layer, or any rules about concurrency. Instead, it's a purely in-memory solution, defined in terms of data actions and WebSocket frames. RFC6455 and Compression Extensions for WebSocket via RFC7692 are fully supported.

https://github.com/python-hyper/wsproto[+https://github.com/python-hyper/wsproto+]

wget
~~~~

GNU Wget (or just Wget, formerly Geturl, also written as its package name, wget) is a computer program that retrieves content from web servers. It is part of the GNU Project. Its name derives from World Wide Web and get. It supports downloading via HTTP, HTTPS, and FTP.

Its features include recursive download, conversion of links for offline viewing of local HTML, and support for proxies. It appeared in 1996, coinciding with the boom of popularity of the Web, causing its wide use among Unix users and distribution with most major Linux distributions. Written in portable C, Wget can be easily installed on any Unix-like system. Wget has been ported to Microsoft Windows, Mac OS X, OpenVMS, HP-UX, MorphOS and AmigaOS. Since version 1.14 Wget has been able to save its output in the web archiving standard WARC format.

Wget has been designed for robustness over slow or unstable network connections. If a download does not complete due to a network problem, Wget will automatically try to continue the download from where it left off, and repeat this until the whole file has been retrieved. It was one of the first clients to make use of the then-new Range HTTP header to support this feature. 

Wget can optionally work like a web crawler by extracting resources linked from HTML pages and downloading them in sequence, repeating the process recursively until all the pages have been downloaded or a maximum recursion depth specified by the user has been reached. The downloaded pages are saved in a directory structure resembling that on the remote server. This "recursive download" enables partial or complete mirroring of web sites via HTTP. Links in downloaded HTML pages can be adjusted to point to locally downloaded material for offline viewing. When performing this kind of automatic mirroring of web sites, Wget supports the Robots Exclusion Standard (unless the option -e robots=off is used).

Recursive download works with FTP as well, where Wget issues the LIST command to find which additional files to download, repeating this process for directories and files under the one specified in the top URL. Shell-like wildcards are supported when the download of FTP URLs is requested.

When downloading recursively over either HTTP or FTP, Wget can be instructed to inspect the timestamps of local and remote files, and download only the remote files newer than the corresponding local ones. This allows easy mirroring of HTTP and FTP sites, but is considered inefficient and more error-prone when compared to programs designed for mirroring from the ground up, such as rsync. On the other hand, Wget doesn't require special server-side software for this task. 

https://www.gnu.org/software/wget/[+https://www.gnu.org/software/wget/+]

https://en.wikipedia.org/wiki/Wget[+https://en.wikipedia.org/wiki/Wget+]

https://www.gnu.org/software/wget/manual/wget.html[+https://www.gnu.org/software/wget/manual/wget.html+]

https://www.booleanworld.com/download-files-web-pages-wget/[+https://www.booleanworld.com/download-files-web-pages-wget/+]

https://www.geeksforgeeks.org/wget-command-in-linux-unix/[+https://www.geeksforgeeks.org/wget-command-in-linux-unix/+]

https://en.wikipedia.org/wiki/CURL[+https://en.wikipedia.org/wiki/CURL+]

https://fasterdata.es.net/data-transfer-tools/other/[+https://fasterdata.es.net/data-transfer-tools/other/+]

Whonix
~~~~~~

Whonix is a desktop operating system designed for advanced security and privacy. Whonix mitigates the threat of common attack vectors while maintaining usability. Online anonymity is realized via fail-safe, automatic, and desktop-wide use of the Tor network. A heavily reconfigured Debian base is run inside multiple virtual machines, providing a substantial layer of protection from malware and IP address leaks. Commonly used applications are pre-installed and safely pre-configured for immediate use. The user is not jeopardized by installing additional applications or personalizing the desktop. Whonix is under active development and is the only operating system designed to be run inside a VM and paired with Tor.

https://www.whonix.org/[+https://www.whonix.org/+]

wi4mpi
~~~~~~

This is WI4MPI, Wrapper Interface For MPI performing a light translation between MPI constants and MPI objects from an MPI implementation to another one.

https://github.com/cea-hpc/wi4mpi[+https://github.com/cea-hpc/wi4mpi+]

windspharm
~~~~~~~~~~

windspharm is a Python package for performing computations on global wind fields in spherical geometry. It provides a high level interface for computations using spherical harmonics. windspharm is capable of computing the following quantities from an input vector wind:

* divergence
* vorticity (relative and absolute)
* streamfunction
* velocity potential
* irrotational and non-divergent components of the wind (Helmholtz decomposition)
* vector gradient of a scalar function
* triangular truncation of a scalar field
* magnitude (wind speed)

https://ajdawson.github.io/windspharm/latest/[+https://ajdawson.github.io/windspharm/latest/+]

https://github.com/ajdawson/windspharm[+https://github.com/ajdawson/windspharm+]

https://github.com/JiaweiZhuang/process_real_wind[+https://github.com/JiaweiZhuang/process_real_wind+]

WINE
~~~~

Wine (recursive backronym for Wine Is Not an Emulator) is a free and open-source compatibility layer that aims to allow computer programs (application software and computer games) developed for Microsoft Windows to run on Unix-like operating systems. Wine also provides a software library, known as Winelib, against which developers can compile Windows applications to help port them to Unix-like systems.

Wine emulates the Windows runtime environment by translating Windows system calls into POSIX-compliant system calls,[12] recreating the directory structure of Windows systems, and providing alternative implementations of Windows system libraries,[13] system services through wineserver[14] and various other components (such as Internet Explorer, the Windows Registry Editor,[15] and msiexec[16]). Wine is predominantly written using black-box testing reverse-engineering, to avoid copyright issues.

https://www.winehq.org/[+https://www.winehq.org/+]

https://en.wikipedia.org/wiki/Wine_(software)[+https://en.wikipedia.org/wiki/Wine_(software)+]

DOSBox
^^^^^^

DOSBox emulates an Intel x86 PC, complete with sound, graphics, mouse, joystick, modem, etc., necessary for running many old MS-DOS games that simply cannot be run on modern PCs and operating systems, such as Microsoft Windows XP, Windows Vista, Linux and FreeBSD. However, it is not restricted to running only games. In theory, any MS-DOS or PC-DOS (referred to commonly as "DOS") application should run in DOSBox, but the emphasis has been on getting DOS games to run smoothly, which means that communication, networking and printer support are still in early development.

DOSBox also comes with its own DOS-like command prompt. It is still quite rudimentary and lacks many of the features found in MS-DOS, but it is sufficient for installing and running most DOS games.

DOSBox has a lively, user-supported community forum hosted at VOGONS (Very Old Games On New Systems).

https://www.dosbox.com/wiki/[+https://www.dosbox.com/wiki/+]

DXVK
^^^^

A Vulkan-based translation layer for Direct3D 10/11 which allows running 3D applications on Linux using Wine.

https://github.com/doitsujin/dxvk[+https://github.com/doitsujin/dxvk+]

Lutris
^^^^^^

Lutris is a gaming client for Linux. It gives you access to all your video games with the exception of the current console generation. You can, in a single interface, run any game from your childhood memories to the current multiplayer games. Integrations with stores like GOG and Steam allow you to import your existing game library and community maintained install scripts give you a completely automated setup. 

Lutris uses what we refer as runners. Runners are programs that the client can control and launch. Our most popular runners include RetroArch, DosBox, FS-UAE, ScummVM, MESS, Dolphin and of course Wine TkG, a build of Wine optimized to give you the best performance out of your system. It is also possible to launch Linux native games, we have a large selection of free and Open Source games ready to install from the client. Download the client and start playing in minutes.

https://lutris.net/[+https://lutris.net/+]

PlayOnLinux
^^^^^^^^^^^

PlayOnLinux is a piece of software which allows you to easily install and use numerous games and apps designed to run with Microsoft® Windows®. Few games are compatible with GNU/Linux at the moment and it certainly is a factor preventing the migration to this system. PlayOnLinux brings a cost-free, accessible and efficient solution to this problem.

https://github.com/PlayOnLinux/POL-POM-4[+https://github.com/PlayOnLinux/POL-POM-4+]

https://www.playonlinux.com/en/[+https://www.playonlinux.com/en/+]

Q4Wine
^^^^^^

Q4Wine is a Qt GUI for Wine. It will help you manage wine prefixes and installed applications. It currently supported on Linux, FreeBSD and OS X platforms.

https://q4wine.brezblock.org.ua/[+https://q4wine.brezblock.org.ua/+]

Winetricks
^^^^^^^^^^

Winetricks is an easy way to work around problems in Wine.

It has a menu of supported games/apps for which it can do all the workarounds automatically. It also allows the installation of missing DLLs and tweaking of various Wine settings.

https://github.com/Winetricks/winetricks[+https://github.com/Winetricks/winetricks+]

WireGuard
~~~~~~~~~

WireGuard is a free and open-source software application and protocol that implements virtual private network (VPN) techniques to create secure point-to-point connections in routed or bridged configurations. It is run as a module inside the Linux kernel and aims for better performance than the IPsec and OpenVPN tunneling protocols.[

WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry.

WireGuard uses state-of-the-art cryptography, like the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, and secure trusted constructions. It makes conservative and reasonable choices and has been reviewed by cryptographers. 

https://www.wireguard.com/[+https://www.wireguard.com/+]

https://github.com/WireGuard/WireGuard[+https://github.com/WireGuard/WireGuard+]

https://archive.fosdem.org/2017/schedule/event/wireguard/[+https://archive.fosdem.org/2017/schedule/event/wireguard/+]

WorldView
~~~~~~~~~

This app from NASA's EOSDIS provides the capability to interactively browse over 800 global, full-resolution satellite imagery layers on desktop and mobile devices. Many of the imagery layers are updated within three hours of observation, showing the entire Earth as it is "right now". This supports time-critical applications such as wildfire management, air quality measurements, and flood monitoring. Some satellite imagery layers span almost 30 years, providing a long term view of our dynamic planet. The underlying data is available for download, and Arctic and Antarctic views of several imagery layers are available for a “full globe” perspective.

Worldview uses OpenLayers to display imagery from the Global Imagery Browse Services (GIBS). This imagery can also be used with libraries such as Leaflet, Cesium, Google Maps or custom GDAL scripts. We encourage interested developers to fork Worldview or build their own clients using GIBS services.

https://github.com/nasa-gibs/worldview[+https://github.com/nasa-gibs/worldview+]

WorldWind
~~~~~~~~~

WorldWind is an open-source (released under the NOSA license) virtual globe. It was first developed by NASA in 2003 for use on personal computers and then further developed in concert with the open source community since 2004. As of 2017, a web based version of WorldWind is available online. An Android version is also available.

Though widely available since 2003, WorldWind was released with the NASA Open Source Agreement license in 2004. The latest Java-based version (2.1.0), was released in December 2016. As of 2015 a web based version of WorldWind is under development and available online. An Android version is also available.

The previous .NET-based version was an application with an extensive suite of plugins. Apart from the Earth there are several worlds: Moon, Mars, Venus, Jupiter (with the four Galilean moons of Io, Ganymede, Europa and Callisto) and SDSS (imagery of stars and galaxies).

Users could interact with the selected planet by rotating it, tilting the view, and zooming in and out. Five million place names, political boundaries, latitude/longitude lines, and other data can be displayed. WorldWind.NET provided the ability to browse maps and geospatial data on the internet using the OGC's WMS servers (version 1.4 also uses WFS for downloading place names), import ESRI shapefiles and kml/kmz files. This is an example of how WorldWind allows anyone to deliver their data.

Other features of WorldWind.NET included support for .X (DirectX 3D polygon mesh) models and advanced visual effects such as atmospheric scattering or sun shading.

The resolution inside the US is high enough to clearly discern individual buildings, houses, cars (USGS Digital Ortho layer) and even the shadows of people (metropolitan areas in USGS Urban Ortho layer). The resolution outside the US is at least 15 meters per pixel.

Microsoft has allowed WorldWind to incorporate Virtual Earth high resolution data for non-commercial use.

WorldWind uses digital elevation model (DEM) data collected by NASA's Shuttle Radar Topography Mission (SRTM), National Elevation Dataset (NED) and Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER). This means one can view topographic features such as the Grand Canyon or Mount Everest in three dimensions. In addition, WW has bathymetry data which allows users to see ocean features, such as trenches and ridges, in 3D.

Many people using the applications are adding their own data and are making them available through various sources, such as the WorldWind Central or blogs mentioned in the link section below.

All images and movies created with WorldWind using Blue Marble, Landsat, or USGS public domain data can be freely modified, re-distributed, and used on web sites, even for commercial purposes. 

https://en.wikipedia.org/wiki/NASA_WorldWind[+https://en.wikipedia.org/wiki/NASA_WorldWind+]

https://worldwind.arc.nasa.gov/[+https://worldwind.arc.nasa.gov/+]

wormhole
~~~~~~~~

This package provides a library and a command-line tool named wormhole, which makes it possible to get arbitrary-sized files and directories (or short pieces of text) from one computer to another. The two endpoints are identified by using identical “wormhole codes”: in general, the sending machine generates and displays the code, which must then be typed into the receiving machine.

The codes are short and human-pronounceable, using a phonetically-distinct wordlist. The receiving side offers tab-completion on the codewords, so usually only a few characters must be typed. Wormhole codes are single-use and do not need to be memorized.

The sender enters:

------
% wormhole send README.md
Sending 7924 byte file named 'README.md'
On the other computer, please run: wormhole receive
Wormhole code is: 7-crossover-clockwork
 
Sending (<-10.0.1.43:58988)..
100%|=========================| 7.92K/7.92K [00:00<00:00, 6.02MB/s]
File sent.. waiting for confirmation
Confirmation received. Transfer complete.
------

and the receiver enters:

------
% wormhole receive
Enter receive wormhole code: 7-crossover-clockwork
Receiving file (7924 bytes) into: README.md
ok? (y/n): y
Receiving (->tcp:10.0.1.43:58986)..
100%|===========================| 7.92K/7.92K [00:00<00:00, 120KB/s]
Received file written to README.md
------

https://magic-wormhole.readthedocs.io/en/latest/[+https://magic-wormhole.readthedocs.io/en/latest/+]

https://github.com/warner/magic-wormhole[+https://github.com/warner/magic-wormhole+]

#XXXX

X10
~~~

X10 is a statically-typed object-oriented language, extending a sequential core language with places, activities, clocks, (distributed, multi-dimensional) arrays and struct types. All these changes are motivated by the desire to use the new language for high-end, high-performance, high-productivity computing. 

X10 is designed specifically for parallel computing using the partitioned global address space (PGAS) model. A computation is divided among a set of places, each of which holds some data and hosts one or more activities that operate on those data. It has a constrained type system for object-oriented programming, a form of dependent types. Other features include user-defined primitive struct types; globally distributed arrays, and structured and unstructured parallelism.

http://x10-lang.org/[+http://x10-lang.org/+]

https://github.com/x10-lang[+https://github.com/x10-lang+]

https://grothoff.org/christian/xtc/x10/[+https://grothoff.org/christian/xtc/x10/+]

https://en.wikipedia.org/wiki/X10_(programming_language)[+https://en.wikipedia.org/wiki/X10_(programming_language)+]

Xanthos
~~~~~~~

Xanthos is an open-source hydrologic model, written in Python, designed to quantify and analyze global water availability. Xanthos simulates historical and future global water availability on a monthly time step at a spatial resolution of 0.5 geographic degrees. Xanthos was designed to be extensible and used by scientists that study global water supply and work with the Global Change Assessment Model (GCAM). Xanthos uses a user-defined configuration file to specify model inputs, outputs and parameters. Xanthos has been tested using actual global data sets and the model is able to provide historical observations and future estimates of renewable freshwater resources in the form of total runoff, average streamflow, potential evapotranspiration, actual evapotranspiration, accessible water, hydropower potential, and more.

https://github.com/JGCRI/xanthos[+https://github.com/JGCRI/xanthos+]

https://openresearchsoftware.metajnl.com/articles/10.5334/jors.181/[+https://openresearchsoftware.metajnl.com/articles/10.5334/jors.181/+]

https://github.com/JGCRI/xanthos/wiki/Available-modules[+https://github.com/JGCRI/xanthos/wiki/Available-modules+]

xarray
~~~~~~

xarray (formerly xray) is an open source project and Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun!

Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience. The package includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.

Xarray was inspired by and borrows heavily from pandas, the popular data analysis package focused on labelled tabular data. It is particularly tailored to working with netCDF files, which were the source of xarray's data model, and integrates tightly with dask for parallel computing.

Multi-dimensional (a.k.a. N-dimensional, ND) arrays (sometimes called "tensors") are an essential part of computational science. They are encountered in a wide range of fields, including physics, astronomy, geoscience, bioinformatics, engineering, finance, and deep learning. In Python, NumPy provides the fundamental data structure and API for working with raw ND arrays. However, real-world datasets are usually more than just raw numbers; they have labels which encode information about how the array values map to locations in space, time, etc.

https://github.com/pydata/xarray[+https://github.com/pydata/xarray+]

http://xarray.pydata.org/en/stable/[+http://xarray.pydata.org/en/stable/+]

downscale
^^^^^^^^^

Tools for downscaling gridded data in xarray.

https://downscale.readthedocs.io/en/latest/readme.html[+https://downscale.readthedocs.io/en/latest/readme.html+]

https://github.com/ClimateImpactLab/downscale[+https://github.com/ClimateImpactLab/downscale+]

esmlab
^^^^^^

Tools for working with earth system multi-model analyses with xarray.
It includes statistics, climatology and regridding functions.

https://github.com/NCAR/esmlab[+https://github.com/NCAR/esmlab+]

extendXarray
^^^^^^^^^^^^

Classes and functions providing high-level interchange between vector and array processing formats. Intended for use with the xarray Python package to simplify common GIS operations (for geojson/shapefile masking, zonal statistics, regional weighting, etc).

Coversion classes and functions are for gridding vector data and switching between array formats.

https://github.com/jjmcnelis/extendXarray[+https://github.com/jjmcnelis/extendXarray+]

ndcsv
^^^^^

NDCSV is a file format that allows storing N-dimensional labelled arrays into human-readable CSV files and read them back without needing any configuration, load hints, or sidecar configuration files.

The fundamental concept is that, unlike pandas.DataFrame.to_csv() and pandas.read_csv(), reading and writing objects is fully automated and reversible. One does not need to specify how many rows and/or columns of header are available - the file format is unambiguous and the library automatically does the right thing.

The format was designed around xarray, so it supports, out of the box:

* Arrays with any number of dimensions
* Labelled, named indices
* Non-index coordinates

https://github.com/crusaderky/ndcsv[+https://github.com/crusaderky/ndcsv+]

https://ndcsv.readthedocs.io/en/latest/[+https://ndcsv.readthedocs.io/en/latest/+]

pyXpcm
^^^^^^

Profile Classification Modelling is a scientific analysis approach based on vertical profiles classification that can be used in a variety of oceanographic problems (front detection, water mass identification, natural region contouring, reference profile selection for validation, etc …). It is being developed at Ifremer-LOPS in collaboration with IMT-Atlantique since 2015, and has become mature enough (with publication and communications) to be distributed and made publicly available for continuous improvements with a community development.

pyXpcm is a package consuming and producing Xarray objects. Xarray objects are N-D labeled arrays and datasets in Python. In future release, pyXpcm will be able to digest very large datasets, following the Pangeo initiative.

https://pyxpcm.readthedocs.io/en/latest/[+https://pyxpcm.readthedocs.io/en/latest/+]

https://github.com/obidam/pyxpcm[+https://github.com/obidam/pyxpcm+]

https://archimer.ifremer.fr/doc/00387/49816/[+https://archimer.ifremer.fr/doc/00387/49816/+]

Salem
^^^^^

Salem is a small library to do geoscientific data processing and plotting. It extends xarray to add geolocalised subsetting, masking, and plotting operations to xarray’s DataArray and DataSet structures.

https://salem.readthedocs.io/en/v0.2.3/[+https://salem.readthedocs.io/en/v0.2.3/+]

https://github.com/fmaussion/salem[+https://github.com/fmaussion/salem+]

xarray_extras
^^^^^^^^^^^^^

This module offers several extensions to xarray.
The API of xarray-extras is unstable by definition, as features will be progressively migrated upwards towards xarray, dask, numpy, pandas, etc. The features include;

* csv - Multi-threaded CSV writer, much faster than pandas.DataFrame.to_csv(), with full support for dask and dask distributed.
* cumulatives - Advanced cumulative sum/productory/mean functions
* interpolate - dask-optimized n-dimensional spline interpolation
* numba-extras - Additions to numba
* sort - Advanced sort/take functions
* stack - Tools for stacking/unstacking dimensions

https://xarray-extras.readthedocs.io/en/latest/[+https://xarray-extras.readthedocs.io/en/latest/+]

https://github.com/crusaderky/xarray_extras[+https://github.com/crusaderky/xarray_extras+]

xarray-simlab
^^^^^^^^^^^^^

xarray-simlab is a Python library that provides both a generic framework for building computational models in a modular fashion and a xarray extension for setting and running simulations using the xarray.Dataset structure. It is designed for interactive and exploratory modeling.

xarray-simlab provides a framework for easily building custom computational models from a set of modular components (i.e., Python classes), called processes.

The framework handles issues that scientists who are developing models should not care too much about, like the model interface and the overall workflow management. Both are automatically determined from the succint, declarative-like interfaces of the model processes.

xarray-simlab is being developped with the idea of reducing the gap between the environments used for building and running computational models and the ones used for processing and analyzing simulation results. If the latter environments become more powerful and interactive, progress has still to be done for the former ones.

https://xarray-simlab.readthedocs.io/en/latest/[+https://xarray-simlab.readthedocs.io/en/latest/+]

https://github.com/benbovy/xarray-simlab[+https://github.com/benbovy/xarray-simlab+]

xclim
^^^^^

xclim is a library of functions computing climate indices. It is based on xarray and can benefit from the parallelization provided by dask. It's objective is to make it as simple as possible for users to compute indices from large climate datasets, and for scientists to write new indices with very little boilerplate.

For applications where meta-data and missing values are important to get right, xclim also provides a class for each index that validates inputs, checks for missing values, converts units and assigns metadata attributes to the output. This provides a mechanism for users to customize the indices to their own specifications and preferences.

https://github.com/Ouranosinc/xclim[+https://github.com/Ouranosinc/xclim+]

https://xclim.readthedocs.io/en/latest/[+https://xclim.readthedocs.io/en/latest/+]

xcube-server
^^^^^^^^^^^^

xcube-server is a light-weight web server that provides various services based on xcube datasets:

* Catalogue services to query for datasets and their variables and dimensions, and feature collections.
* Tile map service, with some OGC WMTS 1.0 compatibility (REST and KVP APIs)
* Dataset services to extract subsets like time-series and profiles for e.g. JS clients

xcube datasets are any datasets that

* that comply to Unidata's CDM and to the CF Conventions;
* that can be opened with the xarray Python library;
* that have variables that have at least the dimensions and shape (time, lat, lon), in exactly this order;
* that have 1D-coordinate variables corresponding to the dimensions;
* that have their spatial grid defined in the WGS84 (EPSG:4326) coordinate reference system.

xcube-server supports local NetCDF files or local or remote Zarr directories. Remote Zarr directories must be stored in publicly accessible, AWS S3 compatible object storage (OBS).

https://github.com/dcs4cop/xcube-server[+https://github.com/dcs4cop/xcube-server+]

xscale
^^^^^^

A library of multi-dimensional signal processing tools using parallel computing.
Xscale is defined to work with multi-dimensional self-described arrays using the xarray objects. Xscale also benefits from parallel computation of several numpy and scipy functions implemented in the dask library. Most of the tools found in xscale are developed to analyse and separate time and spatial scales.

https://xscale.readthedocs.io/en/latest/[+https://xscale.readthedocs.io/en/latest/+]

Xastir
~~~~~~

Xastir is an APRS(tm) client program that uses amateur radio and Internet services to convey GPS mapping, weather, and positional data in a graphical application. It has been developed by and for amateur radio enthusiasts to provide real-time data in an easy to use package. 
Xastir is an acronym for X Amateur Station Tracking and Information Reporting.

Automatic Packet Reporting System (APRS) (also known as "Automatic Position Reporting System") is an amateur radio based digital communication system for real-time exchange of digital information to users on the network.
 Xastir is a computer application that provides client access to this network using the APRS protocol.

http://xastir.org/index.php/Main_Page[+http://xastir.org/index.php/Main_Page+]

https://github.com/Xastir/Xastir[+https://github.com/Xastir/Xastir+]

XBOS
~~~~

XBOS (eXtensible Building Operating System) is an open-source large-scale distributed operating system for smart buildings. XBOS provides secure:

* Real-time monitoring of building sensors
* Control of building actuators
* Collection, modeling, and analytics of building data
* Advanced management and coordination of building systems/subsystems

XBOS aims to push the state-of-the-art in the operation, management, analysis and control of a software-defined building. XBOS integrates traditional vertical silos of building mangement -- lighting, HVAC, electrical -- by combining abstractions of underlying hardware, sensors and actuators with rich metadata description of those components and the relationships between them.

The XBOS features include:

* A common abstraction for IoT sensors and actuators, smart devices, building management systems, etc.
* Long-term secure storage of building telemetry
* Rich standardized metadata describing the building environment and its components
* Secure NAT-friendly communication (via a publish/subscribe bus)
* Decentralized authentication/authorization across multiple administrative domains
* Secure management of persistent processes
* Tools for developing and deploying portable analytics, schedules, controllers, and models
* Tools for developing and deploying user-facing applications

XBOS is assembled from a family of open-source technologies, which can be found via the XBOS Github. 
https://docs.xbos.io/[+https://docs.xbos.io/+]

https://github.com/SoftwareDefinedBuildings/XBOS[+https://github.com/SoftwareDefinedBuildings/XBOS+]

xCAT
~~~~

xCAT is Extreme Cloud Administration Toolkit, xCAT offers complete management for HPC clusters, RenderFarms, Grids, WebFarms, Online Gaming Infrastructure, Clouds, Datacenters, and whatever tomorrow's buzzwords may be. It is agile, extensible, and based on years of system administration best practices and experience. It enables you to:

* Provision Operating Systems on physical or virtual machines: RHEL, CentOS, Fedora, SLES, Ubuntu, AIX, Windows, VMWare, KVM, PowerVM, PowerKVM, zVM.
* Provision using scripted install, stateless, statelite, iSCSI, or cloning
* Remotely manage systems: lights-out management, remote console, and distributed shell support
* Quickly configure and control management node services: DNS, HTTP, DHCP, TFTP, NFS

Differentiating features are:

* xCAT Scales - Beyond all IT budgets. 1,000s of nodes with distributed architecture
* Open Source - Eclipse Public License. You can also buy support contracts.
* More Installation Options - Install to Hard Disk, run stateless (Diskless), run diskless with a little bit of state (Statelite), install to iSCSI (no special hardware required), virtual machines (VMWare, KVM, Xen, PowerVM, zVM)
* Built in Automagic discovery - no need to power on one machine at a time to discover. Also, nodes that fail can be replaced and back in action by just powering them on

http://xcat.org/[+http://xcat.org/+]

https://github.com/xcat2/xcat-core/[+https://github.com/xcat2/xcat-core/+]

https://xcat-docs.readthedocs.io/en/stable/[+https://xcat-docs.readthedocs.io/en/stable/+]

XDMF
~~~~

The need for a standardized method to exchange scientific data between High Performance Computing codes and tools lead to the development of the eXtensible Data Model and Format (XDMF) . Uses for XDMF range from a standard format used by HPC codes to take advantage of widely used visualization programs like ParaView, to a mechanism for performing coupled calculations using multiple, previously stand alone codes.

Data format refers to the raw data to be manipulated. Information like number type ( float, integer, etc.), precision, location, rank, and dimensions completely describe the any dataset regardless of its size. The description of the data is also separate from the values themselves. We refer to the description of the data as Light data and the values themselves as Heavy data. Light data is small and can be passed between modules easily. Heavy data may be potentially enormous; movement needs to be kept to a minimum. Due to the different nature of heavy and light data, they are stored using separate mechanisms. Light data is stored using XML, Heavy data is typically stored using HDF5. While we could have chosen to store the light data using HDF5 attributes using XML does not require every tool to have access to the compiled HDF5 libraries in order to perform simple operations. 

XDMF uses XML to store Light data and to describe the data Model. HDF5 is used to store Heavy data. The data Format is stored redundantly in both XML and HDF5. This allows tools to parse XML to determine the resources that will be required to access the Heavy data.

The data model in XDMF stored in XML provides the knowledge of what is represented by the Heavy data. In this model, HPC data is viewed as a hierarchy of Domains. A Domain must contain at least one Grid. A Grid is the basic representation of both the geometric and computed/measured values. A Grid is considered to be a group of elements with Structured or Unstructured Topology and their associated values. In addition to the topology of the Grid, Geometry, specifying the X, Y, and Z positions of the Grid is required. Finally, a Grid may have one or more Attributes. Attributes are used to store any other value associated with the grid and may be referenced to the Grid or to individual cells that comprise the Grid.

The concept of separating the light data from the heavy data is critical to the performance of this data model and format. HPC codes can read and write data in large, contiguous chunks that are natural to their internal data storage, to achieve optimal I/O performance. If codes were required to significantly re-arrange data prior to I/O operations, data locality, and thus performance, could be adversely affected, particularly on codes that attempt to make maximum use of memory cache. The complexity of the dataset is described in the light data portion, which is small and transportable. For example, the light data might specify a topology of one million hexaherda while the heavy data would contain the geometric XYZ values of the mesh and pressure values at the cell centers stored in large, contiguous arrays. This key feature will allow reusable tools to be built that do not put onerous requirements on HPC codes. Despite the complexity of the organization described in the XML below, the HPC code only needs to produce the three HDF5 datasets for geometry, connectivity, and pressure values.

While not required, a Cxx API is provided to read and write XDMF data. This API has also been wrapped so it is available from popular languages like Python, Tcl, and Java. The API is not necessary in order to produce or consume XDMF data. Currently several HPC codes that already produced HDF5 data, use native text output to produce the XML necessary for valid XDMF. 

http://xdmf.org/index.php/Main_Page[+http://xdmf.org/index.php/Main_Page+]

http://xdmf.org/index.php/Get_Xdmf[+http://xdmf.org/index.php/Get_Xdmf+]

XH5For
^^^^^^

A library to read and write parallel partitioned FEM meshes taking advantage of the Collective/Independent IO provided by the HDF5 library. XH5For is not a general-purpose XDMF library, it only reads XDMF files written by itself.

https://github.com/victorsndvg/XH5For[+https://github.com/victorsndvg/XH5For+]

xerus
~~~~~

The xerus library is a general purpose library for numerical calculations with higher order tensors, Tensor-Train Decompositions / Matrix Product States and general Tensor Networks. The focus of development was the simple usability and adaptibility to any setting that requires higher order tensors or decompositions thereof.

The key features include:

* Modern code and concepts incorporating many features of the Cxx11 standard.
* Full python bindings with very similar syntax for easy transitions from and to cxx.
* Calculation with tensors of arbitrary orders using an intuitive Einstein-like notation A(i,j) = B(i,k,l) * C(k,j,l);.
* Full implementation of the Tensor-Train decompositions (MPS) with all neccessary capabilities (including Algorithms like ALS, ADF and CG).
* Lazy evaluation of (multiple) tensor contractions featuring heuristics to automatically find efficient contraction orders.
* Direct integration of the blas and lapack, as high performance linear algebra backends.
* Fast sparse tensor calculation by usage of the suiteSparse sparse matrix capabilities.
* Capabilites to handle arbitrary Tensor Networks.

https://www.libxerus.org/[+https://www.libxerus.org/+]

https://git.hemio.de/xerus/xerus[+https://git.hemio.de/xerus/xerus+]

http://wias-berlin.de/software/index.jsp?id=ALEA&lang=1[+http://wias-berlin.de/software/index.jsp?id=ALEA&lang=1+]

xgcm
~~~~

xgcm is a python packge for working with the datasets produced by numerical General Circulation Models (GCMs) and similar gridded datasets that are amenable to finite volume analysis. In these datasets, different variables are located at different positions with respect to a volume or area element (e.g. cell center, cell face, etc.) xgcm solves the problem of how to interpolate and difference these variables from one position to another.

xgcm consumes and produces xarray data structures, which are coordinate and metadata-rich representations of multidimensional array data. xarray is ideal for analyzing GCM data in many ways, providing convenient indexing and grouping, coordinate-aware data transformations, and (via dask) parallel, out-of-core array computation. On top of this, xgcm adds an understanding of the finite volume Arakawa Grids commonly used in ocean and atmospheric models and differential and integral operators suited to these grids.

xgcm was motivated by the rapid growth in the numerical resolution of ocean, atmosphere, and climate models. While highly parallel supercomputers can now easily generate tera- and petascale datasets, common post-processing workflows struggle with these volumes. Furthermore, we believe that a flexible, evolving, open-source, python-based framework for GCM analysis will enhance the productivity of the field as a whole, accelerating the rate of discovery in climate science. xgcm is part of the Pangeo initiative.

https://xgcm.readthedocs.io/en/latest/[+https://xgcm.readthedocs.io/en/latest/+]

https://github.com/xgcm/xgcm[+https://github.com/xgcm/xgcm+]

xshape
^^^^^^

Tools for working with shapefiles, topographies, and polygons in xarray.  The features include:

* Read a shapefile and obtain an xarray DataArray of field records
* Draw shapefile boundaries on gridded data
* Plot xarray DataArray data indexed by shapefile records as a choropleth

https://github.com/ClimateImpactLab/xshape[+https://github.com/ClimateImpactLab/xshape+]

xyzpy
^^^^^

xyzpy is python library for efficiently generating, manipulating and plotting data with a lot of dimensions, of the type that often occurs in numerical simulations. It stands wholly atop the labelled N-dimensional array library xarray. The project is hosted on github, please do submit any issues or PRs there.

The aim is to take the pain and errors out of generating and exploring data with a high number of possible parameters. This means:

* you don’t have to write super nested for loops
* you don’t have to remember which arrays/dimensions belong to which variables/parameters
* you don’t have to parallelize over or distribute runs yourself
* you don’t have to worry about loading, saving and merging disjoint data
* you don’t need to guess when a set of runs is going to finish

As well as the ability to automatically parallelize over runs, xyzpy provides the Crop object that allows runs and results to be written to disk, these can then be run by any process with access to the files - e.g. a batch system - or just serve as a convenient persistent progress mechanism.

In terms of post-processing, as well as all the power of xarray, xyzpy adds uneven step differentiation and error propagation, filtering and interpolation - along any axis just specified by name.

The aim of the plotting functionality is to keep the same interface between interactively plotting the data using bokeh, and static, publication ready figures using matplotlib, whilst being able to see the dependence on up to 4 dimensions at once.

https://xyzpy.readthedocs.io/en/latest/[+https://xyzpy.readthedocs.io/en/latest/+]

https://github.com/jcmgray/xyzpy[+https://github.com/jcmgray/xyzpy+]

XBraid
~~~~~~

XBraid is a parallel-in-time software package. It implements an optimal-scaling multigrid solver for the (non)linear systems that arise from the discretization of problems with evolutionary behavior.

Typically, solution algorithms for evolution equations are based on a time-marching approach, solving sequentially for one time step after the other. Parallelism in these traditional time-integration techniques is limited to spatial parallelism. However, current trends in computer architectures are leading towards systems with more, but not faster, processors, i.e., clock speeds are stagnate. Therefore, faster overall runtimes must come from greater parallelism. Our approach to achieve such parallelism in time is with multigrid.

In this software, we implement a non-intrusive, optimal-scaling time-parallel method based on multigrid reduction techniques (multigrid-reduction-in-time or MGRIT).

The field of parallel-in-time methods is in many ways under development, and success has been shown primarily for problems with some parabolic character. While there are ongoing projects (here and elsewhere) looking at varied applications such as hyperbolic problems, computational fluid dynamics, power grids, medical applications, and so on, expectations should take this fact into account. That being said, we strongly encourage new users to try our code for their application. Every new application has its own issues to address and this will help us to improve both the algorithm and the software.

https://github.com/XBraid/xbraid[+https://github.com/XBraid/xbraid+]

https://computation.llnl.gov/projects/parallel-time-integration-multigrid[+https://computation.llnl.gov/projects/parallel-time-integration-multigrid+]

XLA
~~~

XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes TensorFlow computations. The results are improvements in speed, memory usage, and portability on server and mobile platforms. The XLA framework is experimental and in active development.  Initially, most users will not see large benefits from XLA, but are welcome to experiment by using XLA via just-in-time (JIT) compilation or ahead-of-time (AOT) compilation. Developers targeting new hardware accelerators are especially encouraged to try out XLA.

The input language to XLA is called "HLO IR", or just HLO (High Level Optimizer). The semantics of HLO are described on the Operation Semantics page. It is most convenient to think of HLO as a compiler IR.

XLA takes graphs ("computations") defined in HLO and compiles them into machine instructions for various architectures. XLA is modular in the sense that it is easy to slot in an alternative backend to target some novel HW architecture. The CPU backend for x64 and ARM64 as well as the NVIDIA GPU backend are in the TensorFlow source tree.

https://www.tensorflow.org/xla/[+https://www.tensorflow.org/xla/+]

https://ursalabs.org/tech/[+https://ursalabs.org/tech/+]

XMP
~~~

XcalableMP, XMP for short, is a directive-based language extension which allows users to develop parallel programs for distributed memory systems easily and to tune the performance by having minimal and simple notations. 

XMP enables parallelizing the original sequential code using minimal modification with simple directives.  Programmer can use coarray syntax in both XMP/Fortran and XMP/C. In particular, XMP/Fortran is designed to be compatible with Coarray Fortran.  In order to call an MPI program from an XMP program, XMP provides the MPI programming interface. Moreover, OpenMP directives can be combined into XMP as a hybrid programming.

http://xcalablemp.org/[+http://xcalablemp.org/+]

XnViewMP
~~~~~~~~

XnView MP is the enhanced version of XnView Classic. It is a powerful picture viewer, browser and converter for Windows, Mac and Linux.  The features include:

* supports more than 500 image formats (including Multipage and animated still formats APNG, TIFF, GIF, ICO, etc..) and export to about 70 different file formats.
* comes with an easy to use yet powerful batch conversion module.
* Unicode support

https://www.xnview.com/en/[+https://www.xnview.com/en/+]

X.Org Server
~~~~~~~~~~~~

X.Org Server is the free and open-source implementation of the display server for the X Window System stewarded by the X.Org Foundation. 
Implementations of the client side of the protocol are available e.g. in the form of Xlib and XCB.
The X.Org Server implements the server side of the X Window System core protocol version 11 (X11) and extensions to it, e.g. RandR.

The most recent release was X11R7.7 in 2012.
Unlike X11R1 through X11R6.9, X11R7.x releases are not built from one monolithic source tree, but many individual modules. These modules are distributed as individual source code releases, and each one is released when it is ready, instead of only when the overall window system is ready for release. The X11R7.x releases are made by “rolling up” the individual module releases into a collection that is often affectionately called the “katamari” by the developers.

The X11R7.7 release does not include all of the software formerly included in the previous X Window System releases. It is designed to be a reasonable baseline from which to start when building the window system for the first time for a new installation, distribution, or package set. It does not provide a full desktop environment, expecting a more feature rich set of applications to be installed from one of the several excellent desktop environments available for the X Window System. The X.Org developers continue to maintain and produce new releases of much of the software that was formerly in the main window system releases but is no longer included in the katamari releases, including many of the Athena Widgets desktop applications that were provided as samples in previous window system versions.

On most platforms, X11R7.7 has a single hardware-driving X server binary called Xorg. This binary can dynamically load the video drivers, input drivers, and other modules that are needed. Xorg has currently has support for Linux, Solaris, and some BSD OSs on Alpha, PowerPC, IA-64, AMD64, Intel x86, Sparc, and MIPS platforms. 

https://www.x.org/wiki/[+https://www.x.org/wiki/+]

https://en.wikipedia.org/wiki/X.Org_Server[+https://en.wikipedia.org/wiki/X.Org_Server+]

*Documentation* - https://www.x.org/releases/X11R7.7/doc/[+https://www.x.org/releases/X11R7.7/doc/+]
* *commands* - https://www.x.org/releases/X11R7.7/doc/man/man1/index.xhtml[+https://www.x.org/releases/X11R7.7/doc/man/man1/index.xhtml+]
* *drivers* - https://www.x.org/releases/X11R7.7/doc/man/man4/index.xhtml[+https://www.x.org/releases/X11R7.7/doc/man/man4/index.xhtml+]
* *library functions* - https://www.x.org/releases/X11R7.7/doc/man/man3/index.xhtml[+https://www.x.org/releases/X11R7.7/doc/man/man3/index.xhtml+]

xSDK
~~~~

Rapid, efficient production of high-quality, sustainable extreme-scale scientific applications is best accomplished using a rich ecosystem of state-of-the art reusable libraries, tools, lightweight frameworks, and defined software methodologies, developed by a community of scientists who are striving to identify, adapt, and adopt best practices in software engineering. The vision of the xSDK is to provide infrastructure for and interoperability of a collection of related and complementary software elements—developed by diverse, independent teams throughout the high-performance computing (HPC) community—that provide the building blocks, tools, models, processes, and related artifacts for rapid and efficient development of high-quality applications.

The goal of the xSDK is to provide the foundation of this extensible scientific software ecosystem.  The first xSDK release (in April 2016) demonstrated the impact of defining draft xSDK community policies to simplify the combined use and portability of independently developed software packages.  Currently, the xSDK release includes hypre, MAGMA, MFEM, PETSc/TAO, SUNDIALS, SuperLU, and Trilinos.  This release also lays the groundwork for addressing broader issues in software interoperability and performance portability.  This work is especially important as emerging extreme-scale architectures provide unprecedented resources for more complex computational science and engineering simulations, yet the current era of disruptive architectural changes requires refactoring and enhancing software packages in order to effectively use these machines for scientific discovery.

https://xsdk.info/[+https://xsdk.info/+]

https://arxiv.org/abs/1702.08425[+https://arxiv.org/abs/1702.08425+]

xtensor
~~~~~~

xtensor is a Cxx library meant for numerical analysis with multi-dimensional array expressions.
It provides:

* an extensible expression system enabling lazy broadcasting.
* an API following the idioms of the Cxx standard library.
* tools to manipulate array expressions and build upon xtensor.

Containers of xtensor are inspired by NumPy, the Python array programming library. Adaptors for existing data structures to be plugged into the expression system can easily be written.

In fact, xtensor can be used to process numpy data structures in-place using Python’s buffer protocol. For more details on the numpy bindings, check out the xtensor-python project. Language bindings for R and Julia are also available.

xtensor requires gcc 4.9 or greater or a recent version of Clang.

https://xtensor.readthedocs.io/en/latest/index.html[+https://xtensor.readthedocs.io/en/latest/index.html+]

https://github.com/QuantStack/xtensor[+https://github.com/QuantStack/xtensor+]

https://github.com/QuantStack/xtensor-python[+https://github.com/QuantStack/xtensor-python+]

https://github.com/QuantStack/xtensor-blas[+https://github.com/QuantStack/xtensor-blas+]

https://github.com/ClimateImpactLab/xtensor_climate_fun[+https://github.com/ClimateImpactLab/xtensor_climate_fun+]

xtensor-blas
^^^^^^^^^^^^

xtensor-blas is an extension to the xtensor library, offering bindings to BLAS and LAPACK libraries through cxxblas and cxxlapack from the FLENS project.

xtensor-blas currently provides non-broadcasting dot, norm (1- and 2-norm for vectors), inverse, solve, eig, cross, det, slogdet, matrix_rank, inv, cholesky, qr, svd in the xt::linalg namespace (check the corresponding xlinalg.hpp header for the function signatures). The functions, and signatures, are trying to be 1-to-1 equivalent to NumPy. Low-level functions to interface with BLAS or LAPACK with xtensor containers are also offered in the blas and lapack namespace.

https://github.com/QuantStack/xtensor-blas[+https://github.com/QuantStack/xtensor-blas+]

xtensor-julia
^^^^^^^^^^^^^

Julia bindings for the xtensor Cxx multi-dimensional array library.
The Julia bindings for xtensor are based on the libcxxwrap Cxx library.

https://github.com/QuantStack/xtensor-julia[+https://github.com/QuantStack/xtensor-julia+]

xtensor-python
^^^^^^^^^^^^^^

Python bindings for the xtensor Cxx multi-dimensional array library.
The Python bindings for xtensor are based on the pybind11 Cxx library, which enables seemless interoperability between Cxx and Python.

https://github.com/QuantStack/xtensor-python[+https://github.com/QuantStack/xtensor-python+]

#YYYY

Yade
~~~~

Yade is an extensible open-source framework for discrete numerical models, focused on Discrete Element Method. The computation parts are written in cxx using flexible object model, allowing independent implementation of new alogrithms and interfaces. Python is used for rapid and concise scene construction, simulation control, postprocessing and debugging.

https://yade-dev.gitlab.io/trunk/[+https://yade-dev.gitlab.io/trunk/+]

YAPF
~~~~

Most of the current formatters for Python --- e.g., autopep8, and pep8ify --- are made to remove lint errors from code. This has some obvious limitations. For instance, code that conforms to the PEP 8 guidelines may not be reformatted. But it doesn't mean that the code looks good.

YAPF takes a different approach. It's based off of 'clang-format', developed by Daniel Jasper. In essence, the algorithm takes the code and reformats it to the best formatting that conforms to the style guide, even if the original code didn't violate the style guide. The idea is also similar to the 'gofmt' tool for the Go programming language: end all holy wars about formatting - if the whole codebase of a project is simply piped through YAPF whenever modifications are made, the style remains consistent throughout the project and there's no point arguing about style in every code review.

The ultimate goal is that the code YAPF produces is as good as the code that a programmer would write if they were following the style guide. It takes away some of the drudgery of maintaining your code.

https://github.com/google/yapf[+https://github.com/google/yapf+]

https://yapf.now.sh/[+https://yapf.now.sh/+]

YaR
~~~

R is a reversible programming language; that is, it is a high-level language for writing programs that can be executed in either the forward or the reverse time direction. Such languages have a number of applications, such as for rolling back processes to synchronize concurrent systems, or for describing algorithms that permit more energy-efficient hardware implementations.

Created in 1997 [1], R was historically one of the earliest reversible high-level languages, predated only by Lutz and Derby's Janus [2] in 1982, and Henry Baker's Psi-Lisp [3] in 1992. Although R was actually developed independently of Janus, we ended up reinventing some of the same concepts.

The syntax and semantics of R is vaguely C-like, but with a Lisp-like, parenthesis-heavy style of punctuation. For more information, see the various documents contained in the docs/ subdirectory.

Please note that R is unrelated to the statistics language of the same name. When our R was named, in 1997, the other one was not yet widely known. Since then, the R statistics language has become very popular, so it might be a good idea to rename the R reversible language at some point to avoid confusion. One alternate name that I have used for it occasionally is "ЯR" (pronounced "yar"; the backwards R is Cyrillic "ya"). In ASCII this can be rendered "YaR" and taken to mean "Yet another R" or "Yet another Reversible (language)."

Rcomp is written in the Common Lisp programming language, so running it requires a Common Lisp development environment (interpreter and/or compiler). An example environment of this sort that works well is CLISP.

https://github.com/mikepfrank/Rlang-compiler[+https://github.com/mikepfrank/Rlang-compiler+]

Yarn
~~~~

Fast, reliable, and secure dependency management.
Yarn caches every package it has downloaded, so it never needs to download the same package again. It also does almost everything concurrently to maximize resource utilization. This means even faster installs.

Using a detailed but concise lockfile format and a deterministic algorithm for install operations, Yarn is able to guarantee that any installation that works on one system will work exactly the same on another system.
Yarn uses checksums to verify the integrity of every installed package before its code is executed.

The features include:

* Offline Mode. If you've installed a package before, then you can install it again without an internet connection.
* Deterministic. The same dependencies will be installed in the same exact way on any machine, regardless of installation order.
* Network Performance. Yarn efficiently queues requests and avoids request waterfalls in order to maximize network utilization.
* Network Resilience. A single request that fails will not cause the entire installation to fail. Requests are automatically retried upon failure.
* Flat Mode. Yarn resolves mismatched versions of dependencies to a single version to avoid creating duplicates.

https://github.com/yarnpkg/yarn[+https://github.com/yarnpkg/yarn+]

https://yarnpkg.com/en/[+https://yarnpkg.com/en/+]

youtube-dl
~~~~~~~~~~

A command-line program to download videos from YouTube.com and a few more sites.

https://rg3.github.io/youtube-dl/[+https://rg3.github.io/youtube-dl/+]

https://rg3.github.io/youtube-dl/supportedsites.html[+https://rg3.github.io/youtube-dl/supportedsites.html+]

How to set up youtube-dl.conf to download entire channels.

The goals are:

* Audio and Video are downloaded in the highest quality

* Content is remuxed to mkv when necessary, avoiding transcoding

* All non-automatic subtitles downloaded and converted to .srt (every language that is available)

* Video descriptions are saved in a separate file

* Metadata embedded into video file (including title, uploader, description, date, original url)

* Thumbnail saved next to video

https://www.reddit.com/r/DataHoarder/comments/858ny5/my_youtubedl_config_downloading_entire_channels/[+https://www.reddit.com/r/DataHoarder/comments/858ny5/my_youtubedl_config_downloading_entire_channels/+]

#ZZZZ

Zarr
~~~~

Zarr is a Python package providing an implementation of compressed, chunked, N-dimensional arrays, designed for use in parallel computing.
The capabilities include;

* Create N-dimensional arrays with any NumPy dtype.
* Chunk arrays along any dimension.
* Compress and/or filter chunks using any NumCodecs codec.
* Store arrays in memory, on disk, inside a Zip file, on S3, …
* Read an array concurrently from multiple threads or processes.
* Write to an array concurrently from multiple threads or processes.
* Organize arrays into hierarchies via groups.

https://github.com/zarr-developers/zarr[+https://github.com/zarr-developers/zarr+]

https://zarr.readthedocs.io/en/stable/[+https://zarr.readthedocs.io/en/stable/+]

ZBoxFS
~~~~~~

ZboxFS is a zero-details, privacy-focused embeddable file system. Its goal is to help application store files securely, privately and reliably. By encapsulating files and directories into an encrypted repository, it provides a virtual file system and exclusive access to authorised application.

Unlike other system-level file systems, such as ext4, XFS and Btrfs, which provide shared access to multiple processes, ZboxFS is a file system that runs in the same memory space as the application. It only provides access to one process at a time.

By abstracting IO access, ZboxFS supports a variety of underlying storage layers, including memory, OS file system, RDBMS and key-value object store.

The features include:

* Everything is encrypted lock, including metadata and directory structure, no knowledge can be leaked to underlying storage
* State-of-the-art cryptography: AES-256-GCM (hardware), XChaCha20-Poly1305, Argon2 password hashing and etc., empowered by libsodium
* Support multiple storages, including memory, OS file system, RDBMS, Key-value object store and more
* Content-based data chunk deduplication and file-based deduplication
* Data compression using LZ4 in fast mode, optional
* Data integrity is guaranteed by authenticated encryption primitives (AEAD crypto)
* File contents versioning
* Copy-on-write (COW cow) semantics
* ACID transactional operations

https://github.com/zboxfs/zbox[+https://github.com/zboxfs/zbox+]

https://zbox.io/[+https://zbox.io/+]

Zephyr
~~~~~~

he Zephyr OS is based on a small-footprint kernel designed for use on resource-constrained and embedded systems: from simple embedded environmental sensors and LED wearables to sophisticated embedded controllers, smart watches, and IoT wireless applications.

The Zephyr kernel supports multiple architectures, including ARM Cortex-M, Intel x86, ARC, NIOS II, Tensilica Xtensa and RISC-V 32.  Over 100 boards are supported.

The kernel services offered are:

* Multi-threading Services for cooperative, priority-based, non-preemptive, and preemptive threads with optional round robin time-slicing. Includes POSIX pthreads compatible API support.
* Interrupt Services for compile-time registration of interrupt handlers.
* Memory Allocation Services for dynamic allocation and freeing of fixed-size or variable-size memory blocks.
* Inter-thread Synchronization Services for binary semaphores, counting semaphores, and mutex semaphores.
* Inter-thread Data Passing Services for basic message queues, enhanced message queues, and byte streams.
* Power Management Services such as tickless idle and an advanced idling infrastructure.

Other features include:

* Allows an application to incorporate only the capabilities it needs as it needs them, and to specify their quantity and size.
* Implements configurable architecture-specific stack-overflow protection, kernel object and device driver permission tracking, and thread isolation with thread-level memory protection on x86, ARC, and ARM architectures, userspace, and memory domains.
* Allows system resources to be defined at compile-time, which reduces code size and increases performance for resource-limited systems.
* Provides a consistent device model for configuring the drivers that are part of the platform/system and a consistent model for initializing all the drivers configured into the system and Allows the reuse of drivers across platforms that have common devices/IP blocks
* Use of Device Tree (DTS) to describe hardware and configuration information for boards. The DTS information will be used only during compile time. Information about the system is extracted from the compiled DTS and used to create the application image.
* Networking support is fully featured and optimized, including LwM2M and BSD sockets compatible support. OpenThread support (on Nordic chipsets) is also provided - a mesh network designed to securely and reliably connect hundreds of products around the home.
* Bluetooth 5.0 compliant (ESR10) and Bluetooth Low Energy Controller support (LE Link Layer).
* Newtron Flash Filesystem (NFFS) and FATFS Support, FCB (Flash Circular Buffer) for memory constrained applications, and file system enhancements for logging and configuration.
* A multi-instance shell subsystem with user-friendly features such as autocompletion, wildcards, coloring, metakeys (arrows, backspace, ctrl+u, etc.) and history.
* Supports running Zephyr as a Linux application with support for various subsystems and networking.

https://docs.zephyrproject.org/latest/[+https://docs.zephyrproject.org/latest/+]

https://github.com/zephyrproject-rtos/zephyr[+https://github.com/zephyrproject-rtos/zephyr+]

ZeroMQ
~~~~~~

ZeroMQ (also spelled ØMQ, 0MQ or ZMQ) is a high-performance asynchronous messaging library, aimed at use in distributed or concurrent applications. It provides a message queue, but unlike message-oriented middleware, a ZeroMQ system can run without a dedicated message broker. The library's API is designed to resemble that of Berkeley sockets. 

The ZeroMQ API provides sockets (a kind of generalization over the traditional IP and Unix domain sockets), each of which can represent a many-to-many connection between endpoints. Operating with a message-wise granularity, they require that a messaging pattern be used, and are particularly optimized for that kind of pattern.

The basic ZeroMQ patterns are:

* Request-reply - Connects a set of clients to a set of services. This is a remote procedure call and task distribution pattern.
* Publish-subscribe - Connects a set of publishers to a set of subscribers. This is a data distribution pattern.
* Push-pull (pipeline) - Connects nodes in a fan-out / fan-in pattern that can have multiple steps, and loops. This is a parallel task distribution and collection pattern.
* Exclusive pari - Connects two sockets in an exclusive pair. (This is an advanced low-level pattern for specific use cases.)

Each pattern defines a particular network topology. Request-reply defines a so-called "service bus", publish-subscribe defines a "data distribution tree", and push-pull defines "parallelised pipeline". All the patterns are deliberately designed in such a way as to be infinitely scalable and thus usable on Internet scale.[1]

Any message through the socket is treated as an opaque blob of data. Delivery to a subscriber can be automatically filtered by the blob leading string. Available message transports include TCP, PGM (reliable multicast), inter-process communication (IPC) and inter-thread communication (ITC).

The ZeroMQ core library performs very well due to its internal threading model, and can outperform conventional TCP applications in terms of throughput by utilizing an automatic message batching technique.[2][3]

ZeroMQ implements ZMTP, the ZeroMQ Message Transfer Protocol.[4] ZMTP defines rules for backward interoperability, extensible security mechanisms, command and message framing, connection metadata, and other transport-level functionality. A growing number of projects implement ZMTP directly as an alternative to using the full ZeroMQ implementations.

http://zeromq.org/[+http://zeromq.org/+]

https://github.com/zeromq[+https://github.com/zeromq+]

https://github.com/nanomsg/nng[+https://github.com/nanomsg/nng+]

ZMCintegral
~~~~~~~~~~~

ZMCintegral (Numba backened) is an easy to use python package which uses Monte Carlo Evaluation Method to do numerical integrations on Multi-GPU devices. It supports integrations with up to 16 multi-variables, and it is capable of even more than 16 variables if time is not of the priori concern. ZMCintegral usually takes a few minutes to finish the task.

https://github.com/Letianwu/ZMCintegral/[+https://github.com/Letianwu/ZMCintegral/+]

https://arxiv.org/abs/1902.07916[+https://arxiv.org/abs/1902.07916+]

ZOO
~~~

ZOO-Project provides a developer friendly Web Processing Service (WPS) framework for creating and chaining Web Processing Services. A WPS provides web access to functions which run spatial algorithms. ZOO-Project supports many programming languages and comes with three demo applications. The first uses the simple spatialtools services based on the GEOS and OGR libraries, the second showcase Voronoi and Delaunay triangulation (from the CGAL library) and the third present how to interract with OTB applications runninng as WPS services.

The three demontration applications are available from the ZOO-Project demo landing page presented bellow.

ZOO is made of three parts:

* ZOO Kernel : A powerful server-side C Kernel which makes it possible to manage and chain Web services coded in different programming languages.

* ZOO Services : A growing suite of example Web services based on various Open Source libraries.

* ZOO API : A server-side JavaScript API able to call and chain the ZOO Services, which makes the development and chaining processes easier.

ZOO is based on a ‘WPS Service Kernel’ which constitutes the ZOO’s core system (aka ZOO Kernel). The latter is able to load dynamic libraries and to handle them as on-demand Web services. The ZOO Kernel is written in C language, but supports several common programming languages in order to connect to numerous libraries and above all to simplify the Web service end-developer’s job.

A ZOO service is a link composed of a metadata file (.zcfg) and the code for the corresponding implementation. The metadata file describes all the available functions which can be called using a WPS Exec Request, as well as the desired input/output. Services contain the algorithms and functions, and can now be implemented in C/Cxx, Fortran, Java, Python, PHP, Ruby, C# and JavaScript.

http://zoo-project.org/[+http://zoo-project.org/+]

zsync
~~~~~

zsync is a file transfer program. It allows you to download a file from a remote server, where you have a copy of an older version of the file on your computer already. zsync downloads only the new parts of the file. It uses the same algorithm as rsync. However, where rsync is designed for synchronising data from one computer to another within an organisation, zsync is designed for file distribution, with one file on a server to be distributed to thousands of downloaders. zsync requires no special server software â€” just a web server to host the files â€” and imposes no extra load on the server, making it ideal for large scale file distribution.

zsync fills a gap in the technology available for large-scale file distribution. Three key points explain why zsync provides a genuinely new technique for file distribution:

* Client-side rsync â€” zsync uses the rsync algorithm, but runs it on the client side, thus avoiding the high server load associated with rsync.
* Rsync over HTTP â€” zsync provides transfers that are nearly as efficient as rsync -z or cvsup, without the need to run a special server application. All that is needed is an HTTP/1.1-compliant web server. So it works through firewalls and on shared hosting accounts, and gives less security worries.
* Handling for compressed files â€” rsync is ineffective on compressed files, unless they are compressed with a patched version of gzip. zsync has special handling for gzipped files, which enables update transfers of files which are distributed in compressed form.

The special handling of compressed files is, as far as I know, entirely new and unique to zsync. The combination of client-side rsync and HTTP is also unique to zsync, to the best of my knowledge.

http://zsync.moria.org.uk/[+http://zsync.moria.org.uk/+]

zyGrib
~~~~~~

Visualization of weather data from files in GRIB Format 1 and 2. The features include:

* Visualization of meteorologic data from files in GRIB format
* Automatic download of weather and wave forecasts
* Automatic download from IAC (fleetcode) data
* Play animations of 8-day forecasts
* Create your own regional weather maps (worldwide), or view a detailed quantitative forecast for a particular location.
* Plot wind, pressure, temperature, humidity, rain, snow, cloud cover, dew point, wave height, and high altitude data.

http://www.zygrib.org/[+http://www.zygrib.org/+]

Zypper
~~~~~~

Zypper is a command line package manager which makes use of libzypp. Zypper provides functions like repository access, dependency solving, package installation, etc.

YaST2 and RPM MetaData package repositories are supported. Zypper repositories are similar to the ones used in YaST, which also makes use of libzypp. Zypper can also handle repository extensions like patches, patterns, and products. 

The package management features include:

* install/remove packages by name or by capability they provide
* install/remove packages with names
* install/remove specific versions of packages
* install plain RPM files and satisfy dependencies from repositories
* install and remove packages in one go (using +/- or ~/!)
* choose the repository per package by prefixing the name with 'repo_alias:'
* update all installed packages with newer available version where possible
* install patterns/products/patches
* comprehensive installation summary
* avoid installing recommended packages (only required)

The repository management features include:

* easily add/remove/export/import repositories
* use a temporary repository
* restrict operations to specified repositories
* modify multiple repositories at once - select by medium type, localness, or select all

The query features include:

* look for packages by name, or also by summary and description
* look for packages matching substring, glob expressions (wildcards), whole words, exact string
* group search result by package name and type or show all available versions of packages from all repositories
* show detailed information about package/patch/pattern/product
* list all available packages/patches/patterns/products

https://en.opensuse.org/Portal:Zypper[+https://en.opensuse.org/Portal:Zypper+]

https://en.opensuse.org/Portal:Libzypp[+https://en.opensuse.org/Portal:Libzypp+]

https://github.com/openSUSE/zypper[+https://github.com/openSUSE/zypper+]

== NEW SECTION

xref:N0:[0] | xref:NA[A] | xref:NB[B] | xref:NC[C] | xref:ND[D]
| xref:NE[E]
| xref:NF[F]
| xref:NG[G]
| xref:NH[H]
| xref:NI[I]
| xref:NJ[J]
| xref:NK[K]
| xref:NL[L]
| xref:NM[M]
| xref:NN[N]
| xref:NO[O]
| xref:NP[P]
| xref:NQ[Q]
| xref:NR[R]
| xref:NS[S]
| xref:NT[T]
| xref:NU[U]
| xref:NV[V]
| xref:NW[W]
| xref:NX[X]
| xref:NY[Y]
| xref:NZ[Z]


[[N0]]
////
N000
////

[[NA]]
////
NAAA
////

accept
~~~~~~

An approximate compiler for C and `Cxx` programs based on Clang. Think of it as
your assistant in breaking your program in small ways to trade off correctness
for performance.

https://github.com/uwsampa/accept[+https://github.com/uwsampa/accept+]

https://sampa.cs.washington.edu/accept/[+https://sampa.cs.washington.edu/accept/+]

[[ACML]]
ACML
~~~~

AMD Core Math Library, or ACML, provides a free set of thoroughly optimized
and threaded math routines for HPC, scientific, engineering and related
compute-intensive applications. ACML is ideal for weather modeling,
computational fluid dynamics, financial analysis, oil and gas applications and
more. ACML consists of the following main components:

* A full implementation of Level 1, 2 and 3 Basic Linear Algebra Subroutines
(BLAS), with key routines optimized for high performance on AMD Opteron™
processors.  The BLAS level 3 routines will take advantage of heterogeneous
computing through OpenCL if detected.

* A full suite of xref:LAPACK[LAPACK] routines. As well as taking
advantage of the highly-tuned BLAS kernels, a key set of LAPACK routines has
been further optimized to achieve considerably higher performance than
standard LAPACK implementations.

* Beginning version 6 of ACML, a subset of FFTW interfaces are supported for
Fourier transform functionality. Heterogeneous compute with GPU/APU and
OpenCL is supported through the FFTW interfaces. A comprehensive set of FFTs
through ACML specific API (found in version 5 and older) continues to be
available in version 6.

* Random Number Generators in both single- and double-precision.

http://developer.amd.com/tools-and-sdks/cpu-development/amd-core-math-library-acml/[+http://developer.amd.com/tools-and-sdks/cpu-development/amd-core-math-library-acml/+]

Active Papers
~~~~~~~~~~~~~

ActivePapers is a research and development project whose aim is to make
computational science more open and more reliable, by making computational
reproducible and publishable. It is a file format for storing computations.

An ActivePaper is a file combining datasets and programs working on these
datasets in a single package, which also contains a detailed history of which
data was produced when, by running which code, and on which machine. It is a
complete record of the state of a computational research project that can be
shared among collaborators and in the end published as supplementary material
to a journal article.

http://www.activepapers.org/[+http://www.activepapers.org/+]

AD3
~~~

An approximate MAP decoder with Alternating Direction Dual Decomposition.
AD3 (Alternating Directions Dual Decomposition) is an LP-MAP decoder for
undirected constrained factor graphs. In other words, it is an approximate MAP
decoder that retrieves the solution of an LP relaxation of the original
problem. 

The input is a factor graph, which may contain both soft factors, associated
with log-potentials, and hard constraint factors, associated with a logic
function. Factors can be dense, sparse, or combinatorial. Specialized factors
can be implemented by the practitioner.

The output is the LP-MAP assignment, with a posterior value for each variable.
If all variables are integer, the relaxation is tight and the solution is the
true MAP. Otherwise, some entries can be in the unit interval. External tools
can be used to obtain a valid solution using rounding heuristics. Optionally,
a flag can be set that applies a branch-and-bound procedure and retrieves the
true MAP (but it can be slow if the relaxation has many fractional
components).

https://github.com/andre-martins/AD3[+https://github.com/andre-martins/AD3+]

AdH
~~~

Adaptive Hydraulics (AdH) is a modern, multi-dimensional modeling system for
saturated and unsaturated groundwater, overland flow, three-dimensional
Navier-Stokes flow, and two- or three-dimensional shallow water problems.
Developed by the Coastal and Hydraulics Laboratory at the Engineer Research
and Development Center in Vicksburg, MS, the 2-dimensional (2D) shallow water
module of AdH was released to the public in September 2007.

http://adh.usace.army.mil/new_webpage/main/main_page.htm[+http://adh.usace.army.mil/new_webpage/main/main_page.htm+]

AESS
~~~~

The Stochastic Simulation Algorithm (SSA) developed by Gillespie provides a
powerful mechanism for exploring the behavior of chemical systems with small
species populations or with important noise contributions. Gene circuit
simulations for systems biology commonly employ the SSA method, as do
ecological applications. This algorithm tends to be computationally expensive,
so researchers seek an efficient implementation of SSA. In this program
package, the Accelerated Exact Stochastic Simulation Algorithm (AESS) contains
optimized implementations of Gillespieʼs SSA that improve the performance of
individual simulation runs or ensembles of simulations used for sweeping
parameters or to provide statistically significant results.

http://www.sciencedirect.com/science/article/pii/S0010465511002608[+http://www.sciencedirect.com/science/article/pii/S0010465511002608+]

[[Agave]]
Agave
~~~~~

An open source, science-as-a-service API platform for powering your digital lab. Agave allows you to bring together your public, private, and shared high performance computing (HPC), high throughput computing (HTC), Cloud, and Big Data resources under a single, web-friendly REST API.
It enables you to run scientific codes on HPC, HTC or cloud resources, manage your data from
a web interface, and remember how you did it.

http://preview.agaveapi.co/[+http://preview.agaveapi.co/+]

https://bitbucket.org/taccaci[+https://bitbucket.org/taccaci+]

[[agavepy]]
agavepy
^^^^^^^

A simple Python binding for the Agave API.

https://github.com/TACC/agavepy[+https://github.com/TACC/agavepy+]

AMD LibM
~~~~~~~~

AMD LibM is a software library containing a collection of basic math functions
optimized for x86-64 processor based machines. It provides many routines from
the list of standard C99 math functions. AMD LibM is a C library, which users
can link in to their applications to replace compiler-provided math functions.
Generally, programmers access basic math functions through their compiler. But
those who want better accuracy or performance than their compiler’s math
functions can use this library to help improve their applications. Users can
also take advantage of the vector functions in this library. The vector
variants can be used to speed up loops and perform math operations on multiple
elements conveniently.

http://developer.amd.com/tools-and-sdks/cpu-development/libm/[+http://developer.amd.com/tools-and-sdks/cpu-development/libm/+]

amgcl
~~~~~

AMGCL is a `Cxx` header only library for constructing an algebraic multigrid
(AMG) hierarchy. AMG is one the most effective methods for solution of large
sparse unstructured systems of equations, arising, for example, from
discretization of PDEs on unstructured grids [5,6]. The method can be used as
a black-box solver for various computational problems, since it does not
require any information about the underlying geometry. AMG is often used not
as a standalone solver but as a preconditioner within an iterative solver
(e.g. Conjugate Gradients, BiCGStab, or GMRES).

AMGCL builds the AMG hierarchy on a CPU and then transfers it to one of the
provided backends. This allows for transparent acceleration of the solution
phase with help of OpenCL, xref:CUDA:[CUDA], or xref:OpenMP[OpenMP] technologies. Users may provide
their own backends which enables tight integration between AMGCL and the user
code.

https://github.com/ddemidov/amgcl[+https://github.com/ddemidov/amgcl+]

Anaconda Accelerate
~~~~~~~~~~~~~~~~~~~

Accelerate is an add-on to Continuum’s free enterprise Python distribution,
Anaconda. It opens up the full capabilities of your GPU or multi-core
processor to Python. Accelerate includes two packages that can be added to
your Python installation: NumbaPro and MKL Optimizations. MKL Optimizations
makes linear algebra, random number generation, Fourier transforms, and many
other operations run faster and in parallel. NumbaPro builds fast GPU and
multi-core machine code from easy-to-read Python and NumPy code with a
Python-to-GPU compiler. 

If you are an academic at a degree-granting institution, all of these add-ons
are free of charge. Simply click Anaconda Academic License and fill out the
form. If your email address ends in .edu or is in our list of approved
academic institutions, the license will be automatically sent to the provided
email.

https://store.continuum.io/cshop/accelerate/[+https://store.continuum.io/cshop/accelerate/+]

https://store.continuum.io/cshop/academicanaconda[+https://store.continuum.io/cshop/academicanaconda+]

Accelerated Computing with Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Python is one of the fastest growing and most popular programming languages
available. However, as an interpreted language, it has been considered too
slow for high-performance computing.  That has now changed with the release of
the NumbaPro Python compiler from Continuum Analytics.

CUDA Python – Using the NumbaPro Python compiler, which is part of the
Anaconda Accelerate package from Continuum Analytics, you get the best of both
worlds: rapid iterative development and all other benefits of Python combined
with the speed of a compiled language targeting both CPUs and NVIDIA GPUs.

https://developer.nvidia.com/how-to-cuda-python[+https://developer.nvidia.com/how-to-cuda-python+]

Ansible
~~~~~~~

Ansible is a radically simple IT automation platform that makes your
applications and systems easier to deploy. Avoid writing scripts or custom
code to deploy and update your applications— automate in a language that
approaches plain English, using SSH, with no agents to install on remote
systems.

Not only can it be used for automated configuration management, but it also
excels at orchestration, provisioning of systems, zero-time rolling updates
and application deployment. Ansible can be used to keep all your systems
configured exactly the way you want them, and if you have many identical
systems, Ansible will ensure they stay identical. For Linux system
administrators, Ansible is an indispensable tool in implementing and
maintaining a strong security posture. 

Ansible can be used to deploy and configure multiple Linux servers (Red Hat,
Debian, CentOS, OS X, any of the BSDs and others) using secure shell (SSH)
instead of the more common client-server methodologies used by other
configuration management packages.

https://github.com/ansible/ansible[+https://github.com/ansible/ansible+]

http://releases.ansible.com/ansible/[+http://releases.ansible.com/ansible/+]

http://docs.ansible.com/[+http://docs.ansible.com/+]

http://www.linuxjournal.com/content/security-hardening-ansible[+http://www.linuxjournal.com/content/security-hardening-ansible+]

http://www.ansible.com/blog/2013/11/29/ansibles-architecture-beyond-configuration-management[+http://www.ansible.com/blog/2013/11/29/ansibles-architecture-beyond-configuration-management+]

http://lowendbox.com/blog/getting-started-with-ansible/[+http://lowendbox.com/blog/getting-started-with-ansible/+]

os-ansible-deployment
^^^^^^^^^^^^^^^^^^^^^

Ansible playbooks for deploying OpenStack.

https://github.com/stackforge/os-ansible-deployment[+https://github.com/stackforge/os-ansible-deployment+]

Sovereign
^^^^^^^^^

A set of Ansible playbooks to build and maintain your own private cloud:
email, calendar, contacts, file sync, IRC bouncer, VPN, and more.

https://github.com/al3x/sovereign[+https://github.com/al3x/sovereign+]

AnyDSL
~~~~~~

AnyDSL is a framework for the rapid development of domain-specific languages
(DSLs). AnyDSL's main ingredient is AnyDSL's intermediate representation
Thorin. In contrast to other intermediate representations, Thorin features
certain abstractions which allow to maintain domain-specific types and
control-flow.

As creating a front-end for some language is a complex and time-consuming
endeavor, we offer Impala. This is an imperative language which features as a
basis well-known imperative constructs. A DSL developer can hijack Impala such
that desired domain-specific types and constructs are available in Impala
simply by declaring them. The DSL developer just reuses Impala's
infrastructure (lexer, parser, semantic analysis, and code generator). He does
not need to develop his own front-end. Even more important: The decision how
to implement domain-specific details is postponed to the expert of the target
machine. 

https://github.com/AnyDSL[+https://github.com/AnyDSL+]

https://anydsl.github.io/[+https://anydsl.github.io/+]

APP SDK
~~~~~~~

AMD OpenCL™ Accelerated Parallel Processing (APP) technology is a set of
advanced hardware and software technologies that enable AMD graphics
processing cores (GPU), working in concert with the system’s x86 cores (CPU),
to execute heterogeneously to accelerate many applications beyond just
graphics. This enables better balanced platforms capable of running demanding
computing tasks faster than ever, and sets software developers on the path to
optimize for AMD Accelerated Processing Units (APUs). The AMD APP Software
Development Kit (SDK) is a complete development platform created by AMD to
allow you to quickly and easily develop applications accelerated by AMD APP
technology. The SDK provides samples, documentation, and other materials to
quickly get you started leveraging accelerated compute using OpenCL™, Bolt, or
`Cxx` AMP in your C/`Cxx` application, or Aparapi for your Java application.

http://developer.amd.com/tools-and-sdks/opencl-zone/amd-accelerated-parallel-processing-app-sdk/[+http://developer.amd.com/tools-and-sdks/opencl-zone/amd-accelerated-parallel-processing-app-sdk/+]

ARPREC
~~~~~~

This package supports a flexible, arbitrarily high level of numeric precision
-- the equivalent of hundreds or even thousands of decimal digits (up to
approximately ten million digits if needed). Special routines are provided for
extra-high precision (above 1000 digits). The entire library is written in
`Cxx`. High-precision real, integer and complex datatypes are supported. Both
`Cxx` and Fortran-90 translation modules are also provided that permit one to
convert an existing `Cxx` or Fortran-90 program to use the library with only
minor changes to the source code. In most cases only the type statements and
(in the case of Fortran-90 programs) read/write statements need be changed.
Six implementations of PSLQ (one-, two- and three-level, regular and
multi-pair) are included, as well as three high-precision quadrature programs.
New users are encouraged to use this package, rather than MPFUN90 or MPFUN77
(see below).

This verion of the ARPREC package now includes "The Experimental
Mathematician's Toolkit", which is available as the program "mathtool" in the
subdirectory "toolkit". This is a complete interactive high-precision
arithmetic computing environment. One enters expressions in a
Mathematica-style syntax, and the operations are performed using the ARPREC
package, with a level of precision that can be set from 100 to 1000 decimal
digit accuracy. Variables and vector arrays can be defined and referenced.
This program supports all basic arithmetic operations, common transcendental
and combinatorial functions, multi-pair PSLQ (one-, two- or three-level
versions), high-precision quadrature, i.e. numeric integration (Gaussian,
error function or tanh-sinh), and summation of series.

http://crd-legacy.lbl.gov/\~dhbailey/mpdist/[+http://crd-legacy.lbl.gov/~dhbailey/mpdist/+]

ArrayFire
~~~~~~~~~

ArrayFire is a high performance software library for parallel computing with
an easy-to-use API. Its array based function set makes parallel programming
simple.
ArrayFire's multiple backends (xref:CUDA[CUDA], xref:OpenCL[OpenCL] and native CPU) make it platform
independent and highly portable.
A few lines of code in ArrayFire can replace dozens of lines of parallel
computing code, saving you valuable time and lowering development costs.

https://github.com/arrayfire/arrayfire[+https://github.com/arrayfire/arrayfire+]

http://arrayfire.com/[+http://arrayfire.com/+]

ArrayFire.jl
^^^^^^^^^^^^

Julia wrapper for the ArrayFire library.

https://github.com/JuliaComputing/ArrayFire.jl[+https://github.com/JuliaComputing/ArrayFire.jl+]

[[ASCIIDOC]]
ASCIIDOC
~~~~~~~~

AsciiDoc is a text document format for writing notes, documentation, articles,
books, ebooks, slideshows, web pages, man pages and blogs. AsciiDoc files can
be translated to many formats including HTML, PDF, EPUB, man page.
AsciiDoc is highly configurable: both the AsciiDoc source file syntax and the
backend output markups (which can be almost any type of SGML/XML markup) can
be customized and extended by the user.

See also xref:Magic-Book-Project[Magic-Book-Project].

http://www.methods.co.nz/asciidoc/[+http://www.methods.co.nz/asciidoc/+]

http://www.methods.co.nz/asciidoc/plugins.html[+http://www.methods.co.nz/asciidoc/plugins.html+]

AsciidocToGo
^^^^^^^^^^^^

AsciidocToGo is a full featured portable version of asciidoc that contains the
complete toolchain to build html or docbook/latex based PDF documentation out
of plain ascii txt files. Just download AsciidocToGo and start writing instead
of seaching day or maybe weeks to put together all of the the required
software parts.

http://dbcb.github.io/asciidocToGo/[+http://dbcb.github.io/asciidocToGo/+]

asciidoctor-backends
xxxxxxxxxxxxxxxxxxxx

An assortment of backends (i.e., templates) for Asciidoctor, a pure Ruby port
of the AsciiDoc markup language.
In this repository, you’ll find replicas of both the html5 and docbook45
backends from AsciiDoc (and Asciidoctor) written in both Haml and Slim, as
well as backends for generating HTML5 presentations from AsciiDoc.

https://github.com/asciidoctor/asciidoctor-backends[+https://github.com/asciidoctor/asciidoctor-backends+]

asciidoctor-epub3
xxxxxxxxxxxxxxxx+

Asciidoctor EPUB3 is a set of Asciidoctor extensions for converting AsciiDoc
to EPUB3 & KF8/MOBI. 

https://github.com/asciidoctor/asciidoctor-epub3[+https://github.com/asciidoctor/asciidoctor-epub3+]

asciidoctor-gradle-plugin
xxxxxxxxxxxxxxxxxxxxxxxx+

A Gradle plugin that uses Asciidoctor via JRuby to process AsciiDoc source
files within the project. 

https://github.com/asciidoctor/asciidoctor-gradle-plugin[+https://github.com/asciidoctor/asciidoctor-gradle-plugin+]

asciidoctor-latex
xxxxxxxxxxxxxxxx+

Asciidoctor LaTeX is a set of Asciidoctor extensions for converting AsciiDoc
to LaTeX.

https://github.com/asciidoctor/asciidoctor-latex[+https://github.com/asciidoctor/asciidoctor-latex+]

asciidoctor-pdf
xxxxxxxxxxxxxx+

A native PDF renderer for AsciiDoc based on Asciidoctor and
xref:Prawn[Prawn].

https://github.com/asciidoctor/asciidoctor-pdf[+https://github.com/asciidoctor/asciidoctor-pdf+]

asciidoctor.js
^^^^^^^^^^^^^^

JavaScript port of Asciidoctor produced by Opal, a Ruby to JavaScript cross
compiler.

https://github.com/asciidoctor/asciidoctor.js[+https://github.com/asciidoctor/asciidoctor.js+]

DocGist
^^^^^^^

DocGist is a URL proxy tool that converts AsciiDoc documents fetched from
Gists, GitHub repositories, Dropbox folders and other sources to HTML. The
conversion to HTML is performed in the browser (client-side) using the
Asciidoctor.js JavaScript library. DocGist can render documents located
anywhere, as long as the host permits cross-domain access.

http://gist.asciidoctor.org/[+http://gist.asciidoctor.org/+]

Magic-Book-Project
^^^^^^^^^^^^^^^^^^

The Magic Book Project is an open-source framework that facilitates the design
and production of electronic and print books for authors.
Rather than type into a word processor, the Magic Book Project allows an
author to write a book once (using xref:ASCIIDOC[ASCIIDOC], a simple text
document format)
and procedurally generate the layout for a variety of formats using modern
code-based design tools, such as CSS, the stylesheet standard. Write your book
once, press a magic button, and out come multiple versions: printed hardcopy,
digital PDF, HTML, MOBI, and EPUB.

https://github.com/runemadsen/Magic-Book-Project[+https://github.com/runemadsen/Magic-Book-Project+]

MPLW
^^^^

The MPLW is Matplotlib (MPL) wrapper, which can work as AsciiDoc filter. Using
this filter you can generate plots from inline matplotlib scripts.

http://volnitsky.com/project/mplw/[+http://volnitsky.com/project/mplw/+]

rTextDoc
^^^^^^^^

A complete editor for structured text documents with proofreading features.
RTextDoc is designed for typesetting professional research papers using LaTeX
that are heavy on mathematics and images. In addition, it is designed for
writing notes, books, ebooks, slideshows, web pages, man pages and blogs using
AsciiDoc mark-up language. RTextDoc also supports DocBook.

http://jwork.org/rtextdoc/[+http://jwork.org/rtextdoc/+]

when-websocket-met-asciidoctor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Real-time collaborative editor for AsciiDoc file.

https://github.com/mgreau/when-websocket-met-asciidoctor[+https://github.com/mgreau/when-websocket-met-asciidoctor+]

http://wildfly-mgreau.rhcloud.com/ad-editor/[+http://wildfly-mgreau.rhcloud.com/ad-editor/+]

asciinema
~~~~~~~~~

asciinema is a free and open source solution for recording the terminal
sessions and sharing them on the web.
When you run asciinema rec in your terminal the recording starts, capturing
all output that is being printed to your terminal while you're issuing the
shell commands. When the recording finishes (by hitting Ctrl-D or typing exit)
then the captured output is uploaded to asciinema.org website and prepared for
playback on the web.

https://asciinema.org/[+https://asciinema.org/+]

ASP
~~~

All three methods have been implemented in the new MAPLE package ASP
(Automated Symmetry Package) which is an add-on to the MAPLE symmetry package
DESOLVII (Vu, Jefferson and Carminati (2012)  [25]). To our knowledge, this is
the first computer package to automate all three methods of determining
approximate symmetries for differential systems. Extensions to the theory have
also been suggested for the third method and which generalise the first method
to systems of differential equations. Finally, a number of approximate
symmetries and corresponding solutions are compared with results in the
literature.

http://www.sciencedirect.com/science/article/pii/S0010465512003888[+http://www.sciencedirect.com/science/article/pii/S0010465512003888+]

assembler software
~~~~~~~~~~~~~~~~~~

*Software Optimization Resources* - http://www.agner.org/optimize/[+http://www.agner.org/optimize/+]

[[flat_assembler]]
flat assembler
^^^^^^^^^^^^^^

A fast and efficient self-assembling x86 assembler for DOS, Windows and Linux operating systems. Currently it supports x86 and x86-64 instructions sets with MMX, 3DNow!, SSE up to SSE4, AVX, AVX2 and XOP extensions, can produce output in plain binary, MZ, PE, COFF or ELF format. It includes the powerful but easy to use macroinstruction support and does multiple passes to optimize the instruction codes for size. The flat assembler is self-compilable and the complete source code is included.

http://www.flatassembler.net/download.php[+http://www.flatassembler.net/download.php+]

[[HeavyThing]]
HeavyThing
^^^^^^^^^^

A library for working with assembler code.
The HeavyThing library includes automatic profiling support, for both user and library code. 

https://2ton.com.au/HeavyThing/[+https://2ton.com.au/HeavyThing/+]

[[likwid]]
likwid
^^^^^^

A simple to install and use toolsuite of command line applications
for performance oriented programmers. It works for Intel and AMD processors
on the Linux operating system.

https://github.com/rrze-likwid/likwid[+https://github.com/rrze-likwid/likwid+]

[[Yeppp]]
Yeppp
^^^^^

A  high-performance SIMD-optimized mathematical library for x86, ARM, and MIPS processors on Windows, Android, Mac OS X, and GNU/Linux systems.  Yeppp officially supports the C, `Cxx`, C#, Java, and FORTRAN programming languages.

http://www.yeppp.info/[+http://www.yeppp.info/+]

asynchronous communication
~~~~~~~~~~~~~~~~~~~~~~~~~~

Bluelet
^^^^^^^

A simple, pure-Python solution for writing intelligible asynchronous socket
applications. It uses PEP 342 coroutines to make concurrent I/O look and act
like sequential programming.
In this way, it is similar to the Greenlet green-threads library and its
associated packages Eventlet and Gevent. Bluelet has a simpler, 100% Python
implementation that comes at the cost of flexibility and performance when
compared to Greenlet-based solutions. However, it should be sufficient for
many applications that don't need serious scalability; it can be thought of as
a less-horrible alternative to asyncore or an asynchronous replacement for
SocketServer (and more).

https://github.com/sampsyo/bluelet[+https://github.com/sampsyo/bluelet+]

Greenlet
^^^^^^^^

The greenlet package is a spin-off of Stackless, a version of CPython that
supports micro-threads called “tasklets”. Tasklets run pseudo-concurrently
(typically in a single or a few OS-level threads) and are synchronized with
data exchanges on “channels”.

A “greenlet”, on the other hand, is a still more primitive notion of
micro-thread with no implicit scheduling; coroutines, in other words. This is
useful when you want to control exactly when your code runs. You can build
custom scheduled micro-threads on top of greenlet; however, it seems that
greenlets are useful on their own as a way to make advanced control flow
structures. For example, we can recreate generators; the difference with
Python’s own generators is that our generators can call nested functions and
the nested functions can yield values too. 

Greenlets are lightweight coroutines for in-process concurrent programming.

https://pypi.python.org/pypi/greenlet[+https://pypi.python.org/pypi/greenlet+]

https://greenlet.readthedocs.org/en/latest/[+https://greenlet.readthedocs.org/en/latest/+]

Eventlet
xxxxxxxx

A concurrent networking library for Python that allows you to change how you
run your code, not how you write it.

http://eventlet.net/[+http://eventlet.net/+]

Gevent
xxxxxx

A coroutine-based Python networking library that uses greenlet to provide a
high-level synchronous API on top of the libev event loop.
Gevent is inspired by eventlet but features more consistent API, simpler
implementation and better performance. 

http://www.gevent.org/[+http://www.gevent.org/+]

nanomsg
^^^^^^^

A socket library that provides several common communication patterns. It aims
to make the networking layer fast, scalable, and easy to use. Implemented in
C, it works on a wide range of operating systems with no further dependencies.
The communication patterns, also called "scalability protocols", are basic
blocks for building distributed systems. By combining them you can create a
vast array of distributed applications.

Scalability protocols are layered on top of the transport layer in the network
stack. At the moment, the nanomsg library supports INPROC, TCP and IPC.

http://nanomsg.org/[+http://nanomsg.org/+]

http://bravenewgeek.com/a-look-at-nanomsg-and-scalability-protocols/[+http://bravenewgeek.com/a-look-at-nanomsg-and-scalability-protocols/+]

Netty
^^^^^

An asynchronous event-driven network application framework
for rapid development of maintainable high performance protocol servers &
clients. 
Netty is a NIO client server framework which enables quick and easy
development of network applications such as protocol servers and clients. It
greatly simplifies and streamlines network programming such as TCP and UDP
socket server.

'Quick and easy' doesn't mean that a resulting application will suffer from a
maintainability or a performance issue. Netty has been designed carefully with
the experiences earned from the implementation of a lot of protocols such as
FTP, SMTP, HTTP, and various binary and text-based legacy protocols. As a
result, Netty has succeeded to find a way to achieve ease of development,
performance, stability, and flexibility without a compromise.

http://netty.io/index.html[+http://netty.io/index.html+]

RabbitMQ
^^^^^^^^

Robust messaging for applications.

http://www.rabbitmq.com/[+http://www.rabbitmq.com/+]

ZeroMQ
^^^^^^

ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking
library but acts like a concurrency framework. It gives you sockets that carry
atomic messages across various transports like in-process, inter-process, TCP,
and multicast. You can connect sockets N-to-N with patterns like fan-out,
pub-sub, task distribution, and request-reply. It's fast enough to be the
fabric for clustered products. Its asynchronous I/O model gives you scalable
multicore applications, built as asynchronous message-processing tasks. It has
a score of language APIs and runs on most operating systems. 

http://zeromq.org/[+http://zeromq.org/+]

https://github.com/zeromq[+https://github.com/zeromq+]

CurveZMQ
xxxxxxxx

An authentication and encryption protocol for ZeroMQ.

http://curvezmq.org/[+http://curvezmq.org/+]

atom
~~~~

A text editor for the 21st century.

https://atom.io/[+https://atom.io/+]

Authorea
~~~~~~~~

Authorea is the collaborative platform for research. Write and manage your
technical documents in one place.

https://authorea.com/[+https://authorea.com/+]

https://authorea.com/users/3/articles/6316/_show_article[+https://authorea.com/users/3/articles/6316/_show_article+]

[[axasm]]
axasm
~~~~~

A universal cross assembler. The C compiler is used to process your source
along with a set of macro definitions to do code generation for your
specific processor.

https://github.com/wd5gnr/axasm[+https://github.com/wd5gnr/axasm+]

http://hackaday.com/2015/08/06/hacking-a-universal-assembler/[+http://hackaday.com/2015/08/06/hacking-a-universal-assembler/+]

Azimuth Project
~~~~~~~~~~~~~~~

The Azimuth Project is an international collaboration to create a focal point
for scientists and engineers interested in saving the planet. Our goal is to
make clearly presented, accurate information on the relevant issues easy to
find, and to help people work together on our common problems.

http://www.azimuthproject.org/azimuth/show/HomePage[+http://www.azimuthproject.org/azimuth/show/HomePage+]

[[Aztec]]
Aztec
~~~~~

Aztec is a library that provides algorithms for the iterative solution of large sparse linear systems arising in scientific and engineering applications. It is a stand-alone package comprising a set of iterative solvers, preconditioners and matrix-vector multiplication routines. Users are not required to provide their own matrix-vector multiplication routines or preconditioners in order to solve a linear system.

The Aztec library is written in C and is also callable from Fortran. Overall, the package was designed to be portable and easy to use. The user may input the linear system in a simple format and Aztec will perform the necessary transformations for the matrix-vector multiplication and preconditioning. After the transformations, the iterative solvers can run efficiently. If the input matrix is suitably partitioned, the efficiency can be further enhanced.

The major components of Aztec are implementations of iterative solvers (CG, CGS, BiCGSTAB, GMRES and TFQMR) and preconditioners (point Jacobi, block Jacobi, Gauss-Seidel, least-squares polynomials, and overlapping domain decomposition using sparse LU, ILU, ILUT, BILU and ICC within domains). Aztec supports two different sparse matrix notations: a) a point-entry modified sparse row (MSR) format; b) a block-entry variable block row (VBR) format. These two formats have been generalized for parallel implementation and the library includes highly optimized matrix-vector multiply kernels and preconditioners for both types of data structures.

See also xref:AztecOO[AztecOO].

http://www.cs.sandia.gov/CRF/aztec1.html[+http://www.cs.sandia.gov/CRF/aztec1.html+]

http://acts.nersc.gov/aztec/[+http://acts.nersc.gov/aztec/+]

[[NB]]
////
NBBB
////

[[BaCon]]
BaCon
~~~~~

BaCon is a free BASIC to C translator for Unix-based systems, which runs on most Unix/Linux/BSD platforms, including MacOSX. It intends to be a programming aid in creating tools which can be compiled on different platforms (including 64bit environments), while trying to revive the days of the good old BASIC.
BaCon can be described as a translator, a converter, a source-to-source compiler, a transcompiler or a transpiler. It also can be described as a very elaborate preprocessor to C.

BaCon is implemented in generic shell script and in itself. Therefore, to start using Bacon, the target system must have either Korn Shell, or ZShell, or Bourne Again Shell (BASH) available. If none of these shells are available, download and install the free Public Domain Korn Shell which can execute BaCon also. Furthermore, BaCon also works with a newer Kornshell implementation like the MirBSD Korn Shell.
The shell script implementation can convert and compile the BaCon version of BaCon. This will deliver the binary version of BaCon which has an extremely high conversion performance. On newer systems, the average conversion rate usually lies above 10.000 lines per second.
Code converted by BaCon can be compiled by GCC, the Compaq C Compiler, TCC, the clang/LLVM compiler and possibly also by other C compilers.

http://www.basic-converter.org/[+http://www.basic-converter.org/+]

Baudline
~~~~~~~~

Baudline is a time-frequency browser designed for scientific visualization of
the spectral domain.  Signal analysis is performed by Fourier, correlation,
and raster transforms that create colorful spectrograms with vibrant detail.
Conduct test and measurement experiments with the built in function generator,
or play back audio files with a multitude of effects and filters.  The
baudline signal analyzer combines fast digital signal processing, versatile
high speed displays, and continuous capture tools for hunting down and
studying elusive signal characteristics. 

http://www.baudline.com/[+http://www.baudline.com/+]

Beaker
~~~~~~

Beaker is a notebook-style development environment for working interactively
with large and complex datasets. Its plugin-based architecture allows you to
switch between languages or add new ones with ease, ensuring that you always
have the right tool for any of your analysis and visualization needs.

http://beakernotebook.com/[+http://beakernotebook.com/+]

[[Bertini]]
Bertini
~~~~~~~

Bertini is a general-purpose solver, written in C, that was created for
research about polynomial continuation.

See also xref:PHCpack[PHCpack].

http://www3.nd.edu/\~sommese/bertini/[+http://www3.nd.edu/~sommese/bertini/+]

BID Data Project
~~~~~~~~~~~~~~~~

The BID Data Suite is a collection of hardware, software and design patterns
that enable fast, large-scale data mining at very low cost.
The software consists of two parts:

* BIDMat, an interactive matrix library that integrates CPU and GPU
 acceleration and novel computational kernels.
* BIDMach, a machine learning system that includes very efficient model
optimizers and mixing strategies.

BIDMach is an interactive environment designed to make it extremely easy to
build and use machine learning models.
BIDMach includes core classes that take care of managing data sources,
optimization and distributing data over CPUs or GPUs. It’s very easy to write
your own models by generalizing from the models already included in the
Toolkit.

http://bid2.berkeley.edu/bid-data-project/[+http://bid2.berkeley.edu/bid-data-project/+]

https://github.com/BIDData/BIDMat[+https://github.com/BIDData/BIDMat+]

https://github.com/BIDData/BIDMach[+https://github.com/BIDData/BIDMach+]

http://devblogs.nvidia.com/parallelforall/bidmach-machine-learning-limit-gpus/[+http://devblogs.nvidia.com/parallelforall/bidmach-machine-learning-limit-gpus/+]

[[biggus]]
biggus
~~~~~~

This packages enables virtual arrays of arbitrary size, with arithmetic and statistical operations, and conversion to NumPy ndarrays.
Virtual arrays can be stacked to increase their dimensionality, or tiled to increase their extent.
Biggus includes support for easily wrapping data sources which produce NumPy ndarray objects via slicing, e.g. netcdf4python Variable instances, and NumPy ndarray instances.
All operations are performed in a lazy fashion to avoid overloading system resources. Conversion to a concrete NumPy ndarray requires an explicit method call.

https://github.com/SciTools/biggus[+https://github.com/SciTools/biggus+]

http://biggus.readthedocs.org/en/latest/[+http://biggus.readthedocs.org/en/latest/+]

https://pypi.python.org/pypi/Biggus/0.5.0[+https://pypi.python.org/pypi/Biggus/0.5.0+]

https://www.youtube.com/watch?v=2K8_jgiNqUc[+https://www.youtube.com/watch?v=2K8_jgiNqUc+]

BigView
~~~~~~~

BigView allows for interactive panning and zooming of images of arbitrary size
on desktop PCs running Linux. Additionally, it can work in a multi-screen
environment where multiple PCs cooperate to view a single, large image. Using
this software, one can explore — on relatively modest machines — images such
as the Mars Orbiter Camera mosaic [92,160×33,280 pixels].

The images must be first converted into “paged” format, where the image is
stored in 256×256 “pages” to allow rapid movement of pixels into texture
memory. The format contains an “image pyramid”: a set of scaled versions of
the original image. Each scaled image is 1/2 the size of the previous,
starting with the original down to the smallest, which fits into a single
256×256 page.

http://ti.arc.nasa.gov/opensource/projects/bigview/[+http://ti.arc.nasa.gov/opensource/projects/bigview/+]

http://www.techbriefs.com/component/content/article/6-ntb/tech-briefs/software/2355[+http://www.techbriefs.com/component/content/article/6-ntb/tech-briefs/software/2355+]

Binstar
~~~~~~~

A repository for Conda binaries, amongst other things.

https://binstar.org/[+https://binstar.org/+]

https://conda.binstar.org/[+https://conda.binstar.org/+]

Rich Signell's Binstar -
https://binstar.org/rsignell[+https://binstar.org/rsignell+]

BLAS
~~~~

The Basic Linear Algebra Subprograms (BLAS) are a specified set of low-level
subroutines that perform common linear algebra operations such as copying,
vector scaling, vector dot products, linear combinations, and matrix
multiplication. They were first published as a Fortran library in 1979 and
are still used as a building block in higher-level math programming languages
and libraries.

See also xref:ACML[ACML] and xref:OSKI[OSKI].

http://www.netlib.org/blas/[+http://www.netlib.org/blas/+]

ATLAS
^^^^^

Automatically Tuned Linear Algebra Software (ATLAS) is a software library for
linear algebra. It provides a mature open source implementation of BLAS APIs
for C and Fortran77.
ATLAS is often recommended as a way to automatically generate an optimized
BLAS library. While its performance often trails that of specialized libraries
written for one specific hardware platform, it is often the first or even only
optimized BLAS implementation available on new systems and is a large
improvement over the generic BLAS available at Netlib. For this reason, ATLAS
is sometimes used as a performance baseline for comparison with other
products.

This site contains the official reference implementation of BLAS, from
which all others have flowed.  There are Fortran and C versions which should
compile just about anywhere, although they are not optimized for a specific
processor beyond the capabilities of the compiler used.

http://math-atlas.sourceforge.net/[+http://math-atlas.sourceforge.net/+]

http://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software[+http://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software+]

BLIS
^^^^

A software framework for instantiating high-performance BLAS-like dense linear
algebra libraries.
The BLAS-like Library Instantiation Software (BLIS) is a framework for rapid
instantiation of high-performance libraries with Basic Linear Algebra
Subprograms (BLAS) functionality.

https://github.com/flame/blis[+https://github.com/flame/blis+]

http://code.google.com/p/blis/[+http://code.google.com/p/blis/+]

http://www.cs.utexas.edu/users/flame/pubs/BLISTOMSrev2.pdf[+http://www.cs.utexas.edu/users/flame/pubs/BLISTOMSrev2.pdf+]

http://www.cs.utexas.edu/users/flame/pubs/BLIS_TOMS2.pdf[+http://www.cs.utexas.edu/users/flame/pubs/BLIS_TOMS2.pdf+]

http://www.cs.utexas.edu/users/flame/BLISRetreat2014/[+http://www.cs.utexas.edu/users/flame/BLISRetreat2014/+]

Build to Order BLAS
^^^^^^^^^^^^^^^^^^^

The Build to Order BLAS system is a compiler that generates high-performance
implementations of basic linear algebra kernels.

The term BLAS in the name is for Basic Linear Algebra Subprograms. The BLAS is
a standard API for important linear algebra operations. The BLAS are
implemented by most hardware vendors. Traditionally, each routine in the BLAS
is implemented by hand by a highly skilled programmer. The Build to Order BLAS
compiler automates the implementation of not only the BLAS standard but also
any sequence of basic linear algebra operations.

The user of the Build to Order BLAS compiler writes down a specification for a
sequence of matrix and vector operations together with a description of the
input and output parameters. The compiler then tries out many different
choices of how to implement, optimize, and tune those operations for the
user’s computer hardware. The compiler choices the best option, which is
output as a C file containing a function that implements the specified
operations.

https://github.com/nelsonth/btoblas[+https://github.com/nelsonth/btoblas+]

http://ecee.colorado.edu/wpmu/btoblas/[+http://ecee.colorado.edu/wpmu/btoblas/+]

http://arxiv.org/abs/1205.1098[+http://arxiv.org/abs/1205.1098+]

clMath
^^^^^^

clMath is the open-source project for OpenCL based BLAS and FFT libraries.
The complete set of BLAS level 1, 2 & 3 routines is implemented.

http://developer.amd.com/tools-and-sdks/opencl-zone/amd-accelerated-parallel-processing-math-libraries/[+http://developer.amd.com/tools-and-sdks/opencl-zone/amd-accelerated-parallel-processing-math-libraries/+]

cuBLAS
^^^^^^

The NVIDIA CUDA Basic Linear Algebra Subroutines (cuBLAS) library is a
GPU-accelerated version of the complete standard BLAS library that delivers 6x
to 17x faster performance than the latest MKL BLAS. New in CUDA 6.0 is
multi-GPU support in cuBLAS-XT.

https://developer.nvidia.com/cublas[+https://developer.nvidia.com/cublas+]

cuBLAS-XT
xxxxxxxx+

A set of routines which accelerate Level 3 BLAS (Basic Linear Algebra
Subroutine) calls by spreading work across more than one GPU. By using a
streaming design, cuBLAS-XT efficiently manages transfers across the
PCI-Express bus automatically, which allows input and output data to be stored
on the host’s system memory. This provides out-of-core operation – the size of
operand data is only limited by system memory size, not by GPU on-board memory
size.

Starting with CUDA 6.0, a free version of cuBLAS-XT is included in the CUDA
toolkit as part of the cuBLAS library. 

https://developer.nvidia.com/cublasxt[+https://developer.nvidia.com/cublasxt+]

KBLAS
^^^^^

KBLAS (KAUST-BLAS) is a small open-source library that optimizes critical
numerical kernels on CUDA-enabled GPUs. KBLAS provides a subset of standard
BLAS functions. It also proposes some function with BLAS-like interface that
target both single and multi- GPU systems.


The ultimate goal for KBLAS is performance. KBLAS has a set of tuning
parameters that affect its performance according to the GPU architecture, and
the CUDA runtime version. While we cannot guarantee optimal performance with
the default tuning parameters, the user can easily edit such parameters on his
local system. KBLAS might be shipped with autotuners in the future.

http://ecrc.kaust.edu.sa/Pages/Res-kblas.aspx[+http://ecrc.kaust.edu.sa/Pages/Res-kblas.aspx+]

http://arxiv.org/abs/1410.1726[+http://arxiv.org/abs/1410.1726+]

LinAlg
^^^^^^

This `Cxx` class library introduces Matrix, Vector, subMatrices, and LAStreams
over the real domain. The library contains efficient and fool-proof
implementations of level 1 and 2 BLAS (element-wise operations and various
multiplications), transposition, determinant evaluation and matrix inverse.
There are operations on a single row/col/diagonal of a matrix. Distinct
features of the package are Matrix views, Matrix streams, and LazyMatrices.
Lazy construction allows us to write matrix expressions in a natural way
without introducing any hidden temporaries, deep copying, and any reference
counting.

http://okmij.org/ftp/NumMath.html[+http://okmij.org/ftp/NumMath.html+]

OpenBLAS
^^^^^^^^

OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. 

http://www.openblas.net/[+http://www.openblas.net/+]

ulmBLAS
^^^^^^^

A high performance `Cxx` implementation of BLAS (Basic Linear Subprograms).
Standard conforming interfaces for C and Fortran are provided.

http://apfel.mathematik.uni-ulm.de/\~lehn/ulmBLAS/[+http://apfel.mathematik.uni-ulm.de/~lehn/ulmBLAS/+]

[[Blitz]]
Blitz
~~~~~

`Blitzxx` is a `Cxx` class library for scientific computing which provides performance on par with Fortran 77/90. It uses template techniques to achieve high performance. 
It provides dense arrays and vectors, random number generators, and small vectors (useful for representing multicomponent or vector fields).
It uses advanced `Cxx` template metaprogramming techniques, including expression templates, to provide speed-optimized mathematical operations on sequences of data without sacrificing the natural syntax provided by other mathematical programming systems. 

http://sourceforge.net/projects/blitz/[+http://sourceforge.net/projects/blitz/+]

BLOPEX
~~~~~~

Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) is a package,
written in C and MATLAB/OCTAVE, that includes an eigensolver implemented with
the Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG).
Its main features are: a matrix-free iterative method for computing several
extreme eigenpairs of symmetric positive generalized eigenproblems; a
user-defined symmetric positive preconditioner; robustness with respect to
random initial approximations, variable preconditioners, and ill-conditioning
of the stiffness matrix; and apparently optimal convergence speed.

BLOPEX supports parallel MPI-based computations. BLOPEX is incorporated in the
HYPRE package and is available as an external block to the PETSc package.
SLEPc and PHAML have interfaces to call BLOPEX eigensolvers.

http://code.google.com/p/blopex/[+http://code.google.com/p/blopex/+]

Boltons
~~~~~~~

Boltons is a set of over 100 BSD-licensed, pure-Python utilities in the same
spirit as — and yet conspicuously missing from — the standard library,
including:

* Atomic file saving, bolted on with fileutils 
* A highly-optimized OrderedMultiDict, in dictutils
* Two types of PriorityQueue, in queueutils 
* Chunked and windowed iteration, in iterutils 
* A full-featured TracebackInfo type, for representing stack traces, in
tbutils

https://github.com/mahmoud/boltons[+https://github.com/mahmoud/boltons+]

https://boltons.readthedocs.org/en/latest/[+https://boltons.readthedocs.org/en/latest/+]

[[Boost]]
Boost
~~~~~

A set of libraries for the `Cxx` programming language that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing. It contains over eighty individual libraries.

http://www.boost.org/[+http://www.boost.org/+]

https://en.wikipedia.org/wiki/Boost_%28C%2B%2B_libraries%29[+https://en.wikipedia.org/wiki/Boost_%28C%2B%2B_libraries%29+]

Bower
~~~~~

Web sites are made of lots of things — frameworks, libraries, assets,
utilities, and rainbows. Bower manages all these things for you.

Bower works by fetching and installing packages from all over, taking care of
hunting, finding, downloading, and saving the stuff you’re looking for. Bower
keeps track of these packages in a manifest file, bower.json. How you use
packages is up to you. Bower provides hooks to facilitate using packages in
your tools and workflows.

Bower is optimized for the front-end. Bower uses a flat dependency tree,
requiring only one version for each package, reducing page load to a minimum.

http://bower.io/[+http://bower.io/+]a

A very useful thing is the search engine for packages that can be installed
by Bower.

http://bower.io/search/[+http://bower.io/search/+]

BRL-CAD
~~~~~~~

BRL-CAD is a powerful cross-platform open source solid modeling system that
includes interactive geometry editing, high-performance ray-tracing for
rendering and geometric analysis, image and signal-processing tools, a system
performance analysis benchmark suite, libraries for robust geometric
representation, with more than 20 years of active development.

http://brlcad.org/[+http://brlcad.org/+]

broadcasting
~~~~~~~~~~~~

OpenCaster
^^^^^^^^^^

A free and open source MPEG2 transport stream data generator and packet
manipulator. 

http://www.avalpa.com/the-key-values/15-free-software/33-opencaster[+http://www.avalpa.com/the-key-values/15-free-software/33-opencaster+]

Bundler
~~~~~~~

Bundler is a structure-from-motion (SfM) system for unordered image collections (for instance, images from the Internet) written in C and Cxx. An earlier version of this SfM system was used in the Photo Tourism project. For structure-from-motion datasets, please see the BigSFM page.

Bundler takes a set of images, image features, and image matches as input, and produces a 3D reconstruction of camera and (sparse) scene geometry as output. The system reconstructs the scene incrementally, a few images at a time, using a modified version of the Sparse Bundle Adjustment package of Lourakis and Argyros as the underlying optimization engine. Bundler has been successfully run on many Internet photo collections, as well as more structured collections.

The Bundler source distribution also contains potentially userful implementations of several computer vision algorithms, including:

* F-matrix estimation
* Calibrated 5-point relative pose
* Triangulation of multiple rays

Bundler produces sparse point clouds. For denser points, Dr. Yasutaka Furukawa has written a beautiful software package called PMVS2 for running dense multi-view stereo. A typical pipeline is to run Bundler to get camera parameters, use the provided Bundle2PMVS program to convert the results into PMVS2 input, then run PMVS2. You might also be interested in Dr. Furukawa's CMVS view clustering software, which is a helpful preprocess to running PMVS2. 

http://www.cs.cornell.edu/\~snavely/bundler/[+http://www.cs.cornell.edu/~snavely/bundler/+]

https://github.com/snavely/bundler_sfm[+https://github.com/snavely/bundler_sfm+]

http://www.cs.cornell.edu/projects/bigsfm/[+http://www.cs.cornell.edu/projects/bigsfm/+]

bup
~~~

Very efficient backup system based on the git packfile format, providing fast
incremental saves and global deduplication (among and within files, including
virtual machine images).

https://bup.github.io/[+https://bup.github.io/+]

[[NC]]
////
NCCC
////

[[Cactus]]
Cactus
~~~~~~

The Cactus Framework is an open-source, modular, portable programming environment for the collaborative development and deployment of scientific applications using high-performance computing.
The name Cactus comes from the design of a central core ("flesh") which connects to application modules ("thorns") through an extensible interface. Thorns can implement custom developed scientific or engineering applications, such as computational fluid dynamics. Other thorns from a standard computational toolkit provide a range of computational capabilities, such as parallel I/O, data distribution, or checkpointing. 

Cactus runs on many architectures. Applications, developed on standard workstations or laptops, can be seamlessly run on clusters or supercomputers. Cactus provides easy access to many cutting edge software technologies being developed in the academic research community, including the Globus Metacomputing Toolkit, HDF5 parallel file I/O, the PETSc scientific library, adaptive mesh refinement, web interfaces, and advanced visualization tools. 

http://cactuscode.org/[+http://cactuscode.org/+]

*Cactus: Issues for Sustainable Simulation Software* (online article) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.au/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.au/+]

CAD
~~~

[[OpenSCAD]]
OpenSCAD
^^^^^^^^

OpenSCAD is a software for creating solid 3D CAD models. It is free software
and available for Linux/UNIX, Windows and Mac OS X. Unlike most free software
for creating 3D models (such as Blender) it does not focus on the artistic
aspects of 3D modelling but instead on the CAD aspects. Thus it might be the
application you are looking for when you are planning to create 3D models of
machine parts but pretty sure is not what you are looking for when you are
more interested in creating computer-animated movies.

OpenSCAD is not an interactive modeller. Instead it is something like a
3D-compiler that reads in a script file that describes the object and renders
the 3D model from this script file. This gives you (the designer) full control
over the modelling process and enables you to easily change any step in the
modelling process or make designs that are defined by configurable parameters.

OpenSCAD provides two main modelling techniques: First there is constructive
solid geometry (aka CSG) and second there is extrusion of 2D outlines. As data
exchange format format for this 2D outlines Autocad DXF files are used. In
addition to 2D paths for extrusion it is also possible to read design
parameters from DXF files. Besides DXF files OpenSCAD can read and create 3D
models in the STL and OFF file formats.

http://www.openscad.org/[+http://www.openscad.org/+]

Calaos
~~~~~~

Calaos is a free software project (GPLv3) that lets you control and monitor
your home.
You can easily install and use it to transform your home into a smart home. 

https://calaos.fr/en/[+https://calaos.fr/en/+]

Catalyst
~~~~~~~~

It is already common for simulations to discard most of what they compute in
order to minimize time spent on I/O. As we enter the exascale age the problem
of scarce I/O capability continues to grow. Since storing data is no longer
viable for many simulation applications, data analysis and visualization must
now be performed in situ with the simulation to ensure that it is running
smoothly and to fully understand the results that the simulation produces.
Catalyst is a light-weight version of the ParaView server library that is
designed to be directly embedded into parallel simulation codes to perform in
situ analysis at run time.

http://www.paraview.org/in-situ/[+http://www.paraview.org/in-situ/+]

CDI
~~~

A C and Fortran Interface to access Climate and NWP model Data.
Supported data formats are GRIB, netCDF, SERVICE, EXTRA and IEG.

https://code.zmaw.de/projects/cdi[+https://code.zmaw.de/projects/cdi+]

cctools
~~~~~~~

Software  that enables our collaborators to easily harness large scale
distributed systems such as clusters, clouds, and grids. We perform
fundamental computer science research in that enables new discoveries through
computing in fields such as physics, chemistry, bioinformatics, biometrics,
and data mining.  The tools are:

* Parrot - Parrot is a tool for attaching existing programs to remote I/O
systems through the filesystem interface. Parrot "speaks" a variety of
remote I/O services include HTTP, FTP, GridFTP, iRODS, HDFS, XRootD, GROW,
and Chirp on behalf of ordinary programs.

* Chirp - A user-level file system for collaboration across distributed
systems such as clusters, clouds, and grids. Chirp allows ordinary users to
discover, share, and access storage, whether within a single machine room or
over a wide area network. 

* Makeflow - A workflow engine for executing large complex workflows on
clusters, clouds, and grids. Makeflow is very similar to traditional Make,
so if you can write a Makefile, then you can write a Makeflow.

* Work Queue - A framework for building large master-worker applications that
span many computers including clusters, clouds, and grids. Work Queue
applications are written in C, Perl, or Python using a simple API that
allows users to define tasks, submit them to the queue, and wait for
completion. Tasks are executed by a standard worker process that can run on
any available machine. Each worker calls home to the master process,
arranges for data transfer, and executes the tasks. The system handles a
wide variety of failures, allowing for dynamically scalable and robust
applications. 

* SAND - A set of modules for genome assembly that are built atop the Work
* Queue platform for large-scale distributed computation on clusters, clouds,
* or grids.

http://ccl.cse.nd.edu/software/[+http://ccl.cse.nd.edu/software/+]

[[CDO]]
CDO
~~~

A  large tool set for working on climate and NWP model data. xref:NetCDF[NetCDF] 3/4, 
xref:GRIB[GRIB]
1/2 including SZIP and JPEG compression, EXTRA, SERVICE and IEG are supported
as IO-formats. Apart from that CDO can be used to analyse any kind of gridded
data not related to climate science.
CDO has very small memory requirements and can process files larger than the
physical memory.

https://code.zmaw.de/projects/cdo/wiki[+https://code.zmaw.de/projects/cdo/wiki+]

-----
configure --enable-cdi-lib --with-fftw3 --with-jasper=/usr/lib64
--with-libxml2=yes --with-udunits2=/usr/lib64 --with-curl=/usr/lib64
--with-proj=/usr/lib64 --with-netcdf=yes --with-hdf5=yes --with-szlib=yes
--with-threads=yes --with-grib-api=yes

...

configure: CDO is configured with the following options:
{
   "CC"                 : "gcc -std=gnu99",
   "CPP"                : "gcc -E",
   "CPPFLAGS"           : "-I/usr/lib64/include -I/usr/lib64/include -I/usr/lib64/include -I/usr/lib64/include -I/usr/include/libxml2",
   "CFLAGS"             : "-g -O2 -fopenmp ",
   "LDFLAGS"            : "-L/usr/lib64/lib -L/usr/lib64/lib  -L/usr/lib64/lib -L/usr/lib64/lib",
   "LIBS"               : "-lxml2 -ludunits2 -lcurl -lproj -lfftw3 -lgrib_api -ljasper -lnetcdf -lhdf5_hl -lhdf5 -lsz -lz  -lm ",
   "FCFLAGS"            : "",
   "INCLUDES"           : "@INCLUDES@",
   "LD"                 : "/usr/bin/ld -m elf_x86_64",
   "NM"                 : "/usr/bin/nm -B",
   "AR"                 : "ar",
   "AS"                 : "as",
   "DLLTOOL"            : "false",
   "OBJDUMP"            : "objdump",
   "STRIP"              : "strip",
   "RANLIB"             : "ranlib",
   "INSTALL"            : "/usr/bin/install -c",
   "cdi"                : {
     "enable_cdi_lib" : true
   },
  "threads"    : {
    "lib"      : "",
    "include"  : ""
  },
  "zlib"       : {
    "lib"      : " -lz",
  },
  "szlib"      : {
    "lib"      : " -lsz",
    "include"  : ""
  },
  "hdf5"       : {
    "lib"      : " -lhdf5",
    "include"  : ""
  },
  "netcdf"     : {
    "lib"      : " -lnetcdf",
    "include"  : ""
  },
  "udunits2"   : {
    "lib"      : " -L/usr/lib64/lib -ludunits2",
    "include"  : " -I/usr/lib64/include"
  },
  "proj"       : {
    "lib"      : " -L/usr/lib64/lib -lproj",
    "include"  : " -I/usr/lib64/include"
  },
  "USER_NAME"          : "baum",
  "HOST_NAME"          : "max",
  "SYSTEM_TYPE"        : "x86_64-unknown-linux-gnu"
}
configure:
-----

[[CDOpy]]
CDOpy
^^^^^

Cdo{rb,py} allows you to use CDO in the context of Python and Ruby as if it
would be a native library.

https://code.zmaw.de/projects/cdo/wiki/Cdo%7Brbpy%7D[+https://code.zmaw.de/projects/cdo/wiki/Cdo%7Brbpy%7D+]

[[CDSC_Mapper]]
CDSC Mapper
~~~~~~~~~~~

The CDSC Mapper is a compiler package for heterogeneous mapping on various
targets such as multi-core CPUs, GPUs and FPGAs. The objective is to provide
the user with a complete compilation platform to ease the programming of
complex heterogeneous devices, such as a Convey HC1-ex machine. The
architecture of the compiler is based on a collection of production-quality
compilers such as GNU GCC, Nvidia GCC and LLVM; two open-source compilation
infrastructures on top of which development has been performed: the LLNL ROSE
compiler and the LLVM project; and a collection of research compilers and
runtime such as CnC-HC, PolyOpt and SDSLc.

http://cadlab.cs.ucla.edu/mapper/[+http://cadlab.cs.ucla.edu/mapper/+]

CEOP
~~~~

The CEOP Satellite Data Server is actually a gateway with an OPeNDAP front end
and the ability to access data via the OGC WCS protocol on the backend.
Though originally developed for the Coordinated Enhanced Observing Period
(CEOP) effort, it can be used with other WCS servers.  It is implemented as a
plug-in handler to the Hyrax server distributed by OPeNDAP www.opendap.org.

http://opensource.gsfc.nasa.gov/projects/CEOP/index.php[+http://opensource.gsfc.nasa.gov/projects/CEOP/index.php+]

Cetus
~~~~~

Cetus is a compiler infrastructure for the source-to-source transformation of
software programs. It currently supports ANSI C. Since its creation in 2004,
it has grown to over 80,000 lines of Java code, has been made available
publicly on the web, and has become a basis for several research projects. 

http://cetus.ecn.purdue.edu/[+http://cetus.ecn.purdue.edu/+]

CFD Utilities
~~~~~~~~~~~~~

The CFD Utility Software Library (previously known as the Aerodynamics
Division Software Library at NASA Ames Research Center) contains nearly 30
libraries of generalized subroutines and close to 100 applications built upon
those libraries. These utilities have accumulated during four decades or so of
software development in the aerospace field.

All are written in Fortran 90 or FORTRAN 77 with potential reuse in mind. The
only exception is the C translations of a dozen or so numerics routines
grouped as C_utilities.

David Saunders and Robert Kennelly are the primary authors, but miscellaneous
contributions by others are gratefully acknowledged.

See 1-line summaries of the libraries and applications under the Files menu.
Each library folder also contains 1-line summaries of the grouped subroutines,
while each application folder contains READMEs adapted from the main program
headers. NASA permission to upload actual software was granted on Jan. 24,
2014.

http://sourceforge.net/projects/cfdutilities/[+http://sourceforge.net/projects/cfdutilities/+]

http://ti.arc.nasa.gov/opensource/projects/cfdutilities/[+http://ti.arc.nasa.gov/opensource/projects/cfdutilities/+]


[[CFIO]]
CFIO
~~~~

An I/O library for climate models, named CFIO(Climate Fast I/O).
CFIO provides the same interface and feature as xref:PnetCDF[PnetCDF], and adopts an I/O
forwarding technique to provide automatic overlapping of I/O with computing.
CFIO performs better than PnetCDF in terms of decreasing the overall running
time of the program.

This requires xref:MPI[MPI], xref:PnetCDF[PnetCDF] and xref:pthreads[pthreads].

https://github.com/cfio/cfio[+https://github.com/cfio/cfio+]

http://www.geosci-model-dev.net/7/93/2014/gmd-7-93-2014.html[+http://www.geosci-model-dev.net/7/93/2014/gmd-7-93-2014.html+]

Chapel
~~~~~~

An emerging parallel programming language whose design and development are
being led by Cray Inc. in collaboration with academia, computing centers, and
industry. Chapel's goal is to make parallel programming more productive, from
high-end supercomputers to commodity clusters and multicore desktops and
laptops. Chapel is being developed in an open-source manner at SourceForge and
is released under the BSD license.

Chapel supports a multithreaded execution model via high-level abstractions
for data parallelism, task parallelism, concurrency, and nested parallelism.
Chapel's locale type enables users to specify and reason about the placement
of data and tasks on a target architecture in order to tune for locality.
Chapel supports global-view data aggregates with user-defined implementations,
permitting operations on distributed data structures to be expressed in a
natural manner. In contrast to many previous higher-level parallel languages,
Chapel is designed around a multiresolution philosophy, permitting users to
initially write very abstract code and then incrementally add more detail
until they are as close to the machine as their needs require. Chapel supports
code reuse and rapid prototyping via object-oriented design, type inference,
and features for generic programming.

Chapel was designed from first principles rather than by extending an existing
language. It is an imperative block-structured language, designed to be easy
to learn for users of C, `Cxx`, Fortran, Java, Python, Matlab, and other popular
languages. While Chapel builds on concepts and syntax from many previous
languages, its parallel features are most directly influenced by ZPL,
High-Performance Fortran (HPF), and the Cray MTA™/Cray XMT™ extensions to C
and Fortran.

http://chapel.cray.com/[+http://chapel.cray.com/+]

Chapel for Python Programmers -
http://chapel-for-python-programmers.readthedocs.org/[+http://chapel-for-python-programmers.readthedocs.org/+]

http://faculty.knox.edu/dbunde/teaching/chapel/SC12/[+http://faculty.knox.edu/dbunde/teaching/chapel/SC12/+]

http://www4.wittenberg.edu/academics/mathcomp/kburke/chapelTutorial.html[+http://www4.wittenberg.edu/academics/mathcomp/kburke/chapelTutorial.html+]

https://www.ieeetcsc.org/activities/blog/Myths_About_Scalable_Parallel_Programming_Languages_Part_6%3A_Performance_of_Higher-Level_Languages[+https://www.ieeetcsc.org/activities/blog/Myths_About_Scalable_Parallel_Programming_Languages_Part_6%3A_Performance_of_Higher-Level_Languages+]

http://dl.acm.org/citation.cfm?id=2148636&dl=ACM&coll=DL&CFID=239774315&CFTOKEN=87640507[+http://dl.acm.org/citation.cfm?id=2148636&dl=ACM&coll=DL&CFID=239774315&CFTOKEN=87640507+]

Braid
^^^^^

A high-performance language interoperability tool that generates
Babel-compatible bindings for the Chapel programming language. For details on
using the command-line tool, please consult the BRAID man page and the Babel
user's guide.

http://compose-hpc.sourceforge.net/doc/braid/users_guide.html[+http://compose-hpc.sourceforge.net/doc/braid/users_guide.html+]

http://computation.llnl.gov/casc/components/docs/BRAID-README-0.2.2.html[+http://computation.llnl.gov/casc/components/docs/BRAID-README-0.2.2.html+]

PyChapel
^^^^^^^^

This provides interoperability with Chapel in three forms:

* Chapel code inlined in Python
* Chapel code from source-files
* Compile Chapel modules into Python modules

http://pychapel.readthedocs.org/[+http://pychapel.readthedocs.org/+]

https://github.com/chapel-lang/pychapel[+https://github.com/chapel-lang/pychapel+]

http://polaris.cs.uiuc.edu/hpsl/abstracts/a6-lund.pdf[+http://polaris.cs.uiuc.edu/hpsl/abstracts/a6-lund.pdf+]

Chebfun
~~~~~~~

Chebfun is an open-source software system for numerical computing with
functions. The mathematical basis of Chebfun is piecewise polynomial
interpolation implemented with what we call “Chebyshev technology”.
Chebfun has extensive capabilities for dealing with linear and nonlinear
differential and integral operators, and it also includes continuous analogues
of linear algebra notions like QR and singular value decomposition. The
Chebfun2 extension works with functions of two variables defined on a
rectangle in the x-y plane.

http://www.chebfun.org/[+http://www.chebfun.org/+]

pychebfun
^^^^^^^^^

A Python implementation of Chebfun.

https://github.com/olivierverdier/pychebfun[+https://github.com/olivierverdier/pychebfun+]

Cinderella
~~~~~~~~~~

Interactive geometry software.
Besides support for dynamic geometry, Cinderella.2 has many features that
broaden the scope of the program to a wide variety of interaction scenarios.
Compared to the old version of the program, two completely new parts were
added: CindyLab, an environment for doing interactive physical experiments,
and CindyScript, a high-level programming language that allows for fast,
flexible and freely programmable interaction scenarios. Although each of the
three parts of the program (geometry, physical simulation and scripting) can
be used in a standalone manner, the programm unleashes its full power when all
three parts are used in combination. They are designed to interact very
smoothly.

http://www.cinderella.de/tiki-index.php[+http://www.cinderella.de/tiki-index.php+]

CindyJS
^^^^^^^

CindyJS is a framework to create interactive (mathematical) content for the web.
It aims to be compatible with Cinderella, providing an interpreter for the scripting language CindyScript as well as a set of geometric operations which can be used to describe constructions. Together, these components make it very easy to visualize various concepts, from geometry in particular and mathematics in general, but also from various other fields.
 
Freely experiment with masses, springs, charges and fields! Liberated from the constraints of reality, scenarios ranging from atom physics, classical mechanic to planetary orbits may be examined. Effortlessly sketch experiments using the mouse and bring them to life with a simple click. 

CindyJS provides the high-level mathematically oriented user with access to the shader language of the GPU without learning a shader language.

https://cindyjs.org/[+https://cindyjs.org/+]

https://github.com/CindyJS/CindyJS[+https://github.com/CindyJS/CindyJS+]

https://arxiv.org/abs/1808.04579[+https://arxiv.org/abs/1808.04579+]

CKAN
~~~~

CKAN is a powerful data management system that makes data accessible – by
providing tools to streamline publishing, sharing, finding and using data.
CKAN is aimed at data publishers (national and regional governments, companies
and organizations) wanting to make their data open and available.

CKAN is built with Python on the backend and Javascript on the frontend, and
uses the Pylons web framework and SQLAlchemy as its ORM. Its database engine
is PostgreSQL and its search is powered by SOLR. It has a modular architecture
that allows extensions to be developed to provide additional features such as
harvesting or data upload.

CKAN uses its internal model to store metadata about the different records,
and presents it on a web interface that allows users to browse and search this
metadata. It also offers a powerful API that allows third-party applications
and services to be built around it.

http://ckan.org/[+http://ckan.org/+]

https://github.com/ckan/ckan[+https://github.com/ckan/ckan+]

ckanext-spatial
^^^^^^^^^^^^^^^

This extension contains plugins that add geospatial capabilities to CKAN.

https://github.com/ckan/ckanext-spatial[+https://github.com/ckan/ckanext-spatial+]

ClimatePipes
~~~~~~~~~~~~

ClimatePipes uses a web-based application platform due to its widespread
support on mainstream operating systems, ease-of-use, and inherent
collaboration support. The front-end of ClimatePipes uses HTML5 (WebGL, CSS3)
to deliver state-of-the-art visualization and to provide a best-in-class user
experience. The back-end of the ClimatePipes is built using the Visualization
Toolkit (VTK), Climate Data Analysis Tools (CDAT), and other climate and
geospatial data processing tools such as GDAL and PROJ4.

http://www.kitware.com/source/home/post/114[+http://www.kitware.com/source/home/post/114+]

CLFORTRAN
~~~~~~~~~

CLFORTRAN is an open source (LGPL) Fortran module, designed to provide direct
access to GPU, CPU and accelerator based computing resources available by the
OpenCL standard.

http://www.cass-hpc.com/solutions/libraries/clfortran-pure-fortran-interface-to-opencl[+http://www.cass-hpc.com/solutions/libraries/clfortran-pure-fortran-interface-to-opencl+]

clIMAGMA
~~~~~~~~

clMAGMA is an OpenCL port of MAGMA. It supports AMD GPUs. The clMAGMA library
dependancies, in particular optimized GPU OpenCL BLAS and CPU optimized BLAS
and xref:LAPACK[LAPACK] for AMD hardware, can be found in the AMD Accelerated Parallel
Processing Math Libraries (APPML).

http://icl.cs.utk.edu/magma/software/view.html?id=190[+http://icl.cs.utk.edu/magma/software/view.html?id=190+]

Clojure
~~~~~~~

Clojure is a dynamic programming language that targets the Java Virtual
Machine (and the CLR, and JavaScript). It is designed to be a general-purpose
language, combining the approachability and interactive development of a
scripting language with an efficient and robust infrastructure for
multithreaded programming. Clojure is a compiled language - it compiles
directly to JVM bytecode, yet remains completely dynamic. Every feature
supported by Clojure is supported at runtime. Clojure provides easy access to
the Java frameworks, with optional type hints and type inference, to ensure
that calls to Java can avoid reflection.

Clojure is a dialect of Lisp, and shares with Lisp the code-as-data philosophy
and a powerful macro system. Clojure is predominantly a functional programming
language, and features a rich set of immutable, persistent data structures.
When mutable state is needed, Clojure offers a software transactional memory
system and reactive Agent system that ensure clean, correct, multithreaded
designs.

http://clojure.org/[+http://clojure.org/+]

http://en.wikipedia.org/wiki/Clojure[+http://en.wikipedia.org/wiki/Clojure+]

*Modern ClojureScript* - https://github.com/magomimmo/modern-cljs[+https://github.com/magomimmo/modern-cljs+]

ClojureScript
^^^^^^^^^^^^^

ClojureScript is a new compiler for Clojure that targets JavaScript. It is
designed to emit JavaScript code which is compatible with the advanced
compilation mode of the Google Closure optimizing compiler.

https://github.com/clojure/clojurescript[+https://github.com/clojure/clojurescript+]

Immutant
^^^^^^^^

Immutant is an integrated suite of Clojure libraries. It represents an attempt
to reduce the incidental complexity inherent in non-trivial applications. The
services backed by the libraries include Undertow for web, HornetQ for
messaging, Infinispan for caching, Narayana for transactions, and Quartz for
scheduling.

http://immutant.org/[+http://immutant.org/+]

Leiningen
^^^^^^^^^

Leiningen is the easiest way to use Clojure. With a focus on project
automation and declarative configuration, it gets out of your way and lets you
focus on your code.

http://leiningen.org/[+http://leiningen.org/+]

[[Onyx]]
Onyx
^^^^

Distributed, masterless, high performance, fault tolerant data processing for Clojure.

https://github.com/onyx-platform/onyx[+https://github.com/onyx-platform/onyx+]

[[Quil]]
Quil
^^^^

In one hand Quil holds Processing, a carefully crafted API for making drawing and animation extremely easy to get your biscuit-loving chops around. In the other she clutches Clojure, an interlocking suite of exquisite language abstractions forged by an army of hammocks and delicately wrapped in flowing silky parens of un-braided joy.

https://github.com/quil/quil[+https://github.com/quil/quil+]

cluster management
~~~~~~~~~~~~~~~~~~

Circuit
^^^^^^^

Self-managed infrastructure, programmatic monitoring and orchestration.
The circuit is a minimal distributed operating system that enables
programmatic, reactive control over hosts, processes and connections within a
compute cluster. 

http://gocircuit.github.io/circuit/[+http://gocircuit.github.io/circuit/+]

CLyther
~~~~~~~

CLyther is a Python tool similar to Cython and PyPy. CLyther is a just-in-time
specialization engine for OpenCL. The main entry points for CLyther are its
clyther.task and clyther.kernel decorators. Once a function is decorated with
one of these the function will be compiled to OpenCL when called.

CLyther is a Python language extension that makes writing OpenCL code as easy
as Python itself. CLyther currently only supports a subset of the Python
language definition but adds many new features to OpenCL.

CLyther exposes both the OpenCL C library as well as the OpenCL language to
python.

http://srossross.github.io/Clyther/[+http://srossross.github.io/Clyther/+]

CLUTO
~~~~~

CLUTO is a software package for clustering low- and high-dimensional datasets
and for analyzing the characteristics of the various clusters. CLUTO is
well-suited for clustering data sets arising in many diverse application areas
including information retrieval, customer purchasing transactions, web, GIS,
science, and biology.

CLUTO's distribution consists of both stand-alone programs and a library via
which an application program can access directly the various clustering and
analysis algorithms implemented in CLUTO. 

http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview[+http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview+]

gCLUTO
^^^^^^

gCLUTO is a cross-platform graphical application for clustering low- and
high-dimensional datasets and for analyzing the characteristics of the various
clusters. gCLUTO is build on-top of the CLUTO clustering library. 

http://glaros.dtc.umn.edu/gkhome/cluto/gcluto/overview[+http://glaros.dtc.umn.edu/gkhome/cluto/gcluto/overview+]

wCLUTO
^^^^^^

wCLUTO is a web-enabled data clustering application that is designed for the
clustering and data-analysis requirements of gene-expression analysis. wCLUTO
is also built on top of the CLUTO clustering library. Users can upload their
datasets, select from a number of clustering methods, perform the analysis on
the server, and visualize the final results. 

http://glaros.dtc.umn.edu/gkhome/cluto/wcluto/overview[+http://glaros.dtc.umn.edu/gkhome/cluto/wcluto/overview+]

CnC
~~~

Intel Concurrent Collections for `Cxx` is a `Cxx` template library for letting
`Cxx` programmers implement CnC applications which run in parallel on shared and
distributed memory. 

CnC makes it easy to write `Cxx` programs which take full advantage of the
available parallelism. Whether run on multicore systems, Xeon Phi™ or clusters
CnC will seamlessly exploit the performance potential of your hardware.
Through its portabilty and composability (with itself and other tools) it
provides future-proof scalability. 

https://icnc.github.io/[+https://icnc.github.io/+]

https://software.intel.com/en-us/articles/intel-concurrent-collections-for-cc[+https://software.intel.com/en-us/articles/intel-concurrent-collections-for-cc+]

CnC-Python
^^^^^^^^^^

The CnC-Python system under development in the Habanero project at Rice
University builds on past work on the Intel Concurrent Collections (CnC) and
Habanero CnC projects.

https://wiki.rice.edu/confluence/display/HABANERO/CnC-Python[+https://wiki.rice.edu/confluence/display/HABANERO/CnC-Python+]

https://wiki.rice.edu/confluence/display/HABANERO/Habanero+CnC-Python+Download[+https://wiki.rice.edu/confluence/display/HABANERO/Habanero+CnC-Python+Download+]

https://www.usenix.org/conference/hotpar12/cnc-python-multicore-programming-high-productivity[+https://www.usenix.org/conference/hotpar12/cnc-python-multicore-programming-high-productivity+]

COCO
~~~~

COCO (COmparing Continuous Optimisers) is a platform for systematic and sound
comparisons of real-parameter global optimisers. COCO provides benchmark
function testbeds and tools for processing and visualizing data generated by
one or several optimizers. The COCO platform has been used for the
Black-Box-Optimization-Benchmarking (BBOB) workshops that took place during
the GECCO conference in 2009, 2010, 2012, and 2013.

http://coco.gforge.inria.fr/doku.php[+http://coco.gforge.inria.fr/doku.php+]

[[Code:Blocks]]
Code::Blocks
~~~~~~~~~~~~

Code::Blocks is a free C, `Cxx` and Fortran IDE built to meet the most demanding
needs of its users. It is designed to be very extensible and fully
configurable.
An IDE with all the features you need, having a consistent look, feel
and operation across platforms.
Built around a plugin framework, Code::Blocks can be extended with plugins.
Any kind of functionality can be added by installing/coding a plugin. For
instance, compiling and debugging functionality is already provided by
plugins.

http://www.codeblocks.org/[+http://www.codeblocks.org/+]

http://openframeworks.cc/setup/linux-codeblocks/[+http://openframeworks.cc/setup/linux-codeblocks/+]

Code_Saturne
~~~~~~~~~~~~

Code_Saturne solves the Navier-Stokes equations for 2D, 2D-axisymmetric and 3D
flows, steady or unsteady, laminar or turbulent, incompressible or weakly
dilatable, isothermal or not, with scalars transport if required.

It is an open-source CFD software package relying on the finite volume method to simulate the Navier-Stokes equations. It can handle any type of mesh built with any cell/grid structure. Incompressible and compressible flows can be simulated, with or without heat transfer, and a wide range of turbulence models is also available. The solver uses a segregated approach and the velocity-pressure coupling is handled using a projection-like method. The default algorithm to compute the velocity is the Jacobi algorithm and the pressure is solved with the help of an algebraic multigrid (AMG) algorithm.

Parallelism is handled by distributing the domain over MPI processes, with an optional second level of shared memory parallelism based on the OpenMP model. Several partitioning tools are available, i.e. geometry-based (Morton or Hilbert Space Filling Curve) or graph-based (METIS, ParMETIS, SCOTCH and PT-SCOTCH).

Code_Saturne can be used as a standalone package, but extra libraries may also be plugged in, as to read some of the supported mesh formats (CGNS, MED, CCM, for instance), to get access to graph partitioners (METIS, ParMETIS, SCOTCH, PT-SCOTCH) or to additional sets of linear solvers (PETSc, for instance).
MPI-IO is used for input and optional output of potential checkpointing files and meshes and for output of postprocessing files, when using the EnSight Gold format, (also readable by ParaView).

Several turbulence models are available, from Reynolds-Averaged models to
Large-Eddy Simulation models. In addition, a number of specific physical
models are also available as "modules": gas, coal and heavy-fuel oil
combustion, semi-transparent radiative transfer, particle-tracking with
Lagrangian modeling, Joule effect, electrics arcs, weakly compressible flows,
atmospheric flows, rotor/stator interaction for hydraulic machines.

http://code-saturne.org/cms/[+http://code-saturne.org/cms/+]

COFFEE
~~~~~~

The numerical solution of partial differential equations using the finite
element method is one of the key applications of high performance computing.
Local assembly is its characteristic operation. This entails the execution of
a problem-specific kernel to numerically evaluate an integral for each element
in the discretized problem domain. Since the domain size can be huge,
executing efficient kernels is fundamental. Their op- timization is, however,
a challenging issue. Even though affine loop nests are generally present, the
short trip counts and the complexity of mathematical expressions make it hard
to determine a single or unique sequence of successful transformations.
Therefore, we present the design and systematic evaluation of COF- FEE, a
domain-specific compiler for local assembly kernels. COFFEE manipulates
abstract syntax trees generated from a high-level domain-specific language for
PDEs by introducing domain-aware composable optimizations aimed at improving
instruction-level parallelism, especially SIMD vectorization, and register
locality. It then generates C code including vector intrinsics.

http://arxiv.org/abs/1407.0904[+http://arxiv.org/abs/1407.0904+]

https://github.com/coneoproject/COFFEE[+https://github.com/coneoproject/COFFEE+]

Combi
~~~~~

A Pythonic package for combinatorics.
Combi lets you explore spaces of permutations and combinations as if they were
Python sequences, but without generating all the permutations/combinations in
advance. It lets you specify a lot of special conditions on these spaces. It
also provides a few more classes that might be useful in combinatorics
programming.

https://github.com/cool-RR/combi[+https://github.com/cool-RR/combi+]

command-line utilities
~~~~~~~~~~~~~~~~~~~~~~

*9 Linux Commands To Be Known To Secure Your Linux From Danger* - https://www.latesthackingnews.com/9-linux-commands-known-secure-linux-danger/[+https://www.latesthackingnews.com/9-linux-commands-known-secure-linux-danger/+]

*Cool Games You Can Play From Your Linux Command Line*  - https://www.latesthackingnews.com/cool-games-you-can-play-from-your-linux-command-line/[+https://www.latesthackingnews.com/cool-games-you-can-play-from-your-linux-command-line/+]

[[fzf]]
fzf
^^^

A command-line fuzzy finder written in Go.

https://github.com/junegunn/fzf[+https://github.com/junegunn/fzf+]

[[fzf-fs]]
fzf-fs
xxxxxx

A cts like a very simple and configurable file browser/navigator for the command line by taking advantage of the general-purpose fuzzy finder fzf. Although coming without Miller columns, fzf-fs is inspired by tools like lscd and deer, which both follow the example set by ranger.

https://github.com/D630/fzf-fs[+https://github.com/D630/fzf-fs+]

compressive sampling
~~~~~~~~~~~~~~~~~~~~

Compressive sampling is a signal processing technique for efficiently
acquiring and reconstructing a signal, by finding solutions to underdetermined
linear systems.
This is based on the principle that, through optimization, the sparsity of a
signal can be exploited to recover it from far fewer samples than required by
the Shannon-Nyquist sampling theorem. There are two conditions under which
recovery is possible.[1] The first one is sparsity which requires the signal
to be sparse in some domain. The second one is incoherence which is applied
through the isometric property which is sufficient for sparse signals.

http://en.wikipedia.org/wiki/Compressed_sensing[+http://en.wikipedia.org/wiki/Compressed_sensing+]

https://sites.google.com/site/igorcarron2/cs[+https://sites.google.com/site/igorcarron2/cs+]

http://nuit-blanche.blogspot.com/search/label/CS[+http://nuit-blanche.blogspot.com/search/label/CS+]

http://perso.uclouvain.be/laurent.jacques/uploads/Main/cschap.pdf[+http://perso.uclouvain.be/laurent.jacques/uploads/Main/cschap.pdf+]

NESTA
^^^^^

A fast and robust first-order method than solves basis-pursuit problems and a
large number of extensions (including tv-denoising).

http://statweb.stanford.edu/\~candes/nesta/[+http://statweb.stanford.edu/~candes/nesta/+]

http://statweb.stanford.edu/\~candes/nesta/NESTA.pdf[+http://statweb.stanford.edu/~candes/nesta/NESTA.pdf+]

TFOCS
^^^^^

A set of Matlab templates, or building blocks, that can be used to construct
efficient, customized solvers for a variety of convex models, including in
particular those employed in sparse recovery applications.

http://cvxr.com/tfocs/[+http://cvxr.com/tfocs/+]

ConicBundle
~~~~~~~~~~~

ConicBundle is a callable library for C/`Cxx` that implements a bundle method
for minimizing the sum of convex functions that are given by first order
oracles or arise from Lagrangean relaxation of particular conic linear
programs.

https://www-user.tu-chemnitz.de/\~helmberg/ConicBundle/[+https://www-user.tu-chemnitz.de/~helmberg/ConicBundle/+]

[[containers]]
containers
~~~~~~~~~~

A container (Linux Container) at its core is an allocation, portioning, and assignment of host (compute) resources such as CPU Shares, Network I/O, Bandwidth, Block I/O, and Memory (RAM) so that kernel level constructs may jail-off, isolate or “contain” these protected resources so that specific running services (processes) and namespaces may solely utilize them without interfering with the rest of the system. These processes could be lightweight Linux hosts based on a Linux image, multiple web severs and applications, a single subsystem like a database backend, to a single process such as ‘echo “Hello”’ with little to no overhead.

 Commonly known as “operating system-level virtualization” or “OS Virtual Environments” containers differ from hypervisor level virtualization. The main difference is that the container model eliminates the hypervisor layer, redundant OS kernels, binaries, and libraries needed to typically run workloads in a VM.

http://aucouranton.com/2014/06/13/linux-containers-parallels-lxc-openvz-docker-and-more/[+http://aucouranton.com/2014/06/13/linux-containers-parallels-lxc-openvz-docker-and-more/+]

[[Kubernetes]]
Kubernetes
^^^^^^^^^^

An open source system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications.

https://github.com/googlecloudplatform/kubernetes[+https://github.com/googlecloudplatform/kubernetes+]

[[Context_Free_Art]]
Context Free Art
~~~~~~~~~~~~~~~~

Context Free is a program that generates images from written instructions
called a grammar. The program follows the instructions in a few seconds to
create images that can contain millions of shapes.
Chris Coyne created a small language for design grammars called CFDG. These
grammars are sets of non-deterministic rules to produce images. The images are
surprisingly beautiful, often from very simple grammars.
Context Free is a full graphical environment for editing, rendering, and
exploring CFDG design grammars. 

See also xref:Structure_Synth[Structure Synth].

http://contextfreeart.org/index.html[+http://contextfreeart.org/index.html+]

coordinates
~~~~~~~~~~~

A Fortran 90 library that provides functions to manage grids and aribirary
sets of points, including interpolation and mapping between different
coordinate systems.

https://github.com/alex-robinson/coordinates[+https://github.com/alex-robinson/coordinates+]

CRPtoolbox
~~~~~~~~~~

The toolbox contains MATLAB® routines for computing recurrence plots and
related problems.

http://tocsy.pik-potsdam.de/CRPtoolbox/[+http://tocsy.pik-potsdam.de/CRPtoolbox/+]

http://www.recurrence-plot.tk/[+http://www.recurrence-plot.tk/+]

[[CSS]]
CSS
~~~

Cascading Style Sheets (CSS) is a style sheet language used for describing the look and formatting of a document written in a markup language. Although most often used to change the style of web pages and user interfaces written in HTML and XHTML, the language can be applied to any kind of XML document, including plain XML, SVG and XUL. Along with HTML and JavaScript, CSS is a cornerstone technology used by most websites to create visually engaging webpages, user interfaces for web applications, and user interfaces for many mobile applications.

https://en.wikipedia.org/wiki/Cascading_Style_Sheets[+https://en.wikipedia.org/wiki/Cascading_Style_Sheets+]

Basscss
^^^^^^^

Basscss is a lightweight collection of base element styles, immutable
utilities, layout modules, and color styles designed for speed, clarity,
performance, and scalability. 

http://www.basscss.com/[+http://www.basscss.com/+]

[[cTuning]]
cTuning
~~~~~~~

A  free, open source compiler collection that combines multiple tools and techniques including MILEPOST GCC, ICI, CCC framework, cTuning web-services and Collective Optimization Database and cBench as the first practical step toward self-tuning, adaptive computing systems based on industrial tools, empirical techniques, transparent collective optimization, statistical analysis and machine learning. cTuning CC is a wrapper around any compiler such as GCC, LLVM, Open64, Path64, etc that can transparently invoke machine learning mode to correlate program features of a compiled program with the ones stored in the Collective Optimization Database and suggest better optimizations for multi-objective criteria such as improving execution time, compilation time, code size, etc (using optimization space frontier detection).

http://ctuning.org/wiki/index.php/CTools:CTuningCC[+http://ctuning.org/wiki/index.php/CTools:CTuningCC+]

Cubica
~~~~~~

Cubica is a toolkit for efficient finite element simulations of deformable
bodies containing both geometric and material non-linearities. Its main
feature is its use of subspace methods, also known as dimensional model
reduction or reduced order methods, which can accelerate simulations by
several orders of magnitude.

http://www.mat.ucsb.edu/\~kim/cubica/[+http://www.mat.ucsb.edu/~kim/cubica/+]

[[CUDA]]
CUDA
~~~~

CUDA (after the Plymouth Barracuda[1]), which stands for Compute Unified
Device Architecture, is a parallel computing platform and programming model
created by NVIDIA and implemented by the graphics processing units (GPUs) that
they produce.[2] CUDA gives developers direct access to the virtual
instruction set and memory of the parallel computational elements in CUDA
GPUs.

Using CUDA, the GPUs can be used for general purpose processing (i.e., not
exclusively graphics); this approach is known as GPGPU. Unlike CPUs, however,
GPUs have a parallel throughput architecture that emphasizes executing many
concurrent threads slowly, rather than executing a single thread very quickly.

http://en.wikipedia.org/wiki/CUDA[+http://en.wikipedia.org/wiki/CUDA+]

http://www.nvidia.com/object/tesla_software.html[+http://www.nvidia.com/object/tesla_software.html+]

*negativo17 RPM Repository for NVIDIA Drivers* - http://negativo17.org/nvidia-driver/[+http://negativo17.org/nvidia-driver/+]

CUB
^^^

CUB provides state-of-the-art, reusable software components for every layer of
the CUDA programming model.

http://nvlabs.github.io/cub/[+http://nvlabs.github.io/cub/+]

[[cuda4py]]
cuda4py
^^^^^^^

xref:CUDA[CUDA] cffi bindings and helper classes for Python.

https://github.com/Samsung/cuda4py[+https://github.com/Samsung/cuda4py+]

FLAGON
^^^^^^

A way to program NVIDIA graphical processors, from Fortran-9X and eventually
Matlab. 

http://flagon.sourceforge.net/[+http://flagon.sourceforge.net/+]

http://sourceforge.net/projects/flagon/[+http://sourceforge.net/projects/flagon/+]

FortCUDA
^^^^^^^^

The FortCUDA project seeks to generate CUDA bindings in F95/2003 using the
Fortran 2003 ISO_C_BINDINGS module. It is intended to give near native call
syntax to the CUDA SDK in Fortran 2003. Currently, most Fortran compilers are
supporting the ISO_C_BINDINGS module. 

The FortCUDA project mostly consists of a very basic module that contains
appropriate bindings for most of the CUDA function calls found in cuda.h and
cuda_runtime.h. This is not necessarily complete, but is quite comprehensive.
The file parsing capability is provided as part of the distribution and has
been used to generate wrappers around functions from a few projects. Specific
to CUDA, wrappers have been generated for cuda.h and cuda_runtime.h and are
included in the FortCUDA library. In addition Fortran modules have been
generated for cublas.h and cufft.h, but are still being checked for accuracy.
Theoretically the FortCUDA_File_Parser.py script can be used on any C header
file, and, providing our grammars are adequate, will produce a 90% solution to
wrapping the enums, strucs, and functions. 

http://fortcuda.sourceforge.net/[+http://fortcuda.sourceforge.net/+]

Gunrock
^^^^^^^

Gunrock is a CUDA library for graph primitives that refactors, integrates, and
generalizes best-of-class GPU implementations of breadth-first search,
connected components, and betweenness centrality into a unified code base
useful for future development of high-performance GPU graph primitives.

http://gunrock.github.io/gunrock/[+http://gunrock.github.io/gunrock/+]

https://github.com/gunrock/gunrock[+https://github.com/gunrock/gunrock+]

Hemi
^^^^

CUDA C/`Cxx` and the NVIDIA NVCC compiler toolchain support a number of features
designed to make it easier to write portable code, including language
integration of host and device code and data, declaration specifiers (e.g.
__host__ and __device__) and preprocessor definitions (__CUDACC__). These
features combine to enable developers to write code that can be compiled and
run on either the host, the device, or both. Other compilers don't recognize
these features, however, so to really write portable code, we need
preprocessor macros. This is where Hemi comes in.

https://github.com/harrism/hemi[+https://github.com/harrism/hemi+]

http://devblogs.nvidia.com/parallelforall/developing-portable-cuda-cc-code-hemi/[+http://devblogs.nvidia.com/parallelforall/developing-portable-cuda-cc-code-hemi/+]

Modern GPU
^^^^^^^^^^

Modern GPU is code and commentary intended to promote new and productive ways
of thinking about GPU computing.
is project is a library, an algorithms book, a tutorial, and a best-practices
guide.

https://github.com/NVlabs/moderngpu[+https://github.com/NVlabs/moderngpu+]

http://nvlabs.github.io/moderngpu/[+http://nvlabs.github.io/moderngpu/+]

[[rCUDA]]
rCUDA
^^^^^

The  rCUDA framework is the most modern remote GPU virtualization solution today.
It enables the concurrent remote usage of CUDA-enabled devices in a transparent way. Thus, the source code of applications does not need to be modified in order to use remote GPUs but rCUDA takes care of all the necessary details. Furthermore, the overhead introduced by using a remote GPU is very small.

rCUDA provides full compatibility support with CUDA. It implements all of the functions in the CUDA Runtime API and Driver API, excluding only those related with graphics interoperability. It additionally includes highly optimized TCP and low-level InfiniBand pipelined communications as well as full multi-thread and multi-node capabilities. rCUDA targets the same Linux OS distributions as CUDA does, providing also support for x86 and ARM processor architectures. Furthermore, an integration of rCUDA with the SLURM scheduler has been developed, allowing your scheduled jobs to use remote GPUs. The combination of SLURM + rCUDA provides reductions in overall execution times of job batches between 25% and 45%, depending on the exact composition of the job batch.

It has been successfully tested with several applications including
LAMMPS, WideLM, CUDASWxx,
xref:HOOMD-blue[HOOMD-blue], mCUDA-MEME, GPU-Blast, Gromacs, GAMESS, DL-POLY, and HPL.

cuDNN
~~~~~

NVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural
networks. It emphasizes performance, ease-of-use, and low memory overhead.
NVIDIA cuDNN is designed to be integrated into higher-level machine learning
frameworks, such as UC Berkeley’s popular Caffe software. The simple, drop-in
design allows developers to focus on designing and implementing neural net
models rather than tuning for performance, while still achieving the high
performance modern parallel computing hardware affords.

https://developer.nvidia.com/cuDNN[+https://developer.nvidia.com/cuDNN+]

http://devblogs.nvidia.com/parallelforall/embedded-machine-learning-cudnn-deep-neural-network-library-jetson-tk1/[+http://devblogs.nvidia.com/parallelforall/embedded-machine-learning-cudnn-deep-neural-network-library-jetson-tk1/+]

cuwire
~~~~~~

A better microcontroller IDE.

http://apla.github.io/cuwire/[+http://apla.github.io/cuwire/+]

[[ND]]
////
NDDD
////

[[Dakota]]
Dakota
~~~~~~

A toolkit providing a flexible, extensible interface between analysis codes and iterative systems analysis methods.  Dakota contains algorithms for
optimization with gradient and nongradient-based methods; uncertainty quantification with sampling, reliability, stochastic expansion, and epistemic methods; parameter estimation with nonlinear least squares methods; and sensitivity/variance analysis with design of experiments and parameter study methods.

Computational methods developed in structural mechanics, heat transfer, fluid mechanics, shock physics, and many other fields of engineering can be an enormous aid to understanding the complex physical systems they simulate. Often, it is desired to use these simulations as virtual prototypes to obtain an acceptable or optimized design for a particular system. Dakota seeks to enhance the utility of these computational methods by enabling their use as design tools, so that simulations may be used not just for single-point predictions, but also for automated determination of system performance improvements throughout the product life cycle.

https://dakota.sandia.gov/[+https://dakota.sandia.gov/+]

Damaris
~~~~~~~

Damaris is a middleware for I/O and data management targeting large-scale,
MPI-based HPC simulations. It initially proposed to dedicate cores for
asynchronous I/O in multicore nodes of recent HPC platforms, with an emphasis
on ease of integration in existing simulation, efficient resource usage (with
the use of shared memory) and simplicity of extension through plugins.

Over the years, Damaris has evolved into a more elaborate system, providing
the possibility to use dedicated cores or dedicated nodes to data processing
and I/O. It proposes a seamless connection to the VisIt software to enable in
situ visualization with minimum impact on run time. Damaris provides an
extremely simple API and can be easily integrated in existing large-scale
simulations.

http://damaris.gforge.inria.fr/doku.php[+http://damaris.gforge.inria.fr/doku.php+]

https://hal.inria.fr/hal-00859603[+https://hal.inria.fr/hal-00859603+]

https://hal.inria.fr/hal-01025670v1[+https://hal.inria.fr/hal-01025670v1+]

Damsel
~~~~~~

The goal of Damsel project is to enable Exascale computational science
aplications to interact conveniently and efficiently with storage through
abstractions that match their data models.

http://cucis.ece.northwestern.edu/projects/DAMSEL/[+http://cucis.ece.northwestern.edu/projects/DAMSEL/+]

Dart
~~~~

Dart is a cohesive, scalable platform for building apps that run on the web
(where you can use Polymer) or on servers (such as with Google Cloud
Platform). Use the Dart language, libraries, and tools to write anything from
simple scripts to full-featured apps.

https://www.dartlang.org/[+https://www.dartlang.org/+]

DART
~~~~

DART is a community facility for ensemble DA developed and maintained by the
Data Assimilation Research Section (DAReS) at the National Center for
Atmospheric Research (NCAR). DART provides modelers, observational scientists,
and geophysicists with powerful, flexible DA tools that are easy to implement
and use and can be customized to support efficient operational DA
applications. DART is a software environment that makes it easy to explore a
variety of data assimiliation methods and observations with different
numerical models and is designed to facilitate the combination of assimilation
algorithms, models, and real (as well as synthetic) observations to allow
increased understanding of all three. DART includes extensive documentation, a
comprehensive tutorial, and a variety of models and observation sets that can
be used to introduce new users or graduate students to ensemble DA. DART also
provides a framework for developing, testing, and distributing advances in
ensemble DA to a broad community of users by removing the
implementation-specific peculiarities of one-off DA systems. 

DART employs a modular programming approach to apply an Ensemble Kalman Filter
which nudges the underlying models toward a state that is more consistent with
information from a set of observations. Models may be swapped in and out, as
can different algorithms in the Ensemble Kalman Filter. The method requires
running multiple instances of a model to generate an ensemble of states. A
forward operator appropriate for the type of observation being assimilated is
applied to each of the states to generate the model's estimate of the
observation. 

https://www.image.ucar.edu/DAReS/DART/[+https://www.image.ucar.edu/DAReS/DART/+]

dat
~~~

Dat is an open source project that provides a streaming interface between
every file format and data storage backend.

http://dat-data.com/[+http://dat-data.com/+]

DataHub
~~~~~~~

DataHub is a unified, managed, collaborative platform for making
data-processing easy.
Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software
version control systems like git, we propose (a) a dataset version control
system, giving users the ability to create, branch, merge, difference and
search large, divergent collections of datasets, and (b) a platform, DataHub,
that gives users the ability to perform collaborative data analysis building
on this version control system.

http://arxiv.org/abs/1409.0798[+http://arxiv.org/abs/1409.0798+]

http://datahub.csail.mit.edu/www/[+http://datahub.csail.mit.edu/www/+]

Data Transfer and Storage
~~~~~~~~~~~~~~~~~~~~~~~~~

BDM
^^^

A scalable data transfer management tool for GridFTP? transfer protocol. The
goal is to manage as much as 1+ PB with millions of files transfers reliably.

https://sdm.lbl.gov/twiki/bin/view/Software/BDM/WebHome[+https://sdm.lbl.gov/twiki/bin/view/Software/BDM/WebHome+]

CASTOR
^^^^^^

CASTOR, stands for the CERN Advanced STORage manager, is a hierarchical
storage management (HSM) system developed at CERN used to store physics
production files and user files. Files can be stored, listed, retrieved and
accessed in CASTOR using command line tools or applications built on top of
the different data transfer protocols like RFIO (Remote File IO), ROOT
libraries, GridFTP and XROOTD. CASTOR manages disk cache(s) and the data on
tertiary storage or tapes. Currently (2007) there are some 60 million files
and about 7 petabyte of data in CASTOR.

CASTOR provides a UNIX like directory hierarchy of file names. The directories
are always rooted /castor/cern.ch (the cern.ch will be different in other
CASTOR sites). The CASTOR name space can viewed and manipulated only through
CASTOR client commands and library calls. OS commands like ls or mkdir will
not work on CASTOR files. The CASTOR name space holds permanent tape residence
of the CASTOR files, while the more volatile disk residence is only known to
the stager, which is the disk cache management component in CASTOR. When
accessing or modifying a CASTOR file, one must therefore always use a stager.
 
http://castorwww.web.cern.ch/castorwww/[+http://castorwww.web.cern.ch/castorwww/+]

DML
^^^

DataMover-Lite (DML) is a simple file transfer tool with graphical user
interface which supports multi-protocol data movement.DML is available in both
webstart and standalone version.
Currently, DML supports http, https, ftp, gridftp, lahfs and scp.
For GridFTP, DML also supports directory browsing and transferring.

https://sdm.lbl.gov/twiki/bin/view/Software/DML[+https://sdm.lbl.gov/twiki/bin/view/Software/DML+]

SRM-Lite
^^^^^^^^

SRM-Lite is a simple command-line based tool with pluggable file transfer
protocol supports.
SRM-Lite supports scp and sftp in high performance way (hpn-ssh).

https://sdm.lbl.gov/twiki/bin/view/Software/SRMLite[+https://sdm.lbl.gov/twiki/bin/view/Software/SRMLite+]

https://codeforge.lbl.gov/projects/srmlite[+https://codeforge.lbl.gov/projects/srmlite+]

XROOTD
^^^^^^

The XROOTD project aims at giving high performance, scalable fault tolerant
access to data repositories of many kinds. The typical usage is to give access
to file-based ones. It is based on a scalable architecture, a communication
protocol, and a set of plugins and tools based on those. The freedom to
configure it and to make it scale (for size and performance) allows the
deployment of data access clusters of virtually any size, which can include
sophisticated features, like authentication/authorization, integrations with
other systems, WAN data distribution, etc.

XRootD software framework is a fully generic suite for fast, low latency and
scalable data access, which can serve natively any kind of data, organized as
a hierarchical filesystem-like namespace, based on the concept of directory.
As a general rule, particular emphasis has been put in the quality of the core
software parts.

http://xrootd.org/[+http://xrootd.org/+]

DaviX
~~~~~

The DaviX project aims to provide a solution for optimized remote I/O, data
management and large collections of file management over the WebDav (link is
external), Amazon S3 (link is external) and HTTP (link is external) protocols.
Davix is Multi-plateform, Open Source and is written in `Cxx`.

It is composed of two components:

* libdavix: a `Cxx` library. it offers an HTTP API, a remote I/O API and a POSIX
compatibility layer.
* davix-*: several utilities for file transfert, large collections of files
management and large files management.

DaviX supports features like session reuse, redirection caching, vector
operations, Metalink, X509 client certificate, proxy certificate, SOCKS4/5 or
VOMS.

https://dmc.web.cern.ch/projects/davix/home[+https://dmc.web.cern.ch/projects/davix/home+]

http://arxiv.org/abs/1410.4168[+http://arxiv.org/abs/1410.4168+]

DCCRG
~~~~~

DCCRG is an easy to use grid for FVM/FEM simulations written in `Cxx`. It
handles load balancing and neighbour cell data updates between processes
automatically. MPI is used for parallelization. 

The distributed cartesian cell-refinable grid (dccrg) supports adaptive mesh
refinement and allows an arbitrary `Cxx` class to be used as cell data. The
amount of data in grid cells can vary both in space and time allowing dccrg to
be used in very different types of simulations, for example in fluid and
particle codes. Dccrg transfers the data between neighboring cells on
different processes transparently and asynchronously allowing one to overlap
computation and communication. This enables excellent scalability at least up
to 32 k cores in magnetohydrodynamic tests depending on the problem and
hardware. In the version of dccrg presented here part of the mesh metadata is
replicated between MPI processes reducing the scalability of adaptive mesh
refinement (AMR) to between 200 and 600 processes.

https://gitorious.org/dccrg[+https://gitorious.org/dccrg+]

http://arxiv.org/abs/1212.3496[+http://arxiv.org/abs/1212.3496+]

http://www.sciencedirect.com/science/article/pii/S0010465512004237[+http://www.sciencedirect.com/science/article/pii/S0010465512004237+]

Declaratron
~~~~~~~~~~~

We introduce the Declaratron, a system which takes a declarative approach to
specifying mathematically based scientific computation. This uses displayable
mathematical notation (Content MathML) and is both executable and semantically
well defined. We combine domain specific representations of physical science
(e.g. CML, Chemical Markup Language), MathML formulae and computational
specifications (DeXML) to create executable documents which include scientific
data and mathematical formulae. These documents preserve the provenance of the
data used, and build tight semantic links between components of mathematical
formulae and domain objects---in effect grounding the mathematical semantics
in the scientific domain.

http://arxiv.org/abs/1307.3088[+http://arxiv.org/abs/1307.3088+]

https://bitbucket.org/petermr/declaratron[+https://bitbucket.org/petermr/declaratron+]

denoising
~~~~~~~~~

Noise reduction (Wikipedia) -
http://en.wikipedia.org/wiki/Noise_reduction[+http://en.wikipedia.org/wiki/Noise_reduction+]

Video denoising (Wikipedia) -
http://en.wikipedia.org/wiki/Video_denoising[+http://en.wikipedia.org/wiki/Video_denoising+]

A review of image denoising algorithms (paper, PDF, 2005, 41) - A. Buades et
al. -
http://ivm.sjtu.edu.cn/files/dip/pro1/NL2.pdf[+http://ivm.sjtu.edu.cn/files/dip/pro1/NL2.pdf+]

Is denoising dead? (paper, PDF, 2010, 17) - Priyam Chatterjee & Peyman
Milanfar -
https://users.soe.ucsc.edu/\~milanfar/publications/journal/DenoisingBoundsFinal.pdf[+https://users.soe.ucsc.edu/~milanfar/publications/journal/DenoisingBoundsFinal.pdf+]

D-AMP
^^^^^

A denoising algorithm seeks to remove perturbations or errors from a signal.
The last three decades have seen extensive research devoted to this arena, and
as a result, today's denoisers are highly optimized algorithms that
effectively remove large amounts of additive white Gaussian noise. A
compressive sensing (CS) reconstruction algorithm seeks to recover a
structured signal acquired using a small number of randomized measurements.
Typical CS reconstruction algorithms can be cast as iteratively estimating a
signal from a perturbed observation. This paper answers a natural question:
How can one effectively employ a generic denoiser in a CS reconstruction
algorithm? In response, in this paper, we develop a denoising-based
approximate message passing (D-AMP) algorithm that is capable of
high-performance reconstruction. We demonstrate that, for an appropriate
choice of denoiser, D-AMP offers state-of-the-art CS recovery performance for
natural images. We explain the exceptional performance of D-AMP by analyzing
some of its theoretical features. A critical insight in our approach is the
use of an appropriate Onsager correction term in the D-AMP iterations, which
coerces the signal perturbation at each iteration to be very close to the
white Gaussian noise that denoisers are typically designed to remove. 

This packages contains the code to run the BM3D, BM3D-SAPCA, BLS-GSM, and NLM
variants of the denoising-based approximate message passing and
denoising-based iterative thresholding algorithms.

http://dsp.rice.edu/software/DAMP-toolbox[+http://dsp.rice.edu/software/DAMP-toolbox+]

http://arxiv.org/abs/1406.4175[+http://arxiv.org/abs/1406.4175+]

DEOF
~~~~

In this paper it is suggested that a stochastic isotropic diffusive process,
representing a spatial first order auto regressive process (AR(1)-process),
can be used as a null hypothesis for the spatial structure of climate
variability. By comparing the leading empirical orthogonal functions (EOFs) of
a fitted null hypothesis with EOF modes of an observed data set, inferences
about the nature of the observed modes can be made. The concept and procedure
of fitting the null hypothesis to the observed EOFs is in analogy to time
analysis, where an AR(1)-process is fitted to the statistics of the time
series in order to evaluate the nature of the time scale behavior of the time
series. The formulation of a stochastic null hypothesis allows one to define
teleconnection patterns as those modes that are most distinguished from the
stochastic null hypothesis. The method is applied to several artificial and
real data sets including the sea surface temperature of the tropical Pacific
and Indian Ocean and the Northern Hemisphere wintertime and tropical sea level
pressure.

A Matlab script for computing the Distinct EOFs is available.

http://users.monash.edu.au/\~dietmard/deof-analysis.html[+http://users.monash.edu.au/~dietmard/deof-analysis.html+]

DEPOT
~~~~~

DEPOT is a framework for easily storing and serving files in web applications
on Python2.6+ and Python3.2+.
Modern web applications need to rely on a huge amount of stored images,
generated files and other data which is usually best to keep outside of your
database. DEPOT provides a simple and effective interface for storing your
files on a storage backend at your choice (Local, S3, GridFS) and easily
relate them to your application models (SQLAlchemy, Ming) like you would for
plain data.

http://depot.readthedocs.org/en/latest/[+http://depot.readthedocs.org/en/latest/+]

Delite
~~~~~~

Delite is a research project from Stanford University's Pervasive Parallelism
Laboratory (PPL). Delite is a compiler framework and runtime for parallel
embedded domain-specific languages (DSLs). Our goal is enable the rapid
construction of high performance, highly productive DSLs. 

Delite is still in alpha, and there is no official release. However, the
'develop' (Delite) and 'delite-develop' (LMS) branches should be relatively
stable for experimental development of new DSLs. For those interested in
developing their own DSLs, we highly recommend using Forge, which is itself a
DSL that automates much of the process of creating DSLs embedded in Scala. For
those interested in using instead of building DSLs, alpha builds of OptiML, a
DSL for machine learning, OptiQL, a DSL for data querying, and OptiGraph, a
DSL for graph analytics, are currently available.

https://github.com/stanford-ppl/Delite[+https://github.com/stanford-ppl/Delite+]

http://stanford-ppl.github.io/Delite/index.html[+http://stanford-ppl.github.io/Delite/index.html+]

OptiML
^^^^^^

OptiML is an embedded domain-specific language for machine learning. OptiML
is developed as a research project from Stanford University's Pervasive
Parallelism Laboratory (PPL).

OptiML is currently targeted at machine learning researchers and algorithm
developers; it aims to provide a productive, high performance, MATLAB-like
environment for linear algebra supplemented with machine learning specific
abstractions. Our primary goal is to allow machine learning practitioners to
write code in a highly declarative manner and still achieve high performance
on a variety of underlying parallel, heterogeneous devices. The same OptiML
program should run well and scale on a CMP (chip multi-processor), a GPU, a
combination of CMPs and GPUs, clusters of CMPs and GPUs, and eventually even
FPGAs and other specialized accelerators.

In particular, OptiML is designed to allow statistical inference algorithms
expressible by the Statistical Query Model to be both easy to express and very
fast to execute. These algorithms can be expressed in a summation form, and
can be parallelized using fine-grained map-reduce operations. OptiML employs
aggressive optimizations to reduce unnecessary memory allocations and fuse
operations together to make these as fast as possible. OptiML also attempts to
specialize implementations to particular hardware devices as much as possible
to achieve the best performance.

http://stanford-ppl.github.io/Delite/optiml/index.html[+http://stanford-ppl.github.io/Delite/optiml/index.html+]

Forge
^^^^^

A prototype meta DSL that generates Delite DSL implementations from a
specification-like program. 

https://github.com/stanford-ppl/Forge[+https://github.com/stanford-ppl/Forge+]

detrending
~~~~~~~~~~

Codes for detrending Kepler and other light curves.
To study exoplanetary atmospheres, we typically require a 10e-4 to 10e-5 level
of accuracy in flux. Achieving such a precision has become the central
challenge to exoplanetary research and is often impeded by systematic
(nongaussian) noise from either the instrument, stellar activity or both.
Dedicated missions, such as Kepler, feature an a priori instrument calibration
plan to the required accuracy but nonetheless remain limited by stellar
systematics. More generic instruments often lack a sufficiently defined
instrument response function, making it very hard to calibrate. The correct
calibration strategy is hence of paramount importance and requires a dedicated
effort and out of the box thinking. In recent years, we have made significant
advances in exoplanetary spectroscopy through improvements in data
de-trending. 

http://zuserver2.star.ucl.ac.uk/\~ingo/projects.html[+http://zuserver2.star.ucl.ac.uk/~ingo/projects.html+]

http://arxiv.org/abs/1409.2312[+http://arxiv.org/abs/1409.2312+]

http://arxiv.org/abs/1406.3984[+http://arxiv.org/abs/1406.3984+]

https://github.com/exosamsi/detrending/blob/master/documents/existing_detrending_algorithms.md[+https://github.com/exosamsi/detrending/blob/master/documents/existing_detrending_algorithms.md+]

https://github.com/exosamsi/detrending[+https://github.com/exosamsi/detrending+]

distalgo
~~~~~~~~

DistAlgo is a very high-level language for programming distributed algorithms.
This project implements a DistAlgo compiler with Python as the target
language. In the following text, the name 'DistAlgo' refers to the compiler
and not the language.

https://github.com/DistAlgo/distalgo[+https://github.com/DistAlgo/distalgo+]

http://arxiv.org/abs/1412.8461[+http://arxiv.org/abs/1412.8461+]

Distributed Array Protocol
~~~~~~~~~~~~~~~~~~~~~~~~~~

The Distributed Array Protocol (DAP) is a process-local protocol that allows
two subscribers, called the “producer” and the “consumer” or the “exporter”
and the “importer”, to communicate the essential data and metadata necessary
to share a distributed-memory array between them. This allows two
independently developed components to access, modify, and update a distributed
array without copying. The protocol formalizes the metadata and buffers
involved in the transfer, allowing several distributed array projects to
collaborate, facilitating interoperability. By not copying the underlying
array data, the protocol allows for efficient sharing of array data.

http://distributed-array-protocol.readthedocs.org/en/rel-0.10.0/index.html[+http://distributed-array-protocol.readthedocs.org/en/rel-0.10.0/index.html+]

D-LITE
~~~~~~

D-LITe is an universal architecture for building simple application over
heterogenous Sensors Networks.

http://igm.univ-mlv.fr/Networks/project_dlite.html[+http://igm.univ-mlv.fr/Networks/project_dlite.html+]

Docker
~~~~~~

Docker is an open platform for developers and sysadmins to build, ship, and
run distributed applications. Consisting of Docker Engine, a portable,
lightweight runtime and packaging tool, and Docker Hub, a cloud service for
sharing applications and automating workflows, Docker enables apps to be
quickly assembled from components and eliminates the friction between
development, QA, and production environments. As a result, IT can ship faster
and run the same app, unchanged, on laptops, data center VMs, and any cloud. 

https://www.docker.com/whatisdocker/[+https://www.docker.com/whatisdocker/+]

https://registry.hub.docker.com/[+https://registry.hub.docker.com/+]

Atomic
^^^^^^

A package for deploying and managing xref:Docker[Docker]
containers.
Project Atomic integrates the tools and patterns of container-based
application and service deployment with trusted operating system platforms to
deliver an end-to-end hosting architecture that's modern, reliable, and
secure. 

An Atomic Host is a lean operating system designed to run Docker containers,
built from upstream CentOS, Fedora, or Red Hat Enterprise Linux RPMs. It
provides all the benefits of the upstream distribution, plus the ability to
perform atomic upgrades and rollbacks.

http://www.projectatomic.io/[+http://www.projectatomic.io/+]

Docker Machine
^^^^^^^^^^^^^^

A tool that makes it really easy to go from “zero to Docker”. Machine creates
Docker Engines on your computer, on cloud providers, and/or in your data
center, and then configures the Docker client to securely talk to them.

https://blog.docker.com/2015/02/announcing-docker-machine-beta/[+https://blog.docker.com/2015/02/announcing-docker-machine-beta/+]

[[Rancher]]
Rancher
^^^^^^^

A complete infrastructure platform for running Docker in production.

http://rancher.com/[+http://rancher.com/+]

https://github.com/rancher/rancher[+https://github.com/rancher/rancher+]

DOpElib
~~~~~~~

An innovative feature of
DOpElib
is to provide a software toolkit to solve forward PDE
problems as well as optimal control problems constrained by PDE.
DOpElib
concentrates
on a unified approach for both linear and nonlinear problems by interpreting
every PDE
problem as nonlinear and applying a Newton method to solve it. The focus is on
the
numerical solution of both stationary and nonstationary problems which come
from
diㄦent application fields, like elasticity and plasticity, uid dynamics, and
multiphysics
problems such as uid-structure interactions.

http://wwwopt.mathematik.tu-darmstadt.de/dopelib/[+http://wwwopt.mathematik.tu-darmstadt.de/dopelib/+]

[[DSCPACK]]
DSCPACK
~~~~~~~

DSCPACK can be used to solve sparse linear systems using direct methods on multiprocessors and networks-of-workstations. This package is suitable for systems where the coefficient matrix is symmetric and sparse. This solver is written in C; it uses MPI for inter-processor communication and the BLAS library for improved cache-performance.

http://www.cse.psu.edu/\~pxr3/Dscpack/[+http://www.cse.psu.edu/~pxr3/Dscpack/+]

dtk
~~~

A software experimental platform, providing the foundations needed to develop
dedicated modular applications aggregating functionalities embedded using
low-level and interchangeable software entities - plugins - and orchestrated
through high-level software entities - scripts or GUIs.

http://dtk.inria.fr/[+http://dtk.inria.fr/+]

http://dtk.inria.fr/dtk/repositories[+http://dtk.inria.fr/dtk/repositories+]

http://dtk.inria.fr/projects[+http://dtk.inria.fr/projects+]

num3sis
^^^^^^^

Platform for multi-physics simulations built on top of dtk.

http://num3sis.inria.fr/[+http://num3sis.inria.fr/+]

DxT
~~~

DxTer is a system for researching Design by Transformation (DxT).
DxTer takes as input a knowledge base of software design
transformations and a graph representing functionality to be
implemented. It generates a search space of implementations from the
transformations, estimates their costs, and outputs the best code.

DxT can be used as a principled way to re-engineer legacy applications or to
forward engineer new applications.  (The former is used for domains containing
a single application, whereas the latter is used when transformations describe
a combinatorial number of derivable applications).  Our primary application is
to forward engineer dense linear algebra applications using the Flame
methodology and Elemental library.  Our goal is to express the design
knowledge of Elemental as transformations, and to generate Elemental libraries
for new architectures, rather than hand-deriving such libraries.  Doing so
will be a significant accomplishment -- both in software engineering in
general, and dense linear algebra in particular.

https://github.com/DxTer-project/dxter[+https://github.com/DxTer-project/dxter+]

http://www.cs.utexas.edu/users/schwartz/DxT/[+http://www.cs.utexas.edu/users/schwartz/DxT/+]

http://www.cs.utexas.edu/\~bamarker/[+http://www.cs.utexas.edu/~bamarker/+]

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.2790[+http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.2790+]

DZSlides
~~~~~~~~

DZSlides is a one-page-template to build your presentation in HTML5 and CSS3.

http://paulrouget.com/dzslides/[+http://paulrouget.com/dzslides/+]

https://github.com/paulrouget/dzslides[+https://github.com/paulrouget/dzslides+]

[[NE]]
////
NEE
////

Earth Orbit
~~~~~~~~~~~

An astronomically precise and accurate model that offers 3-D visualizations of
Earth's orbital geometry, Milankovitch parameters and the ensuing insolation
forcing. The model is developed in MATLAB® as a user-friendly graphical user
interface. Users are presented with a choice between the Berger (1978a) and
Laskar et al. (2004) astronomical solutions for eccentricity, obliquity and
precession. A "demo" mode is also available, which allows the Milankovitch
parameters to be varied independently of each other, so that users can isolate
the effects of each parameter on orbital geometry, the seasons, and
insolation. A 3-D orbital configuration plot, as well as various surface and
line plots of insolation and insolation anomalies on various time and space
scales are produced. Insolation computations use the model's own orbital
geometry with no additional a priori input other than the Milankovitch
parameter solutions.

http://www.geosci-model-dev.net/7/1051/2014/gmd-7-1051-2014.html[+http://www.geosci-model-dev.net/7/1051/2014/gmd-7-1051-2014.html+]

EAVL
~~~~

EAVL is the Extreme-scale Analysis and Visualization Library.

https://github.com/jsmeredith/EAVL[+https://github.com/jsmeredith/EAVL+]

http://ft.ornl.gov/eavl/[+http://ft.ornl.gov/eavl/+]

Eclipse
~~~~~~~

An integrated development environment (IDE). It contains a base workspace and
an extensible plug-in system for customizing the environment. Written mostly
in Java, Eclipse can be used to develop applications. By means of various
plug-ins, Eclipse may also be used to develop applications in other
programming languages: Ada, ABAP, C, `Cxx`, COBOL, Fortran, Haskell, JavaScript,
Lasso, Lua, Natural, Perl, PHP, Prolog, Python, R, Ruby (including Ruby on
Rails framework), Scala, Clojure, Groovy, Scheme, and Erlang. It can also be
used to develop packages for the software Mathematica. Development
environments include the Eclipse Java development tools (JDT) for Java and
Scala, Eclipse CDT for C/`Cxx` and Eclipse PDT for PHP, among others.

http://www.eclipse.org/[+http://www.eclipse.org/+]

http://marketplace.eclipse.org/[+http://marketplace.eclipse.org/+]

Eigen
~~~~~

Eigen is a `Cxx` template library for linear algebra: matrices, vectors,
numerical solvers, and related algorithms.

http://eigen.tuxfamily.org/[+http://eigen.tuxfamily.org/+]

Ekho
~~~~

A new tool for recording and replaying energy harvesting conditions. Energy
harvesting is a necessity for many small, embedded sensing devices, that must
operate maintenance-free for long periods of time. However, understanding how
the environment changes and it's effects on device behavior has always been a
source of frustration. Ekho allows system designers working with ultra low
power devices, to realistically predict how new hardware and software
configurations will perform before deployment. By taking advantage of
electrical characteristics all energy sources share, Ekho is able to emulate
many different energy sources (e.g., Solar, RF, Thermal, and Vibrational) and
takes much of the guesswork out of experimentation with tiny, energy
harvesting sensing systems.

https://github.com/jhester/ekho[+https://github.com/jhester/ekho+]

http://dl.acm.org/citation.cfm?id=2668332.2668336&coll=DL&dl=ACM[+http://dl.acm.org/citation.cfm?id=2668332.2668336&coll=DL&dl=ACM+]

https://github.com/jhester/ekho/blob/master/presentation/ekho-sensys-notes.pdf[+https://github.com/jhester/ekho/blob/master/presentation/ekho-sensys-notes.pdf+]

ELCIRC
~~~~~~

ELCIRC is an unstructured-grid model designed for the effective simulation of
3D baroclinic circulation across river-to-ocean scales. It uses a
finite-volume/finite-difference Eulerian-Lagrangian algorithm to solve the
shallow water equations, written to realistically address a wide range of
physical processes and of atmospheric, ocean and river forcings. The numerical
algorithm is low-order, but volume conservative, stable and computationally
efficient. It also naturally incorporates wetting and drying of tidal flats.
While originally developed to meet specific modeling challenges for the
Columbia River, ELCIRC has been extensively tested against standard
ocean/coastal benchmarks, and is starting to be applied to estuaries and
continental shelves around the world.

http://www.stccmop.org/CORIE/modeling/elcirc/[+http://www.stccmop.org/CORIE/modeling/elcirc/+]

elm
~~~

A functional reactive language for interactive applications.
Elm is great for 2D and 3D games, diagrams, widgets, and websites.

http://elm-lang.org/[+http://elm-lang.org/+]

https://github.com/elm-lang[+https://github.com/elm-lang+]

eLML
~~~~

The eLesson Markup Language (eLML) is an testopen source XML framework for
creating structured eLessons using XML. For easier lesson authoring eLML we
offer the web-based WYSIWYG Firedocs eLML Editor and to create eLML template
layouts withouth any XSLT-knowledge you can use our new Template Builder. Once
you created your eLML-lesson you can transform it into many different
output-formats like IMS Content Package or SCORM, various HTML-templates,
eBooks (ePub format), PDF, Office-Document (ODF) and many more listed under
"Output Formats".

http://www.elml.org/website/en/html/index.html[+http://www.elml.org/website/en/html/index.html+]

El Topo
~~~~~~~

El Topo is a public domain `Cxx` package for tracking dynamic surfaces
represented as triangle meshes in 3D. It robustly handles topology changes
such as merging and pinching off, while adaptively maintaining a tangle-free,
high-quality triangulation.

The current release contains source for the El Topo library, as well as Talpa,
an executable demonstrating several applications of our method. The code has
been tested on OS/X and Linux and is freely available for download. 

http://www.cs.ubc.ca/labs/imager/tr/2009/eltopo/eltopo.html[+http://www.cs.ubc.ca/labs/imager/tr/2009/eltopo/eltopo.html+]

https://github.com/tysonbrochu/eltopo[+https://github.com/tysonbrochu/eltopo+]

Embree
~~~~~~

Embree is a collection of high-performance ray tracing kernels, developed at
Intel. The target user of Embree are graphics application engineers that want
to improve the performance of their application by leveraging the optimized
ray tracing kernels of Embree. The kernels are optimized for photo-realistic
rendering on the latest Intel® processors with support for SSE, AVX, AVX2, and
the 16-wide Intel® Xeon Phi™ coprocessor vector instructions. Embree supports
runtime code selection to choose the traversal and build algorithms that best
matches the instruction set of your CPU. We recommend using Embree through its
API to get the highest benefit from future improvements. Embree is released as
Open Source under the Apache 2.0 license.

http://embree.github.io/[+http://embree.github.io/+]

EMPIRE
~~~~~~

EMPIRE is the name given to a way of changing the source code of a dynamical
model so that it can interface with sequential data assimilation methods.

EMPIRE should be one of the quickest and easiest ways in which to modify the
source code of the model to use data assimilation. 

http://www.met.reading.ac.uk/\~darc/empire/[+http://www.met.reading.ac.uk/~darc/empire/+]

[[Emscripten]]
Emscripten
~~~~~~~~~~

Emscripten is an LLVM-based project that compiles C and `Cxx` into
highly-optimizable JavaScript in asm.js format. This lets you run C and `Cxx` on
the web at near-native speed, without plugins.

http://kripken.github.io/emscripten-site/[+http://kripken.github.io/emscripten-site/+]

emulators
~~~~~~~~~

[[Dolphin]]
Dolphin
^^^^^^^

Dolphin is an emulator for two recent Nintendo video game consoles: the GameCube and the Wii. It allows PC gamers to enjoy games for these two consoles in full HD (1080p) with several enhancements: compatibility with all PC controllers, turbo speed, networked multiplayer, and even more.

https://dolphin-emu.org/[+https://dolphin-emu.org/+]

http://blog.lse.epita.fr/articles/38-emulating-the-gamecube-audio-processing-in-dolphin.html[+http://blog.lse.epita.fr/articles/38-emulating-the-gamecube-audio-processing-in-dolphin.html+]

Unicorn
^^^^^^^

A lightweight multi-platform, multi-architecture CPU emulator framework.
The features include:

* Multi-architectures: Arm, Arm64 (Armv8), M68K, Mips, PowerPC, Sparc, & X86 (include X86_64);
* Clean/simple/lightweight/intuitive architecture-neutral API;
* Support fine-grained instrumentation at various levels;
* Implemented in pure C language, with bindings for Python available;
* Native support for Windows & *nix (with Mac OSX, iOS, Android, Linux, *BSD & Solaris confirmed);
* Thread-safe by design.

http://www.unicorn-engine.org/[+http://www.unicorn-engine.org/+]

Equelle
~~~~~~~

Equelle is a domain-specific language for the specification of simulators for
systems of PDEs through a high-level syntax. The language allows the user to
focus on equations and numerics while hiding the low-level details of software
and hardware implementations.

http://equelle.org/[+http://equelle.org/+]

https://github.com/sintefmath/equelle[+https://github.com/sintefmath/equelle+]

ESIP Wiki
~~~~~~~~~

The Federation of Earth Science Information Partners (ESIP) is a broad-based,
distributed community of data and information technology practitioners.

http://wiki.esipfed.org/index.php/Main_Page[+http://wiki.esipfed.org/index.php/Main_Page+]

[[ESMF]]
ESMF
~~~~

CIM
^^^

The Common Information Model (CIM) is a metadata standard used by the climate
research community and others to describe the artifacts and processes they
work with.  This includes climate simulations, the specific model components
used to run those simulations, the datasets generated by those components, the
geographic grids upon which those components and data are mapped, the
computing platforms used, and so on.

https://earthsystemcog.org/projects/es-doc-models/cim[+https://earthsystemcog.org/projects/es-doc-models/cim+]

CoG
^^^

Earth System CoG is a web environment that enables users to create project
workspaces, connect projects into networks, share and consolidate information
within those networks, and seamlessly link to tools for data archival,
reformatting and search, data visualization, and metadata collection and
display. CoG is integrated with the Earth System Grid Federation (ESGF) data
distribution software and provides an easy to use interface to its services.

https://earthsystemcog.org/projects/cog/[+https://earthsystemcog.org/projects/cog/+]

Cupid
^^^^^

Cupid is a development and training environment for models that use the Earth
System Modeling Framework (ESMF) and National Unified Operational Capability
(NUOPC) Layer infrastructure. Cupid is implemented as a plug-in for the widely
used Eclipse Integrated Development Environment (IDE). Together, Cupid and
Eclipse form an accessible, appealing training environment that makes it
easier and faster to build NUOPC-based applications.

https://www.earthsystemcog.org/projects/cupid/[+https://www.earthsystemcog.org/projects/cupid/+]

ES-DOC
^^^^^^

ES-DOC is an international effort to develop tools to describe Earth system
models in order to better understand and utilize model data. The tools are
based on the Common Information Model (CIM) standard.

https://earthsystemcog.org/projects/es-doc-models/[+https://earthsystemcog.org/projects/es-doc-models/+]

ESGF
^^^^

The Earth System Grid Federation (ESGF) Peer-to-Peer (P2P) enterprise system
is a collaboration that develops, deploys and maintains software
infrastructure for the management, dissemination, and analysis of model output
and observational data.

http://esgf.llnl.gov/[+http://esgf.llnl.gov/+]

SMF Web Services
^^^^^^^^^^^^^^^^

The option to implement a variety of models as web services was implemented
in the Earth System Modeling Framework (ESMF).  ESMF is based on the idea of
components, which may represent physical domains such as the atmosphere,
ocean, or cryosphere, or specific processes such as ocean biogeochemistry.
These components have a standard interface that includes a specification of
input fields, output fields, and time information.  When running on high
performance computing systems, ESMF components are usually called as
subroutines of a main program.  With ESMF web services, the components can be
run on multiple computer systems, and can communicate with each other through
web protocols.

ESMF web services are currently comprised of a set of SOAP (Simple Object
Access Protocol) interfaces implemented using a combination of Apache Tomcat,
Axis2, and custom Java classes.  The SOAP services provide the gateway between
the ESMF components and the Internet.

https://earthsystemcog.org/projects/esmfwebservices/[+https://earthsystemcog.org/projects/esmfwebservices/+]

ESMPy
^^^^^

ESMPy is a Python interface to the Earth System Modeling Framework (ESMF)
regridding utility.

ESMF is software for building and coupling weather, climate, and related
models.  It has a robust, parallel and scalable remapping package, used to
generate remapping weights.  It can handle a wide variety of grids and
options:  logically rectangular grids and unstructured meshes; regional or
global grids; 2D or 3D; and pole and masking options.  ESMF also has
capabilities to read grid information from NetCDF files in a variety of
formats, including the evolving Climate and Forecast (CF) GridSpec and UGRID
conventions. It is currently being merged with the OpenClimateGIS package so
that it can also support Geographic Information System (GIS) data formats.

ESMPy supports a single-tile logically rectangular discretization type called
Grid and an unstructured discretization type called Mesh (ESMF also supports
observational data streams). ESMPy supports bilinear, finite element patch
recovery and first-order conservative interpolation methods.  There is also an
option to ignore unmapped destination points and mask out points on either the
source or destination.  Regridding on the sphere takes place in 3D Cartesian
space, so the pole problem is not an issue as it can be with other Earth
system grid remapping software.  Grid and Mesh objects can be created in 2D or
3D space, and 3D first-order conservative regridding is fully supported.
Future plans for ESMPy involve the incorporation of observational data streams
and time operations, in addtion to the GIS formats mentioned previously.

https://www.earthsystemcog.org/projects/esmpy/[+https://www.earthsystemcog.org/projects/esmpy/+]

NUOPC
^^^^^

The National Unified Operational Prediction Capability (NUOPC) is a consortium
of Navy, NOAA, and Air Force modelers and their research partners. It aims to
advance the weather prediction modeling systems used by meteorologists,
mission planners, and decision makers. NUOPC partners are working toward a
common model architecture - a standard way of building models - in order to
make it easier to collaboratively build modeling systems.  To this end, they
have developed a NUOPC Layer that defines conventions and templates for using
the Earth System Modeling Framework (ESMF).

https://www.earthsystemcog.org/projects/nuopc/[+https://www.earthsystemcog.org/projects/nuopc/+]

OpenClimateGIS
^^^^^^^^^^^^^^

OpenClimateGIS (OCGIS) is a Python package designed for geospatial
manipulation, subsetting, computation, and translation of climate datasets
stored in local NetCDF files or files served through xref:THREDDS[THREDDS] data servers.
OpenClimateGIS has a straightforward, request-based API that is simple to use
yet complex enough to perform a variety of computational tasks. The software
is built entirely from open source packages. ClimateTranslator is a new web
interface to the OpenClimateGIS functionality. OpenClimateteGIS is currently
being merged with high performance parallel grid remapping capabilities from
ESMF, through the ESMPy package.

https://earthsystemcog.org/projects/openclimategis/[+https://earthsystemcog.org/projects/openclimategis/+]

Essential
~~~~~~~~~

https://www.xda-developers.com/how-to-root-essential-phone-ph-1/[+https://www.xda-developers.com/how-to-root-essential-phone-ph-1/+]

https://www.xda-developers.com/magisk-hub-2/[+https://www.xda-developers.com/magisk-hub-2/+]

https://www.xda-developers.com/install-adb-windows-macos-linux/[+https://www.xda-developers.com/install-adb-windows-macos-linux/+]

https://android.gadgethacks.com/how-to/root-your-essential-ph-1-with-magisk-0187784/[+https://android.gadgethacks.com/how-to/root-your-essential-ph-1-with-magisk-0187784/+]

ExaStencils
~~~~~~~~~~~

The central goal of ExaStencils is to develop a radically new software
technology for applications with exascale performance. To reach this goal, the
project focusses on a comparatively narrow but very important application
domain. The aim is to enable a simple and convenient formulation of problem
solutions in this domain. The software technology developed in ExaStencils
shall facilitate the highly automatic generation of a large variety of
efficient implementations via the judicious use of domain-specific knowledge
in each of a sequence of optimization steps such that, at the end, exascale
performance results.

The application domain chosen is that of stencil codes, i.e.,
compute-intensive algorithms in which data points in a grid are redefined
repeatedly as a combination of the values of neighboring points. The
neighborhood pattern used is called a stencil. Stencils codes are used for the
solution of discrete partial differential equations and the resulting linear
systems. 

http://www.exastencils.org/[+http://www.exastencils.org/+]

http://conferences.computer.org/wolfhpc/2014/papers/7020a042.pdf[+http://conferences.computer.org/wolfhpc/2014/papers/7020a042.pdf+]

Excafe
~~~~~~

An EXpression Capturing Finite Element Library  is a library developed during
my PhD as a means to explore the benefits of using active library techniques
for the performance optimisation of finite-element simulations. In particular
active library techniques facilitate efficient implementations of domain
specific languages.

Excafé only supports triangular meshes with Lagrange basis functions at
present. Furthermore boundary integrals have not yet been implemented.
However, the functionality present is more than sufficient to implement an
incompressible Navier-Stokes solver, which is included in the distribution.

One topic that Excafé has been used to explore is the symbolic analysis of the
expressions in finite element local assembly matrices. Excafé has access to
run-time representations of variational forms and basis functions. It uses
this to build symbolic representations of each entry of the local assembly
matrix. Once it has these, it uses a common sub-expression elimination pass
targeted at polynomial evaluation to find an evaluation strategy for these
expressions that minimizes operation count.

https://www.doc.ic.ac.uk/\~fpr02/excafe/[+https://www.doc.ic.ac.uk/~fpr02/excafe/+]

https://www.doc.ic.ac.uk/\~fpr02/excafe/fenics11_presentation.pdf[+https://www.doc.ic.ac.uk/~fpr02/excafe/fenics11_presentation.pdf+]

https://github.com/FrancisRussell/excafe[+https://github.com/FrancisRussell/excafe+]

EZFIO
~~~~~

EZFIO is the Easy Fortran I/O library generator. It generates automatically an
I/O library from a simple configuration file. The produced library contains
Fortran subroutines to read/write the data from/to disk, and to check if the
data exists. A Python and an Ocaml API are also provided.

With EZFIO, the data is organized in a file system inside a main directory.
This main directory contains subdirectories, which contain files. Each file
corresponds to a data. For atomic data the file is a plain text file, and for
array data the file is a gzipped text file.

https://github.com/scemama/EZFIO[+https://github.com/scemama/EZFIO+]

EvoGrid
~~~~~~~

The EvoGrid is a worldwide, cross-disciplinary effort to create an abstract,
yet plausible simulation of the chemical origins of life on Earth. One could
think of this as an artificial origin of life experiment. Our strategy is to
employ a large number of computers in a grid to simulate a digital primordial
soup along with a distributed set of computers acting as observers looking
into that grid. These observers, modeled after the very successful @Home
scientific computation projects, will be looking for signs of emergent
complexity and reporting back to the central grid. 

http://www.evogrid.org/index.php/Main_Page[+http://www.evogrid.org/index.php/Main_Page+]

http://sourceforge.net/projects/evogrid/[+http://sourceforge.net/projects/evogrid/+]

[[NF]]
////
NFFF
////

fastermath
~~~~~~~~~~

A library of math functions targeted at 32-bit and 64-bit x86 Linux systems.
The purpose of this library is to provide faster drop in replacements for
selected functions of the standard math library 'libm'. These functions
are written so they can be more optimized by compilers and all special
case tests for increased consistency and accuracy have been removed.
They are based on the corresponding implementations from the Cephes math
library by Stephen L. Moshier. The code has been simplified perusing
internal compiler facilities wherever possible and assuming little
endian IEEE-754 single and double precision math.

https://github.com/akohlmey/fastermath[+https://github.com/akohlmey/fastermath+]

FastFlow
~~~~~~~~

A `Cxx` parallel programming framework advocating high-level, pattern-based
parallel programming. It chiefly supports streaming and data parallelism,
targeting heterogenous platforms composed of clusters of shared-memory
platforms, possibly equipped with computing accelerators such as NVidia
GPGPUs, Xeon Phi, Tilera TILE64. 

FastFlow comes as a `Cxx` template library designed as a stack of layers that
progressively abstracts out the programming of parallel applications. The goal
of the stack is threefold: portability, extensibility, and performance. For
this, all the three layers are realised as thin strata of `Cxx` templates that
are 1) seamlessly portable; 2) easily extended via subclassing; and 3)
statically compiled and cross-optimised with the application. The terse design
ensures easy portability on almost all OSes and CPUs with a `Cxx` compiler.

http://calvados.di.unipi.it/[+http://calvados.di.unipi.it/+]

http://arxiv.org/abs/1204.5402[+http://arxiv.org/abs/1204.5402+]

FASTMath
~~~~~~~~

The FASTMathSciDAC Institute develops and deploys scalable mathematical
algorithms and software tools for reliable simulation of complex physical
phenomena and collaborates with application scientists to ensure the
usefulness and applicability of FASTMath technologies.

http://www.fastmath-scidac.org/[+http://www.fastmath-scidac.org/+]

http://www.fastmath-scidac.org/software-catalog.html[+http://www.fastmath-scidac.org/software-catalog.html+]

Faust
~~~~~

FAUST (Functional Audio Stream) is a functional programming language
specifically designed for real-time signal processing and synthesis. FAUST
targets high-performance signal processing applications and audio plug-ins for
a variety of platforms and standards.

http://faust.grame.fr/[+http://faust.grame.fr/+]

faust-lv2
^^^^^^^^^

This project provides an LV2 plugin architecture for the Faust programming
language. The package contains the Faust architecture and templates for the
needed LV2 manifest (ttl) files, a collection of sample plugins written in
Faust, and a generic GNU Makefile for compiling the plugins.

https://bitbucket.org/agraef/faust-lv2[+https://bitbucket.org/agraef/faust-lv2+]

guitarix
^^^^^^^^
A virtual guitar amplifier for Linux running with jack (Jack Audio Connection
Kit).
It takes the signal from your guitar as any real amp would do: as a
mono-signal from your sound card. Your tone is processed by a main amp and a
rack-section. Both can be routed separately and deliver a processed
stereo-signal via Jack. You may fill the rack with effects from more than 25
built-in modules spanning from a simple noise-gate to brain-slashing
modulation-fx like flanger, phaser or auto-wah. Your signal is processed with
minimum latency. On any properly set-up Linux-system you do not need to wait
for more than 10 milli-seconds for your playing to be delivered, processed by
guitarix.
It offers the range of sounds you would expect from a full-featured universal
guitar-amp.  A great part of guitarix effects is written in Faust.

http://faust.grame.fr/index.php/related-projects/guitarix[+http://faust.grame.fr/index.php/related-projects/guitarix+]

http://sourceforge.net/p/guitarix/git/ci/8e763c/tree/trunk/src/faust/[+http://sourceforge.net/p/guitarix/git/ci/8e763c/tree/trunk/src/faust/+]

FBReader
~~~~~~~~

FBReader is a free (and ad-free) multi-platform ebook reader.
It provides access to popular network libraries that contain a large set of
ebooks. Download books for free or for a fee. Add your own catalog.
It supports popular ebook formats: ePub, fb2, mobi, rtf, html, plain text, and
a lot of other formats.
It is highly customizable. Choose colors, fonts, page turning animations,
dictionaries, bookmarks, etc. to make reading as convenient as you want.

http://fbreader.org/[+http://fbreader.org/+]

https://github.com/geometer/FBReader[+https://github.com/geometer/FBReader+]

FEAST
~~~~~

The FEAST solver package is a free high-performance numerical library for
solving the standard or generalized eigenvalue problem, and obtaining all the
eigenvalues and eigenvectors within a given search interval. It is based on an
innovative fast and stable numerical algorithm -- named the FEAST algorithm --
which deviates fundamentally from the traditional Krylov subspace iteration
based techniques (Arnoldi and Lanczos algorithms) or other Davidson-Jacobi
techniques. The FEAST algorithm takes its inspiration from the density-matrix
representation and contour integration technique in quantum mechanics. It is
free from explicit orthogonalization procedures, and its main computational
tasks consist of solving very few inner independent linear systems with
multiple right-hand sides and one reduced eigenvalue problem orders of
magnitude smaller than the original one. The FEAST algorithm combines
simplicity and efficiency and offers many important capabilities for achieving
high performance, robustness, accuracy, and scalability on parallel
architectures.

This general purpose FEAST solver package includes both reverse communication
interfaces and ready to use predefined interfaces for dense, banded and sparse
systems. It includes double and single precision arithmetic, and all the
interfaces are compatible with Fortran (77,90) and C. FEAST is both a
comprehensive library package, and an easy to use software. This solver is
expected to significantly augment numerical performances and capabilities in
large-scale modern applications.

http://www.feast-solver.org/[+http://www.feast-solver.org/+]

http://www.ecs.umass.edu/\~polizzi/feast/index.htm[+http://www.ecs.umass.edu/~polizzi/feast/index.htm+]

http://arxiv.org/abs/1404.2891[+http://arxiv.org/abs/1404.2891+]

http://arxiv.org/abs/1407.8078[+http://arxiv.org/abs/1407.8078+]

http://arxiv.org/abs/1302.0432[+http://arxiv.org/abs/1302.0432+]

Fedora Playground Repository
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Playground repository gives contributors a place to host packages that are
not up to the standards of the main Fedora repository but may still be useful
to other users. For now the Playground repository contains both packages that
are destined for eventual inclusion into the main Fedora repository and
packages that are never going to make it there. Users of the repository should
be willing to endure a certain amount of instability when using packages from
there.

https://fedoraproject.org/wiki/Changes/Playground_repository[+https://fedoraproject.org/wiki/Changes/Playground_repository+]

[[Feldspar]]
Feldspar
~~~~~~~~

Feldspar (Functional Embedded Language for DSP and PARallelism) is a domain-specific language with associated code generator mainly targeting digital signal processing algorithms.
The project has developed a prototype compiler that generates ISO C99 code for programs written in this high-level language, and plans to target real digital signal processing hardware in the future.

http://feldspar.inf.elte.hu/[+http://feldspar.inf.elte.hu/+]

http://feldspar.github.io/[+http://feldspar.github.io/+]

http://feldspar.inf.elte.hu/feldspar/index.html[+http://feldspar.inf.elte.hu/feldspar/index.html+]

http://arxiv.org/abs/1507.07264[+http://arxiv.org/abs/1507.07264+]

[[FESIM]]
FESIM
~~~~~

The Finite-Element Sea Ice Model (FESIM)
includes the elastic-viscous-plastic (EVP) and viscous-plastic (VP) solvers and employs a flux corrected transport algorithm to advect the ice and snow mean thicknesses and concentration.
The model is formulated on unstructured triangular meshes. It assumes a collocated placement of ice velocities, mean thicknesses and concentration at mesh vertices, and relies on piecewise-linear (P1) continuous elements.

http://www.geosci-model-dev.net/8/1747/2015/gmd-8-1747-2015.html[+http://www.geosci-model-dev.net/8/1747/2015/gmd-8-1747-2015.html+]

http://www.geosci-model-dev.net/8/1747/2015/gmd-8-1747-2015-supplement.zip[+http://www.geosci-model-dev.net/8/1747/2015/gmd-8-1747-2015-supplement.zip+]

[[FFmpeg]]
FFmpeg
~~~~~~

FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created. It supports the most obscure ancient formats up to the cutting edge. No matter if they were designed by some standards committee, the community or a corporation.

It contains libavcodec, libavutil, libavformat, libavfilter, libavdevice, libswscale and libswresample which can be used by applications. As well as ffmpeg, ffserver, ffplay and ffprobe which can be used by end users for transcoding, streaming and playing.

http://ffmpeg.org/[+http://ffmpeg.org/+]

[[FFT]]
FFT
~~~

[[FFTF]]
FFTF
^^^^

A library that puts various Fast Fourier Transform implementations (built-in and third party) under a single interface.  The supported backends are
KissFFT, xref:Ooura_FFT[Ooura FFT], xref:libavcodec[libavcodec], xref:FFTW[FFTW], Intel IPP, Intel MKL,
NVIDA cuFFT, AMD OpenCL and ViennaCL.

https://github.com/Samsung/FFTF[+https://github.com/Samsung/FFTF+]

[[FFTW]]
FFTW
^^^^

A C subroutine library for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST).

http://www.fftw.org/[+http://www.fftw.org/+]

fourpack
^^^^^^^^

Library fourpack provides a conveninent and uniform interface to Fast Fourier
Transform impelemted in Intel Mathemtatical Kernel Library and FFTW.
This contains routines that:

* implements an interface to really fast FFT libraries, FFTW and Intel Math
Kernel Library;
* perform linear filtration with the use of FFT;
* compute direct and inverse spherical function transform; and
* compute tuning parameters for FFTW.

This requires xref:petools[petools].

http://astrogeo.org/fourpack/[+http://astrogeo.org/fourpack/+]

NFFT
^^^^

The NFFT (nonequispaced fast Fourier transform or nonuniform fast Fourier
transform) is a C subroutine library for computing the nonequispaced discrete
Fourier transform (NDFT) and its generalisations in one or more dimensions, of
arbitrary input size, and of complex data.

http://www-user.tu-chemnitz.de/\~potts/nfft/index.php[+http://www-user.tu-chemnitz.de/~potts/nfft/index.php+]

http://www-user.tu-chemnitz.de/\~potts/paper/nfft3.pdf[+http://www-user.tu-chemnitz.de/~potts/paper/nfft3.pdf+]

pyNFFT
xxxxxx

A Python interface for NFFT.

https://github.com/ghisvail/pyNFFT[+https://github.com/ghisvail/pyNFFT+]

NUFFT
^^^^^

When the data is irregular in either the "physical" or "frequency" domain,
unfortunately, the FFT does not apply. Over the last twenty years, a number of
algorithms have been developed to overcome this limitation - generally
referred to as non-uniform FFTs (NUFFT), non-equispaced FFTs (NFFT) or
unequally-spaced FFTs (USFFT). They achieve the same O(N log N) computational
complexity, but with a larger, precision-dependent, and dimension-dependent
constant.

We have developed some NUFFT libraries in Fortran 77 and Fortran 90 that are
freely available under the GPL license. 

http://www.cims.nyu.edu/cmcl/nufft/nufft.html[+http://www.cims.nyu.edu/cmcl/nufft/nufft.html+]

OpenFFT
^^^^^^^

OpenFFT is an open source parallel package for computing three-dimensional
Fast Fourier Transforms (3-D FFTs) of both real and complex numbers of
arbitrary input size. It originates from OpenMX (Open source package for
Material eXplorer). OpenFFT adopts a communication-optimal domain
decomposition method that is adaptive and capable of localizing data when
transposing from one dimension to another for reducing the total volume of
communication. It is written in C and MPI, with support for Fortran through
the Fortran interface, and employs FFTW3 for computing 1-D FFTs.

http://www.openmx-square.org/openfft/[+http://www.openmx-square.org/openfft/+]

http://arxiv.org/abs/1501.07350[+http://arxiv.org/abs/1501.07350+]

[[Ooura_FFT]]
Ooura FFT
^^^^^^^^^

A package to calculate Discrete Fourier/Cosine/Sine Transforms of 1-dimensional sequences of length 2^N. This package contains C and Fortran FFT codes.

http://www.kurims.kyoto-u.ac.jp/\~ooura/fft.html[+http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html+]

[[P3DFFT]]
P3DFFT
^^^^^^

Parallel Three-Dimensional Fast Fourier Transforms is a
library for large-scale computer simulations on parallel platforms. 3D FFT is
an important algorithm for simulations in a wide range of fields, including
studies of turbulence, climatology, astrophysics and material science.

http://code.google.com/p/p3dfft/[+http://code.google.com/p/p3dfft/+]

PFFT
^^^^

A parallel FFT software library based on MPI.

http://www-user.tu-chemnitz.de/\~mpip/software.php[+http://www-user.tu-chemnitz.de/~mpip/software.php+]

PNFFT
^^^^^

A parallel software library for the calculation of three-dimensional
nonequispaced FFTs based. It is available under GPL licence. The
parallelization is based on MPI. PNFFT depends on the PFFT and FFTW software
library.

http://www-user.tu-chemnitz.de/\~mpip/software.php[+http://www-user.tu-chemnitz.de/~mpip/software.php+]

SFFT
^^^^

The Sparse Fast Fourier Transform is a recent algorithm developed by Hassanieh
et al. [2, 3] for computing the the discrete Fourier Transforms on signals
with a sparse (exact or approximately) frequency domain. The algorithm
improves the asymptotic runtime compared to the prior methods based on
pruning.

http://groups.csail.mit.edu/netmit/sFFT/[+http://groups.csail.mit.edu/netmit/sFFT/+]

Feelxx
~~~~~~

`Feelxx` is a `Cxx` library for partial differential equation solves using
generalized Galerkin methods such as the finite element method, the h/p finite
element method, the spectral element method or the reduced basis method.

http://www.feelpp.org/[+http://www.feelpp.org/+]

FiberViewerLight
~~~~~~~~~~~~~~~~

Fiber ViewerLight is an open-source `Cxx` application to analyze fiber bundles.
FiberViewerLight is now available as a 3D Slicer extension.

https://github.com/NIRALUser/FiberViewerLight[+https://github.com/NIRALUser/FiberViewerLight+]

http://www.nitrc.org/projects/fvlight/[+http://www.nitrc.org/projects/fvlight/+]

FIGTree
~~~~~~~

A library for fast computation of Gauss transforms in multiple dimensions,
using the Improved Fast Gauss Transform and Approximate Nearest Neighbor
searching.
This software allows for efficient computation of probabilities by Kernel
Density Estimation (KDE), and can reduce complexity of algorithms commonly
used in Computer Vision, Machine Learning, etc, that must evaluate the Gauss
transform. 

http://www.umiacs.umd.edu/\~morariu/figtree/[+http://www.umiacs.umd.edu/~morariu/figtree/+]

https://github.com/vmorariu/figtree[+https://github.com/vmorariu/figtree+]

[[file_system]]
file system
~~~~~~~~~~~

aufs
^^^^

The advanced multi layered unification filesystem implements a union mount for
Linux file systems.


http://aufs.sourceforge.net/[+http://aufs.sourceforge.net/+]

http://en.wikipedia.org/wiki/Aufs[+http://en.wikipedia.org/wiki/Aufs+]

BeeGFS
^^^^^^

A high-performance parallel file system from the Fraunhofer Center for High
Performance Computing.
BeeGFS is a pure software solution for scale-out parallel network-accessible
storage, developed with a strong focus on performance and designed for very
easy installation and management. If I/O intensive workloads are your problem,
BeeGFS is the solution.

BeeGFS provides a common file system for shared access to multiple clients and
transparently spreads user data across multiple servers. By increasing the
number of servers and/or disks in the system, you can simply scale performance
and capacity of the file system to the level that you need.

http://www.fhgfs.com/cms/[+http://www.fhgfs.com/cms/+]

http://moo.nac.uci.edu/\~hjm/fhgfs_vs_gluster.html[+http://moo.nac.uci.edu/~hjm/fhgfs_vs_gluster.html+]

[[Ceph]]
Ceph
^^^^

Ceph is a free software storage platform designed to present object, block,
and file storage from a single distributed computer cluster. Ceph's main goals
are to be completely distributed without a single point of failure, scalable
to the exabyte level, and freely-available. The data is replicated, making it
fault tolerant.
Ceph software runs on commodity hardware. The system is designed to be both
self-healing and self-managing and strives to reduce both administrator and
budget overhead.

http://ceph.com/[+http://ceph.com/+]

http://en.wikipedia.org/wiki/Ceph_%28software%29[+http://en.wikipedia.org/wiki/Ceph_%28software%29+]

CernVM-fS
^^^^^^^^^

A network file system based on HTTP and optimized to deliver experiment
software in a fast, scalable, and reliable way. Files and file metadata are
aggressively cached and downloaded on demand. Thereby the CernVM-FS decouples
the life cycle management of the application software releases from the
operating system.

http://cernvm.cern.ch/portal/filesystem[+http://cernvm.cern.ch/portal/filesystem+]

[[Chirp]]
Chirp
^^^^^

Chirp is a user-level file system for collaboration across distributed systems
such as clusters, clouds, and grids. Chirp allows ordinary users to discover,
share, and access storage, whether within a single machine room or over a wide
area network.

Chirp requires no special privileges. Unlike most standard filesystems or
storage services, Chirp does not require root access, kernel changes, special
modules, or anything like that. It can be run by ordinary users to export
ordinary filesystems on any machine or port that you like.

Chirp is transparent. When used with xref:Parrot[Parrot] or
xref:FUSE[FUSE], Chirp servers can be
transparently attached to existing ordinary applications -- like tcsh, vi, and
perl -- without any sort of kernel changes or special privileges. Chirp is
designed to give maximum compatibility with standard Unix semantics. 

Chirp is easy to deploy. Chirp is designed to be deployed with a minimum of
fuss. One simple command starts a Chirp server or a Chirp client. There is no
complex configuration, installation, or setup to mess up. It just works. This
makes Chirp ideal for on-the-fly storage management in batch computing and
grid computing environments. 

http://www.cse.nd.edu/\~ccl/software/chirp/[+http://www.cse.nd.edu/~ccl/software/chirp/+]

[[FUSE]]
FUSE
^^^^

Filesystem in Userspace (FUSE) is an operating system mechanism for Unix-like
computer operating systems that lets non-privileged users create their own
file systems without editing kernel code. This is achieved by running file
system code in user space while the FUSE module provides only a "bridge" to
the actual kernel interfaces.

FUSE is particularly useful for writing virtual file systems. Unlike
traditional file systems that essentially save data to and retrieve data from
disk, virtual filesystems do not actually store data themselves. They act as a
view or translation of an existing file system or storage device.

http://fuse.sourceforge.net/[+http://fuse.sourceforge.net/+]

http://sourceforge.net/p/fuse/wiki/FileSystems/[+http://sourceforge.net/p/fuse/wiki/FileSystems/+]

http://en.wikipedia.org/wiki/Filesystem_in_Userspace[+http://en.wikipedia.org/wiki/Filesystem_in_Userspace+]

http://www.ibm.com/developerworks/linux/library/l-fuse/index.html[+http://www.ibm.com/developerworks/linux/library/l-fuse/index.html+]

http://lwn.net/Articles/68104/[+http://lwn.net/Articles/68104/+]

Gluster
^^^^^^^

GlusterFS is an open source, distributed file system capable of scaling to
several petabytes (actually, 72 brontobytes!) and handling thousands of
clients. GlusterFS clusters together storage building blocks over Infiniband
RDMA or TCP/IP interconnect, aggregating disk and memory resources and
managing data in a single global namespace. GlusterFS is based on a stackable
user space design and can deliver exceptional performance for diverse
workloads.

http://www.gluster.org/[+http://www.gluster.org/+]

HTTPFS
^^^^^^

The goal of the present HTTPFS project is to enable access to remote files,
directories, and other containers (e.g., structured text documents, OS tables)
through an HTTP pipe. HTTPFS system permits retrieval, creation and
modification of these resources as if they were regular files and directories
on a local filesystem. The remote host can be any UNIX or Win9x/WinNT box that
is capable of running a Perl CGI script, and accessible either directly or via
a web proxy or a gateway. HTTPFS runs entirely in user space. The current
implementation fully supports reading as well as creating, writing, appending,
and truncating of files on a remote HTTP host. HTTPFS provides an isolation
level for concurrent file access stronger than the one mandated by POSIX file
system semantics, closer to that of AFS. Both a programmatic interface with
familiar open(), read(), write(), close(), etc. calls, and an interactive
interface, via the popular Midnight Commander file browser, are provided. 

http://okmij.org/ftp/HTTP-VFS.html[+http://okmij.org/ftp/HTTP-VFS.html+]

http://okmij.org/USENIX99/[+http://okmij.org/USENIX99/+]

Lustre
^^^^^^

An open-source, parallel file system that provides a POSIX compliant file
system interface, can scale to thousands of clients, petabytes of storage and
hundreds of gigabytes per second of I/O bandwidth. The key components of the
Lustre file system are the Metadata Servers (MDS), the Metadata Targets (MDT),
Object Storage Servers (OSS), Object Server Targets (OST) and the Lustre
clients.

The ability of a Lustre file system to scale capacity and performance for any
need reduces the need to deploy many separate file systems, such as one for
each compute cluster. Storage management is simplified by avoiding the need to
copy data between compute clusters. In addition to aggregating storage
capacity of many servers, the I/O throughput is also aggregated and scales
with additional servers. Moreover, throughput and/or capacity can be easily
increased by adding servers dynamically.

http://lustre.org/[+http://lustre.org/+]

[[Parrot]]
Parrot
^^^^^^

Parrot is a tool for attaching old programs to new storage systems. Parrot
makes a remote storage system appear as a file system to a legacy application.
Parrot does not require any special privileges, any recompiling, or any change
whatsoever to existing programs. It can be used by normal users doing normal
tasks.

Parrot is useful to users of distributed systems, because it frees them from
rewriting code to work with new systems and relying on remote administrators
to trust and install new software. Parrot is also useful to developers of
distributed systems, because it allows rapid deployment of new code to real
applications and real users that do not have the time, inclination, or
permissions to build a kernel-level filesystem. 

Parrot "speaks" a variety of remote I/O services include HTTP, FTP, GridFTP,
iRODS, HDFS, XRootD, GROW, and Chirp on behalf of ordinary programs. It works
by trapping a program's system calls through the ptrace debugging interface,
and replacing them with remote I/O operations as desired.

http://www.cse.nd.edu/\~ccl/software/parrot/[+http://www.cse.nd.edu/~ccl/software/parrot/+]

Robinhood Policy Engine
^^^^^^^^^^^^^^^^^^^^^^^

Robinhood Policy Engine is a versatile tool to manage contents of large file systems. It maintains a replicate of filesystem medatada in a database that can be queried at will. It makes it possible to schedule mass action on filesystem entries by defining attribute-based policies, provides fast 'find' and 'du' enhanced clones, gives to administrators an overall view of filesystem contents through its web UI and command line tools.
It supports any POSIX filesystem and implements advanced features for Lustre filesystems (list/purge files per OST or pool, read MDT changelogs...)

Originally developped for HPC, it has been designed to perform all its tasks in parallel, so it is particularly adapted for running on large filesystems with millions of entries and petabytes of data. But of course, you can take benefits of all its features for managing smaller filesystems, like '/tmp' of workstations.

https://github.com/cea-hpc/robinhood/wiki[+https://github.com/cea-hpc/robinhood/wiki+]

http://arxiv.org/abs/1505.01448[+http://arxiv.org/abs/1505.01448+]

[[file_transfer]]
file transfer
~~~~~~~~~~~~~

[[WDT]]
WDT
^^^

Warp speed Data Transfer (WDT) is an embeddedable library (and command line tool) aiming to transfer data between 2 systems as fast as possible over multiple TCP paths.

https://github.com/facebook/wdt[+https://github.com/facebook/wdt+]

Fiona
~~~~~

Fiona is designed to be simple and dependable. It focuses on reading and
writing data in standard Python IO style, and relies upon familiar Python
types and protocols such as files, dictionaries, mappings, and iterators
instead of classes specific to OGR. Fiona can read and write real-world data
using multi-layered GIS formats and zipped virtual file systems and integrates
readily with other Python GIS packages such as pyproj, Rtree, and Shapely.

https://github.com/Toblerity/Fiona[+https://github.com/Toblerity/Fiona+]

Fireflies
~~~~~~~~~

Fireflies (formally "DSvis") is a tool that allows you to visually explore (in 2D or 3D) the dynamics of arbitrary systems of ordinary differential equations (ODEs). It does this by using the GPU (via the OpenCL library) to simulate a large number of independent particles according to the specified system of ODEs. By integrating one group of particles forwards in time and the other backwards, the stable and unstable attractors of the system are revealed and the way in which they change with the system's parameters can be seen interactively.

https://bitbucket.org/rmerrison/fireflies[+https://bitbucket.org/rmerrison/fireflies+]

http://arxiv.org/abs/1505.00344[+http://arxiv.org/abs/1505.00344+]

[[Flex_Projector]]
Flex Projector
~~~~~~~~~~~~~~

Flex Projector is a freeware, cross-platform application for creating custom world map projections. The intuitive interface allows users to easily modify dozens of popular world map projections—the possibilities range from slight adjustments to making completely new projections. Flex Projector is intended as a tool for practicing mapmakers and students of cartography.

http://www.flexprojector.com/[+http://www.flexprojector.com/+]

FortranCL
~~~~~~~~~

FortranCL is an OpenCL interface for Fortran 90. It allows programmers to call
the OpenCL parallel programming framework directly from Fortran, so developers
can accelerate their Fortran code using graphical processing units (GPU) and
other accelerators.

The interface is designed to be as close to C OpenCL interface as possible,
while written in native Fortran 90 with type checking. It was originally
designed as an OpenCL interface to be used by the Octopus code.

The interface is not complete but provides all the basic calls required to
write a full Fortran 90 OpenCL program. 

http://code.google.com/p/fortrancl/[+http://code.google.com/p/fortrancl/+]

Freenet
~~~~~~~

Freenet is free software which lets you anonymously share files, browse and
publish "freesites" (web sites accessible only through Freenet) and chat on
forums, without fear of censorship. Freenet is decentralised to make it less
vulnerable to attack, and if used in "darknet" mode, where users only connect
to their friends, is very difficult to detect.

Communications by Freenet nodes are encrypted and are routed through other
nodes to make it extremely difficult to determine who is requesting the
information and what its content is.

Users contribute to the network by giving bandwidth and a portion of their
hard drive (called the "data store") for storing files. Files are
automatically kept or deleted depending on how popular they are, with the
least popular being discarded to make way for newer or more popular content.
Files are encrypted, so generally the user cannot easily discover what is in
his datastore, and hopefully can't be held accountable for it. Chat forums,
websites, and search functionality, are all built on top of this distributed
data store. 

https://freenetproject.org/[+https://freenetproject.org/+]

furious.js
~~~~~~~~~~

Furious.js is a scientific computing package for JavaScript that
was inspired by Numpy.

https://github.com/amd/furious.js[+https://github.com/amd/furious.js+]

FVOM
~~~~

A finite volume ocean model.

http://aforge.awi.de/gf/project/fvom/[+http://aforge.awi.de/gf/project/fvom/+]

http://aforge.awi.de/gf/project/fvom/scmsvn/[+http://aforge.awi.de/gf/project/fvom/scmsvn/+]

[[NG]]
////
NGGG
////

Gaigen
~~~~~~

Gaigen is a program which can generate implementations of geometric algebras.
It generates `Cxx` and C source code which implements a geometric algebra
requested by the user. The choice to create a program which generates
implementations of these algebras was made because we wanted performance
similar to optimized hand-written code, while maintaining full generality; for
(scientific) research and experimentation, many geometric algebras with
different dimensionality, signatures and other properties may be required.
Instead of coding each algebra by hand, Gaigen provides the possibility to
generate the code for exactly the geometric algebra the user requires. This
code may be less efficient than fully optimized hand-written code, but is
likely to be much more efficient than one library which tries to support all
possible algebras at once. Gaigen supports algebras with a dimension from 0 to
8. The implementation of products used in Gaigen becomes infeasable for
dimensions higher than about 7 or 8. For basis vectors, all 3 signatures are
supported (-1, 0, +1). It is also possible to create reciprocal pairs of null
vectors, which square to 0 with themselves, but to +1 or -1 with the other. 7
basic products are implemented (geometric product, outer product, left and
right contraction, scalar product, (modified) Hestenes inner product) plus the
outer morphism operator and the delta product. Several useful functions (such
as factorization, meet and join) have been implemented. Everything has been
designed with memory and time efficiency in mind. It is possible to optimize
Gaigen for your platform, application or processor by replacing the lowest
computation layer. Gaigen can suggest optimizations for the algebras you
generate with it by using the provided profiler function. Benchmarks in a ray
tracing application show that Gaigen is 30 to 60 times faster than CLU (`Cxx`).
In another application, Gaigen was 6000 times faster than Gable (Matlab).

http://g25.sourceforge.net/[+http://g25.sourceforge.net/+]

http://www.swmath.org/software/4958[+http://www.swmath.org/software/4958+]

http://staff.science.uva.nl/\~fontijne/g25.html[+http://staff.science.uva.nl/~fontijne/g25.html+]

http://www.science.uva.nl/research/isla/pub/gpce32-fontijne.pdf[+http://www.science.uva.nl/research/isla/pub/gpce32-fontijne.pdf+]

GALAHAD
~~~~~~~

GALAHAD is a thread-safe library of Fortran 2003 packages for solving
nonlinear optimization problems. At present, the areas covered by the library
are unconstrained and bound-constrained optimization, quadratic programming,
nonlinear programming, systems of nonlinear equations and inequalities, and
nonlinear least squares problems.

http://www.galahad.rl.ac.uk/[+http://www.galahad.rl.ac.uk/+]

Galois
~~~~~~

Galois is a system that automatically executes "Galoized" serial `Cxx` or Java
code in parallel on shared-memory machines. It works by exploiting amorphous
data-parallelism, which is present even in irregular codes that are organized
around pointer-based data structures such as graphs and trees. The Galois
system includes the Lonestar benchmark suite and the ParaMeter profiler. 

Multicore processors are becoming increasingly the norm. As a result, we need
to find ways to make it easier to write parallel programs. Galois allows the
programmer to write serial `Cxx` or Java code while still getting the
performance of parallel execution. All the programmer has to do is use
Galois-provided data structures, which are necessary for correct concurrent
execution, and annotate which loops should be run in parallel. The Galois
system then speculatively extracts as much parallelism as it can. The current
release includes a dozen sample benchmarks applications from a broad range of
domains that are written using the Galois extensions and classes. 

Lonestar and LonestarGPU benchmark collections are collections of widely-used real-world
applications that exhibit irregular behavior.

http://iss.ices.utexas.edu/?p=projects/galois[+http://iss.ices.utexas.edu/?p=projects/galois+]

Galry
~~~~~

Galry is a high performance interactive visualization package in Python based
on OpenGL. It allows to interactively visualize very large plots (tens of
millions of points) in real time, by using the graphics card as much as
possible.

Galry's high-level interface is directly inspired by Matplotlib and Matlab.
The low-level interface can be used to write complex interactive visualization
GUIs with Qt that deal with large 2D/3D datasets.

Visualization capabilities of Galry are not restricted to plotting, and
include textures, 3D meshes, graphs, shapes, etc. Custom shaders can also be
written for advanced uses.

https://github.com/rossant/galry[+https://github.com/rossant/galry+]

GASPI
~~~~~

PGAS (Partitioned Global Address Space) programming models have been discussed as an alternative to MPI for some time. The PGAS approach offers the developer an abstract shared address space which simplifies the programming task and at the same time facilitates: data-locality, thread-based programming and asynchronous communication. The goal of the GASPI project is to develop a suitable programming tool for the wider HPC-Community by defining a standard with a reliable basis for future developments through the PGAS-API of Fraunhofer ITWM. Furthermore, an implementation of the standard as a highly portable open source library will be available. The standard will also define interfaces for performance analysis, for which tools will be developed in the project. The evaluation of the libraries is done via the parallel re-implementation of industrial applications up to and including production status.

http://www.gaspi.de/en/project.html[+http://www.gaspi.de/en/project.html+]

http://arxiv.org/abs/1505.04628[+http://arxiv.org/abs/1505.04628+]

GPI-2
^^^^^

GPI-2 implements the GASPI specification (www.gaspi.de), an API specification
which originates from the ideas and concepts GPI. GPI-2 is an API for
asynchronous communication. It provides a flexible, scalable and fault
tolerant interface for parallel applications.

http://www.gpi-site.com/gpi2/[+http://www.gpi-site.com/gpi2/+]

http://www.gaspi.de/en/documents.html[+http://www.gaspi.de/en/documents.html+]

http://www.gpi-site.com/gpi2/tutorial/[+http://www.gpi-site.com/gpi2/tutorial/+]

Gazebo
~~~~~~

Robot simulation is an essential tool in every roboticist's toolbox. A
well-designed simulator makes it possible to rapidly test algorithms, design
robots, and perform regression testing using realistic scenarios. Gazebo
offers the ability to accurately and efficiently simulate populations of
robots in complex indoor and outdoor environments. At your fingertips is a
robust physics engine, high-quality graphics, and convenient programmatic and
graphical interfaces.

http://gazebosim.org/[+http://gazebosim.org/+]

G-Code
~~~~~~

G-code (also RS-274), which has many variants, is the common name for the most
widely used numerical control (NC) programming language. It is used mainly in
computer-aided manufacturing for controlling automated machine tools. G-code
is sometimes called G programming language.

In fundamental terms, G-code is a language in which people tell computerized
machine tools how to make something. The how is defined by instructions on
where to move, how fast to move, and through what path to move. The most
common situation is that, within a machine tool, a cutting tool is moved
according to these instructions through a toolpath, cutting away excess
material to leave only the finished workpiece. The same concept also extends
to noncutting tools such as forming or burnishing tools, photoplotting,
additive methods such as 3D printing, and measuring instruments.

http://en.wikipedia.org/wiki/G-code[+http://en.wikipedia.org/wiki/G-code+]

Skeinforge
^^^^^^^^^^

Skeinforge is a tool chain composed of Python scripts that converts your 3D
model into G-Code instructions for RepRap.

http://fabmetheus.crsndoo.com/wiki/index.php/Skeinforge[+http://fabmetheus.crsndoo.com/wiki/index.php/Skeinforge+]

http://reprap.org/wiki/Skeinforge[+http://reprap.org/wiki/Skeinforge+]

GDAL
~~~~

A translator library for raster and vector geospatial data formats that is
released under an X/MIT style Open Source license by the Open Source
Geospatial Foundation. As a library, it presents a single raster abstract data
model and vector abstract data model to the calling application for all
supported formats. It also comes with a variety of useful commandline
utilities for data translation and processing.

http://www.gdal.org/[+http://www.gdal.org/+]

Fiona
^^^^^

Reads and writes shapefiles using GDAL in the background and is therefore
quite fast. Intended as an easier to use alternative than the original Python
GDAL bindings.

Fiona is designed to be simple and dependable. It focuses on reading and
writing data in standard Python IO style and relies upon familiar Python types
and protocols such as files, dictionaries, mappings, and iterators instead of
classes specific to OGR. Fiona can read and write real-world data using
multi-layered GIS formats and zipped virtual file systems and integrates
readily with other Python GIS packages such as pyproj, Rtree, and Shapely.

https://github.com/Toblerity/Fiona[+https://github.com/Toblerity/Fiona+]

GDAL Python
^^^^^^^^^^^

Python bindings for the GDAL library.

https://pypi.python.org/pypi/GDAL/[+https://pypi.python.org/pypi/GDAL/+]

http://trac.osgeo.org/gdal/wiki/GdalOgrInPython[+http://trac.osgeo.org/gdal/wiki/GdalOgrInPython+]

http://www.gis.usu.edu/\~chrisg/python/[+http://www.gis.usu.edu/~chrisg/python/+]

geometryIO
^^^^^^^^^^

GDAL Python wrapper for reading and writing geospatial data to a variety of vector
formats.

https://github.com/invisibleroads/geometryIO[+https://github.com/invisibleroads/geometryIO+]

GeM
~~~

The GeM software is designed to automate the generation of determining
equations and related operations, in order to compute symmetries and
conservation laws for any ODE/PDE system, generally without limitations in DE
order and number of variables.

ODE/PDE systems containing arbitrary functions and/or constants can be
analyzed, and classes of functions for which additional symmetries /
conservation laws occur can be isolated.

GeM output (determining equations) is usually fed into Maple "rifsimp" (a
stable routine for differential reduction), which simplifies determining
equations, and performs case splits when the given system contains arbitrary
functions and/or constants.

GeM also contains special routines to output computed symmetries as well
as fluxes/densities of computed conservation laws.

http://math.usask.ca/\~shevyakov/gem/[+http://math.usask.ca/~shevyakov/gem/+]

http://math.usask.ca/\~shevyakov/publ/papers/afs_gem_cpc.pdf[+http://math.usask.ca/~shevyakov/publ/papers/afs_gem_cpc.pdf+]

http://math.usask.ca/\~shevyakov/research/students/olinov_2011.pdf[+http://math.usask.ca/~shevyakov/research/students/olinov_2011.pdf+]

GeoBases
~~~~~~~~

This project provides tools to play with geographical data. It also works with
non-geographical data, except for map visualizations.
There are embedded data sources in the project, but you can easily play with
your own data in addition to the available ones. Csv files containing data
about airports, train stations, countries, … are loaded, then you can:

* performs various types of queries ( find this key, or find keys with this
property)
* fuzzy searches based on string distance ( find things roughly named like
this)
* geographical searches ( find things next to this place)
* get results on a map, or export it as csv data, or as a Python object

http://opentraveldata.github.io/geobases/[+http://opentraveldata.github.io/geobases/+]

https://pypi.python.org/pypi/GeoBases/4.17.4[+https://pypi.python.org/pypi/GeoBases/4.17.4+]

GeoExt
~~~~~~

A JavaScript framework that combines the GIS functionality of OpenLayers with
the user interface of the ExtJS library.
It enables the construction of desktop-like GIS applications on the web.

http://geoext.github.io/geoext2/[+http://geoext.github.io/geoext2/+]

gxp
^^^

High-level components for GeoExt-based applications.

https://github.com/justb4/gxp[+https://github.com/justb4/gxp+]

Geogram
~~~~~~~

Geogram is a programming library of geometric algorithms. It includes a simple
yet efficient Mesh data structure (for surfacic and volumetric meshes), exact
computer arithmetics (a-la Shewchuck, implemented in GEO::expansion), a
predicate code generator (PCK: Predicate Construction Kit), standard geometric
predicates (orient/insphere), Delaunay triangulation, Voronoi diagram, spatial
search data structures, spatial sorting) and less standard ones (more general
geometric predicates, intersection between a Voronoi diagram and a triangular
or tetrahedral mesh embedded in n dimensions). The latter is used by
FWD/WarpDrive, the first algorithm that computes semi-discrete Optimal
Transport in 3d that scales up to 1 million Dirac masses.

http://alice.loria.fr/software/geogram/doc/html/index.html[+http://alice.loria.fr/software/geogram/doc/html/index.html+Geogram

geojs
~~~~~

JavaScript Geo visualization and Analysis Library.

https://github.com/OpenGeoscience/geojs[+https://github.com/OpenGeoscience/geojs+]

http://code.google.com/p/earth-api-utility-library/[+http://code.google.com/p/earth-api-utility-library/+]

GeoJSON
~~~~~~~

GeoJSON[1] is an open standard format for encoding collections of simple
geographical features along with their non-spatial attributes using JavaScript
Object Notation. The features include points (therefore addresses and
locations), line strings (therefore streets, highways and boundaries),
polygons (countries, provinces, tracts of land), and multi-part collections of
these types. GeoJSON features need not represent entities of the physical
world only; mobile routing and navigation apps, for example, might describe
their service coverage using GeoJSON.[2]

The GeoJSON format differs from other GIS standards in that it was written and
is maintained not by a formal standards organization, but by an Internet
working group of developers.

http://geojson.org/[+http://geojson.org/+]

http://en.wikipedia.org/wiki/GeoJSON[+http://en.wikipedia.org/wiki/GeoJSON+]

PyGeoj
^^^^^^

A simple Python GeoJSON file reader and writer.
PyGeoj treats GeoJSON as an actual file format instead of a set
of formatting rules.

https://github.com/karimbahgat/PyGeoj[+https://github.com/karimbahgat/PyGeoj+]

https://thepythongischallenge.wordpress.com/2014/07/25/pygeoj-a-simple-python-geojson-file-reader-and-writer/[+https://thepythongischallenge.wordpress.com/2014/07/25/pygeoj-a-simple-python-geojson-file-reader-and-writer/+]

python-geojson
^^^^^^^^^^^^^^

Python bindings and utilities for GeoJSON.

https://pypi.python.org/pypi/geojson[+https://pypi.python.org/pypi/geojson+]

https://github.com/frewsxcv/python-geojson[+https://github.com/frewsxcv/python-geojson+]

TopoJSON
^^^^^^^^

An extension of GeoJSON that encodes topology. Rather than representing
geometries discretely, geometries in TopoJSON files are stitched together from
shared line segments called arcs.[18] Arcs are sequences of points, while line
strings and polygons are defined as sequences of arcs. Each arc is defined
only once, but can be referenced several times by different shapes, thus
reducing redundancy and decreasing the file size.[19] In addition, TopoJSON
facilitates applications that use topology, such as topology-preserving shape
simplification, automatic map coloring, and cartograms.

https://github.com/mbostock/topojson[+https://github.com/mbostock/topojson+]

https://github.com/mbostock/topojson/wiki[+https://github.com/mbostock/topojson/wiki+]

GeoKettle
~~~~~~~~~

A powerful, metadata-driven Spatial ETL tool dedicated to the integration of
different spatial data sources for building and updating geospatial data
warehouses. GeoKettle enables the Extraction of data from data sources, the
Transformation of data in order to correct errors, make some data cleansing,
change the data structure, make them compliant to defined standards, and the
Loading of transformed data into a target DataBase Management System (DBMS) in
OLTP or OLAP/SOLAP mode, GIS file or Geospatial Web Service.

http://www.spatialytics.org/projects/geokettle/[+http://www.spatialytics.org/projects/geokettle/+]

GeoMapApp
~~~~~~~~~

GeoMapApp is an earth science exploration and visualization application that
is continually being expanded as part of the Marine Geoscience Data System
(MGDS) at the Lamont-Doherty Earth Observatory of Columbia University. The
application provides direct access to the Global Multi-Resolution Topography
(GMRT) compilation that hosts high resolution (~100 m node spacing) bathymetry
from multibeam data for ocean areas and ASTER (Advanced Spaceborne Thermal
Emission and Reflection Radiometer) and NED (National Elevation Dataset)
topography datasets for the global land masses.

http://www.geomapapp.org/[+http://www.geomapapp.org/+]

GEOS-Chem
~~~~~~~~~

The GEOS–Chem model is a global three-dimensional model of tropospheric
chemistry driven by assimilated meteorological observations from the Goddard
Earth Observing System (GEOS) of the NASA Global Modeling Assimilation Office.

GEOS–Chem began as a merging of Mian Chin's GEOS–CTM code with the emissions,
dry deposition, and chemistry routines from the old Harvard–GISS 9-layer
model. Since then, we have added many updates and improvements to GEOS–Chem.
The model now uses detailed inventories for fossil fuel, biomass burning,
biofuel burning, biogenic, and aerosol emissions. GEOS–Chem includes
state-of-the-art transport (TPCORE) and photolysis (FAST–J) routines, as well
as the SMVGEAR II chemistry solver package. Detailed aerosol microphysical
simulations using GEOS–Chem may performed with the TOMAS aerosol microphysics
code or the APM aerosol microphysics code.

GEOS–Chem has been parallelized using the xref:OpenMP[OpenMP] compiler directives, and it
scales well when running across multiple CPU's on shared-memory machines. We
are currently building a Grid-Independent version of GEOS-Chem in order to
take advantage of distributed memory architectures and MPI parallelization.

Several software tools facilitate the visualization of GEOS-Chem model
outputs. The IDL-based GAMAP package—which is developed and maintained by the
GEOS–Chem Support Team—allows for easy generation of a wide variety of plots
and animations. Furthermore, several members of the GEOS-Chem user community
are now developing open-source software visualization tools for other computer
languages, including Matlab, NCL, R, and Python.

http://wiki.seas.harvard.edu/geos-chem/index.php/Main_Page[+http://wiki.seas.harvard.edu/geos-chem/index.php/Main_Page+]

geoscience data servers
~~~~~~~~~~~~~~~~~~~~~~~

DChart/Dapper
^^^^^^^^^^^^^

The Dapper Data Viewer (aka DChart) allows you to visualize and download
in-situ oceanographic or atmospheric data from file or OpenDap server.
Features include an interactive map that is draggable, an in-situ station
layer that allows you to select data stations, and a plot window that allows
you to plot data from one or more stations. Three plot types are supported
(profile, property-property, and time series) and users can interact directly
with the plot to pan or zoom in and out.

Dapper is an OPeNDAP/Java-based web server developed by the EPIC group at PMEL
that provides networked access to in-situ and gridded data.
The Dapper servlet contains a set of configurable services that convert
in-situ or gridded data to the OPeNDAP protocol. 

http://www.epic.noaa.gov/epic/software/dchart/index.html[+http://www.epic.noaa.gov/epic/software/dchart/index.html+]

ERDDAP
^^^^^^

ERDDAP is a data server that gives you a simple, consistent way to download
subsets of gridded and tabular scientific datasets in common file formats and
make graphs and maps. This particular ERDDAP installation has oceanographic
data.

http://coastwatch.pfeg.noaa.gov/erddap/index.html[+http://coastwatch.pfeg.noaa.gov/erddap/index.html+]

http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html[+http://coastwatch.pfeg.noaa.gov/erddap/download/setup.html+]

GI-cat
^^^^^^

An implementation of a broker catalog service that
allows clients to discover and evaluate geoinformation resources over a
federation of data sources, and publishes different catalog interfaces,
allowing different clients to use the service.
A data provider can deploy his/her own GI-cat instance, grouping together
disparate data sources, to accommodate his/her users' needs.

GI-cat features caching and mediation capabilities and can act as a broker
towards disparate catalog and access services: by implementing metadata
harmonization and protocol adaptation, it is able to transform query results
to a uniform and consistent interface. GI-cat is based on a service-oriented
framework of modular components and can be customized and tailored to support
different deployment scenarios.

GI-cat can access a multiplicity of catalogs services, as well as inventory
and access services to discover, and possibly access, heterogeneous ESS
resources. Specific components implement mediation services for interfacing
heterogeneous service providers which expose multiple standard specifications;
they are called Accessors. These mediating components map the heterogeneous
providers metadata models into a uniform data model which implements 
ISO 19115, based on official ISO 19139 schemas and its extensions
(check out more information about the internal GI-cat format) . Accessors also
implement the query protocol mapping; they translate the query requests
expressed according to the interface protocols exposed by GI-cat, into the
multiple query dialects spoken by the resource service providers. Currently, a
number of well-accepted catalog and inventory services are supported,
including several OGC Web Services (e.g. WCS, WMS), xref:THREDDS[THREDDS] Data Server,
SeaDataNet Common Data Index, and GBIF.

http://essi-lab.eu/do/view/GIcat[+http://essi-lab.eu/do/view/GIcat+]

Hyrax
^^^^^

Hyrax is a new data server which combines the efforts at UCAR/HAO to build a
high performance DAP-compliant data server for the Earth System Grid II
project with existing software developed by OPeNDAP. 

Hyrax uses the Java servlet mechanism to hand off requests from a general web
daemon to DAP format-specific software. This results in higher performance for
small requests. The servlet front end, which we call the OPeNDAP Lightweight
Front end Server (OLFS) looks at each request and formulates a query to a
second server (which may or may not on the same machine as the OLFS) called
the Back End Server (BES).

The BES is the high-performance server software from HAO. It handles reading
data from the data stores and returning DAP-compliant responses to the OLFS.
In turn, the OLFS may pass these response back to the requestor with little or
no modification or it may use them to build more complex responses. The nature
of the Inter Process Communication (IPC) between the OLFS and BES is such that
they should both be on the same machine or be able to communicate over a very
high bandwidth channel. 

http://www.opendap.org/download/hyrax[+http://www.opendap.org/download/hyrax+]

LAS
^^^

The Live Access Server (LAS) is a highly configurable web server designed to
provide flexible access to geo-referenced scientific data. It can present
distributed data sets as a unified virtual data base through the use of DODS
networking. Ferret is the default visualization application used by LAS.

http://ferret.pmel.noaa.gov/Ferret/LAS/home/[+http://ferret.pmel.noaa.gov/Ferret/LAS/home/+]

[[TdsConfig]]
TdsConfig
xxxxxxxx+

Contents of TDS configuration directories for several variants including a TDS serving all Unidata IDD data.

https://github.com/Unidata/TdsConfig[+https://github.com/Unidata/TdsConfig+]

thredds-styles
xxxxxxxxxxxxxx

Styles for a THREDDS server.

https://github.com/asascience-open/thredds-styles[+https://github.com/asascience-open/thredds-styles+]

GeoVis
~~~~~~

Python Geographic Visualizer (GeoVis) is a standalone geographic visualization
module for the Python programming language intended for easy everyday-use by
novices and power-programmers alike. It has one-liners for quickly visualizing
a shapefile, building and styling basic maps with multiple shapefile layers,
and/or saving to imagefiles. Uses the built-in Tkinter or other third-party
rendering modules to do its main work.

https://github.com/karimbahgat/geovis[+https://github.com/karimbahgat/geovis+]

gestalt
~~~~~~~

A control system framework for personal fabrication. 

https://github.com/imoyer/gestalt[+https://github.com/imoyer/gestalt+]

pygestalt
^^^^^^^^^

Gestalt is a framework for building controllers for automated tools. It
enables you to import your machines as Python modules, and makes it easy to
connect machines to browser-based user interfaces.

http://pygestalt.org/[+http://pygestalt.org/+]

http://www.pygestalt.org/VMC_IEM.pdf[+http://www.pygestalt.org/VMC_IEM.pdf+]

http://mtm.cba.mit.edu/machines/stages/[+http://mtm.cba.mit.edu/machines/stages/+]

http://mtm.cba.mit.edu/[+http://mtm.cba.mit.edu/+]

GeStore
~~~~~~~

Up-to-date meta-databases are vital for the analysis of biological data. The
current exponential increase in biological data is also exponentially
increasing meta-database sizes. Large-scale meta-database management is
therefore an important challenge for platforms providing services for
biological data analysis. In particular, there is a need either to run an
analysis with a particular version of a meta-database, or to rerun an analysis
with an updated meta-database. We present our GeStore approach for biological
meta-database management. It provides efficient storage and runtime generation
of specific meta-database versions, and efficient incremental updates for
biological data analysis tools. The approach is transparent to the tools, and
we provide a framework that makes it easy to integrate GeStore with biological
data analysis frameworks. We present the GeStore system, as well as an
evaluation of the performance characteristics of the system, and an evaluation
of the benefits for a biological data analysis workflow. 

http://arxiv.org/abs/1503.07759[+http://arxiv.org/abs/1503.07759+]

https://github.com/EdvardPedersen/GeStore[+https://github.com/EdvardPedersen/GeStore+]

[[GHOST]]
GHOST
~~~~~

The General, Hybrid, and Optimized Sparse Toolkit (GHOST)  is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the "MPI+X" paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. 

https://bitbucket.org/essex/ghost[+https://bitbucket.org/essex/ghost+]

http://arxiv.org/abs/1507.08101[+http://arxiv.org/abs/1507.08101+]

[[GHOST-apps]]
GHOST-apps
^^^^^^^^^^

Applications for the GHOST package.

https://bitbucket.org/essex/ghost-apps[+https://bitbucket.org/essex/ghost-apps+]

[[GIS]]
GIS
~~~

[[SAGA]]
SAGA
^^^^

The System for Automated Geoscientific Analyses is a GIS
package with immense capabilities for geodata processing and analysis. SAGA is programmed in the object oriented `Cxx` language and supports the implementation of new functions with a very effective Application Programming Interface (API). Functions are organised as modules in framework independent Module Libraries and can be accessed via SAGA’s Graphical User Interface (GUI) or various scripting environments (shell scripts, Python, R, etc.).

http://www.saga-gis.org/en/index.html[+http://www.saga-gis.org/en/index.html+]

http://sourceforge.net/projects/saga-gis/[+http://sourceforge.net/projects/saga-gis/+]

http://www.geosci-model-dev.net/8/1991/2015/gmd-8-1991-2015.html[+http://www.geosci-model-dev.net/8/1991/2015/gmd-8-1991-2015.html+]

git
~~~

Da bomb.

*Building maintainable step-by-step tutorials with Git* - http://info.meteor.com/blog/step-by-step-tutorials-with-git[+http://info.meteor.com/blog/step-by-step-tutorials-with-git+]

git-arr
^^^^^^^

A  git repository browser that can generate static HTML instead of having to
run dynamically.

It is smaller, with less features and a different set of tradeoffs than other
similar software, so if you're looking for a robust and featureful git
browser, please look at gitweb or cgit instead.

However, if you want to generate static HTML at the expense of features, then
it can be useful.

http://blitiri.com.ar/p/git-arr/[+http://blitiri.com.ar/p/git-arr/+]

git-cola
^^^^^^^^

A sleek and powerful git GUI that
is written in Python.

http://git-cola.github.io/[+http://git-cola.github.io/+]

gitfs
^^^^^

A version controlled file system.

http://www.presslabs.com/gitfs/[+http://www.presslabs.com/gitfs/+]

git-hub
^^^^^^^

The hub subcommand for git, allows you to perform many of the operations made
available by GitHub's v3 REST API, from the git commandline command.

You can fork, create, delete and modify repositories. You can get information
about users, repositories and issues. You can star, watch and follow things,
and find out who else is doing the same. The API is quite extensive. With this
command you can do many of your day to day GitHub actions without needing a
web browser.

You can also chain commands together using the output of one as the input of
another. For example you could use this technique to clone all the repos of a
GitHub user or organization, with one command. 

https://github.com/ingydotnet/git-hub[+https://github.com/ingydotnet/git-hub+]

GitLab
^^^^^^

GitLab is an advanced Git-repository manager. It introduces a powerful code
review and issue-tracking system, complete with GitLab CI: a powerful
continuous integration tool.

https://about.gitlab.com/[+https://about.gitlab.com/+]

Gitless
^^^^^^^

Gitless is an experimental version control system built on top of Git. Many
people complain that Git is hard to use. We think the problem lies deeper than
the user interface, in the concepts underlying Git. Gitless is an experiment
to see what happens if you put a simple veneer on an app that changes the
underlying concepts. Because Gitless is implemented on top of Git (could be
considered what Git pros call a 'porcelain' of Git), you can always fall back
on Git. And of course your coworkers you share a repo with need never know
that you're not a Git aficionado. 

http://gitless.com/[+http://gitless.com/+]

git-subrepo
^^^^^^^^^^^

This git command "clones" an external git repo into a subdirectory of your
repo. Later on, upstream changes can be pulled in, and local changes can be
pushed back. Simple.

https://github.com/ingydotnet/git-subrepo[+https://github.com/ingydotnet/git-subrepo+]

Git Town
^^^^^^^^

An open-source Git plugin.
It adds Git commands that make collaborative software development more efficient and safe.

http://www.git-town.com/[+http://www.git-town.com/+]

Gogs
^^^^

A self-hosted Git service written in Go.

http://gogs.io/[+http://gogs.io/+]

http://jbrodriguez.io/gogs-an-alternative-to-gitlab/[+http://jbrodriguez.io/gogs-an-alternative-to-gitlab/+]


kallithea
^^^^^^^^^

A source code management system that supports two leading version control
systems, Mercurial and Git, and has a web interface that is easy to use for
users and admins. You can install Kallithea on your own server and host
repositories for the version control system of your choice.

https://kallithea-scm.org/[+https://kallithea-scm.org/+]

paper-now
^^^^^^^^^

Create, edit and (optionally) display a journal article, entirely in GitHub.
In contrast to the more traditional process of +submit > peer review >
publish+
at PeerJ, or even the less formal preprints at PeerJ Preprints or arXiv, Paper
Now is an experiment to see where the future may go with scholarly
communication. Initially, it may be that co-authors collaborate either
privately or publicly on GitHub and then proceed to submitting to PeerJ or
other journals for formal peer-review or preprinting. Or perhaps this is where
the traditional medium of publication begins to diverge. There is no end goal
other than to see what the academic community wants, which is why this is
completely open to fork, extend, and build upon.

https://github.com/PeerJ/paper-now[+https://github.com/PeerJ/paper-now+]

Givaro
~~~~~~

Givaro is a `Cxx` library for arithmetic and algebraic computations.
Its main features are implementations of the basic arithmetic of many
mathematical entities: Primes fields, Extensions Fields, Finite Fields, Finite
Rings, Polynomials, Algebraic numbers, Arbitrary precision integers and
rationals (`Cxx` wrappers over gmp) It also provides data-structures and
templated classes for the manipulation of basic algebraic objects, such as
vectors, matrices (dense, sparse, structured), univariate polynomials (and
therefore recursive multivariate).
It contains different program modules and is fully compatible with the LinBox
linear algebra library and the
xref:KAAPI[KAAPI] kernel for Adaptative, Asynchronous
Parallel and Interactive programming.

http://givaro.forge.imag.fr/[+http://givaro.forge.imag.fr/+]

Gizeh
~~~~~

Gizeh is a Python library for vector graphics.
Gizeh is written on top of the module cairocffi, which is a Python binding of
the popular C library Cairo. Cairo is powerful, but difficult to learn and
use. Gizeh implements a few classes on top of Cairo that make it more
intuitive.

https://github.com/Zulko/gizeh[+https://github.com/Zulko/gizeh+]

GLMnet
~~~~~~

Lasso and elastic-net regularized generalized linear models.
This is a Matlab port for the extremely efficient procedures for fitting the
entire lasso or elastic-net path for linear regression, logistic and
multinomial regression, Poisson regression the Cox model.

http://web.stanford.edu/\~hastie/glmnet_matlab/[+http://web.stanford.edu/~hastie/glmnet_matlab/+]

Granger-causality
^^^^^^^^^^^^^^^^^

This page contains the codes for learning the Granger causality in different
settings. The codes are written in Matlab and depend on the GLMnet package for
performing Lasso.
Lasso-Granger is an efficient algorithm for learning the temporal dependency
among multiple time series based on variable selection using Lasso.
Copula-Granger extends the power of Lasso-Granger to non-linear datasets. It
uses the copula technique to separate the marginal properties of the joint
distribution from its dependency structure.

https://github.com/USC-Melady/Granger-causality/wiki[+https://github.com/USC-Melady/Granger-causality/wiki+]

glsim
~~~~~

We describe glsim, a `Cxx` library designed to provide routines to perform basic
housekeeping tasks common to a very wide range of simulation programs, such as
reading simulation parameters or reading and writing self-describing binary
files with simulation data. The design also provides a framework to add
features to the library while preserving its structure and interfaces.

http://www.sciencedirect.com/science/article/pii/S0010465511001597[+http://www.sciencedirect.com/science/article/pii/S0010465511001597+]

Glumpy
~~~~~~

Glumpy is a python library for scientific visualization that is both fast,
scalable and beautiful. Glumpy offers an intuitive interface between numpy and
modern OpenGL.

http://glumpy.github.io/[+http://glumpy.github.io/+]

GNU Radio
~~~~~~~~~

GNU Radio is a free software development toolkit that provides the signal
processing runtime and processing blocks to implement software radios using
readily-available, low-cost external RF hardware and commodity processors. It
is widely used in hobbyist, academic and commercial environments to support
wireless communications research as well as to implement real-world radio
systems.

GNU Radio applications are primarily written using the Python programming
language, while the supplied, performance-critical signal processing path is
implemented in `Cxx` using processor floating point extensions where available.
Thus, the developer is able to implement real-time, high-throughput radio
systems in a simple-to-use, rapid-application-development environment.

http://gnuradio.org/redmine/projects/gnuradio[+http://gnuradio.org/redmine/projects/gnuradio+]

http://www.slideshare.net/AlbertHuang314/intro-dsp[+http://www.slideshare.net/AlbertHuang314/intro-dsp+]

https://srlabs.de/decrypting_gsm/[+https://srlabs.de/decrypting_gsm/+]

http://www.rs-online.com/designspark/electronics/eng/blog/taking-the-raspberry-pi-2-for-a-test-drive-with-gnu-radio-2[+http://www.rs-online.com/designspark/electronics/eng/blog/taking-the-raspberry-pi-2-for-a-test-drive-with-gnu-radio-2+]

http://blog.nikseetharaman.com/gsm-network-characterization-using-software-defined-radio/[+http://blog.nikseetharaman.com/gsm-network-characterization-using-software-defined-radio/+]

gqrx
^^^^

Gqrx is a software defined radio receiver powered by the GNU Radio SDR
framework and the Qt graphical toolkit.
Gqrx supports many of the SDR hardware available, including Funcube Dongles,
rtl-sdr, HackRF and USRP devices.

http://gqrx.dk/[+http://gqrx.dk/+]

Go
~~

Go is an open source programming language that makes it easy to build simple,
reliable, and efficient software. 

https://golang.org/[+https://golang.org/+]

https://github.com/sbinet/golang-go[+https://github.com/sbinet/golang-go+]

Boom Filters
^^^^^^^^^^^^

Probabilistic data structures for processing continuous, unbounded streams.

Boom Filters are probabilistic data structures for processing continuous,
unbounded streams. This includes Stable Bloom Filters, Scalable Bloom Filters,
Counting Bloom Filters, Inverse Bloom Filters, Cuckoo Filters, several
variants of traditional Bloom filters, HyperLogLog, Count-Min Sketch, and
MinHash.

https://github.com/tylertreat/BoomFilters[+https://github.com/tylertreat/BoomFilters+]

[[GOLD]]
GOLD
~~~~

The Generalized Ocean Layer Dynamics and is a hybrid coordinate finite volume ocean model code funded by NOAA and developed by the ocean group at NOAA-GFDL and Princeton University. The GOLD code was used for the ocean model component of the Earth System Model ESM2G.

https://code.google.com/p/gold-omod/[+https://code.google.com/p/gold-omod/+]

http://www.gfdl.noaa.gov/ocean-model[+http://www.gfdl.noaa.gov/ocean-model+]

Google Earth Engine
~~~~~~~~~~~~~~~~~~~

Google Earth Engine brings together the world's satellite imagery — trillions
of scientific measurements dating back almost 40 years — and makes it
available online with tools for scientists, independent researchers, and
nations to mine this massive warehouse of data to detect changes, map trends
and quantify differences on the Earth's surface. Applications include:
detecting deforestation, classifying land cover, estimating forest biomass and
carbon, and mapping the world’s roadless areas.

https://earthengine.google.org/[+https://earthengine.google.org/+]

GPTIPS
~~~~~~

GPTIPS is a free symbolic data mining platform and interactive modelling
environment for MATLAB.

https://sites.google.com/site/gptips4matlab/[+https://sites.google.com/site/gptips4matlab/+]

http://arxiv.org/abs/1412.4690[+http://arxiv.org/abs/1412.4690+]

Gradle
~~~~~~

Gradle is an open source build automation system. Gradle can automate the
building, testing, publishing, deployment and more of software packages or
other types of projects such as generated static websites, generated
documentation or indeed anything else.

Gradle is a project automation tool that builds upon the concepts of Apache
Ant and Apache Maven and introduces a Groovy-based domain-specific language
(DSL) instead of the more traditional XML form of declaring the project
configuration.

Gradle was designed for multi-project builds which can grow to be quite large,
and supports incremental builds by intelligently determining which parts of
the build tree are up-to-date, so that any task dependent upon those parts
will not need to be re-executed.

The initial plugins are primarily focused around Java, Groovy and Scala
development and deployment, but more languages and project workflows are on
the roadmap.

https://github.com/gradle/gradle[+https://github.com/gradle/gradle+]

http://gradle.org/[+http://gradle.org/+]

Graphite
~~~~~~~~

Graphite is an open-source, distributed parallel simulator for multicore
architectures. Graphite is designed from the ground up for exploration of
future multicore processors containing dozens, hundreds, or even thousands of
cores. It provides high performance for fast design space exploration and
software development.

https://github.com/mit-carbon/Graphite[+https://github.com/mit-carbon/Graphite+]

http://groups.csail.mit.edu/carbon/?page_id=111[+http://groups.csail.mit.edu/carbon/?page_id=111+]

Graphite (3D)
~~~~~~~~~~~~~

Graphite is a research platform for computer graphics, 3D modeling and
numerical geometry.

http://alice.loria.fr/index.php/software/3-platform/22-graphite.html[+http://alice.loria.fr/index.php/software/3-platform/22-graphite.html+]

http://alice.loria.fr/WIKI/index.php/Graphite/Graphite[+http://alice.loria.fr/WIKI/index.php/Graphite/Graphite+]

Grappa
~~~~~~

Grappa makes an entire cluster look like a single, powerful, shared-memory
machine. By leveraging the massive amount of concurrency in large-scale
data-intensive applications, Grappa can provide this useful abstraction with
high performance. Unlike classic distributed shared memory (DSM) systems,
Grappa does not require spatial locality or data reuse to perform well.

Data-intensive, or "Big Data", workloads are an important class of large-scale
computations. However, the commodity clusters they are run on are not well
suited to these problems, requiring careful partitioning of data and
computation. A diverse ecosystem of frameworks have arisen to tackle these
problems, such as MapReduce, xref:Spark[Spark], Dryad, and GraphLab, which ease
development of large-scale applications by specializing to particular
algorithmic structure and behavior.

Grappa provides abstraction at a level high enough to subsume many performance
optimizations common to these data-intensive platforms. However, its
relatively low-level interface provides a convenient abstraction for building
data-intensive frameworks on top of. Prototype implementations of (simplified)
MapReduce, GraphLab, and a relational query engine have been built on Grappa
that out-perform the original systems.

http://grappa.io/[+http://grappa.io/+]

https://github.com/uwsampa/grappa[+https://github.com/uwsampa/grappa+]

http://sampa.cs.washington.edu/papers/grappa-tr-2014-02.pdf[+http://sampa.cs.washington.edu/papers/grappa-tr-2014-02.pdf+]

http://grappa.io/docs/grappa-uwt-may2014.pdf[+http://grappa.io/docs/grappa-uwt-may2014.pdf+]

[[GRIB]]
GRIB
~~~~

A nightmarish data format that's driven many a geoscientist to drink.

Other packages that can deal with this format are xref:CDO[CDO],
xref:IDV[IDV], xref:NetCDF_Java[NetCDF Java] and xref:PyNIO[PyNIO].

http://www.nco.ncep.noaa.gov/pmb/docs/on388/[+http://www.nco.ncep.noaa.gov/pmb/docs/on388/+]

[[GRIB_API]]
GRIB API
^^^^^^^^

An application program interface accessible from C, FORTRAN and Python programs developed for encoding and decoding WMO FM-92 GRIB edition 1 and edition 2 messages. A useful set of command line tools is also provided to give quick access to GRIB messages.

https://software.ecmwf.int/wiki/display/GRIB/Home[+https://software.ecmwf.int/wiki/display/GRIB/Home+]

*GRIB API Examples* - https://software.ecmwf.int/wiki/display/GRIB/GRIB%20API%20examples[+https://software.ecmwf.int/wiki/display/GRIB/GRIB%20API%20examples+]

[[wgrib]]
wgrib
^^^^^

A program to manipulate, inventory and decode GRIB files.

http://www.cpc.ncep.noaa.gov/products/wesley/wgrib.html[+http://www.cpc.ncep.noaa.gov/products/wesley/wgrib.html+]

[[wgrib2]]
wgrib2
^^^^^^

A greatly extended version of xref:wgrib[wgrib] that can handle both GRIB1 and GRIB2
files.

http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/index.html[+http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/index.html+]

[[zyGrib]]
zyGrib
^^^^^^

Visualization of weather data from files in GRIB 1 format.

http://www.zygrib.org/[+http://www.zygrib.org/+]

gRPC
~~~~

In gRPC a client application can directly call methods on a server application
on a different machine as if it was a local object, making it easier for you
to create distributed applications and services. As in many RPC systems, gRPC
is based around the idea of defining a service, specifying the methods that
can be called remotely with their parameters and return types. On the server
side, the server implements this interface and runs a gRPC server to handle
client calls. On the client side, the client has a stub that provides exactly
the same methods as the server.

gRPC clients and servers can run and talk to each other in a variety of
environments - from servers inside Google to your own desktop - and can be
written in any of gRPC's supported languages. So, for example, you can easily
create a gRPC server in Java with clients in Go, Python, or Ruby. In addition,
the latest Google APIs will have gRPC versions of their interfaces, letting
you easily build Google functionality into your applications.

http://www.grpc.io/[+http://www.grpc.io/+]

https://github.com/grpc[+https://github.com/grpc+]

GSI
~~~

The community GSI system is a variational data assimilation system, designed
to be flexible, state-of-art, and run efficiently on various parallel
computing platforms. The GSI system is in the public domain and is freely
available for community use. 

The Developmental Testbed Center (DTC) currently maintains and supports a
community version of the GSI system (now at Version 3.3). The testing and
support of this GSI system at the DTC currently focus on regional numerical
weather prediction (NWP) applications coupled with the Weather Research and
Forecasting (WRF) Model , but the GSI can be applied to Global Forecast
System(GFS) as well as other modelling systems. 

http://www.dtcenter.org/com-GSI/users/[+http://www.dtcenter.org/com-GSI/users/+]

GSL
~~~

The GNU Scientific Library (GSL) is a numerical library for C and `Cxx`
programmers.
The library provides a wide range of mathematical routines such as random
number generators, special functions and least-squares fitting. There are over
1000 functions in total with an extensive test suite.

http://www.gnu.org/software/gsl/[+http://www.gnu.org/software/gsl/+]

CythonGSL
^^^^^^^^^

CythonGSL provides a Cython interface for the GNU Scientific Library (GSL).
Cython is the ideal tool to speed up numerical computations by converting
typed Python code to C and generating Python wrappers so that these compiled
functions can be called from Python. Scientific programming often requires use
of various numerical routines (e.g. numerical integration, optimization).
While SciPy provides many of those tools, there is an overhead associated with
using these functions within your Cython code. CythonGSL allows you to shave
off that last layer by providing Cython declarations for the GSL which allow
you to use this high-quality library from within Cython without any Python
overhead.

https://github.com/twiecki/CythonGSL[+https://github.com/twiecki/CythonGSL+]

FGSL
^^^^

A portable, object-based Fortran interface to the GNU scientific library, a
collection of numerical routines for scientific computing.

http://www.lrz.de/services/software/mathematik/gsl/fortran/index.html[+http://www.lrz.de/services/software/mathematik/gsl/fortran/index.html+]

GSLL
^^^^

The GNU Scientific Library for Lisp (GSLL) allows you to use the GNU
Scientific Library (GSL) from Common Lisp. This library provides a full range
of common mathematical operations useful to scientific and engineering
applications. The design of the GSLL interface is such that access to most of
the GSL library is possible in a Lisp-natural way; the intent is that the user
not be hampered by the restrictions of the C language in which GSL has been
written. GSLL thus provides interactive use of GSL for getting quick answers,
even for someone not intending to program in Lisp. 

https://common-lisp.net/project/gsll/[+https://common-lisp.net/project/gsll/+]

O2scl
^^^^^

O2scl is a `Cxx` library for object-oriented numerical programming. It includes
interpolation, differentiation, integration, roots of polynomials, equation
solving, minimization, constrained minimization, Monte Carlo integration,
simulated annealing, least-squares fitting, solution of ordinary differential
equations, two-dimensional interpolation, Chebyshev approximation, unit
conversions, and file I/O with xref:HDF5[HDF5].

http://o2scl.sourceforge.net/[+http://o2scl.sourceforge.net/+]

https://github.com/awsteiner/o2scl[+https://github.com/awsteiner/o2scl+]

[[gtool5]]
gtool5
~~~~~~

A Fortran 90 input/output library, "gtool5", is developed for use with
numerical simulation models in the fields of Earth and planetary sciences. The
use of this library will simplify implementation of input/output operations
into program code in a consolidated form independent of the size and
complexity of the software and data. The library also enables simple
specification of the metadata needed for post-processing and visualization of
the data. These aspects improve the readability of simulation code, which
facilitates the simultaneous performance of multiple numerical experiments
with different software and efficiency in examining and comparing the
numerical results.

http://www.geosci-model-dev.net/5/449/2012/gmd-5-449-2012.html[+http://www.geosci-model-dev.net/5/449/2012/gmd-5-449-2012.html+]


http://www.gfd-dennou.org/library/gtool/gtool5.htm.en[+http://www.gfd-dennou.org/library/gtool/gtool5.htm.en+]

GUESS
~~~~~

GUESS is an exploratory data analysis and visualization tool for graphs and
networks. The system contains a domain-specific embedded language called
Gython (an extension of Python, or more specifically Jython) which supports
the operators and syntactic sugar necessary for working on graph structures in
an intuitive manner. An interactive interpreter binds the text that you type
in the interpreter to the objects being visualized for more useful
integration. GUESS also offers a visualization front end that supports the
export of static images and dynamic movies. 

http://graphexploration.cond.org/[+http://graphexploration.cond.org/+]

gun
~~~

Gun is a persisted distributed cache, part of a NoDB movement. It requires
zero maintenance and runs on your own infrastructure. Think of it as "Dropbox
for Databases" or a "Self-hosted Firebase". This is an early preview, so check
out the github and read on.

Everything gets cached, so your users experience lightning fast response
times. Since gun can be embedded anywhere javascript can run, that cache can
optionally be right inside your user's browser using localstorage fallbacks.
Updates are then pushed up to the servers when the network is available. 

http://gundb.io/[+http://gundb.io/+]

gvSIG CE
~~~~~~~~

gvSIG Community Edition (CE) is a community driven GIS project fork of gvSIG
that will be bundled with SEXTANTE and GRASS GIS. This project is not
supported by the gvSIG Association. gvSIG CE is not an official project of
gvSIG.
gvSIG CE is a fully functional Open Source Desktop GIS that provides powerful
visualization (including thematic maps, advanced symbology and labelling),
cartography, raster, vector and geoprocessing in a single, integrated software
suite.

http://gvsigce.org/[+http://gvsigce.org/+]

[[NH]]
////
NHHH
////

h20
~~~

An optimized HTTP server with support for HTTP/1.x and HTTP/2.
H2O is a very fast HTTP server written in C. It can also be used as a library.

https://github.com/h2o/h2o/[+https://github.com/h2o/h2o/+]

Habanero-C
~~~~~~~~~~

The Habanero-C (HC) language under development in the Habanero project at Rice
University builds on past work on Habanero-Java, which in turn was derived
from X10 v1.5.  HC serves as a research testbed for new compiler and runtime
software technologies for extreme scale systems for homogeneous and
heterogeneous processors.

Habanero-C is designed to be mapped onto hardware platforms with lightweight
system software stacks, such as the Customizable Heterogeneous Platform (CHP)
being developed in the NSF Expeditions Center for Domain-Specific Computing
(CDSC) which includes CPUs, GPUs, and FPGAs.  The C foundation also makes it
easier to integrate HC with communication middleware for cluster systems, such
as MPI and GASNet.

The Habanero-C compiler is written in `Cxx` and is built on top of the ROSE
compiler infrastructure, which was also used in the DARPA-funded PACE project
at Rice University.  The bulk of the Habanero-C runtime has been written from
scratch in portable ANSI C.  However, a few library routines for low-level
synchronization and atomic operations are written in assembly language for the
target platform.  To date, the Habanero-C runtime has been ported and tested
on Intel X86, Cyclops 64, Power7, Sun Niagara 2 and Intel SCC multicore
platforms.

https://wiki.rice.edu/confluence/display/HABANERO/Habanero-C[+https://wiki.rice.edu/confluence/display/HABANERO/Habanero-C+]

http://arxiv.org/abs/1407.4859[+http://arxiv.org/abs/1407.4859+]

hadoop_g5k
~~~~~~~~~~

The Apache Hadoop project provides an open-source framework for reliable,
scalable, distributed computing. As such, it can be deployed and used in the
Grid 5000 platform. However, its configuration and management may be sometimes
difficult, specially under the dynamic nature of clusters within Grid 5000
reservations. In turn, Execo offers a Python API to manage processes
execution. It is well suited for quick and easy creation of reproducible
experiments on distributed hosts.

The project presented here is called hadoop_g5k and provides a layer built on
top of Execo that allows to manage Hadoop clusters and prepare reproducible
experiments in Hadoop. It offers a set of scripts to be used in command-line
interfaces and a Python interface. 

https://www.grid5000.fr/mediawiki/index.php/Hadoop_On_Execo[+https://www.grid5000.fr/mediawiki/index.php/Hadoop_On_Execo+]

execo
^^^^^

A Python library that allows you to finely manage unix processes on thousands
of remote hosts. 

http://execo.gforge.inria.fr/doc/latest-stable/[+http://execo.gforge.inria.fr/doc/latest-stable/+]

https://github.com/lpouillo/execo-g5k-tools[+https://github.com/lpouillo/execo-g5k-tools+]

HClib
^^^^^

HClib is a library implementation of the Habanero-C language. The reference
HClib implementation is built on top of the Open Community Runtime (OCR).

http://habanero-rice.github.io/hclib/[+http://habanero-rice.github.io/hclib/+]

Haxe
~~~~

The Haxe programming language is a high level strictly typed programming
language which is used by the Haxe compiler to produce cross-platform native
code. The Haxe programming language is easy to learn if you are familiar
already with either Java, `Cxx`, PHP, AS3 or similar object oriented languages. The
Haxe programming language has been especially designed in order to adapt the
various platforms native behaviors and allow efficient cross-platform
development. 

The Haxe Compiler is responsible for translating the Haxe programming language
to the target platform native source code or binary. Each platform is natively
supported, without any overhead coming from running inside a virtual machine.
The Haxe Compiler is very efficient and can compile thousands of classes in
seconds. 

The Haxe standard library provides a common set of highly tested APIs that
gives you complete cross-platform behavior. This includes data structures,
maths and date, serialization, reflection, bytes, crypto, file system,
database access, etc. The Haxe standard library also includes
platform-specific API that gives you access to important parts of the platform
capabilities, and can be easily extended. 

The compiler targets include Flash, Neko, Javascript,
Actionscript 3, PHP, `Cxx`, Java, Csharp and Python.

Haxe is written in xref:OCaml[OCaml].

http://haxe.org/[+http://haxe.org/+]

https://github.com/HaxeFoundation/haxe[+https://github.com/HaxeFoundation/haxe+]

Haxe UI
^^^^^^^

Create cross-platform, rich user interfaces.
Quickly with a single framework.

http://haxeui.org/[+http://haxeui.org/+]

Massive
^^^^^^^

Massive provide a number of open source libraries and tools that are intended
to increase the quality, efficiency and consistency of cross-platform
development with Haxe.

http://open.massiveinteractive.com/[+http://open.massiveinteractive.com/+]

https://github.com/massiveinteractive[+https://github.com/massiveinteractive+]

node-webkit-haxelib
^^^^^^^^^^^^^^^^^^^

Haxelib which downloads node-webkit binary for your platform and makes it
accessible via +haxelib run node-webkit path/to/index.html+.
Node Webkit lets you run a Webkit shell on the desktop, meaning you can use
Haxe and HTML5 / JS technologies to build your app. It provides full access to
the NodeJS APIs so your app can integrate with the system.

https://github.com/as3boyan/node-webkit-haxelib[+https://github.com/as3boyan/node-webkit-haxelib+]

Waxe
^^^^

Use WxWidgets to create desktop apps with a truly native look and feel on all
major platforms. Works with the `Cxx` and Neko targets, and integrates with NME.

http://nmehost.com/waxe/[+http://nmehost.com/waxe/+]

https://github.com/nmehost/waxe[+https://github.com/nmehost/waxe+]

[[HDF5]]
HDF5
~~~~

This...

ESIO
^^^^

The ExaScale IO (ESIO) library provides simple, high throughput input and
output of structured data sets using parallel HDF5. ESIO is designed to
support reading and writing turbulence simulation restart files but it may be
useful in other contexts. The library is written in C99 and may be used by C89
or `Cxx` applications. A Fortran API built atop the F2003 standard ISO_C_BINDING
is also available.

https://red.ices.utexas.edu/projects/esio[+https://red.ices.utexas.edu/projects/esio+]

ExaHDF5
^^^^^^^

Our proposed work consists of three thrust areas that address these
contemporary challenges. First, we will provide high performance I/O
middleware that makes effective use of computational platforms, researching a
number of optimization strategies and deploying them through the HDF5
software. Second, we will improve the productivity of application developers
by hiding the complexity of parallel I/O via new auto-tuning and transparent
data re-organization techniques, and by extending our existing work in
easy-to-use, high-level APIs that expose scientific data models. Third, we
will facilitate scientific analysis for users by extending query-based
techniques, developing novel in situ analysis capabilities, and making sure
that visualization tools use best practices when reading HDF5 data.

https://sdm.lbl.gov/exahdf5/[+https://sdm.lbl.gov/exahdf5/+]

HDF5.jl
^^^^^^^

An interface to the HDF5 library for xref:Julia[Julia].

H5FDdsm
^^^^^^^

The H5FDdsm project provides a Virtual File Driver for HDF5, which can be
used to link two applications via a virtual file system. One application
(server/host) owns a memory buffer, which may be distributed over N processes
(DSM buffer) - the second application (client) writes to HDF5 in parallel
using M processes and the data is diverted to the DSM host, where it can be
read in parallel as if from disk. The file system is bypassed completely and
the data is transmitted using one of several network protocols (MPI or TCP
over sockets currently supported). Note that the interface can also be used
within the same application as a parallel data staging layer, in this case, no
connection is required and information is exchanged between processes using
MPI.

https://hpcforge.org/projects/h5fddsm/[+https://hpcforge.org/projects/h5fddsm/+]

https://hpcforge.org/plugins/mediawiki/wiki/h5fddsm/index.php/Main_Page[+https://hpcforge.org/plugins/mediawiki/wiki/h5fddsm/index.php/Main_Page+]

H5Part
^^^^^^

H5Part is a very simple data storage schema and provides an API that
simplifies the reading/writing of the data to the HDF5 file format.

H5Part is a very simple data storage schema and provides an API that
simplifies the reading/writing of the data to the HDF5 file format. An
important foundation for a stable visualization and data analysis environment
is a stable and portable file storage format and its associated APIs. The
presence of a "common file storage format," including associated APIs, will
help foster a fundamental level of interoperability across the project's
software infrastructure. It will also help ensure that key data analysis
capabilities are present during the earliest phases of the software
development effort.

https://codeforge.lbl.gov/projects/h5part/[+https://codeforge.lbl.gov/projects/h5part/+]

http://vis.lbl.gov/Research/AcceleratorSAPP/[+http://vis.lbl.gov/Research/AcceleratorSAPP/+]

http://vis.lbl.gov/Research/H5Part/[+http://vis.lbl.gov/Research/H5Part/+]

ICARUS
xxxxxx

ICARUS is a ParaView plug-in interfaced around the H5FDdsm driver for steering
and visualizing in-situ HDF5 output of simulation codes.

https://hpcforge.org/projects/icarus/[+https://hpcforge.org/projects/icarus/+]

h5py
^^^^

A Pythonic interface to the HDF5 binary data format.

http://www.h5py.org/[+http://www.h5py.org/+]

https://github.com/h5py/h5py[+https://github.com/h5py/h5py+]

h5serv
^^^^^^

A web service that implements a REST-based web service for HDF5 data stores.

https://github.com/HDFGroup/h5serv[+https://github.com/HDFGroup/h5serv+]

http://h5serv.readthedocs.org/en/latest/[+http://h5serv.readthedocs.org/en/latest/+]

HDVis
~~~~~

An important goal of scientific data analysis is to understand the behavior of
a system or process based on a sample of the system. In many instances it is
possible to observe both input parameters and system outputs, and characterize
the system as a high-dimensional function. Such data sets arise, for instance,
in large numerical simulations, as energy landscapes in optimization problems,
or in the analysis of image data relating to biological or medical parameters.
This paper proposes an approach to analyze and visualizing such data sets. The
proposed method combines topological and geometric techniques to provide
interactive visualizations of discretely sampled high-dimensional scalar
fields. The method relies on a segmentation of the parameter space using an
approximate Morse-Smale complex on the cloud of point samples. For each
crystal of the Morse-Smale complex, a regression of the system parameters with
respect to the output yields a curve in the parameter space. The result is a
simplified geometric representation of the Morse-Smale complex in the high
dimensional input domain. Finally, the geometric representation is embedded in
2D, using dimension reduction, to provide a visualization platform. The
geometric properties of the regression curves enable the visualization of
additional information about each crystal such as local and global shape,
width, length, and sampling densities. The method is illustrated on several
synthetic examples of two dimensional functions. Two use cases, using data
sets from the UCI machine learning repository, demonstrate the utility of the
proposed approach on real data. Finally, in collaboration with domain experts
the proposed method is applied to two scientific challenges. The analysis of
parameters of climate simulations and their relationship to predicted global
energy flux and the concentrations of chemical species in a combustion
simulation and their integration with temperature.

http://www.sci.utah.edu/software/hdvis.html[+http://www.sci.utah.edu/software/hdvis.html+]

Heartbeats
~~~~~~~~~~

Adaptive, or self-aware, computing has been proposed as one method to help
application programmers confront the growing complexity of multicore software
development.

However, existing approaches to adaptive systems are largely ad hoc and often
do not manage to incorporate the true performance goals of the applications
they are designed to support.

This project proposed an enabling technology for adaptive computing systems:
Application Heartbeats. The Application Heartbeats framework provides a
simple, standard programming interface that applications can use to indicate
their performance and system software (and hardware) can use to query an
application's performance. 

http://code.google.com/p/heartbeats/[+http://code.google.com/p/heartbeats/+]

http://dspace.mit.edu/handle/1721.1/61950[+http://dspace.mit.edu/handle/1721.1/61950+]

http://groups.csail.mit.edu/carbon/?page_id=94[+http://groups.csail.mit.edu/carbon/?page_id=94+]

Hector
~~~~~~

An open source, object-oriented, simple global climate carbon-cycle model. It runs essentially instantaneously while still representing the most critical global scale earth system processes, and is one of a class of models heavily used for for complex climate model emulation and uncertainty analyses.

https://github.com/JGCRI/hector[+https://github.com/JGCRI/hector+]

https://github.com/JGCRI/hector/wiki[+https://github.com/JGCRI/hector/wiki+]

http://www.geosci-model-dev.net/8/939/2015/gmd-8-939-2015.html[+http://www.geosci-model-dev.net/8/939/2015/gmd-8-939-2015.html+]

Hermes
~~~~~~

Hermes2D (Higher-order modular finite element system) is a `Cxx`/Python library
of algorithms for rapid development of adaptive hp-FEM solvers. hp-FEM is a
modern version of the finite element method (FEM) that is capable of extremely
fast, exponential convergence.

The Hermes library can be used for a large variety of PDE problems ranging
from linear elliptic equations to time-dependent nonlinear multi-physics PDE
systems arising in elasticity, structural mechanics, fluid mechanics,
acoustics, electromagnetics, and other fields of computational engineering and
science.

The Documentation for the Hermes libraries is an extensive set of
instructions, information and tutorials related to the use of Hermes and the
Finite Element Method. Hermes includes instructions for the installation of
collaborating Third Party Libraries (TPLs) as well as an introduction to the
mathematics behind the hp-FEM method and detailed instructions on the use and
modification of the code.

http://en.wikipedia.org/wiki/Hermes_Project[+http://en.wikipedia.org/wiki/Hermes_Project+]

HHVM
~~~~

HHVM is an open-source virtual machine designed for executing programs written
in Hack and PHP. HHVM uses a just-in-time (JIT) compilation approach to
achieve superior performance while maintaining the development flexibility
that PHP provides.

http://hhvm.com/[+http://hhvm.com/+]

HipGISAXS
~~~~~~~~~

A massively-parallel high-performance x-ray scattering data analysis code.
HipGISAXS is a massively parallel software, which we have developed using `Cxx`,
augmented with MPI, Nvidia xref:CUDA[CUDA], xref:OpenMP[OpenMP], and parallel-HDF5 libraries, on
large-scale clusters of multi/many-cores and graphics processors. HipGISAXS
currently supports *NIX based systems, and is able to harness computational
power from any general-purpose CPUs including state-of-the-art multicores, as
well as Nvidia GPUs and Intel MIC coprocessors. It is able to handle large
input data including any custom complex morphology as described in the
following, and perform GISAXS simulations at high resolutions.

http://portal.nersc.gov/project/als/hipgisaxs/[+http://portal.nersc.gov/project/als/hipgisaxs/+]

https://github.com/HipGISAXS/HipGISAXS/wiki[+https://github.com/HipGISAXS/HipGISAXS/wiki+]

HLib
~~~~

HLib is a program library for hierarchical matrices and H2-matrices.
H-matrices are a powerful tool for representing and working with dense (and
sparse) matrices, e.g. from integral or partial differential equations. They
allow the complete matrix algebra, e.g. matrix-vector multiplication, matrix
addition, multiplication, inversion and factorisation in almost linear time
with respect to the number of rows and columns.

HLIBpro contains various algorithms for the approximation of dense matrices,
e.g. ACA and HCA, the complete set of available H-algebra, various clustering
techniques, e.g. geometric and algebraic clustering, many functions for
discretising integral equations, e.g. Laplace, Helmholtz and Maxwell
equations.
A special focus of HLIBpro lies in the parallelisation of these methods to
shared (threads) and distributed memory machines (MPI).

http://www.hlib.org/[+http://www.hlib.org/+]

http://www.hlibpro.com/[+http://www.hlibpro.com/+]

HOMsPy
~~~~~~

Higher Order (Symplectic) Methods in Python are explicit algorithms for higher
order symplectic integration of a large class of Hamilton’s equations have
recently been discussed by Mushtaq et al. Here we present a Python program for
automatic numerical implementation of these algorithms for a given
Hamiltonian, both for double precision and multiprecision computations. We
provide examples of how to use this program, and illustrate behavior of both
the code generator and the generated solver module(s).

http://www.sciencedirect.com/science/article/pii/S0010465514000253[+http://www.sciencedirect.com/science/article/pii/S0010465514000253+]

http://arxiv.org/abs/1301.7736[+http://arxiv.org/abs/1301.7736+]

http://arxiv.org/abs/1310.2111[+http://arxiv.org/abs/1310.2111+]

[[HOOMD-blue]]
HOOMD-blue
~~~~~~~~~~

A general-purpose particle simulation toolkit. It scales from a single CPU core to thousands of GPUs.
You define particle initial conditions and interactions in a high-level python script. Then tell HOOMD-blue how you want to execute the job and it takes care of the rest. Python job scripts give you unlimited flexibility to create custom initialization routines, control simulation parameters, and perform in situ analysis.

http://codeblue.umich.edu/hoomd-blue/[+http://codeblue.umich.edu/hoomd-blue/+]

HOP
~~~

HOP is a multi-tier programming language for the Web 2.0 and the so-called
diffuse Web. It is designed for programming interactive web applications in
many fields such as multimedia (web galleries, music players, ...), ubiquitous
and house automation (SmartPhones, personal appliance), mashups, office (web
agendas, mail clients, ...), etc. 

HOP features include:

* an extensive set of widgets for programming fancy and portable Web GUIs,
* full compatibility with traditional Web technologies (JavaScript, HTML,
CSS),
* HTML5 support,
* a versatile Web server supporting HTTP/1.0 and HTTP/1.1,
* native multimedia support for enabling ubiquitous Web multimedia
applications,
* fast WebDAV level 1 support,
* an optimizing native code compiler for server code,
* an on-the-fly JavaScript compiler for client code,
* an extensive set of libraries for the mail, calendars, databases,
Telephony

http://hop.inria.fr/[+http://hop.inria.fr/+]

HOPE
~~~~

A Python Just-In-Time compiler for astrophysical computations.
In order to combine the ease of Python and the speed of `Cxx`, we developed
HOPE, a specialised Python just-in-time (JIT) compiler designed for numerical
astrophysical applications. HOPE focuses on a subset of the language and is
able to translate Python code into `Cxx` while performing numerical optimisation
on mathematical expressions at runtime. To enable the JIT compilation, the
user only needs to add a decorator to the function definition. We assess the
performance of HOPE by performing a series of benchmarks and compare its
execution speed with that of plain Python, `Cxx` and the other existing
frameworks. We find that HOPE improves the performance compared to plain
Python by a factor of 2 to 120, achieves speeds comparable to that of `Cxx`, and
often exceeds the speed of the existing solutions. 

https://github.com/cosmo-ethz/hope[+https://github.com/cosmo-ethz/hope+]

http://arxiv.org/abs/1410.4345[+http://arxiv.org/abs/1410.4345+]

HPC-GAP
~~~~~~~

HPC-GAP is the EPSRC funded project to reengineer the software for computation
in algebra and discrete mathematics to take advantage of the power of current
and future high-performance computers. Our main focus is on the GAP system and
the more recent SymGridPar middleware, which provide flexible and effective
computation on single processors and small clusters. We will adapt the
software to efficiently use large clusters of multi-core processors to perform
larger computations. To demonstrate the effectiveness of our adaptations we
will apply our new software to problems from a number of important areas of
pure mathematics. 

http://www-circa.mcs.st-and.ac.uk/hpcgap.php[+http://www-circa.mcs.st-and.ac.uk/hpcgap.php+]

ftp://ftp.gap-system.org/pub/gap/hpcgap-alpha/[+ftp://ftp.gap-system.org/pub/gap/hpcgap-alpha/+]

HPGL
~~~~

The High Performance Geostatistics Library is written in `Cxx`/Python to
realize some geostatistical algorithms.  The algorithms are called in Python,
by executing the corresponding commands.

http://hpgl.mit-ufa.com/[+http://hpgl.mit-ufa.com/+]

https://github.com/hpgl/hpgl[+https://github.com/hpgl/hpgl+]

HPGMG
~~~~~

HPGMG implements full multigrid (FMG) algorithms using finite-volume and
finite-element methods. Different algorithmic variants adjust the arithmetic
intensity and architectural properties that are tested. These FMG methods
converge up to discretization error in one F-cycle, thus may be considered
direct solvers. An F-cycle visits the finest level a total of two times, the
first coarsening (8x smaller) 4 times, the second coarsening 6 times, etc.

HPGMG-FV solves constant- and variable-coefficient elliptic problems on
isotropic Cartesian grids using Full Multigrid (FMG). The method is
second-order accurate in the max norm, as demonstrated by the FMG convergence.
FMG interpolation (prolongation) is linear and V-cycle interpolation and
restriction are piecewise constant. Recursive decomposition is used to
construct a space filling curve akin to Z-Mort in order to distribute work
among processes. Chebyshev polynomials are used for smoothing, preconditioned
by the diagonal. FMG convergence is observed with a fourth order Chebyshev
polynomial using a V(4,4) cycle. Thus convergence is reached in a total of 9
fine-grid operator applications (4 presmooths, residual, 4 postsmooths). This
makes HPGMG-FV extremeley fast and energy efficient.

https://bitbucket.org/hpgmg/hpgmg/[+https://bitbucket.org/hpgmg/hpgmg/+]

http://crd.lbl.gov/departments/computer-science/performance-and-algorithms-research/research/hpgmg/[+http://crd.lbl.gov/departments/computer-science/performance-and-algorithms-research/research/hpgmg/+]

HPX
~~~

HPX (High Performance ParalleX) is a general purpose `Cxx` runtime system for
parallel and distributed applications of any scale. It strives to provide a
unified programming model which transparently utilizes the available resources
to achieve unprecedented levels of scalability.  This library strictly adheres
to the `Cxx11` Standard and leverages the xref:Boost[Boost] `Cxx` Libraries which makes HPX
easy to use, highly optimized, and very portable.  HPX is developed for
conventional architectures including Linux-based systems, Windows, Mac, and
the BlueGene/Q, as well as accelerators such as the Xeon Phi.

The goal of HPX is to create a high quality, freely available, open source
implementation of the ParalleX model for conventional systems, such as classic
Linux based Beowulf clusters or multi-socket highly parallel SMP nodes. At the
same time, we want to have a very modular and well designed runtime system
architecture which would allow us to port our implementation onto new computer
system architectures. We want to use real world applications to drive the
development of the runtime system, coining out required functionalities and
converging onto a stable API which will provide a smooth migration path for
developers. The API exposed by HPX is modelled after the interfaces defined by
the `Cxx11/14` ISO standard and adheres to the programming guidelines used by
the xref:Boost[Boost] collection of `Cxx` libraries.

https://github.com/STEllAR-GROUP/hpx[+https://github.com/STEllAR-GROUP/hpx+]

http://stellar.cct.lsu.edu/projects/hpx/[+http://stellar.cct.lsu.edu/projects/hpx/+]

hpxpi
^^^^^

HPXPI is an implementation of the XPI specification on top of the HPX runtime
system. It is currently based on the XPI document version r313.

XPI (eXtreme Parallex Interface) is a programming interface for parallel
applications and systems based on the ParalleX execution model. XPI provides a
simple abstraction layer to the family of ParalleX implementation HPX runtime
system software. As HPX evolves, XPI insulates application codes from such
changes, ensuring stability of experimental application codes. XPI serves both
as a target for source-to-source compilers of high-level languages and as a
readible low-level programming interface syntax. XPI is experimental and
supports current on-going sponsored research projects. Its long term future is
entirely dependent on its resulting value; an unknown at this time. But it is
motivated by a shortterm need to advance key project goals.

https://github.com/STEllAR-GROUP/hpxpi[+https://github.com/STEllAR-GROUP/hpxpi+]

hpx_script
^^^^^^^^^^

Bindings for HPX for various scripting languages, currently Python and Lua. 

https://github.com/STEllAR-GROUP/hpx_script[+https://github.com/STEllAR-GROUP/hpx_script+]

HSL
~~~

HSL (formerly the Harwell Subroutine Library) is a collection of
state-of-the-art packages for large-scale scientific computation written and
developed by the Numerical Analysis Group at the STFC Rutherford Appleton
Laboratory and other experts. HSL offers users a high standard of reliability
and has an international reputation as a source of robust and efficient
numerical software. Among its best known packages are those for the solution
of sparse linear systems of equations and sparse eigenvalue problems. MATLAB
interfaces are offered for selected packages.

The Library was started in 1963 and was originally used at the Harwell
Laboratory on IBM mainframes running under OS and MVS. Over the years, the
Library has evolved and has been extensively used on a wide range of
computers, from supercomputers to modern PCs. Recent additions include
optimised support for multicore processors.

HSL packages are available at no cost for academic research and teaching. See
download links for individual packages in the catalogue. 

http://www.hsl.rl.ac.uk/[+http://www.hsl.rl.ac.uk/+]

htmlPy
~~~~~~

A wrapper around PySide's QtWebKit library. It helps with creating beautiful GUIs using HTML5, CSS3 and Javascript for standalone Python applications. It is built on Qt which makes it highly customizable and cross-platform. htmlPy is compatible with both Python2 and Python3. It can be used with any python library or environment like django, flask, scipy, virtualenv etc. You can use front-end libraries and frameworks like bootstrap, jQuery, jQuery UI etc. and create GUIs for your applications in no time.

http://amol-mandhane.github.io/htmlPy/[+http://amol-mandhane.github.io/htmlPy/+]

htmlwidgets
~~~~~~~~~~~

Bring the best of JavaScript data visualization to R.
Use JavaScript visualization libraries at the R console, just like plots.
Embed widgets in R Markdown documents and Shiny web applications.
Develop new widgets using a framework that seamlessly bridges R and
JavaScript.

http://www.htmlwidgets.org/[+http://www.htmlwidgets.org/+]

http://blog.rstudio.org/2014/12/18/htmlwidgets-javascript-data-visualization-for-r/[+http://blog.rstudio.org/2014/12/18/htmlwidgets-javascript-data-visualization-for-r/+]

HTTP servers
~~~~~~~~~~~~

Caddy
^^^^^

Other web servers were designed for the Web, but Caddy was designed for
humans, with today's Web in mind. 
Caddy supports HTTP/2, IPv6, Markdown, WebSockets, FastCGI, templates and
more, right out of the box. 

https://caddyserver.com/[+https://caddyserver.com/+]

Seastar
^^^^^^^

Seastar is an advanced, open-source `Cxx` framework for high-performance server
applications on modern hardware. Applications using Seastar can run on Linux
or OSv.

http://www.seastar-project.org/[+http://www.seastar-project.org/+]

https://github.com/cloudius-systems/seastar[+https://github.com/cloudius-systems/seastar+]

Hugo
~~~~

Hugo is a static site generator written in Go. It is optimized for speed, easy use and configurability. Hugo takes a directory with content and templates and renders them into a full HTML website.

Hugo relies on Markdown files with front matter for meta data. And you can run Hugo from any directory. This works well for shared hosts and other systems where you don’t have a privileged account.

https://github.com/spf13/hugo[+https://github.com/spf13/hugo+]

http://gohugo.io/[+http://gohugo.io/+]

hwloc
~~~~~

The Portable Hardware Locality (hwloc) software package provides a portable
abstraction (across OS, versions, architectures, ...) of the hierarchical
topology of modern architectures, including NUMA memory nodes, sockets, shared
caches, cores and simultaneous multithreading. It also gathers various system
attributes such as cache and memory information as well as the locality of I/O
devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims
at helping applications with gathering information about modern computing
hardware so as to exploit it accordingly and efficiently.

http://www.open-mpi.org/projects/hwloc/[+http://www.open-mpi.org/projects/hwloc/+]

HyperDex
~~~~~~~~

An open source distributed consistent key-value datastore. It differentiates
from other distributed key-value datastores by claiming to offer consistency
and multi-dimensional hashing, on top of the usual performance, availability,
and throughput guarantees.  A performance test that measures performance of
HyperDex using identical setup to an independent study that evaluates the
performance of Cassandra, MongoDB, and HBase side-by-side shows HyperDex to
have superior throughput and latency.
Multi-dimensional hashing is achieved through a different mechanism called
hyperspace hashing than BigTable's multiple column approach. The consistency
guarantee is achieved through a novel chaining protocol.

HyperDex provides one of the richest APIs in the NoSQL space. Its support
datatypes is unparalleled by other sharded data stores. It supports bulk
asynchronous operations. And it boasts the fastest, consistent, online backups
in the industry. All of these are accessible with bindings from C, `Cxx`,
Python, Ruby, Java, Go, and Rust.

http://hyperdex.org/[+http://hyperdex.org/+]

http://en.wikipedia.org/wiki/HyperDex[+http://en.wikipedia.org/wiki/HyperDex+]

HyperSQL
~~~~~~~~

HSQLDB (HyperSQL DataBase) is the leading SQL relational database software
written in Java. It offers a small, fast multithreaded and transactional
database engine with in-memory and disk-based tables and supports embedded and
server modes. It includes a powerful command line SQL tool and simple GUI
query tools.

http://hsqldb.org/[+http://hsqldb.org/+]

[[NI]]
////
NIII
////

IBEX
~~~~

IBEX is a `Cxx` library for constraint processing over real numbers.
It provides reliable algorithms for handling non-linear constraints. In
particular, roundoff errors are also taken into account. It is based on
interval arithmetic and affine arithmetic.
The main feature of Ibex is its ability to build strategies declaratively
through the contractor programming paradigm. It can also be used as a
black-box solver.

It can be used to solve a variety of problems that can roughly be formulated
as to
find a reliable characterization with boxes (Cartesian product of
intervals) of sets implicitely defined by constraints.
Reliable means that all sources of uncertainty should be taken into account,
including:

* approximation of real numbers by floating-point numbers
* round-off errors
* linearization truncatures
* model parameter uncertainty
* measurement noise

http://www.ibex-lib.org/[+http://www.ibex-lib.org/+]

ICALAB
~~~~~~

ICALAB for Signal Processing and ICALAB for Image Processing are two
independent demo packages for MATLAB that implement a number of efficient
algorithms for ICA (independent component analysis) employing HOS (higher
order statistics), BSS (blind source separation) employing SOS (second order
statistics) and LP (linear prediction), and BSE (blind signal extraction)
employing various SOS and HOS methods. 

http://www.bsp.brain.riken.jp/ICALAB/[+http://www.bsp.brain.riken.jp/ICALAB/+]

[[IDV]]
IDV
~~~

A Java-based software framework for analyzing and visualizing geoscience data.
It uses the xref:VisAD[VisAD] and xref:NetCDF_Java[NetCDF Java] 
libraries and other Java-based utility packages. 

The IDV application is a geoscience display and analysis software system with many of the standard data displays that other Unidata software (e.g. GEMPAK and McIDAS) provide. It brings together the ability to display and work with satellite imagery, gridded data (for example, numerical weather prediction model output), surface observations, balloon soundings, NWS WSR-88D Level II and Level III RADAR data, and NOAA National Profiler Network data, all within a unified interface. It also provides 3-D views of the earth system and allows users to interactively slice, dice, and probe the data, creating cross-sections, profiles, animations and value read-outs of multi-dimensional data sets. The IDV can display any Earth-located data if it is provided in a known format.

http://www.unidata.ucar.edu/software/idv/[+http://www.unidata.ucar.edu/software/idv/+]

[[ImageJ]]
ImageJ
~~~~~~

Image processing and analysis in Java.
ImageJ can display, edit, analyze, process, save and print 8-bit, 16-bit and 32-bit images. It can read many image formats including TIFF, GIF, JPEG, BMP, DICOM, FITS and "raw". It supports "stacks", a series of images that share a single window. It is multithreaded, so time-consuming operations such as image file reading can be performed in parallel with other operations.

 It can calculate area and pixel value statistics of user-defined selections. It can measure distances and angles. It can create density histograms and line profile plots. It supports standard image processing functions such as contrast manipulation, sharpening, smoothing, edge detection and median filtering.

It does geometric transformations such as scaling, rotation and flips. Image can be zoomed up to 32:1 and down to 1:32. All analysis and processing functions are available at any magnification factor. The program supports any number of windows (images) simultaneously, limited only by available memory.

http://rsb.info.nih.gov/ij/[+http://rsb.info.nih.gov/ij/+]

[[IJBlob]]
IJBlob
^^^^^^

An ImageJ ilbrary to detect and analyze connected components (blobs) in
binary images.

https://github.com/thorstenwagner/ij-blob[+https://github.com/thorstenwagner/ij-blob+]

*IJBlob: An ImageJ Library for Connected Component Analysis and Shape Analysis* (online paper) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ae/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ae/+]

InfluxDB
~~~~~~~~

An open-source, distributed, time series database
with no external dependencies.
InfluxDB is a time series, metrics, and analytics database. It’s written in Go
and has no external dependencies. That means once you install it there’s
nothing else to manage (like Redis, ZooKeeper, HBase, or whatever).
InfluxDB is targeted at use cases for DevOps, metrics, sensor data, and
real-time analytics.

http://influxdb.com/[+http://influxdb.com/+]

https://github.com/influxdb[+https://github.com/influxdb+]

http://www.xaprb.com/blog/2014/03/02/time-series-databases-influxdb/[+http://www.xaprb.com/blog/2014/03/02/time-series-databases-influxdb/+]

Instrumentino
~~~~~~~~~~~~~

Instrumentino is an open-source modular graphical user interface framework for
controlling Arduino based experimental instruments. It expands the control
capability of Arduino by allowing instruments builders to easily create a
custom user interface program running on an attached personal computer.It
enables the definition of operation sequences and their automated running
without user intervention.

Acquired experimental data and a usage log are automatically saved on the
computer for further processing.

Complex devices, which are difficult to control using an Arduino, may be
integrated as well by incorporating third party application programming
interfaces (APIs) into the Instrumentino framework.

https://pypi.python.org/pypi/instrumentino[+https://pypi.python.org/pypi/instrumentino+]

https://github.com/yoelk/instrumentino[+https://github.com/yoelk/instrumentino+]

http://www.chemie.unibas.ch/\~hauser/open-source-lab/instrumentino/index.html[+http://www.chemie.unibas.ch/~hauser/open-source-lab/instrumentino/index.html+]

Interactive Spaces
~~~~~~~~~~~~~~~~~~

Interactive Spaces is a software platform which allows you to merge the
virtual world with the physical world. By making it easy to connect sensors to
applications running on different machines in a space, quite complex behaviors
can be built.

Interactive Spaces applications are build from units called Activities which
can easily communicate with each other no matter where they are on the local
network. Through the use of Interactive Spaces communication system, called a
route, any activity in the space can speak to or listen to messages from any
other activities that it chooses to. This means you can easily control and
synchronize events across a collection of machines.

http://www.interactive-spaces.org/[+http://www.interactive-spaces.org/+]

[[Internet-in-a-Box]]
Internet-in-a-Box
~~~~~~~~~~~~~~~~~

The Internet-in-a-Box is a small, inexpensive device which provides essential
Internet resources without any Internet connection. It provides a local copy
of half a terabyte of the world's Free information.
It provides:

* Wikipedia: Complete Wikipedia in a dozen different languages
* Maps: Zoomable world-wide maps down to street level
* E-books: Over 35 thousand e-books in a variety of languages
* Software: Huge library of Open Source Software, including installable Ubuntu
Linux OS with all software package repositories. Includes full source code
for study or modification.
* Video: Hundreds of hours of instructional videos
* Chat: Simple instant messaging across the community

While the complete dataset for the Internet-in-a-Box project is over 700 GB,
you can install the 500 MB QuickStart Sampler dataset to try the software
without needing the full dataset.

There are several methods to install and run Internet-in-a-Box (IIAB).
For installation: you can install IIAB as a Python package using the Python
package manager pip.  Or you can install from source using git.
To run: you can run IIAB as a stand-alone server, or you can integrate
it with your Apache installation using WSGI as a gateway.

https://github.com/braddockcg/internet-in-a-box[+https://github.com/braddockcg/internet-in-a-box+]

http://internet-in-a-box.org/[+http://internet-in-a-box.org/+]

IOOS Github
~~~~~~~~~~~

https://github.com/ioos[+https://github.com/ioos+]

Conda Recipes -
https://github.com/ioos/conda-recipes[+https://github.com/ioos/conda-recipes+]

DMAC System Integration Test -
https://github.com/ioos/system-test[+https://github.com/ioos/system-test+]

IoT
~~~

Trendy stuff about wee hardware running this sort of software.

http://arxiv.org/abs/1501.07438[+http://arxiv.org/abs/1501.07438+]

http://arxiv.org/abs/1502.01181[+http://arxiv.org/abs/1502.01181+]

http://www.mdpi.com/2224-2708/1/3/217/htm[+http://www.mdpi.com/2224-2708/1/3/217/htm+]

http://www.mdpi.com/2224-2708/2/4/717/htm[+http://www.mdpi.com/2224-2708/2/4/717/htm+]

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.8897&rank=1[+http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.8897&rank=1+]

AllJoyn
^^^^^^^

A collaborative open-source software framework that makes it easy for devices
and apps to discover and communicate with each other. It supports many
language bindings and can be easily integrated into platforms small and large.
The AllJoyn framework defines a common way for devices and apps to communicate
with one another ushering a new wave of interoperable devices to make the
Internet of Things a reality.

The AllJoyn framework handles the complexities of discovering nearby devices,
creating sessions between devices, and communicating securely between those
devices. It abstracts out the details of the physical transports and provides
a simple-to-use API. Multiple connection session topologies are supported,
including point-to-point and group sessions. The security framework is
flexible, supporting many mechanisms and trust models. And the types of data
transferred are also flexible, supporting raw sockets or abstracted objects
with well-defined interfaces, methods, properties, and signals.

One of the defining traits of the AllJoyn framework is its inherent
flexibility.
It was designed to run on multiple platforms, ranging from small embedded RTOS
platforms to full-featured OSes. It supports multiple language bindings and
transports. And since the AllJoyn framework is open-source, this flexibility
can be extended further in the future to support even more transports,
bindings, and features.

https://allseenalliance.org/[+https://allseenalliance.org/+]

Apollo
^^^^^^

ActiveMQ Apollo is a faster, more reliable, easier to maintain messaging
broker built from the foundations of the original ActiveMQ. It accomplishes
this using a radically different threading and message dispatching
architecture. Like ActiveMQ, Apollo is a multi-protocol broker and supports
STOMP, AMQP, MQTT, Openwire, SSL, and WebSockets.

http://activemq.apache.org/apollo/[+http://activemq.apache.org/apollo/+]

[[CoAP]]
CoAP
^^^^

The Constrained Application Protocol (CoAP) is a specialized web transfer
protocol for use with constrained nodes and constrained networks in the
Internet of Things.
The protocol is designed for machine-to-machine (M2M) applications such as
smart energy and building automation.

See also xref:Erbium[Erbium] and xref:txThings[txThings].

http://coap.technology/[+http://coap.technology/+]

Californium
xxxxxxxxxx+

Californium (Cf) is an open source implementation of the Constrained
Application Protocol (xref:CoAP[CoAP]). It is written in Java and targets
unconstrained
environments such as back-end service infrastructures (e.g., proxies, resource
directories, or cloud services) and less constrained environments such as
embedded devices running Linux (e.g., smart home/factory controllers or
cellular gateways). Californium (Cf) has been running code for the IETF
standardization of CoAP and was recently reimplemented from scratch having all
the experience. In particular, Cf focuses now on service scalability for
large-scale Internet of Things applications. The new implementation was
successfully tested at the ETSI CoAP and OMA LWM2M Plugtests in November 2013
and March 2014. It complies with all mandatory and optional test cases.

https://projects.eclipse.org/projects/technology.californium[+https://projects.eclipse.org/projects/technology.californium+]

http://www.eclipse.org/californium/[+http://www.eclipse.org/californium/+]

[[libcoap]]
libcoap
xxxxxx+

This implements a lightweight application-protocol for devices that are
constrained their resources such as computing power, RF range, memory,
bandwith, or network packet sizes. This protocol,
xref:CoAP[CoAP] was standardized in the
IETF as RFC 7252. 

http://libcoap.sourceforge.net/[+http://libcoap.sourceforge.net/+]

GSN
^^^

GSN is a Java environment that runs on one or more computers composing the
backbone of the acquisition network. A set of wrappers allow to feed live data
into the system. Then, the data streams are processed according to XML
specification files. The system is built upon a concept of sensors (real
sensors or virtual sensors, that is a new data source created from live data)
that are connected together in order to built the required processing path.
For example, one can imagine an anemometer that would sent its data into GSN
through a wrapper (various wrappers are already available and writing new ones
is quick), then that data stream could be sent to an averaging mote, the
output of this mote could then be split and sent for one part to a database
for recording and to a web site for displaying the average measured wind in
real time. All of this example could be done by editing only a few XML files
in order to connect the various motes together.

GSN is designed to make the sensor network application development a pleasure.
The applications based on GSN are hardware-independent making the sensor
network changes invisible to the application, for instance you can change the
underlying sensor network from the Mica2 nodes to the BTNodes (with compatible
sensing boards) without ever touching a single line of code in the
application.

Now you have all the common sensor network requirements in one package plus
the support for dozens of well known sensing hardware. 

https://github.com/LSIR/gsn/wiki[+https://github.com/LSIR/gsn/wiki+]

IoTivity
^^^^^^^^

IoTivity is an open source software framework enabling seamless
device-to-device connectivity to address the emerging needs of the Internet of
Things.

https://www.iotivity.org/[+https://www.iotivity.org/+]

IoTSyS
^^^^^^

An integration middleware for the Internet of Things. It provides a
communication stack for embedded devices based on IPv6, Web services and oBIX
to provide interoperable interfaces for smart objects. Using 6LoWPAN for
constrained wireless networks and the Constrained Application Protocol
together with Efficient XML Interchange an efficient stack is provided
allowing using interoperable Web technologies in the field of sensor and
actuator networks and systems while remaining nearly as efficient regarding
transmission message sizes as existing automation systems. The IoTSyS
middleware aims providing a gateway concept for existing sensor and actuator
systems found in nowadays home and building automation systems, a stack which
can be deployed directly on embedded 6LoWPAN devices and further addresses
security, discovery and scalability issues.

http://code.google.com/p/iotsys/[+http://code.google.com/p/iotsys/+]

http://www.ipso-alliance.org/ipso-challenge-2013-interviews/iotsys-internet-of-things-integration-middleware[+http://www.ipso-alliance.org/ipso-challenge-2013-interviews/iotsys-internet-of-things-integration-middleware+]

Kura
^^^^

Kura is a Java/OSGi-based framework for IoT gateways. Kura APIs offer access
to the underlying hardware (serial ports, GPS, watchdog, GPIOs, I2C, etc.),
management of network configurations, communication with M2M/IoT Integration
Platforms, and gateway management.

https://eclipse.org/kura/[+https://eclipse.org/kura/+]

makeSense
^^^^^^^^^

A project to improve the ease of wireless sensor network programming by
allowing programmers to express high-level objectives, and leave the low-level
details to the compiler and run-time system. The goal is also to enable easy
integration with other systems, such as business systems, mainly via the usage
of business process modeling.

A makeSense tutorial is available.  The tutorial comes in a virtual machine
(and the file to download is therefore quite large) that has all the software
you need to learn and understand how to develop sensor network applications
using makeSense.  The tutorial has two parts. In the first part, domain
experts can use our model editor (with extended BPMN) to develop sensor
network applications. In the second part, programmers (also with limited or no
knowledge on sensor networks) can learn how to use the makeSense
macroprogramming language on how to develop sensor network applications.

http://www.project-makesense.eu/[+http://www.project-makesense.eu/+]

Mihini
^^^^^^

The Mihini project delivers an embedded runtime running on top of Linux, that
exposes a high-level Lua API for building Machine-to-Machine applications. 

https://eclipse.org/mihini/[+https://eclipse.org/mihini/+]

MQTT
^^^^

MQTT stands for MQ Telemetry Transport. It is a publish/subscribe,
extremely simple and lightweight messaging protocol, designed for constrained
devices and low-bandwidth, high-latency or unreliable networks. The design
principles are to minimise network bandwidth and device resource requirements
whilst also attempting to ensure reliability and some degree of assurance of
delivery. These principles also turn out to make the protocol ideal of the
emerging “machine-to-machine” (M2M) or “Internet of Things” world of connected
devices, and for mobile applications where bandwidth and battery power are at
a premium.

http://mqtt.org/[+http://mqtt.org/+]

Mosquitto
xxxxxxxx+

The Mosquitto project provides an open-source implementation of an MQTT
broker. 
It implements the MQ Telemetry Transport protocol versions 3.1 and 3.1.1. MQTT
provides a lightweight method of carrying out messaging using a
publish/subscribe model. This makes it suitable for "machine to machine"
messaging such as with low power sensors or mobile devices such as phones,
embedded computers or microcontrollers like the Arduino.

The Mosquitto broker is the focus of the project and aims to be a lightweight
and function MQTT broker that can run on relatively constrained systems, but
still be powerful enough for a wide range of applications.
The +mosquitto_pub+ and +mosquitto_sub+ command line utilities provide a
straightforward and powerful way of interacting with your broker. The client
library that the utilities use for their MQTT support can be used to develop
your own MQTT applications.

http://www.eclipse.org/mosquitto/[+http://www.eclipse.org/mosquitto/+]

http://mosquitto.org/[+http://mosquitto.org/+]

http://www.slideshare.net/andysc/the-house-that-twitters[+http://www.slideshare.net/andysc/the-house-that-twitters+]

mqtt-panel
xxxxxxxxxx

A web interface for MQTT.
A simple web interface which is able to subscribe to a MQTT topic and display
the information. 

https://github.com/fabaff/mqtt-panel[+https://github.com/fabaff/mqtt-panel+]

mqtt-spy
xxxxxxxx

An open source utility intended to help you with monitoring activity on MQTT
topics. It's been designed to deal with high volumes of messages, as well as
occasional publications. 
A JavaFX application that should work on any operating system with an
appropriate version of Java 8 installed.
mqtt-spy-daemon is a Java-based command line tool that does not require a GUI
environment. Basic functionality works with Java 7, whereas some of the
advanced features like scripting require Java 8 to be installed.

https://code.google.com/p/mqtt-spy/[+https://code.google.com/p/mqtt-spy/+]

Paho
xxxx

The Paho project provides open-source client implementations of open and
standard messaging protocols aimed at new, existing, and emerging applications
for Machine‑to‑Machine (M2M) and Internet of Things (IoT). 

http://www.eclipse.org/paho/[+http://www.eclipse.org/paho/+]

Node-RED
^^^^^^^^

Node-RED is a tool for wiring together hardware devices, APIs and online services in new and interesting ways.
Node-RED provides a browser-based flow editor that makes it easy to wire together flows using the wide range nodes in the palette. Flows can be then deployed to the runtime in a single-click.

http://nodered.org/[+http://nodered.org/+]

Node-SGS
^^^^^^^^

The Internet of Things (IoT) is set to occupy a substantial component of
future Internet. The IoT connects sensors and devices that record physical
observations to applications and services of the Internet. As a successor to
technologies such as RFID and Wireless Sensor Networks (WSN), the IoT has
stumbled into vertical silos of proprietary systems, providing little or no
interoperability with similar systems. As the IoT represents future state of
the Internet, an intelligent and scalable architecture is required to provide
connectivity between these silos, enabling discovery of physical sensors and
interpretation of messages between things. This paper proposes a gateway and
Semantic Web enabled IoT architecture to provide interoperability between
systems using established communication and data standards. The Semantic
Gateway as Service (SGS) allows translation between messaging protocols such
as XMPP, CoAP and MQTT via a multi-protocol proxy architecture. Utilization of
broadly accepted specifications such as W3C's Semantic Sensor Network (SSN)
ontology for semantic annotations of sensor data provide semantic
interoperability between messages and support semantic reasoning to obtain
higher-level actionable knowledge from low-level sensor data.

http://arxiv.org/abs/1410.4977[+http://arxiv.org/abs/1410.4977+]

https://github.com/chheplo/node-sgs[+https://github.com/chheplo/node-sgs+]

openHAB
^^^^^^^

A software for integrating different home automation systems and technologies
into one single solution that allows over-arching automation rules and that
offers uniform user interfaces. 
The open Home Automation Bus (openHAB) project aims at providing a universal
integration platform for all things around home automation. It is a pure Java
solution, fully based on OSGi. The Equinox OSGi runtime and Jetty as a web
server build the core foundation of the runtime.

It is designed to be absolutely vendor-neutral as well as
hardware/protocol-agnostic. openHAB brings together different bus systems,
hardware devices and interface protocols by dedicated bindings. These bindings
send and receive commands and status updates on the openHAB event bus. This
concept allows designing user interfaces with a unique look&feel, but with the
possibility to operate devices based on a big number of different
technologies. Besides the user interfaces, it also brings the power of
automation logics across different system boundaries.

https://github.com/openhab/openhab[+https://github.com/openhab/openhab+]

http://www.openhab.org/[+http://www.openhab.org/+]

OpenRemote
^^^^^^^^^^

OpenRemote is software integration platform for residential and commercial
building automation. OpenRemote platform is automation protocol agnostic,
operates on off-the-shelf hardware and is freely available under an Open
Source license. OpenRemote's architecture enables fully autonomous and
user-independent intelligent buildings. End-user control interfaces are
available for iOS and Android devices, and for devices with modern web
browsers. User interface design, installation management and configuration can
be handled remotely with OpenRemote cloud-based design tools.

The supported protocols/devices include
TCP/IP, Telnet, HTTP/REST, RS-232, 
http://en.wikipedia.org/wiki/AMX192[AMX],
http://en.wikipedia.org/wiki/KNX_%28standard%29[KNX],
http://www.lutron.com/technicaldocumentlibrary/040249.pdf[Lutron],
http://en.wikipedia.org/wiki/Z-Wave[Z-Wave],
http://en.wikipedia.org/wiki/1-Wire[1-Wire],
http://en.wikipedia.org/wiki/EnOcean[EnOcean],
http://en.wikipedia.org/wiki/XPL_Protocol[xPL],
http://en.wikipedia.org/wiki/Insteon[Insteon].
http://en.wikipedia.org/wiki/X10_%28industry_standard%29[X10],
http://en.wikipedia.org/wiki/Infrared_Data_Association[infrared],
http://www.russound.com/[Russound],
http://www.globalcache.com/[GlobalCache],
http://www.irtrans.de/en/[ITRrans],
https://wiki.videolan.org/Real_Time_Messaging_Protocol/[VLC],
http://www.panstamp.com/[panStamp],
http://openrb.com/wp-content/uploads/2012/02/AVR3312CI_AVR3312_PROTOCOL_V7.6.0.pdf[Denon
AVR],
http://en.wikipedia.org/wiki/Freebox[Freebox],
https://www.mythtv.org/wiki/Myth_Protocol/Guide[MythTV] and more.

http://www.openremote.org/[+http://www.openremote.org/+]

OSGi
^^^^

The OSGi specification describes a modular system and a service platform for
the Java programming language that implements a complete and dynamic component
model, something that does not exist in standalone Java/VM environments.
Applications or components, coming in the form of bundles for deployment, can
be remotely installed, started, stopped, updated, and uninstalled without
requiring a reboot; management of Java packages/classes is specified in great
detail. Application life cycle management is implemented via APIs that allow
for remote downloading of management policies. The service registry allows
bundles to detect the addition of new services, or the removal of services,
and adapt accordingly.

The OSGi specifications have evolved beyond the original focus of service
gateways, and are now used in applications ranging from mobile phones to the
open-source Eclipse IDE. Other application areas include automobiles,
industrial automation, building automation, PDAs, grid computing,
entertainment, fleet management and application servers.

http://en.wikipedia.org/wiki/OSGi[+http://en.wikipedia.org/wiki/OSGi+]

http://en.wikipedia.org/wiki/OSGi#Projects_using_OSGi[+http://en.wikipedia.org/wiki/OSGi#Projects_using_OSGi+]

R-OSGi
xxxxxx

Remote Services for OSGi runs as an OSGi bundle and facilitates distribution
for arbitrary OSGi framework implementations. All that a service provider
framework has to do is registering a service for remote access. Subsequently,
other peers can connect to the service provider peer and get access to the
service. Remote services are accessed in an entirely transparent way. For
every remote service, a local proxy bundle is generated that registers the
same service. Local service clients can hence access the remote service in the
same way and without regarding distribution. 

Even though Remote Services for OSGi is a sophisticated middleware for OSGi
frameworks, it uses a very efficient network protocol and has a small
footprint. This makes it ideal for small and embedded devices with limited
memory and network bandwidth. The service runs on every OSGi-compliant
environment.
Remote Services for OSGi has been tested with Eclipse Equinox, Knopflerfish,
and Oscar / Apache Felix, as well as with our own lightweight OSGi
implementation Concierge. Our test platforms include a variety of different
devices, hardware architectures and Java VMs. 

http://r-osgi.sourceforge.net/index.html[+http://r-osgi.sourceforge.net/index.html+]

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.8897&rank=1[+http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.188.8897&rank=1+]

ProFuN
^^^^^^

Our goal is to provide a programming environment where sensors can be
programmed as an ensemble rather than individually. Here programmers will
focus on the applications and on the services provided by the collection of
sensors rather than on which particular sensor to program or on how
communication will take place. Energy levels and scheduling will to some
extent be controlled by the programmers, e.g., to give priority to specific
application tasks. Briefly put, we will employ macro programming of networks
of sensors rather than micro programming of individual sensors. We will focus
on wireless sensor networks, but the general concept of ensemble programming
is applicable to other multiprocessor areas such as grid computing, multicore,
and server farms, like Google, etc. We will demonstrate that sensor networks
can be programmed as an ensemble under severe constraints, including being
devices with very limited capabilities, frequently failing sensors, mobility,
energy constraints and harsh security threats.

ProFuN TG is a high-level programming environment for wireless sensor
networks. It helps the application programmer with software design, deployment
and maintenance.
The tool is customizable: both user-defined tasks and user-defined objective
functions for task mapping are possible.

http://parapluu.github.io/profun/[+http://parapluu.github.io/profun/+]

http://www.it.uu.se/research/profun[+http://www.it.uu.se/research/profun+]

SmartHome
^^^^^^^^^

Eclipse SmartHome is a framework for building smart home solutions. As such,
it consists of a rich set of OSGi bundles that serve different purposes. Not
all solutions that build on top of Eclipse SmartHome will require all of those
bundles - instead they can choose what parts are interesting for them.

https://eclipse.org/smarthome/[+https://eclipse.org/smarthome/+]

TANGO
^^^^^

TANGO is a software toolkit for connecting things together,
building control systems, and integrating system.
It is free , open source and object-oriented. It is easy to use and is well
adapted to solving simple and complex distributed problems. TANGO Controls has
been used to build solutions for:

* Distributed Control Systems (DCS) in which devices are controlled and
monitored in a local distributed network
* Supervisory Control And Data Acquisition (SCADA) systems in which remote
devices are controlled and monitored centrally
* Integrated Control Systems (ICS) in which different autonomous control
systems are integrated into a central one
* Interface Devices that run on small embedded platforms into a distributed
control system
* Internet of Things (IoT) applications in which arbitrary devices are
controlled through the Internet
* Machine to Machine (M2M) applications in which devices communicates with
each other
* System Integration Platforms in which different kind of software
applications and systems are integrated into a central one

TANGO Controls is operating system independent and supports `Cxx`, Java and
Python for all of the components.

http://www.tango-controls.org/[+http://www.tango-controls.org/+]

Taurus
xxxxxx

Taurus is a python framework for both CLI and GUI tango applications. It is
build on top of PyTango and PyQt. Taurus stands for TAngo User interface ‘R’
US.

https://pypi.python.org/pypi/taurus[+https://pypi.python.org/pypi/taurus+]

http://plone.tango-controls.org/static/taurus/latest/doc/html/[+http://plone.tango-controls.org/static/taurus/latest/doc/html/+]

PyTango
xxxxxx+

Python bindings for TANGO.

http://www.esrf.eu/computing/cs/tango/tango_doc/kernel_doc/pytango/latest/[+http://www.esrf.eu/computing/cs/tango/tango_doc/kernel_doc/pytango/latest/+]

http://sourceforge.net/projects/tango-cs/files/PyTango/[+http://sourceforge.net/projects/tango-cs/files/PyTango/+]

https://pypi.python.org/pypi/PyTango[+https://pypi.python.org/pypi/PyTango+]

the thing system
^^^^^^^^^^^^^^^^

The Thing System is a set of software components and network protocols. Our
steward software is written in node.js making it both portable and easily
extensible. It can run on your laptop, or fit onto a small single board
computer like the Raspberry Pi.

The steward is at the heart of the system and connects to Things in your home,
whether those things are media players such as the Sonos or the Apple TV, your
Nest thermostat, your INSTEON home control system, or your Philips Hue
lightbulbs — whether your things are connected together via Wi-Fi, USB or
Bluetooth Low Energy (BLE). The steward will find them and bring them together
so they can talk to one another and perform magic.

Dozens of "things" are supported, with more on the way.

http://thethingsystem.com/[+http://thethingsystem.com/+]

https://github.com/TheThingSystem[+https://github.com/TheThingSystem+]

http://thethingsystem.com/dev/supported-things.html[+http://thethingsystem.com/dev/supported-things.html+]

WaveScope/WaveScript
^^^^^^^^^^^^^^^^^^^^

WaveScope is a system for developing distributed, high-rate applications that
need to process streams of data from various sources (e.g., sensors) using a
combination of signal processing and database (event stream processing)
operations. The execution environment for these applications ranges from
embedded sensor nodes to multicore/multiprocessor servers.

WaveScript is the programming language used to develop WaveScope applications.
It is a high-level, functional, stream-processing language that aims to
deliver uncompromising performance. WaveScript programs execute in parallel on
multiple cores, or distributed across a network. Its compiler uses aggressive
partial evaluation techniques to remove abstractions and reduce the source
program to a graph of stream operators.

The WaveScript compiler supports multiple backends generating code for several
languages.

* First, the flagship WaveScript backend offering the best performance
generates native code using a C compiler backend. 
* Second, an embedding of WaveScript in Scheme is included with the compiler
and enables low-latency compile-link-load of new programs. 
* Third, because WaveScript is similar to ML, translation is straightforward
and WaveScript can generate code for SML (MLton) or OCaml. 

http://www.cs.indiana.edu/~rrnewton/wavescope/WaveScope_+_WaveScript/WaveScope_Homepage.html[+http://www.cs.indiana.edu/~rrnewton/wavescope/WaveScope_+_WaveScript/WaveScope_Homepage.html+]

https://github.com/rrnewton/WaveScript[+https://github.com/rrnewton/WaveScript+]

IoT Operating Systems
~~~~~~~~~~~~~~~~~~~~~

[[Contiki]]
Contiki
^^^^^^^

Contiki is an open source operating system for the Internet of Things. Contiki
connects tiny low-cost, low-power microcontrollers to the Internet.
Contiki provides powerful low-power Internet communication. Contiki supports
fully standard IPv6 and IPv4, along with the recent low-power wireless
standards: 6lowpan, RPL, CoAP. With Contiki's ContikiMAC and sleepy routers,
even wireless routers can be battery-operated.

Contiki is designed to run on classes of hardware devices that are severely
constrained in terms of memory, power, processing power, and communication
bandwidth. A typical Contiki system has memory on the order of kilobytes, a
power budget on the order of milliwatts, processing speed measured in
megahertz, and communication bandwidth on the order of hundreds of
kilobits/second. This class of systems includes both various types of embedded
systems as well as a number of old 8-bit computers.
Despite providing multitasking and a built-in TCP/IP stack, Contiki only needs
about 10 kilobytes of RAM and 30 kilobytes of ROM.[1] A full system, complete
with a graphical user interface, needs about 30 kilobytes of RAM.

http://www.contiki-os.org/[+http://www.contiki-os.org/+]

https://github.com/contiki-os/contiki[+https://github.com/contiki-os/contiki+]

http://en.wikipedia.org/wiki/Contiki[+http://en.wikipedia.org/wiki/Contiki+]

[[CALIPSO]]
CALIPSO
xxxxxx+

CALIPSO builds Internet Protocol (IP) connected smart object networks, but
with novel methods to attain very low power consumption, thereby providing
both interoperability and long lifetimes. CALIPSO leans on the significant
body of work on sensor networks to integrate radio duty cycling and
data-centric mechanisms into the IPv6 stack, something that existing work has
not previously done. CALIPSO works at three layers: the network, the routing,
and the application layer. We also revisit architectural decisions on naming,
identification, and the use of middle-boxes.

CALIPSO works within the IETF/IPv6 framework, which includes the recent IETF
RPL and CoAP protocols. This gives a structure for evaluation that has not
previously been available. We use xref:Contiki[Contiki] open source OS, Europe’s leading
smart object OS, as the target development environment for prototyping and
experimental evaluation.

http://www.ict-calipso.eu/[+http://www.ict-calipso.eu/+]

https://github.com/sics-iot/calipso[+https://github.com/sics-iot/calipso+]

[[Erbium]]
Erbium
xxxxxx

Erbium (Er) is a low-power REST Engine for xref:Contiki[Contiki] that was developed together
with SICS (and a rare earth element that is found in the Ytterby mine near
Stockholm). The REST Engine includes a comprehensive embedded
xref:CoAP[CoAP]
implementation, which became the official one for the Contiki OS. It supports
RFC 7252 together with blockwise transfers and observing. 

http://people.inf.ethz.ch/mkovatsc/erbium.php[+http://people.inf.ethz.ch/mkovatsc/erbium.php+]

[[RIOT]]
RIOT
^^^^

RIOT is an operating system designed for the particular requirements of
Internet of Things (IoT) scenarios. These requirements comprise a low memory
footprint, high energy efficiency, real-time capabilities, a modular and
configurable communication stack, and support for a wide range of low-power
devices. RIOT provides a microkernel, utilities like cryptographic libraries,
data structures (bloom filters, hash tables, priority queues), or a shell,
different network stacks, and support for various microcontrollers, radio
drivers, sensors, and configurations for entire platforms, e.g. TelosB or
STM32 Discovery Boards.

https://github.com/RIOT-OS/RIOT/wiki[+https://github.com/RIOT-OS/RIOT/wiki+]

https://github.com/siskin/RIOT[+https://github.com/siskin/RIOT+]

IPFS
~~~~

IPFS is a distributed file system that seeks to connect all computing devices
with the same system of files. In some ways, this is similar to the original
aims of the Web, but IPFS is actually more similar to a single bittorrent
swarm exchanging git objects.

It combines good ideas from Git, BitTorrent, Kademlia, SFS, and the Web. It is
like a single bittorrent swarm, exchanging git objects. IPFS provides an
interface as simple as the HTTP web, but with permanence built in. 

http://ipfs.io/[+http://ipfs.io/+]

http://arxiv.org/abs/1407.3561[+http://arxiv.org/abs/1407.3561+]

https://github.com/jbenet/ipfs[+https://github.com/jbenet/ipfs+]

IPredator
~~~~~~~~~

IPredator provides you with an encrypted tunnel from your computer to the
Internet. We are hiding your real IP address behind one of ours. 

https://www.ipredator.se/[+https://www.ipredator.se/+]

IRAF
~~~~

IRAF is the Image Reduction and Analysis Facility, a general purpose
software system for the reduction and analysis of scientific data.  IRAF is
written and supported by the IRAF programming group at the National Optical
Astronomy Observatories (NOAO) in Tucson, Arizona.  IRAF includes a good
selection of programs for general image processing and graphics applications,
plus a large number of programs for the reduction and analysis of optical
astronomy data within the NOAO package.  External or layered packages are
also available for the analysis of HST, XRAY and EUV data.  IRAF provides
a complete programming environment, which includes the Command Language
script facility, the IMFORT Fortran programming interface, and the fully
featured SPP/VOS programming environment in which the portable IRAF system
is written.

See xref:PyRAF[PyRAF].

http://iraf.noao.edu/[+http://iraf.noao.edu/+]

IRPF90
~~~~~~

IRPF90 is a Fortran programming environment which helps the development of
large Fortran codes by applying the Implicit Reference to Parameters method
(IRP).

In Fortran programs, the programmer has to focus on the order of the
instructions: before using a variable, the programmer has to be sure that it
has already been computed in all possible situations. For large codes, it is
common source of error.

In IRPF90 most of the order of instructions is handled by the pre-processor,
and an automatic mechanism guarantees that every entity is built before being
used. This mechanism relies on the +needs/needed by+ relations between the
entities, which are built automatically.

Codes written with IRPF90 execute often faster than Fortran programs, are
faster to write and easier to maintain. 

http://irpf90.ups-tlse.fr/index.php[+http://irpf90.ups-tlse.fr/index.php+]

https://github.com/scemama/irpf90[+https://github.com/scemama/irpf90+]

http://sourceforge.net/projects/irpf90/[+http://sourceforge.net/projects/irpf90/+]

http://arxiv.org/abs/0909.5012[+http://arxiv.org/abs/0909.5012+]

ISIS
~~~~

The Integrated System for Imagers and Spectrometers (ISIS) is a free,
specialized, digital image processing software package developed by the USGS
for NASA. ISIS key feature is the ability to place many types of data in the
correct cartographic location, enabling disparate data to be co-analyzed. ISIS
also includes standard image processing applications such as contrast,
stretch, image algebra, filters, and statistical analysis. ISIS can process
two-dimensional images as well as three-dimensional cubes derived from imaging
spectrometers. The production of USGS topographic maps of extraterrestrial
landing sites relies on ISIS software. ISIS is able to process data from NASA
and International spacecraft missions including Lunar Orbiter, Apollo,
Voyager, Mariner 10, Viking, Galileo, Magellan, Clementine, Mars Global
Surveyor, Cassini, Mars Odyssey, Mars Reconnaissance Orbiter, MESSENGER, Lunar
Reconnaissance Orbiter, Chandrayaan, Dawn, and Kaguya. 

The ISIS software is a valuable resource for planetary missions that require
systematic data processing, products for planning, and research and analysis
of derived data products. By using ISIS, missions can leverage millions of
dollars of software development that NASA has paid for. However, before the
power of ISIS can be applied to an instrument, a camera model and custom
programs to ingest mission-specific ancillary data are necessary. Once an
instrument is added to ISIS, it can support data processing pipelines,
radiometric calibration, photometric calibration, band-to-band registration of
multispectral data, ortho-rectification, construction of scientifically
accurate and cosmetically pleasing mosaics, generation of control networks
solutions and creation of topographic models. 

http://isis.astrogeology.usgs.gov/[+http://isis.astrogeology.usgs.gov/+]

http://planetarygis.blogspot.com/[+http://planetarygis.blogspot.com/+]

http://astrogeology.usgs.gov/facilities/mrctr[+http://astrogeology.usgs.gov/facilities/mrctr+]

ITL
~~~

The Information Theoretic Data Analysis Library.

https://svn.mcs.anl.gov/repos/itl/trunk/[+https://svn.mcs.anl.gov/repos/itl/trunk/+]

https://sites.google.com/site/sdmavtest/itl-documentation[+https://sites.google.com/site/sdmavtest/itl-documentation+]

http://www.irisa.fr/kerdata/data-at-exascale/doku.php?id=activity[+http://www.irisa.fr/kerdata/data-at-exascale/doku.php?id=activity+]

http://sdav-scidac.org/highlights/analysis/27-highlights/analysis/41-multivariate-easy.html[+http://sdav-scidac.org/highlights/analysis/27-highlights/analysis/41-multivariate-easy.html+]

http://abonchaudhuri.net/software.html[+http://abonchaudhuri.net/software.html+]

https://sites.google.com/site/sdmavtest/home[+https://sites.google.com/site/sdmavtest/home+]

https://sites.google.com/site/sdmavtest/related-works/paperlistinformationtheorypaperswithdifferentapplications[+https://sites.google.com/site/sdmavtest/related-works/paperlistinformationtheorypaperswithdifferentapplications+]

[[NJ]]
////
NJJJ
////

[[Javascript]]
Javascript/ECMAScript
~~~~~~~~~~~~~~~~~~~~~

A dynamic computer programming language.[5] It is most commonly used as part
of web browsers, whose implementations allow client-side scripts to interact
with the user, control the browser, communicate asynchronously, and alter the
document content that is displayed.[5] It is also used in server-side network
programming with runtime environments such as Node.js, game development and
the creation of desktop and mobile applications. With the rise of the
single-page web app and JavaScript-heavy sites, it is increasingly being used
as a compile target for source-to-source compilers from both dynamic languages
and static languages. In particular, Emscripten and highly optimised JIT
compilers, in tandem with asm.js which is friendly to AOT compilers like
OdinMonkey, have enabled C and `Cxx` programs to be compiled into JavaScript and
execute at near-native speeds, making JavaScript be considered the "assembly
language of the web",[6] according to its creator and others.

*The Hitchhiker's Guide to Modern JavaScript Tooling* - http://reactkungfu.com/2015/07/the-hitchhikers-guide-to-modern-javascript-tooling/[+http://reactkungfu.com/2015/07/the-hitchhikers-guide-to-modern-javascript-tooling/+]

*Introduction to JavaScript for Fortran programmers* -
http://www.see.ed.ac.uk/\~jwp/MSO/newMSO/lab/JS/[+http://www.see.ed.ac.uk/~jwp/MSO/newMSO/lab/JS/+]

asm.js
^^^^^^

A low-level, extraordinarily optimizable subset of JavaScript. 
It is an intermediate programming language consisting of a strict subset of
the JavaScript language. It enables significant performance improvements for
web applications that are written in statically-typed languages with manual
memory management (such as C) and then translated to JavaScript by a
source-to-source compiler. Asm.js does not aim to improve the performance of
hand-written JavaScript code, nor does it enable anything other than enhanced
performance.

It is intended to have performance characteristics closer to that of native
code than standard JavaScript by limiting language features to those amenable
to ahead-of-time optimization and other performance improvements.[2] By using
a subset of JavaScript, asm.js is already supported by all major web
browsers,[3] unlike alternative approaches such as Google Native Client.
Mozilla Firefox was the first web browser to implement asm.js-specific
optimizations, starting with Firefox 22.[4] The optimizations of Google
Chrome's V8 JavaScript engine in Chrome 28 made asm.js benchmarks more than
twice as fast as prior versions of Chrome.

See xref:Emscripten[Emscripten].

https://github.com/dherman/asm.js[+https://github.com/dherman/asm.js+]

http://asmjs.org/[+http://asmjs.org/+]

http://en.wikipedia.org/wiki/Asm.js[+http://en.wikipedia.org/wiki/Asm.js+]

[[Babel.js]]
Babel.js
^^^^^^^^

Babel.js is a transpiler - it takes your code written in a ECMAScript 2015 standard and produces the code in the older standard that browsers can run. It also allows you to enable experimental ECMAScript 2016 (a.k.a. ECMAScript 7 or ES7) features and has a built-in JSX transpiler. It can take JSX syntax that React uses and produce the JavaScript code out of it.

http://babeljs.io/[+http://babeljs.io/+]

Carnival
^^^^^^^^

Carnival is an unobtrusive, developer-friendly way to add comments to any web
site.

https://carnivalapp.io/docs[+https://carnivalapp.io/docs+]

DynJS
^^^^^

DynJS is an ECMAScript runtime for the JVM. 

http://dynjs.org/[+http://dynjs.org/+]

[[JerryScript]]
JerryScript
^^^^^^^^^^^

JerryScript is the lightweight JavaScript engine intended to run on a very constrained devices such as microcontrollers.

http://samsung.github.io/jerryscript/[+http://samsung.github.io/jerryscript/+]

https://github.com/Samsung/jerryscript[+https://github.com/Samsung/jerryscript+]

Nashorn
^^^^^^^

Nashorn's goal is to implement a lightweight high-performance JavaScript
runtime in Java with a native JVM. This Project intends to enable Java
developers embedding of JavaScript in Java applications via JSR-223 and to
develop free standing JavaScript applications using the jrunscript
command-line tool.

https://blogs.oracle.com/nashorn/[+https://blogs.oracle.com/nashorn/+]

http://openjdk.java.net/projects/nashorn/[+http://openjdk.java.net/projects/nashorn/+]

http://en.wikipedia.org/wiki/Nashorn_%28JavaScript_engine%29[+http://en.wikipedia.org/wiki/Nashorn_%28JavaScript_engine%29+]

[[npm]]
npm
^^^

NPM is a package manager. It does the same job as your system package managers do, but for JavaScript. It is a tool for downloading all pieces of your environment. NPM takes care of downloading packages, resolving dependencies for them and providing a package abstraction around your project. So when another developer wants to work with your codebase all he need to do is to issue the npm install command and all dependencies will install automatically. In such package you can also include license info, name, keywords, version, description and many other metadata about your code. If you are developing a library, npm also helps you to publish it later and make it available for all developers that work within the Node.js environment.

https://www.npmjs.com/[+https://www.npmjs.com/+]

http://reactkungfu.com/2015/07/the-hitchhikers-guide-to-modern-javascript-tooling/[+http://reactkungfu.com/2015/07/the-hitchhikers-guide-to-modern-javascript-tooling/+]

Rhino
^^^^^

Rhino is an open-source implementation of JavaScript written entirely in Java.
It is typically embedded into Java applications to provide scripting to end
users. It is embedded in J2SE 6 as the default Java scripting engine.

https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino[+https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino+]

http://en.wikipedia.org/wiki/Rhino_%28JavaScript_engine%29[+http://en.wikipedia.org/wiki/Rhino_%28JavaScript_engine%29+]

JavaScript Frameworks
~~~~~~~~~~~~~~~~~~~~~

Things that run on top of JavaScript.

Enyo
^^^^

An open source JavaScript framework for Cross-platform mobile, desktop, TV and
web applications emphasizing object-oriented encapsulation and modularity.

http://enyojs.com/[+http://enyojs.com/+]

http://en.wikipedia.org/wiki/Enyo_%28software%29[+http://en.wikipedia.org/wiki/Enyo_%28software%29+]

http://apps.enyojs.com/[+http://apps.enyojs.com/+]

Numeric.js
^^^^^^^^^^

The Numeric Javascript library allows you to perform sophisticated numerical
computations in pure javascript in the browser and elsewhere.

http://numericjs.com/[+http://numericjs.com/+]

SpiderMonkey
^^^^^^^^^^^^

SpiderMonkey is Mozilla's JavaScript engine written in C/`Cxx`. It is used in
various Mozilla products, including Firefox, and is available under the MPL2.

SpiderMonkey is the code name for the first-ever JavaScript engine, written by
Brendan Eich at Netscape Communications, later released as open source and
now maintained by the Mozilla Foundation. SpiderMonkey provides
JavaScript support for Mozilla Firefox and various embeddings such as the
GNOME 3 desktop.

Eich "wrote JavaScript in ten days" in 1995, having been "recruited to
Netscape with the promise of 'doing Scheme' in the browser". (The idea of
using Scheme was abandoned when "engineering management [decided] that the
language must ‘look like Java’".) In the fall of 1996, Eich, needing to
"pay off [the] substantial technical debt" left from the first year, "stayed
home for two weeks to rewrite Mocha as the codebase that became known as
SpiderMonkey". The name SpiderMonkey was chosen as a reference to the movie
Beavis and Butt-head Do America, in which the character Tom Anderson mentions
that the title characters were "whacking off like a couple of spider
monkeys." In 2011, Eich transferred management of the SpiderMonkey code to
Dave Mandelin.

https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey[+https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey+]

http://en.wikipedia.org/wiki/SpiderMonkey_(software)[+http://en.wikipedia.org/wiki/SpiderMonkey_(software)+]

V8
^^

The V8 JavaScript Engine is an open source JavaScript engine developed by
Google for the Google Chrome web browser.
V8 compiles JavaScript to native machine code (IA-32, x86-64, ARM, or MIPS
ISAs) before executing it, instead of more traditional techniques such
as interpreting bytecode or compiling the whole program to machine code and
executing it from a filesystem. The compiled code is additionally optimized
(and re-optimized) dynamically at runtime, based on heuristics of the code's
execution profile. Optimization techniques used include inlining, elision of
expensive runtime properties, and inline caching, among many others.

https://chromium.googlesource.com/v8/v8.git[+https://chromium.googlesource.com/v8/v8.git+]

https://github.com/v8/v8-git-mirror[+https://github.com/v8/v8-git-mirror+]

http://code.google.com/p/v8/[+http://code.google.com/p/v8/+]

http://en.wikipedia.org/wiki/V8_%28JavaScript_engine%29[+http://en.wikipedia.org/wiki/V8_%28JavaScript_engine%29+]

jekyl
~~~~~

Jekyll is a simple, blog-aware, static site generator perfect for personal,
project, or organization sites. Think of it like a file-based CMS, without all
the complexity. Jekyll takes your content, renders Markdown and Liquid
templates, and spits out a complete, static website ready to be served by
Apache, Nginx or another web server. Jekyll is the engine behind GitHub Pages,
which you can use to host sites right from your GitHub repositories.

https://github.com/jekyll/jekyll[+https://github.com/jekyll/jekyll+]

http://jekyllrb.com/[+http://jekyllrb.com/+]

JIDT
~~~~

Complex systems are increasingly being viewed as distributed information
processing systems, particularly in the domains of computational neuroscience,
bioinformatics and Artificial Life. This trend has resulted in a strong uptake
in the use of (Shannon) information-theoretic measures to analyse the dynamics
of complex systems in these fields. We introduce the Java Information Dynamics
Toolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3
licensed) open-source code implementation for empirical estimation of
information-theoretic measures from time-series data. While the toolkit
provides classic information-theoretic measures (e.g. entropy, mutual
information, conditional mutual information), it ultimately focusses on
implementing higher-level measures for information dynamics. That is, JIDT
focusses on quantifying information storage, transfer and modification, and
the dynamics of these operations in space and time. For this purpose, it
includes implementations of the transfer entropy and active information
storage, their multivariate extensions and local or pointwise variants. JIDT
provides implementations for both discrete and continuous-valued data for each
measure, including various types of estimator for continuous data (e.g.
Gaussian, box-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped
at run-time due to Java's object-oriented polymorphism. Furthermore, while
written in Java, the toolkit can be used directly in MATLAB, GNU Octave,
Python and other environments. We present the principles behind the code
design, and provide several examples to guide users. 

http://arxiv.org/abs/1408.3270[+http://arxiv.org/abs/1408.3270+]

http://code.google.com/p/information-dynamics-toolkit/[+http://code.google.com/p/information-dynamics-toolkit/+]

joblib
~~~~~~

Joblib provides a simple helper class to write parallel for loops using
multiprocessing. The core idea is to write the code to be executed as a
generator expression, and convert it to parallel computing.

https://pythonhosted.org/joblib/parallel.html[+https://pythonhosted.org/joblib/parallel.html+]

Jolie
~~~~~

Jolie is an open-source programming language for developing distributed
applications based on microservices. In the programming paradigm proposed with
Jolie, each program is a service that can communicate with other programs by
sending and receiving messages over a network.

http://www.jolie-lang.org/[+http://www.jolie-lang.org/+]

http://arxiv.org/abs/1410.3712[+http://arxiv.org/abs/1410.3712+]

jPort
~~~~~

An application launcher for portable Java applications on every computer
everywhere you go. jPort creates a Java enabled menu to launch dozens of free
applications. 
jPort desktop does not require installation. Simply upload jPort on any
desktop and hundreds of awesome applications will be under your fingertips.

http://jwork.org/jport/[+http://jwork.org/jport/+]

http://jwork.org/jport/pjp/index.html[+http://jwork.org/jport/pjp/index.html+]

JSON-FP
~~~~~~~

In a decentralized computing environment, it's a better practice to pass
programming codes to various machines to execute (and then gather the results)
when the application is dealing with huge amount of data. However, how can
machines of various configurations understand each other? Also, the "moving
code, least moving data" policy may work better with functional programming
than imperative programming.

Those questions/issues lead to the idea of doing functional programming in
JSON. If programs can be coded in JSON, they can be easily shipped around and
understood by machines of vaious settings. Combining JSON and functional
programming also makes security issues easier to track or manage.

JSON-FP is part of an attempt to make data freely and easily accessed,
distributed, annotated, meshed, even re-emerged with new values. To achieve
that, it's important to be able to ship codes to where data reside, and that's
what JSON-FP is trying to achieve.

https://github.com/benlue/jsonfp[+https://github.com/benlue/jsonfp+]

http://www.slideshare.net/BenLue/what-is-jsonfp[+http://www.slideshare.net/BenLue/what-is-jsonfp+]

Julia
~~~~~

A high-level, high-performance dynamic programming language for technical
computing, with syntax that is familiar to users of other technical computing
environments. It provides a sophisticated compiler, distributed parallel
execution, numerical accuracy, and an extensive mathematical function library.
The library, largely written in Julia itself, also integrates mature,
best-of-breed C and Fortran libraries for linear algebra, random number
generation, signal processing, and string processing.

http://julialang.org/[+http://julialang.org/+]

Julia Packages - http://pkg.julialang.org/[+http://pkg.julialang.org/+]

http://www.mit.edu/\~kepner/pubs/JuliaSemiring_HPEC2013_Paper.pdf[+http://www.mit.edu/~kepner/pubs/JuliaSemiring_HPEC2013_Paper.pdf+]

http://arxiv.org/abs/1209.5145[+http://arxiv.org/abs/1209.5145+]

http://arxiv.org/abs/1411.1607[+http://arxiv.org/abs/1411.1607+]

[[ApproxFun]]
ApproxFun
^^^^^^^^^

Julia package for function approximation.

https://github.com/ApproxFun/ApproxFun.jl[+https://github.com/ApproxFun/ApproxFun.jl+]

ApproxFun and solution of differential equations in Julia (slides, PDF, 2014,
41) - Sheehan Olver -
http://www.maths.usyd.edu.au/u/olver/talks/ApproxFun.pdf[+http://www.maths.usyd.edu.au/u/olver/talks/ApproxFun.pdf+]

A practical framework for infinite-dimensional linear algebra (paper, PDF,
2014, 5) - Sheehan Olver & Alex Townsend -
http://arxiv.org/abs/1409.5529[+http://arxiv.org/abs/1409.5529+]

The automatic solution of partial differential equations using a global
spectral method (paper, PDF, 2014, 22) - Alex Townsend & Sheehan Olver -
http://arxiv.org/abs/1409.2789[+http://arxiv.org/abs/1409.2789+]

Compose
^^^^^^^

Compose is a declarative vector graphics system written in Julia. It's designed to simplify the creation of complex graphics and serves as the basis of the Gadfly data visualization package.

http://composejl.org/[+http://composejl.org/+]

Escher
^^^^^^

Escher lets you build beautiful interactive Web UIs in Julia.

Escher's built-in web server allows you to create interactive UIs with very little code. It takes care of messaging between Julia and the browser under-the-hood. It can also hot-load code: you can see your UI evolve as you save your changes to it.

The built-in library functions support Markdown, Input widgets, TeX-style Layouts, Styling, TeX, Code, Behaviors, Tabs, Menus, Slideshows, Plots (via Gadfly) and Vector Graphics (via Compose) – everything a Julia programmer would need to effectively visualize data or to create user-facing GUIs. The API comprehensively covers features from HTML and CSS, and also provides advanced features. Its user merely needs to know how to write code in Julia.

https://shashi.github.io/Escher.jl/[+https://shashi.github.io/Escher.jl/+]

Gadfly
^^^^^^

Gadfly is a system for plotting and visualization based largely on Hadley Wickhams's ggplot2 for R, and Leland Wilkinson's book The Grammar of Graphics.

http://gadflyjl.org/[+http://gadflyjl.org/+]

JuMP
^^^^

JuMP is a domain-specific modeling language for mathematical programming embedded in Julia. It currently supports a number of open-source and commercial solvers for a variety of problem classes, including linear programming, (mixed) integer programming, second-order conic programming, and nonlinear programming.

JuMP makes it easy to specify and solve optimization problems without expert knowledge, yet at the same time allows experts to implement advanced algorithmic techniques such as exploiting efficient hot-starts in linear programming or using callbacks to interact with branch-and-bound solvers. JuMP is also fast - benchmarking has shown that it can create problems at similar speeds to special-purpose commercial tools such as AMPL while maintaining the expressiveness of a generic high-level programming language. JuMP can be easily embedded in complex work flows including simulations and web servers.

https://github.com/JuliaOpt/JuMP.jl[+https://github.com/JuliaOpt/JuMP.jl+]

http://sbebo.github.io/blog/blog/2015/06/10/julia/[+http://sbebo.github.io/blog/blog/2015/06/10/julia/+]

[[Nemo]]
Nemo
^^^^

Nemo is a computer algebra package for the Julia programming language.

http://nemocas.org/[+http://nemocas.org/+]

OnlineStats
^^^^^^^^^^^

On-line statistics for Julia.

https://github.com/joshday/OnlineStats.jl[+https://github.com/joshday/OnlineStats.jl+]

[[Jupyter]]
Jupyter
~~~~~~~

The next generation of IPython notebooks.
IPython will continue to exist as a Python kernel for Jupyter, but the
notebook and other language-agnostic parts of IPython will move to new
projects under the Jupyter name. IPython 3.0 will be the last monolithic
release of IPython. 

https://github.com/jupyter[+https://github.com/jupyter+]

*Interactive Notebooks: Sharing the Code* - http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261[+http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261+]

*Publishing Workflows for Jupyter* - http://odewahn.github.io/publishing-workflows-for-jupyter/#1[+http://odewahn.github.io/publishing-workflows-for-jupyter/#1+]

*Jupyter Advanced Topics Tutorial* (2:48:53 video) - https://www.youtube.com/watch?v=38R7jiCspkw[+https://www.youtube.com/watch?v=38R7jiCspkw+]

* *Notes for Tutorial* - https://github.com/jupyter/scipy-advanced-tutorial[+https://github.com/jupyter/scipy-advanced-tutorial+]

*Teaching with IPython: Jupyter Notebooks and Jupyterhub* (19:57 video) - https://www.youtube.com/watch?v=OuhtpxGuboY[+https://www.youtube.com/watch?v=OuhtpxGuboY+]

bqplot
^^^^^^

Plotting library for IPython/Jupyter Notebooks.

https://github.com/bloomberg/bqplot[https://github.com/bloomberg/bqplot]

jupyter-drive
^^^^^^^^^^^^^

This repository contains custom Contents classes that allows IPython to use
Google Drive for file management. The code is a organized as a python package
that contains functions to install a Jupyter Notebook JavaScript extension,
and activate/deactivate different IPython profiles to be used with Google
drive.

https://github.com/jupyter/jupyter-drive[+https://github.com/jupyter/jupyter-drive+]

jupyterhub
^^^^^^^^^^

Multi-user server for Jupyter notebooks.

https://github.com/jupyter/jupyterhub[+https://github.com/jupyter/jupyterhub+]

nbviewer
^^^^^^^^

Jupyter nbviewer is the web application behind The Jupyter Notebook Viewer,
which is graciously hosted by Rackspace.  Run this locally to get most of
the features of nbviewer on your own network.

https://github.com/jupyter/nbviewer[+https://github.com/jupyter/nbviewer+]

tmpnb
^^^^^

Creates temporary Jupyter Notebook servers using Docker containers.
This  launches a docker container for each user that requests one.
In practice, this gets used to
https://tmpnb.org[provide temporary notebooks], demo the IPython notebook as part of a
http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261[Nature article], or even
http://odewahn.github.io/publishing-workflows-for-jupyter/#1[provide Jupyter kernels for publications].

https://github.com/jupyter/tmpnb[+https://github.com/jupyter/tmpnb+]

JVM
~~~

A Java virtual machine (JVM) is an abstract computing machine. There are three
notions of the JVM: specification, implementation, and instance. The
specification is a book that formally describes what is required of a JVM
implementation. Having a single specification ensures all implementations are
interoperable. A JVM implementation is a computer program that implements
requirements of the JVM specification in a compliant and preferably performant
manner. An instance of the JVM is a process that executes a computer program
compiled into Java bytecode.

http://en.wikipedia.org/wiki/Java_virtual_machine[+http://en.wikipedia.org/wiki/Java_virtual_machine+]

http://en.wikipedia.org/wiki/List_of_Java_virtual_machines[+http://en.wikipedia.org/wiki/List_of_Java_virtual_machines+]

http://en.wikipedia.org/wiki/List_of_JVM_languages[+http://en.wikipedia.org/wiki/List_of_JVM_languages+]

* Clojure - http://en.wikipedia.org/wiki/Clojure[+http://en.wikipedia.org/wiki/Clojure+]

* Groovy -
http://en.wikipedia.org/wiki/Groovy_%28programming_language%29[+http://en.wikipedia.org/wiki/Groovy_%28programming_language%29+]

* Scala -
http://en.wikipedia.org/wiki/Scala_%28programming_language%29[+http://en.wikipedia.org/wiki/Scala_%28programming_language%29+]

* JRuby -
http://en.wikipedia.org/wiki/JRuby[+http://en.wikipedia.org/wiki/JRuby+]

* Jython -
http://en.wikipedia.org/wiki/Jython[+http://en.wikipedia.org/wiki/Jython+]

[[NK]]
////
NKKK
////

Karta
~~~~~

A simple and fast framework for spatial analysis in Python.
It contains clean vector and raster data types that are coordinate
system-aware, implementations of frequently-used of geospatial analysis
methods, and the read/write interfaces to several formats, including GeoJSON,
shapefiles, and ESRI ASCII.

https://github.com/njwilson23/karta[+https://github.com/njwilson23/karta+]

Kartograph
~~~~~~~~~~

Kartograph is a simple and lightweight framework for building interactive map
applications without Google Maps or any other mapping service. It was created
with the needs of designers and data journalists in mind.
Actually, Kartograph is two libraries. One generates beautiful & compact SVG
maps; the other helps you to create interactive maps that run across all major
browsers.

The Kartograph.py library is a Python library for generating beautiful,
Illustrator-friendly SVG maps.
The Kartograph.js library is a JavaScript library for creating interactive
maps based on Kartograph.py SVG maps.

http://kartograph.org/[+http://kartograph.org/+]

kibana
~~~~~~

Visualize logs and time-stamped data.
Elasticsearch works seamlessly with Kibana to let you see and interact with
your data.

http://www.elasticsearch.org/overview/kibana/[+http://www.elasticsearch.org/overview/kibana/+]

[[KiCad]]
KiCad
~~~~~

KiCad is an open source software suite for Electronic Design Automation (EDA). The programs handle Schematic Capture, and PCB Layout with Gerber output.

http://kicad-pcb.org/[+http://kicad-pcb.org/+]

KML
~~~

Keyhole Markup Language (KML) is an XML notation for expressing geographic
annotation and visualization within Internet-based, two-dimensional maps and
three-dimensional Earth browsers.
The KML file specifies a set of features (place marks, images, polygons, 3D
models, textual descriptions, etc.) for display in Here Maps, Google Earth,
Maps and Mobile, or any other geospatial software implementing the KML
encoding. Each place always has a longitude and a latitude. Other data can
make the view more specific, such as tilt, heading, altitude, which together
define a "camera view" along with a timestamp or timespan. KML shares some of
the same structural grammar as GML. Some KML information cannot be viewed in
Google Maps or Mobile.

KML files are very often distributed in KMZ files, which are zipped KML files
with a .kmz extension. These must be legacy (ZIP 2.0) compression compatible
(i.e. stored or deflate method), otherwise the .kmz file might not uncompress
in all geobrowsers. The contents of a KMZ file are a single root KML
document (notionally "doc.kml") and optionally any overlays, images, icons,
and COLLADA 3D models referenced in the KML including network-linked KML
files. The root KML document by convention is a file named "doc.kml" at the
root directory level, which is the file loaded upon opening. By convention the
root KML document is at root level and referenced files are in subdirectories
(e.g. images for overlay images).

http://en.wikipedia.org/wiki/Keyhole_Markup_Language[+http://en.wikipedia.org/wiki/Keyhole_Markup_Language+]

https://developers.google.com/kml/[+https://developers.google.com/kml/+]

[[fastkml]]
fastkml
^^^^^^^

Fastkml is a library to read, write and manipulate KML files. It aims to keep
it simple and fast (using lxml if available). Fast refers to the time you
spend to write and read KML files as well as the time you spend to get
aquainted to the library or to create KML objects. It aims to provide all of
the functionality that KML clients such as OpenLayers, Google Maps, and Google
Earth provides.

https://github.com/cleder/fastkml[+https://github.com/cleder/fastkml+]

https://fastkml.readthedocs.io/en/latest/[+https://fastkml.readthedocs.io/en/latest/+]

KPP
~~~

The KPP kinetic preprocessor is a software tool that assists the computer
simulation of chemical kinetic systems. The concentrations of a chemical
system evolve in time according to the differential law of mass action
kinetics. A numerical simulation requires an implementation of the
differential laws and a numerical integration in time.

KPP translates a specification of the chemical mechanism into Fortran77,
Fortran90, C, or Matlab simulation code that implements the concentration time
derivative function, its Jacobian, and it Hessian, together with a suitable
numerical integration scheme. Sparsity in Jacobian/Hessian is carefully
exploited in order to obtain computational efficiency.

KPP incorporates a library with several widely used atmospheric chemistry
mechanisms; the users can add their own chemical mechanisms to the library.
KPP also includes a comprehensive suite of stiff numerical integrators. The
KPP development environment is designed in a modular fashion and allows for
rapid prototyping of new chemical kinetic schemes as well as new numerical
integration methods. 

http://people.cs.vt.edu/\~asandu/Software/Kpp/[+http://people.cs.vt.edu/~asandu/Software/Kpp/+]

http://en.wikipedia.org/wiki/Kinetic_PreProcessor[+http://en.wikipedia.org/wiki/Kinetic_PreProcessor+]

http://wiki.seas.harvard.edu/geos-chem/index.php/KPP_solvers_FAQ[+http://wiki.seas.harvard.edu/geos-chem/index.php/KPP_solvers_FAQ+]

Krita
~~~~~

Krita is a KDE program for sketching and painting, offering an end–to–end
solution for creating digital painting files from scratch by masters. Fields
of painting that Krita explicitly supports are concept art, creation of comics
and textures for rendering. Modeled on existing real-world painting materials
and workflows, Krita supports creative working by getting out of the way and
with a snappy response.
There are three versions of Krita: Krita Sketch, for touch devices, Krita
Desktop desktop systems and finally Krita Studio, which is like Krita Desktop
but supported by KO GmbH.

https://krita.org/[+https://krita.org/+]

Natron
^^^^^^

A free and open source video compositing software, similar in functionality to
Adobe After Effects or Nuke by The Foundry.
The project is a free node-based compositor that relies on OpenColorIO for
color management, OpenImageIO for file formats support, and Qt for user
interface. It also works with 32bit float per channel precision and supports
OFX plugins, both free and commercial.

http://natron.inria.fr/[+http://natron.inria.fr/+]

https://github.com/MrKepzie/Natron[+https://github.com/MrKepzie/Natron+]

[[KSTAR]]
KSTAR
~~~~~

The KSTAR project supports the development of Klang, a source-to-source
compiler that
turns C programs with xref:OpenMP[OpenMP] pragmas to C programs with calls to either the
StarPU or
the Kaapi runtime system.
The features include OpenMP 3.1, the OpenMP 4.0 +depend+ clause,
Accelerators extensions, and C/Cpp source-to-source translation
based on xref:Clang[Clang].

http://kstar.gforge.inria.fr/[+http://kstar.gforge.inria.fr/+]

[[StarPU]]
StarPU
^^^^^^

A unified runtime system for heterogeneous multicore architectures.

Software that uses StarPU to run on heterogeneous architectures
includes xref:MAGMA[MAGMA],
xref:SkePU[SkePU] and
xref:PaStiX[PaStiX].

http://runtime.bordeaux.inria.fr/StarPU/[+http://runtime.bordeaux.inria.fr/StarPU/+]

[[XKAAPI]]
[[KAAPI]]
XKAAPI
^^^^^^

A runtime for scheduling irregular fine grain tasks with data flow
dependencies. It could be used through xref:OpenMP[OpenMP]-4.0 compliant applications using
GNU C or `Cxx` compiler, Intel compilers or our the research C/`Cxx`
source-to-source compiler KSTAR.
It is a C library that allows to execute multithreaded computation with data
flow synchronization between threads.
The library is able to schedule fine/medium size grain program on distributed
machine. The data flow graph is dynamic (unfold at runtime). Target
architectures are clusters of SMP machines.

http://kaapi.gforge.inria.fr/[+http://kaapi.gforge.inria.fr/+]

[[NL]]
////
NLLL
////

LabPlot
~~~~~~~

LabPlot is an application for interactive graphing and analysis of scientific
data.

https://edu.kde.org/applications/science/labplot/[+https://edu.kde.org/applications/science/labplot/+]

[[LAPACK]]
LAPACK
~~~~~~

The Linear Algebra Package is a standard software library for numerical linear algebra. It provides routines for solving systems of linear equations and linear least squares, eigenvalue problems, and singular value decomposition. It also includes routines to implement the associated matrix factorizations such as LU, QR, Cholesky and Schur decomposition. LAPACK was originally written in FORTRAN 77, but moved to Fortran 90 in version 3.2 (2008).[1] The routines handle both real and complex matrices in both single and double precision.

LAPACK was designed to effectively exploit the caches on modern cache-based architectures and can run very fast given a well-tuned xref:BLAS[BLAS] implementation.
The routines are written so that as much as possible of the computation is performed by calls to the Basic Linear Algebra Subprograms (BLAS). LAPACK is designed at the outset to exploit the Level 3 BLAS — a set of specifications for Fortran subprograms that do various types of matrix multiplication and the solution of triangular systems with multiple right-hand sides. Because of the coarse granularity of the Level 3 BLAS operations, their use promotes high efficiency on many high-performance computers, particularly if specially coded implementations are provided by the manufacturer.
LAPACK has also been extended to run on distributed-memory systems in later packages such as ScaLAPACK and PLAPACK.

http://www.netlib.org/lapack/[+http://www.netlib.org/lapack/+]

[[Leaflet]]
Leaflet
~~~~~~~

A modern open-source JavaScript library for mobile-friendly interactive maps.
Leaflet is designed with simplicity, performance and usability in mind. It
works efficiently across all major desktop and mobile platforms out of the
box, taking advantage of HTML5 and CSS3 on modern browsers while still being
accessible on older ones. It can be extended with a huge amount of plugins.

http://leafletjs.com/[+http://leafletjs.com/+]

https://github.com/Leaflet/Leaflet[+https://github.com/Leaflet/Leaflet+]

http://leafletjs.com/plugins.html[+http://leafletjs.com/plugins.html+]

*Getting Started with Leaflet* - https://switch2osm.org/using-tiles/getting-started-with-leaflet/[+https://switch2osm.org/using-tiles/getting-started-with-leaflet/+]

*Basics of Making Maps with Leaflet and Browswerify* - http://learnjs.io/blog/2013/11/08/leaflet-basics/[+http://learnjs.io/blog/2013/11/08/leaflet-basics/+]

[[Folium]]
Folium
^^^^^^

Folium builds on the data wrangling strengths of the Python ecosystem and the
mapping strengths of the Leaflet.js library. Manipulate your data in Python,
then visualize it in on a Leaflet map via Folium.

Folium makes it easy to visualize data that’s been manipulated in Python on an
interactive Leaflet map. It enables both the binding of data to a map for
choropleth visualizations as well as passing Vincent/Vega visualizations as
markers on the map.

The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and
Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. Folium
supports both GeoJSON and TopoJSON overlays, as well as the binding of data to
those overlays to create choropleth maps with color-brewer color schemes.

https://folium.readthedocs.org/en/latest/[+https://folium.readthedocs.org/en/latest/+]

[[Leptonica]]
Leptonica
~~~~~~~~~

A pedagogically-oriented open source site containing software that is broadly useful for image processing and image analysis applications.

http://www.leptonica.com/[+http://www.leptonica.com/+]

[[libavcodec]]
libavcodec
~~~~~~~~~~

A free and open-source library of codecs for encoding and decoding video and audio data.[5] Because of a project fork, libraries with this name are
provided by xref:FFmpeg[FFmpeg] and xref:libav[libav], but they are incompatible.

This is an integral part of many open-source multimedia applications and frameworks. The popular MPlayer, xine and VLC media players use it as their main, built-in decoding engine that enables playback of many audio and video formats on all supported platforms. It is also used by the ffdshow tryouts decoder as its primary decoding library. libavcodec is also used in video editing and transcoding applications like Avidemux, MEncoder or Kdenlive for both decoding and encoding.

Libavcodec contains decoder and sometimes encoder implementations of several proprietary formats, including ones for which no public specification has been released. As such, a significant reverse engineering effort is part of libavcodec development. Having such codecs available within the standard libavcodec framework gives a number of benefits over using the original codecs, most notably increased portability, and in some cases also better performance, since libavcodec contains a standard library of highly optimized implementations of common building blocks, such as DCT and color space conversion.

https://en.wikipedia.org/wiki/Libavcodec[+https://en.wikipedia.org/wiki/Libavcodec+]

[[libav]]
libav
^^^^^

Provides cross-platform tools and libraries to convert, manipulate and stream a wide range of multimedia formats and protocols.

https://libav.org/[+https://libav.org/+]

libcloudphysics
~~~~~~~~~~~~~~~

A `Cxx` library of algorithms for representing cloud microphysics in numerical models.
Currently (6/15) the library covers three warm-rain schemes: the single- and double-moment bulk schemes, and the particle-based scheme with Monte-Carlo coalescence.
This requires xref:Boost[Boost] and xref:Thrust[Thrust].

http://libcloudphxx.igf.fuw.edu.pl/[+http://libcloudphxx.igf.fuw.edu.pl/+]

https://github.com/igfuw/libcloudphxx[+https://github.com/igfuw/libcloudphxx+]

http://www.geosci-model-dev.net/8/1677/2015/gmd-8-1677-2015.html[+http://www.geosci-model-dev.net/8/1677/2015/gmd-8-1677-2015.html+]

libeemd
~~~~~~~

libeemd is a C library for performing the ensemble empirical mode
decomposition (EEMD), its complete variant (CEEMDAN) or the regular empirical
mode decomposition (EMD). It includes a Python interface called pyeemd. The
details of what libeemd actually computes are available as a separate article,
which you should read if you are unsure about what EMD, EEMD and CEEMDAN are.

https://bitbucket.org/luukko/libeemd[+https://bitbucket.org/luukko/libeemd+]

http://pyeemd.readthedocs.org/en/latest/index.html[+http://pyeemd.readthedocs.org/en/latest/index.html+]

libfabric
~~~~~~~~~

The Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric
communication services to applications. Libfabric is a software library
instantiation of OFI.

The goal of OFI and libfabric is to define interfaces that enable a tight
semantic map between applications and underlying fabric services.
Specifically, libfabric software interfaces have been co-designed with fabric
hardware providers and application developers, with a focus on the needs of
HPC users. OFI supports multiple interface semantics, is fabric and hardware
implementation agnostic, and leverages and expands the existing RDMA open
source community.

Libfabric is designed to minimize the impedance mismatch between applications,
including middleware such as MPI, SHMEM, and PGAS, and fabric communication
hardware. Its interfaces target high-bandwidth, low-latency NICs, with a goal
to scale to tens of thousands of nodes.

http://ofiwg.github.io/libfabric/[+http://ofiwg.github.io/libfabric/+]

https://github.com/ofiwg/libfabric[+https://github.com/ofiwg/libfabric+]

LibFlatArray
~~~~~~~~~~~~

This acts as a highly efficient multi-dimensional array of arbitrary objects,
but really uses a struct of arrays memory layout. It's great for writing
vectorized code and its lightning-fast iterators give you access to
neighboring elements with zero address generation overhead. 

http://www.libgeodecomp.org/libflatarray.html[+http://www.libgeodecomp.org/libflatarray.html+]

LibGeoDecomp
~~~~~~~~~~~~

LibGeoDecomp (Library for Geometric Decomposition codes) is an
auto-parallelizing library for computer simulations. It is written in
`Cxx` and works best with kernels written in `Cxx`, but other languages
(e.g. Fortran) may be linked in, too. Thanks to its modular design the
library can harness all state of the art hardware architectures, e.g.
multi-core CPUs,
GPUs (currently only NVIDIA GPUs, via CUDA),
Intel Xeon Phi, MPI clusters and Raspberry Pi.

The library takes over the spatial and temporal
loops of the simulation as well as storage of the simulation data. It
will call back the user code for performing the actual computations.
User code in turn calls back the brary to access simulation data.
Thanks to this two-way callback the library can control which part of
the code runs when.

Users can build custom computer simulations (e.g. engineering or
natural sciences problems) by encapsulating their model in a `Cxx`
class. This class is then supplied to the library as a template
parameter. The library essentially relieves the user from the pains of
parallel programming, but is limited to applications which perform
space- and time-discrete simulations with only local interactions.

https://github.com/STEllAR-GROUP/libgeodecomp[+https://github.com/STEllAR-GROUP/libgeodecomp+]

http://www.libgeodecomp.org/[+http://www.libgeodecomp.org/+]

libguestfs
~~~~~~~~~~

A set of tools for accessing and modifying virtual machine (VM) disk images.
You can use this for viewing and editing files inside guests, scripting
changes to VMs, monitoring disk used/free statistics, creating guests, P2V,
V2V, performing backups, cloning VMs, building VMs, formatting disks, resizing
disks, and much more.

libguestfs can access almost any disk image imaginable. It can do it securely
— without needing root and with multiple layers of defence against rogue disk
images. It can access disk images on remote machines or on CDs/USB sticks. It
can access proprietary systems like VMware and Hyper-V.

All this functionality is available through a scriptable shell called
guestfish, or an interactive rescue shell virt-rescue.

libguestfs is a C library that can be linked with C and `Cxx` management
programs and has bindings for about a dozen other programming languages. Using
our FUSE module you can also mount guest filesystems on the host. 

http://libguestfs.org/[+http://libguestfs.org/+]

https://github.com/libguestfs/libguestfs[+https://github.com/libguestfs/libguestfs+]

https://apps.fedoraproject.org/packages/libguestfs[+https://apps.fedoraproject.org/packages/libguestfs+]

*Using libguestfs to Easily Access Linux Virtual Machine File Systems* - http://searchenterpriselinux.techtarget.com/tip/Using-libguestfs-to-easily-access-Linux-virtual-machine-file-systems[+http://searchenterpriselinux.techtarget.com/tip/Using-libguestfs-to-easily-access-Linux-virtual-machine-file-systems+]

libguestfs-tools
^^^^^^^^^^^^^^^^

This package contains miscellaneous system administrator command line tools for virtual machines.

https://apps.fedoraproject.org/packages/libguestfs-tools[+https://apps.fedoraproject.org/packages/libguestfs-tools+]

libmpdata
~~~~~~~~~

A library of parallel forward-in-time solvers for systems of generalised transport equations. The solvers belong to the Multidimensional Positive Definite Advection Transport Algorithm (MPDATA) family of numerical schemes.
This requires xref:Blitz[Blitz], xref:Boost[Boost] and xref:HDF5[HDF5].

http://libmpdataxx.igf.fuw.edu.pl/[+http://libmpdataxx.igf.fuw.edu.pl/+]

https://github.com/igfuw/libmpdataxx[+https://github.com/igfuw/libmpdataxx+]

http://arxiv.org/abs/1407.1309[+http://arxiv.org/abs/1407.1309+]

http://www.geosci-model-dev.net/8/1005/2015/gmd-8-1005-2015.html[+http://www.geosci-model-dev.net/8/1005/2015/gmd-8-1005-2015.html+]

Libra
~~~~~

The Libra Toolkit is a collection of algorithms for learning and inference
with discrete probabilistic models, including Bayesian networks (BNs), Markov
networks (MNs), dependency networks (DNs), sum-product networks (SPNs), and
arithmetic circuits (ACs). Compared to other toolkits, Libra focuses more on
structure learning, especially for tractable models in which exact inference
is efficient. Each algorithm in Libra is implemented as a command-line program
suitable for interactive use or scripting, with consistent options and file
formats throughout the toolkit. 

All methods are implemented in OCaml, in order to obtain the best possible
speed while keeping the code compact and easy to work with. To compile the
source code, you must have a UNIX-like environment (Linux, OS X) with a recent
version of OCaml installed. 

http://libra.cs.uoregon.edu/[+http://libra.cs.uoregon.edu/+]

http://arxiv.org/abs/1504.00110[+http://arxiv.org/abs/1504.00110+]


libRoadRunner
~~~~~~~~~~~~~

An extensible, high-performance, cross-platform, open-source software library
for the simulation and analysis of models expressed using Systems Biology
Markup Language (SBML). SBML is the most widely used standard for representing
dynamic networks, especially biochemical networks. libRoadRunner supports
solution of both large models and multiple replicas of a single model on
desktop, mobile and cluster computers. libRoadRunner is a self-contained
library, able to run both as a component inside other tools via its Cxx and C
bindings andnteractively through its Python interface.
libRoadRunner uses a custom Just-In-Time (JIT) compiler built on the
widely-used LLVM JIT compiler framework to compile SBML-specified models
directly into very fast native machine code for a variety of processors,
making it appropriate for solving very large models or multiple replicas of
smaller models. libRoadRunner is flexible, supporting the bulk of the SBML
specification (except for delay and nonlinear algebraic equations) and several
of its extensions. It offers multiple deterministic and stochastic
integrators, as well as tools for steady-state, stability analyses and flux
balance analysis.

http://arxiv.org/abs/1503.01095[+http://arxiv.org/abs/1503.01095+]

http://libroadrunner.org/[+http://libroadrunner.org/+]

libuv
~~~~~

A multi-platform support library with a focus on asynchronous I/O. It was
primarily developed for use by Node.js, but it's also used by Luvit, Julia,
pyuv, and others.

https://github.com/libuv/libuv[+https://github.com/libuv/libuv+]

https://github.com/libuv/libuv/wiki/Projects-that-use-libuv[+https://github.com/libuv/libuv/wiki/Projects-that-use-libuv+]

*An Introduction to libuv* - http://nikhilm.github.io/uvbook/index.html[+http://nikhilm.github.io/uvbook/index.html+]

pyuv
^^^^

A Python module which provides an interface to libuv.

https://pypi.python.org/pypi/pyuv[+https://pypi.python.org/pypi/pyuv+]

http://pyuv.readthedocs.org/en/v1.x/[+http://pyuv.readthedocs.org/en/v1.x/+]

Lighthouse
~~~~~~~~~~

Lighthouse is a framework for creating, maintaining, and using a taxonomy of
available software that can be used to build highly-optimized matrix algebra
computations. The taxonomy provides an organized anthology of software
components and programming tools needed for that task. The taxonomy will serve
as a guide to practitioners seeking to learn what is available for their
programming tasks, how to use it, and how the various parts fit together. It
builds upon and improves existing collections of numerical software, adding
tools for the tuning of matrix algebra computations.

https://github.com/LighthouseHPC/lighthouse[+https://github.com/LighthouseHPC/lighthouse+]

http://arxiv.org/abs/1408.1363[+http://arxiv.org/abs/1408.1363+]

Limulus
~~~~~~~

Limulus is an acronym for LInux MULti-core Unified Supercomputer. The Limulus
project goal is to create and maintain an open specification and software
stack for a personal workstation cluster. Ideally, a user should be able to
build or purchase a small personal workstation cluster using the Limulus
reference design and low cost hardware. In addition, a freely available
turn-key Linux based software stack will be created and maintained for use on
the Limulus design. A Limulus is inteneded to be a workstation cluster
platform where users can develop software, test ideas, run small scale
applications, and teach HPC methods.

http://limulus.basement-supercomputing.com/[+http://limulus.basement-supercomputing.com/+]

LinBox
~~~~~~

LinBox is a Cxx template library for exact, high-performance linear algebra
computation with dense, sparse, and structured matrices over the integers and
over finite fields.  LinBox aims to provide world-class high performance
implementations of the most advanced algorithms for exact linear algebra. 

LinBox was originally designed primarily to work with sparse and structured
matrices, which are defined in this context as those matrices where the
computational cost of application of an m by n matrix to a vector is
significantly less than O(mn), the cost for a dense matrix. Now, increasingly,
LinBox also has codes for dense matrix computations using floating point BLAS
routines for speed while not sacrificing exactness. 

LinBox implements iterative system-solving methods such as those of Wiedemann
and Lanczos to operate on very large, sparse linear systems. This avoids the
potential fill-in associated with elimination-based methods and keeps memory
use relatively constant through the computation. These methods allow LinBox to
be used to solve systems with hundreds of thousands of equations and hundreds
of thousands of variables. 

http://linalg.org/[+http://linalg.org/+]

LinuxCNC
~~~~~~~~

LinuxCNC (the Enhanced Machine Control) is a software system for computer
control of machine tools such as milling machines and lathes.
It provides:

* several graphical user interfaces including one for touch screens
* an interpreter for "G-code" (the RS-274 machine tool programming language)
* a realtime motion planning system with look-ahead
* operation of low-level machine electronics such as sensors and motor
drives
* an easy to use "breadboard" layer for quickly creating a unique
configuration for your machine
* a software PLC programmable with ladder diagrams
* easy installation with .deb packages or a Live-CD

It does not provide drawing (CAD - Computer Aided Design) or G-code generation
from the drawing (CAM - Computer Automated Manufacturing) functions.

It can simultaneously move up to 9 axes and supports a variety of
interfaces.
The control can operate true servos (analog or PWM) with the feedback loop
closed by the LinuxCNC software at the computer, or open loop with
"step-servos" or stepper motors.
Motion control features include: cutter radius and length compensation,
path deviation limited to a specified tolerance, lathe threading, synchronized
axis motion, adaptive feedrate, operator feed override, and constant velocity
control.
Support for non-Cartesian motion systems is provided via custom kinematics
modules. Available architectures include hexapods (Stewart platforms and
similar concepts) and systems with rotary joints to provide motion such as
PUMA or SCARA robots.
LinuxCNC runs on Linux using real time extensions. Support currently
exists for version 2.4 and 2.6 Linux kernels with real time extensions applied
by RT-Linux or RTAI patches.

http://www.linuxcnc.org/[+http://www.linuxcnc.org/+]

http://linuxcnc.org/docs/html/gui/axis.html[+http://linuxcnc.org/docs/html/gui/axis.html+]

Linux Diminutives
~~~~~~~~~~~~~~~~~

Linux distributions that aren't (necessarily) lesser but rather smaller in size.

http://en.wikipedia.org/wiki/Linux_for_mobile_devices[+http://en.wikipedia.org/wiki/Linux_for_mobile_devices+]

CoreOS
^^^^^^

CoreOS is a new Linux distribution that has been rearchitected to provide
features needed to run modern infrastructure stacks. The strategies and
architectures that influence CoreOS allow companies like Google, Facebook and
Twitter to run their services at scale with high resilience.

https://coreos.com/[+https://coreos.com/+]

OpenEmbedded
^^^^^^^^^^^^

Welcome to OpenEmbedded, the build framework for embedded Linux. OpenEmbedded
offers a best-in-class cross-compile environment. It allows developers to
create a complete Linux Distribution for embedded systems.
The OpenEmbedded-Core Project (OE-Core for short) resulted from the merge of
the Yocto Project with OpenEmbedded.

http://www.openembedded.org/wiki/Main_Page[+http://www.openembedded.org/wiki/Main_Page+]

http://en.wikipedia.org/wiki/OpenEmbedded[+http://en.wikipedia.org/wiki/OpenEmbedded+]

Yocto
xxxx+

The Yocto Project is an open source collaboration project that provides
templates, tools and methods to help you create custom Linux-based systems for
embedded products regardless of the hardware architecture. It was founded in
2010 as a collaboration among many hardware manufacturers, open-source
operating systems vendors, and electronics companies to bring some order to
the chaos of embedded Linux development.

The Yocto Project provides resources and information catering to both new and
experienced users, and includes core system component recipes provided by the
OpenEmbedded project. The Yocto Project also provides pointers to example code
built demonstrating its capabilities. These community-tested images include
the Yocto Project kernel and cover several build profiles across multiple
architectures including ARM, PPC, MIPS, x86, and x86-64. Specific platform
support takes the form of Board Support Package (BSP) layers for which a
standard format has been developed. The project also provides an Eclipse IDE
plug-in and a graphical user interface to the build system called Hob.

https://www.yoctoproject.org/[+https://www.yoctoproject.org/+]

Replicant
^^^^^^^^^

Replicant is a fully free Android distribution running on several devices,
a free software mobile operating system putting the emphasis on freedom and
privacy/security.
It aims to replace all proprietary Android components with their free software
counterparts. This also makes it a security focused operating system as it
closes discovered Android backdoors.[4] It is available for several
smartphones and tablet computers.

http://www.replicant.us/[+http://www.replicant.us/+]

http://en.wikipedia.org/wiki/Replicant_%28operating_system%29[+http://en.wikipedia.org/wiki/Replicant_%28operating_system%29+]

Tizen
^^^^^

An  operating system based on the Linux kernel and the GNU C Library
implementing the Linux API. It targets a very wide range of devices including
smartphones, tablets, in-vehicle infotainment (IVI) devices, smart TVs, PCs,
smart cameras, wearable computing (such as smartwatches), Blu-ray players,
printers and smart home appliances[3] (such as refrigerators, lighting,
washing machines, air conditioners, ovens/microwaves and a robotic vacuum
cleaner[4]). Its purpose is to offer a consistent user experience across
devices. Tizen is a project within the Linux Foundation and is governed by a
Technical Steering Group (TSG) composed of Samsung and Intel among others.

HTML5 applications run on Tizen, Android, Firefox OS, Ubuntu Touch, Windows
Phone, and webOS without a browser.
Applications based on Qt, GTK+ and EFL frameworks can run on Tizen IVI.[28]
While there is no official support for these third-party frameworks, according
to the explanation on the Tizen SDK Web site,[29] Tizen applications for
mobile devices can be developed without relying on an official Tizen IDE as
long as the application complies with Tizen packaging rules. In May 2013, a
community port of Qt to Tizen focused on delivering native GUI controls and
integration of Qt with Tizen OS features for smartphones.[30] Based on the Qt
port to Tizen, Tizen and mer can interchange code.

https://www.tizen.org/[+https://www.tizen.org/+]

http://en.wikipedia.org/wiki/Tizen[+http://en.wikipedia.org/wiki/Tizen+]

webOS
^^^^^

WebOS, also known as webOS, LG webOS, Open webOS, or HP webOS, is a Linux
kernel-based multitask operating system for smart devices like TVs,[1] and
smartwatches;[2] and was formerly a mobile operating system.[3] Initially
developed by Palm, which was acquired by Hewlett-Packard, HP made the platform
open source, and it became Open webOS.

The WebOS mobile platform introduced features so innovative that some are
still in use by Apple, Microsoft and Google on their mobile operating systems
iOS, Windows Phone, and Android, respectively.

http://www.openwebosproject.org/[+http://www.openwebosproject.org/+]

http://www.theverge.com/2012/6/5/3062611/palm-webos-hp-inside-story-pre-postmortem[+http://www.theverge.com/2012/6/5/3062611/palm-webos-hp-inside-story-pre-postmortem+]

http://en.wikipedia.org/wiki/WebOS[+http://en.wikipedia.org/wiki/WebOS+]

http://www.hpwebos.com/us/[+http://www.hpwebos.com/us/+]

Linux Distributions
~~~~~~~~~~~~~~~~~~~

More for unusual and/or cute ones than those that are more well known.

Linux From Scratch
^^^^^^^^^^^^^^^^^^

Linux From Scratch (LFS) is a project that provides you with step-by-step
instructions for building your own custom Linux system, entirely from source
code. 

http://www.linuxfromscratch.org/[+http://www.linuxfromscratch.org/+]

LOCKSS
^^^^^^

The LOCKSS Program, based at Stanford University Libraries, provides libraries
and publishers with award-winning, low-cost, open source digital preservation
tools to preserve and provide access to persistent and authoritative digital
content.

We recommend installing the LOCKSS software on a dedicated server or a virtual
machine. We provide the LOCKSS software integrated with a Linux installation
based on CentOS 6.

http://lockss.stanford.edu/[+http://lockss.stanford.edu/+]

NixOS
^^^^^

A Linux distribution with a unique approach to package and configuration
management. Built on top of the Nix package manager, it is completely
declarative, makes upgrading systems reliable, and has many other advantages.

http://nixos.org/[+http://nixos.org/+]

Openwall
^^^^^^^^

Openwall GNU/*/Linux (or Owl for short) is a small security-enhanced Linux
distribution for servers, appliances, and virtual appliances.
Owl live CDs with remote SSH access are also good for recovering or installing
systems (whether with Owl or not). Another secondary use is for operating
systems and/or computer security courses, which benefit from the simple
structure of Owl and from our inclusion of the complete build environment.

http://www.openwall.com/[+http://www.openwall.com/+]

Linux Initialization
~~~~~~~~~~~~~~~~~~~~

See below.

http://en.wikipedia.org/wiki/Init[+http://en.wikipedia.org/wiki/Init+]

http://arxiv.org/abs/0706.2748[+http://arxiv.org/abs/0706.2748+]

etcd
^^^^

A highly-available key value store for shared configuration and service
discovery.  A component of CoreOS.

https://github.com/coreos/etcd[+https://github.com/coreos/etcd+]

fleet
^^^^^

This ties together systemd and etcd into a distributed init system. Think of
it as an extension of systemd that operates at the cluster level instead of
the machine level. This project is very low level and is designed as a
foundation for higher order orchestration.

https://github.com/coreos/fleet[+https://github.com/coreos/fleet+]

https://coreos.com/using-coreos/clustering/[+https://coreos.com/using-coreos/clustering/+]

init
^^^^

In Unix-based computer operating systems, init (short for initialization) is
the first process started during booting of the computer system. Init is a
daemon process that continues running until the system is shut down. It is the
direct or indirect ancestor of all other processes and automatically adopts
all orphaned processes. Init is started by the kernel using a hard-coded
filename; a kernel panic will occur if the kernel is unable to start it. Init
is typically assigned process identifier 1.

The design of init has diverged in Unix systems such as System III and System
V, from the functionality provided by the init in Research Unix and its BSD
derivatives. The usage on most Linux distributions is somewhat compatible with
System V, but some distributions, such as Slackware, use a BSD-style and
others, such as Gentoo, have their own customized version.

Several replacement init implementations have been written with attempt to
address design limitations in the standard versions. These include launchd,
the Service Management Facility, systemd and Upstart.

http://en.wikipedia.org/wiki/Init[+http://en.wikipedia.org/wiki/Init+]

systemd
^^^^^^^

A suite of basic building blocks for a Linux system. It provides a system and
service manager that runs as PID 1 and starts the rest of the system. systemd
provides aggressive parallelization capabilities, uses socket and D-Bus
activation for starting services, offers on-demand starting of daemons, keeps
track of processes using Linux control groups, supports snapshotting and
restoring of the system state, maintains mount and automount points and
implements an elaborate transactional dependency-based service control logic.
systemd supports SysV and LSB init scripts and works as a replacement for
sysvinit. Other parts include a logging daemon, utilities to control basic
system configuration like the hostname, date, locale, maintain a list of
logged-in users and running containers and virtual machines, system accounts,
runtime directories and settings, and daemons to manage simple network
configuration, network time synchronization, log forwarding, and name
resolution.

http://www.freedesktop.org/wiki/Software/systemd/[+http://www.freedesktop.org/wiki/Software/systemd/+]

http://0pointer.de/blog/projects/systemd.html[+http://0pointer.de/blog/projects/systemd.html+]

http://en.wikipedia.org/wiki/Systemd[+http://en.wikipedia.org/wiki/Systemd+]

http://0pointer.de/blog/projects/why.html[+http://0pointer.de/blog/projects/why.html+]

Upstart
^^^^^^^

Upstart is an event-based replacement for the /sbin/init daemon which handles
starting of tasks and services during boot, stopping them during shutdown and
supervising them while the system is running.
It was originally developed for the Ubuntu distribution, but is intended to be
suitable for deployment in all Linux distributions as a replacement for the
venerable System-V init. 

http://upstart.ubuntu.com/[+http://upstart.ubuntu.com/+]

Linux Package Managers
~~~~~~~~~~~~~~~~~~~~~~

dpkg
^^^^

The software at the base of the package management system in the free
operating system Debian and its numerous derivatives. dpkg is used to install,
remove, and provide information about .deb packages.

dpkg itself is a low level tool; higher level tools, such as APT, are used to
fetch packages from remote locations or deal with complex package relations.
Tools like aptitude or synaptic are more commonly used than dpkg on its own,
as they have a more sophisticated way of dealing with package relationships
and a friendlier interface.

http://en.wikipedia.org/wiki/Dpkg[+http://en.wikipedia.org/wiki/Dpkg+]

aptitude
xxxxxxxx

Aptitude is an Ncurses based FrontEnd to Apt, the debian package manager.
Since it is text based, it is run from a terminal or a CLI (command line
interface).

https://wiki.debian.org/Aptitude[+https://wiki.debian.org/Aptitude+]

http://anonscm.debian.org/cgit/aptitude/aptitude.git/[+http://anonscm.debian.org/cgit/aptitude/aptitude.git/+]

dkpg on Fedora
xxxxxxxxxxxxxx

An RPM file containing dpkg for Fedora distributions.

https://apps.fedoraproject.org/packages/dpkg[+https://apps.fedoraproject.org/packages/dpkg+]

Nix
^^^

Nix is a powerful package manager for Linux and other Unix systems that makes
package management reliable and reproducible. It provides atomic upgrades and
rollbacks, side-by-side installation of multiple versions of a package,
multi-user package management and easy setup of build environments.

http://nixos.org/nix/[+http://nixos.org/nix/+]

Nixpkgs
xxxxxx+

The Nix Packages collection (Nixpkgs) is a set of nearly 6,500 packages for
the Nix package manager, released under a permissive MIT/X11 license.
On GNU/Linux, the packages in Nixpkgs are ‘pure’, meaning that they have no
dependencies on packages outside of the Nix store. This means that they should
work on pretty much any GNU/Linux distribution.

http://nixos.org/nixpkgs/[+http://nixos.org/nixpkgs/+]

OpenPKG
^^^^^^^

OpenPKG provides a flexible and extensive toolkit of about 1500 portable and
high-quality Unix server software packages within a fully self-contained
packaging framework.
OpenPKG 4 supports all major Unix server platforms, including BSD, GNU/Linux,
Solaris and MacOS X flavors, and can be deployed multiple times on a single
system without virtualization technologies and with minimum intrusion.
The OpenPKG software distribution is updated daily
and hence always provides you with the latest Open Source
server software.

http://www.openpkg.org/[+http://www.openpkg.org/+]

http://download.openpkg.org/packages/current/source/[+http://download.openpkg.org/packages/current/source/+]

RPM
^^^

The RPM Package Manager (RPM) is a powerful command line driven package
management system capable of installing, uninstalling, verifying, querying,
and updating computer software packages. Each software package consists of an
archive of files along with information about the package like its version, a
description, and the like. There is also a library API, permitting advanced
developers to manage such transactions from programming languages such as C or
Python. 

http://www.rpm.org/[+http://www.rpm.org/+]

http://en.wikipedia.org/wiki/RPM_Package_Manager[+http://en.wikipedia.org/wiki/RPM_Package_Manager+]

LISP
~~~~

Antik
^^^^^

Antik provides a foundation for scientific and engineering computation in
Common Lisp. It is designed not only to facilitate numerical computations, but
to permit the use of numerical computation libraries and the interchange of
data and procedures, whether foreign (non-lisp) or Lisp libraries. It is named
after the Antikythera mechanism, one of the oldest examples of a scientific
computer known. 

http://repo.or.cz/w/antik.git[+http://repo.or.cz/w/antik.git+]

https://www.common-lisp.net/project/antik/[+https://www.common-lisp.net/project/antik/+]

CL21
^^^^

CL21 is an experimental project redesigning Common Lisp.

http://cl21.org/[+http://cl21.org/+]

FEMLISP
^^^^^^^

FEMLISP is a Common Lisp framework for solving partial differential equations
with the help of the finite element method (FEM).

http://www.femlisp.org/[+http://www.femlisp.org/+]

Hy
^^

In a nutshell, Hy is a Lisp dialect, but one that converts its structure into
Python ... literally a conversion into Python’s abstract syntax tree! (Or to
put it in more crude terms, Hy is lisp-stick on a Python!)

This is pretty cool because it means Hy is several things:

* A Lisp that feels very Pythonic
* For Lispers, a great way to use Lisp’s crazy powers but in the wide world of
Python’s libraries (why yes, you now can write a Django application in
Lisp!)
* For Pythonistas, a great way to start exploring Lisp, from the comfort of
Python!
* For everyone: a pleasant language that has a lot of neat ideas!

http://docs.hylang.org/en/latest/[+http://docs.hylang.org/en/latest/+]

https://pypi.python.org/pypi/hy[+https://pypi.python.org/pypi/hy+]

https://github.com/hylang/hy[+https://github.com/hylang/hy+]

mal
^^^

Mal is an Clojure inspired Lisp interpreter.

Mal is implemented in 26 different languages.

Mal is a learning tool. Each implementation of mal is separated into 11
incremental, self-contained (and testable) steps that demonstrate core
concepts of Lisp. The last step is capable of self-hosting (running the mal
implemenation of mal).

https://github.com/kanaka/mal[+https://github.com/kanaka/mal+]

Parenscript
^^^^^^^^^^^

Parenscript is a translator from an extended subset of Common Lisp to JavaScript. Parenscript code can run almost identically on both the browser (as JavaScript) and server (as Common Lisp).

Parenscript code is treated the same way as Common Lisp code, making the full power of Lisp macros available for JavaScript. This provides a web development environment that is unmatched in its ability to reduce code duplication and provide advanced metaprogramming facilities to web developers.

https://common-lisp.net/project/parenscript/[+https://common-lisp.net/project/parenscript/+]

Sigil
xxxx+

A Parenscript to Javascript command line compiler and REPL.

https://www.npmjs.com/package/sigil-cli[+https://www.npmjs.com/package/sigil-cli+]

Quicklisp
^^^^^^^^^

Quicklisp makes it easy to get started with a rich set of community-developed
Common Lisp libraries. 
Quicklisp is a library manager for Common Lisp. It works with your existing
Common Lisp implementation to download, install, and load any of over 1,100
libraries with a few simple commands. 
Quicklisp is easy to install and works with ABCL, Allegro CL, Clozure CL,
CLISP, CMUCL, ECL, LispWorks, SBCL, and Scieneer CL, on Linux.

http://www.quicklisp.org/[+http://www.quicklisp.org/+]

http://www.quicklisp.org/beta/releases.html[+http://www.quicklisp.org/beta/releases.html+]

Livingstone2
~~~~~~~~~~~~

Livingstone2 is a reusable artificial intelligence (AI) software system
designed to assist spacecraft, life support systems, chemical plants or other
complex systems in operating robustly with minimal human supervision, even in
the face of hardware failures or unexpected events. Livingstone2 diagnoses the
current state of the spacecraft or other system and recommends commands or
repair actions that will allow the system to continue operations.

Livingstone2 is an enhancement and re-engineering of the Livingstone diagnosis
system that was flight tested on-board the Deep Space One spacecraft in May
1999. It contains significant enhancements to robustness, performance and
usability. Livingstone2 is able to track multiple diagnostic hypotheses, as
opposed to a single hypothesis in Livingstone. It is also able to revise
diagnostic decisions made in the past when additional observations become
available. In such cases, Livingstone might find the incorrect hypothesis.
These improvements increase robustness.

Re-architecting and re-implementing the system in Cxx has increased
performance. Usability has been vastly improved by creating a set of
development tools which are closely integrated with the Livingstone2 engine.
In addition to the core diagnosis engine, Livingstone2 now includes a compiler
than translates diagnostic models written in a Java-like language into
Livingstone2's language, and a broad set of graphical tools for model
development. These software tools support the rapid deployment of model-based
representations of complex systems for Livingstone2 via a visual model
builder/tester (Stanley), and two graphical user interface tools (Candidate
Manager and History Table) which provide Livingstone2 status information
during testing. Runtime support is provided by the real-time interface (RTI)
which converts analog sensor readings to the digital values required by
Livingstone2.

Also included in the Livingstone2 download is Oliver, a prototype model
builder/tester, which is however incomplete, but could be used as a starting
place for a new model builder/tester. 

http://ti.arc.nasa.gov/opensource/projects/livingstone2/[+http://ti.arc.nasa.gov/opensource/projects/livingstone2/+]

LLVM
~~~~

The LLVM compiler infrastructure project (formerly Low Level Virtual Machine)
is a compiler infrastructure designed as a set of reusable libraries with
well-defined interfaces. It is written in Cxx and is designed for
compile-time, link-time, run-time, and "idle-time" optimization of programs
written in arbitrary programming languages. Originally implemented for C and
Cxx, the language-agnostic design (and the success) of LLVM has since spawned
a wide variety of front ends: languages with compilers that use LLVM include
Common Lisp, ActionScript, Ada, D, Fortran, OpenGL Shading Language, Go,
Haskell, Java bytecode, Julia, Objective-C, Swift, Python, Ruby, Rust,
Scala, C# and Lua.

http://en.wikipedia.org/wiki/LLVM[+http://en.wikipedia.org/wiki/LLVM+]

http://llvm.org/[+http://llvm.org/+]

http://llvm.org/docs/tutorial/[+http://llvm.org/docs/tutorial/+]

*LLMV Weekly Newsletter* - http://llvmweekly.org/[+http://llvmweekly.org/+]

*LLVM for Grad Students* - http://adriansampson.net/blog/llvm.html[+http://adriansampson.net/blog/llvm.html+]

LLVMLinux
^^^^^^^^^

This project aims to fully build the Linux kernel using Clang which is the C
front end for the LLVM compiler infrastructure project. Together Clang and
LLVM have many positive attributes and features which many developers and
system integrators would like to take advantage of when developing and
deploying the Linux Kernel as a part of their own projects.

http://llvm.linuxfoundation.org/index.php/Main_Page[+http://llvm.linuxfoundation.org/index.php/Main_Page+]

Pure
^^^^

Pure is a modern-style functional programming language based on term
rewriting. It offers equational definitions with pattern matching, full
symbolic rewriting capabilities, dynamic typing, eager and lazy evaluation,
lexical closures, built-in list and matrix support and an easy-to-use C
interface. The interpreter uses LLVM as a backend to JIT-compile Pure programs
to fast native code.

http://purelang.bitbucket.org/[+http://purelang.bitbucket.org/+]

pykaleidoscope
^^^^^^^^^^^^^^

Implementation of the LLVM tutorial in Python.

https://github.com/eliben/pykaleidoscope/[+https://github.com/eliben/pykaleidoscope/+]

Souper
^^^^^^

Souper is a superoptimizer for LLVM IR. It uses an SMT solver to help identify
missing peephole optimizations in LLVM's midend optimizers.

https://github.com/google/souper[+https://github.com/google/souper+]

loo.py
~~~~~~

A large amount of numerically-oriented code is written and is being written in
legacy languages. Much of this code could, in principle, make good use of
data-parallel throughput-oriented computer architectures. Loo.py, a
transformation-based programming system targeted at GPUs and general
data-parallel architectures, provides a mechanism for user-controlled
transformation of array programs. This transformation capability is designed
to not just apply to programs written specifically for Loo.py, but also those
imported from other languages such as Fortran. It eases the trade-off between
achieving high performance, portability, and programmability by allowing the
user to apply a large and growing family of transformations to an input
program. These transformations are expressed in and used from Python and may
be applied from a variety of settings, including a pragma-like manner from
other languages. 

http://wiki.tiker.net/Loopy[+http://wiki.tiker.net/Loopy+]

https://github.com/inducer/loopy[+https://github.com/inducer/loopy+]

http://arxiv.org/abs/1503.07659[+http://arxiv.org/abs/1503.07659+]

http://mathema.tician.de/software/loopy/[+http://mathema.tician.de/software/loopy/+]

https://pypi.python.org/pypi/loo.py[+https://pypi.python.org/pypi/loo.py+]

[[LOVECLIM]]
LOVECLIM
~~~~~~~~

An acronym made from the names of the five different models that have been coupled to built the Earth system model: LOch–Vecode-Ecbilt-CLio-agIsm Model (LOVECLIM).

http://www.elic.ucl.ac.be/modx/elic/index.php?id=81[+http://www.elic.ucl.ac.be/modx/elic/index.php?id=81+]

http://www.elic.ucl.ac.be/modx/elic/index.php?id=289[+http://www.elic.ucl.ac.be/modx/elic/index.php?id=289+]

http://www.geosci-model-dev.net/3/603/2010/gmd-3-603-2010.html[+http://www.geosci-model-dev.net/3/603/2010/gmd-3-603-2010.html+]

LROSE
~~~~~

LROSE is an NSF-backed project to develop common software for the LIDAR, RADAR
and PROFILER community.

http://www.eol.ucar.edu/content/lrose[+http://www.eol.ucar.edu/content/lrose+]

LTRANS
~~~~~~

The Larval TRANSport Lagrangian model (LTRANS v.2b) is an off-line
particle-tracking model that runs with the stored predictions of a 3D
hydrodynamic model, specifically the Regional Ocean Modeling System (ROMS).
Although LTRANS was built to simulate oyster larvae, it can easily be adapted
to simulate passive particles and other planktonic organisms. LTRANS v.2 is
written in Fortran 90 and is designed to track the trajectories of particles
in three dimensions. It includes a 4th order Runge-Kutta scheme for particle
advection and a random displacement model for vertical turbulent particle
motion. Reflective boundary conditions, larval behavior, and settlement
routines are also included. 

http://northweb.hpl.umces.edu/LTRANS.htm[+http://northweb.hpl.umces.edu/LTRANS.htm+]

[[Lua]]
Lua
~~~

A powerful, fast, lightweight, embeddable scripting language. 
Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping. 

http://www.lua.org/[+http://www.lua.org/+]

http://luaforge.net/[+http://luaforge.net/+]

[[LuaJIT]]
LuaJIT
^^^^^^

A Just-In-Time Compiler (JIT) for the Lua programming language. Lua is a powerful, dynamic and light-weight programming language. It may be embedded or used as a general-purpose, stand-alone language. 
LuaJIT has been successfully used as a scripting middleware in games, appliances, network and graphics apps, numerical simulations, trading platforms and many other specialty applications. It scales from embedded devices, smartphones, desktops up to server farms. It combines high flexibility with high performance and an unmatched low memory footprint. 

http://luajit.org/[+http://luajit.org/+]

http://luajit.org/luajit.html[+http://luajit.org/luajit.html+]

https://github.com/LuaDist/luajit[+https://github.com/LuaDist/luajit+]

LuxMark
~~~~~~~

LuxMark is a OpenCL benchmark tool. The idea for the program was conceived in
2009 by Jean-Francois Romang. It was intended as a promotional tool
for LuxRender (to quote original Jromang's words: "LuxRender propaganda with
OpenCL"). The idea was quite simple, wrap SLG inside an easy to use graphical
user interface and use it as a benchmark for OpenCL.

http://www.luxrender.net/wiki/LuxMark[+http://www.luxrender.net/wiki/LuxMark+]

LVS
~~~

The Linux Virtual Server is a highly scalable and highly available server
built on a cluster of real servers, with the load balancer running on the
Linux operating system. The architecture of the server cluster is fully
transparent to end users, and the users interact as if it were a single
high-performance virtual server.

The real servers and the load balancers may be interconnected by either
high-speed LAN or by geographically dispersed WAN. The load balancers can
dispatch requests to the different servers and make parallel services of the
cluster to appear as a virtual service on a single IP address, and request
dispatching can use IP load balancing technolgies or application-level load
balancing technologies. Scalability of the system is achieved by transparently
adding or removing nodes in the cluster. High availability is provided by
detecting node or daemon failures and reconfiguring the system appropriately.

http://www.linuxvirtualserver.org/[+http://www.linuxvirtualserver.org/+]

[[NM]]
////
NMMM
////

[[MAGMA]]
MAGMA
~~~~~

Matrix algebra on GPU and multicore architectures.
The MAGMA project aims to develop a dense linear algebra library similar to
xref:LAPACK[LAPACK] but for heterogeneous/hybrid architectures, starting with current
Multicore+GPU systems.
MAGMA provides implementations for CUDA, Intel Xeon Phi, and OpenCL.

It is designed to be similar to LAPACK in functionality, data storage, and interface, and
to easily allow scientists to easily port their existing software components from LAPACK to MAGMA.
There are two types of LAPACK-style interfaces. The first one, referred to as the CPU interface, takes the input and produces the result in the CPU's memory. The second, referred to as the GPU interface, takes the input and produces the result in the GPU's memory. In both cases, a hybrid CPU/GPU algorithm is used. Also included is MAGMA BLAS, a complementary to CUBLAS routines. 

http://icl.cs.utk.edu/magma/index.html[+http://icl.cs.utk.edu/magma/index.html+]

cIMAGMA
^^^^^^^

clMAGMA is an OpenCL port of MAGMA. It supports AMD GPUs. The clMAGMA library
dependancies, in particular optimized GPU OpenCL BLAS and CPU optimized BLAS
and xref:LAPACK[LAPACK] for AMD hardware, can be found in the AMD clMath Libraries
(formerly APPML).

http://icl.cs.utk.edu/magma/software/index.html[+http://icl.cs.utk.edu/magma/software/index.html+]

[[make]]
make & variants
~~~~~~~~~~~~~~~

A useful albeit confusing tool on Linux systems.

Some alternatives to GNU classic make are listed below.

*Makefiles in Linux: An Overview* - http://www.codeproject.com/Articles/31488/Makefiles-in-Linux-An-Overview[+http://www.codeproject.com/Articles/31488/Makefiles-in-Linux-An-Overview+]

*Tutorial on Writing makefiles* - http://makepp.sourceforge.net/1.19/makepp_tutorial.html[+http://makepp.sourceforge.net/1.19/makepp_tutorial.html+]

*How We Use Make* -
https://segment.com/blog/how-we-use-make/[+https://segment.com/blog/how-we-use-make/+]

Bazel
^^^^^

Bazel is a build tool that builds code quickly and reliably. It is used to
build the majority of Google's software, and thus it has been designed to
handle build problems present in Google's development environment.

http://bazel.io/[+http://bazel.io/+]

ekam
^^^^

Ekam ("make" backwards) is a build system which automatically figures out what
to build and how to build it purely based on the source code. No separate
"makefile" is needed.

Ekam works by exploration. For example, when it encounters a file ending in
".cpp", it tries to compile the file, intercepting system calls to find out
its dependencies (e.g. included headers). If some of these are missing, Ekam
continues to explore until it finds headers matching them. When Ekam builds an
object file and discovers that it contains a "main" symbol, it tries to link
it, searching for other object files to satisfy all symbol references therein.
When Ekam sees a test, it runs the test, again intercepting system calls to
dynamically discover what inputs the test needs (which may not have been built
yet). Ekam can be extended to understand new file types by writing simple
shell scripts telling it what to do with them.

Thus Ekam is, in fact, Make in reverse. Make starts by reading a Makefile,
sees what executables it wants to build, then from there figures out what
source files need to be compiled to link into them, then compiles them. Ekam
starts by looking for source files to compile, then determines what
executables it can build from them, and, in the end, might output a Makefile
describing what it did.

https://github.com/sandstorm-io/ekam[+https://github.com/sandstorm-io/ekam+]

[[SEAPT]]
SEAPT
^^^^^

A feature rich set of Autotools configuration files and scripts targetting C/Cxx projects, with built-in Doxygen and Google Testing Framework. It works only on UNIX systems and casual build does not depend on anything but Shell interpreter and GNU make. SEAPT is Eclipse CDT friendly.

SEAPT is designed entirely on the basis of Autotools stack: aclocal, autoconf, autoheader, automake and libtool. It is much similar to "classic" open source build system (e.g., needs "configure" to be run), but extends it with a Doxygen documentation and Google Test Framework support out of the box, as well as supplies the necessary Eclipse CDT Autotools Project files. SEAPT suggests a development workflow in which Eclipse is used to write the code and the actual build process is completely independent of any IDE. Though Eclipse is not required at all, it enables deep integration with Autotools and is recommended.

Like cmake, SEAPT provides a nicely formatted colored output. It supports building subdirectories in parallel, compiler version checking and much more.

https://github.com/vmarkovtsev/SEAPT[+https://github.com/vmarkovtsev/SEAPT+]

[[Mantevo]]
Mantevo
~~~~~~~

Mantevo is a multi-faceted application performance project. It provides
application performance proxies known as miniapps.   Miniapps combine some or
all of the dominant numerical kernels contained in an actual stand-alone
application. Miniapps include libraries wrapped in a test driver providing
representative inputs. They may also be hard-coded to solve a particular test
case so as to simplify the need for parsing input files and mesh descriptions.
Mini apps range in scale from partial, performance-coupled components of the
application to a simplified representation of a complete execution path
through the application.

https://mantevo.org/[+https://mantevo.org/+]

https://mantevo.org/MantevoOverview.pdf[+https://mantevo.org/MantevoOverview.pdf+]

https://github.com/peihunglin/Mantevo-ROSE[+https://github.com/peihunglin/Mantevo-ROSE+]

https://github.com/UK-MAC[+https://github.com/UK-MAC+]

https://github.com/akohlmey/miniMD-omp[+https://github.com/akohlmey/miniMD-omp+]

TeaLeaf
^^^^^^^

TeaLeaf is a xref:Mantevo[Mantevo] mini-app that solves the linear heat conduction equation on a
spatially decomposed regularly grid using a 5 point stencil with implicit
solvers. TeaLeaf currently solves the equations in two dimensions, but three
dimensional support is in beta. 

The solvers have been written in Fortran with xref:OpenMP[OpenMP] and MPI and they have
also been ported to OpenCL to provide an accelerated capability.
Other versions invoke third party linear solvers and currently include Petsc,
xref:Trilinos[Trilinos] and Hypre, which are in beta release. For each of these version there
are instructions on how to download, build and link in the relevant library. 

http://uk-mac.github.io/TeaLeaf/[+http://uk-mac.github.io/TeaLeaf/+]

Mapbox Studio
~~~~~~~~~~~~~

Mapbox Studio gives you instant streaming access to massive global datasets
like Mapbox Streets, Mapbox Terrain, and Mapbox Satellite without importing
any data onto your computer.

Create your own vector tiles using Mapbox Studio. Convert data from
traditional formats (Shapefile, GeoJSON, KML, GPX) and upload directly to
Mapbox to deploy your vector tiles at scale.

https://www.mapbox.com/mapbox-studio/#linux[+https://www.mapbox.com/mapbox-studio/#linux+]

https://github.com/mapbox/mapbox-studio[+https://github.com/mapbox/mapbox-studio+]

[[MapFish]]
MapFish
~~~~~~~

A  flexible and complete framework for building rich web-mapping applications.
It emphasizes high productivity, and high-quality development.
MapFish is based on the Pylons Python web framework. MapFish extends Pylons
with geospatial-specific functionality. For example MapFish provides specific
tools for creating web services that allows querying and editing geographic
objects.

MapFish also provides a complete RIA-oriented JavaScript toolbox, a JavaScript
testing environment, and tools for compressing JavaScript code. The JavaScript
toolbox is composed of the ExtJS, OpenLayers , GeoExt JavaScript toolkits.

MapFish is compliant with the Open Geospatial Consortium standards. This is
achieved through OpenLayers or GeoExt supporting several OGC norms, like WMS,
WFS, WMC, KML, GML etc..

http://mapfish.org/[+http://mapfish.org/+]

GeoMapFish
^^^^^^^^^^

The GeoMapFish application allows to build rich and extensible WebGIS in an easy and flexible way. It is composed of a desktop WebGIS interface, an administration interface, an API for map integration in thirdparty websites and a mobile version.
Besides the OGC-Standard web services, a MapFish protocol adapted to the efficient communication between Client and Server is available. On this basis, complex and high performance web mapping applications can be built.

http://geomapfish.org/[+http://geomapfish.org/+]

MapFish Print
^^^^^^^^^^^^^

MapFish Print allows printing maps as PDFs.
It is written in Java and typically executed as a servlet in a
servlet container such as Apache Tomcat.

http://www.mapfish.org/doc/print/index.html[+http://www.mapfish.org/doc/print/index.html+]

MapServer
~~~~~~~~~

An Open Source platform for publishing spatial data and interactive mapping
applications to the web.
MapServer is an Open Source geographic data rendering engine written in C.
Beyond browsing GIS data, MapServer allows you create “geographic image maps”,
that is, maps that can direct users to content.

http://www.mapserver.org/[+http://www.mapserver.org/+]

EOxServer
^^^^^^^^^

EOxServer is a Python application and framework for presenting Earth
Observation (EO) data and metadata.
It implements the  OGC Implementation Specifications EO-WCS and EO-WMS on top
of  MapServer's  WCS and  WMS implementations. 

EOxServer is an open source software for registering, processing, and
publishing Earth Observation (EO) data via different Web Services. EOxServer
is written in Python and relies on widely-used libraries for geospatial data
manipulation.

The core concept of the EOxServer data model is the one of a coverage. In this
context, a coverage is a mapping from a domain set (a geographic region of the
Earth described by its coordinates) to a range set. For original EO data, the
range set usually consists of measurements of some physical quantity (e.g.
radiation for optical instruments).

http://eoxserver.org/doc/en/[+http://eoxserver.org/doc/en/+]

MapCache
^^^^^^^^

A server that implements tile caching to speed up access to WMS layers. The primary objectives are to be fast and easily deployable, while offering the essential features (and more!) expected from a tile caching solution.
MapCache is part of MapServer Suite, but provided as a distinct module.

TinyOWS
^^^^^^^

A lightweight and fast implementation of the OGC WFS-T specification. Web Feature Service (WFS) allows to query and to retrieve features. The transactional profile (WFS-T) allows then to insert, update or delete such features.  From a technical point of view WFS-T is a Web Service API in front of a spatial database.
TinyOWS is part of MapServer Suite, but provided as a distinct module.

http://www.mapserver.org/tinyows/index.html[+http://www.mapserver.org/tinyows/index.html+]

Markdown
~~~~~~~~

A text-to-HTML conversion tool for web writers. Markdown allows you to write using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid XHTML (or HTML).
Markdown is two things: (1) a plain text formatting syntax; and (2) a software tool, written in Perl, that converts the plain text formatting to HTML.

http://daringfireball.net/projects/markdown/[+http://daringfireball.net/projects/markdown/+]

http://daringfireball.net/projects/markdown/dingus[+http://daringfireball.net/projects/markdown/dingus+]

*Markdown Cheatsheet* - https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet[+https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet+]

*awesome-markdown: A Collection of Markdown Goodies* - https://github.com/writekit/awesome-markdown[+https://github.com/writekit/awesome-markdown+]

CommonMark
^^^^^^^^^^

A strongly specified, highly compatible implementation of Markdown.

http://commonmark.org/[+http://commonmark.org/+]

https://github.com/jgm/CommonMark[+https://github.com/jgm/CommonMark+]

MultiMarkdown
^^^^^^^^^^^^^

MultiMarkdown, or MMD, is a tool to help turn minimally marked-up plain text into well
formatted documents, including HTML, PDF (by way of LaTeX), OPML, or
OpenDocument (specifically, Flat OpenDocument or .fodt, which can in turn be converted into RTF, Microsoft Word, or virtually any other word-processing format).

MMD is a superset of the Markdown syntax, originally created by John Gruber. It adds multiple syntax features (tables, footnotes, and citations, to name a few), in addition to the various output formats listed above (Markdown only creates HTML). Additionally, it builds in “smart” typography for various languages (proper left- and right-sided quotes, for example).

http://fletcherpenney.net/multimarkdown/[+http://fletcherpenney.net/multimarkdown/+]

Mathfu
~~~~~~

MathFu is a Cxx math library developed primarily for games focused on
simplicity and efficiency.

It provides a suite of vector, matrix and quaternion classes to perform basic
geometry suitable for game developers. This functionality can be used to
construct geometry for graphics libraries like OpenGL or perform calculations
for animation or physics systems.

http://google.github.io/mathfu/[+http://google.github.io/mathfu/+]

Matlab/Octave Translation
~~~~~~~~~~~~~~~~~~~~~~~~~

mat2cpp
^^^^^^^

A package for converting Matlab or Octave to Cxx.

http://moby.ihme.washington.edu/bradbell/mat2cpp/mat2cpp.htm[xxhttp://moby.ihme.washington.edu/bradbell/mat2cpp/mat2cpp.htm+]

[[Matlab]]
Matlab
~~~~~~

LiberMate
^^^^^^^^^

A Matlab to Python (Scipy/Numpy) translator.

https://github.com/awesomebytes/libermate[+https://github.com/awesomebytes/libermate+]

[[SMOP]]
SMOP
^^^^

Translates Matlab to Python.

https://github.com/victorlei/smop[+https://github.com/victorlei/smop+]


Matlab Toolboxes
~~~~~~~~~~~~~~~~

BCS
^^^

Bayesian Compressive Sensing (BCS) is a Bayesian framework for solving the
inverse problem of compressive sensing (CS).
The basic BCS algorithm adopts the relevance vector machine (RVM), and later
it is extended by marginalizing the noise variance  with improved robustness.
This is
a MatLab 7.0 implementation of BCS, VB-BCS (BCS implemented via a variational
Bayesian (VB) approach), TS-BCS for wavelet and for block-DCT implemented via
both MCMC approach and VB approach.

http://people.ee.duke.edu/\~lcarin/BCS.html[+http://people.ee.duke.edu/~lcarin/BCS.html+]

[[BeamLab]]
BeamLab
^^^^^^^

A a collection of Matlab functions that have been used by the authors and collaborators to implement a variety of computational algorithms related to beamlet, curvelet,  ridgelet analysis.  It includes about 900 Matlab files, datasets, and demonstration scripts. Some computationally expensive routines have been implemented as Matlab MEX functions.

http://statweb.stanford.edu/\~beamlab/[+http://statweb.stanford.edu/~beamlab/+]

[[Braidlab]]
Braidlab
^^^^^^^^

Matlab package for analyzing data using braids.

https://github.com/jeanluct/braidlab[+https://github.com/jeanluct/braidlab+]

http://github.com/jeanluct/braidlab/raw/master/doc/braidlab_guide.pdf[+http://github.com/jeanluct/braidlab/raw/master/doc/braidlab_guide.pdf+]

http://arxiv.org/abs/1410.0849[+http://arxiv.org/abs/1410.0849+]

http://arxiv.org/abs/1502.02162[+http://arxiv.org/abs/1502.02162+]

CurveLab
^^^^^^^^

The Curvelet transform is a higher dimensional generalization of the Wavelet
transform designed to represent images at different scales and different
angles.  Curvelets enjoy two unique mathematical properties, namely:

* Curved singularities can be well approximated with very few coefficients and
in a non-adaptive manner - hence the name "curvelets."
* Curvelets remain coherent waveforms under the action of the wave equation in
a smooth medium.

By releasing the CurveLab toolbox, we hope to encourage the dissemination of
curvelets to image processing, inverse problems and scientific computing.

http://www.curvelet.org/[+http://www.curvelet.org/+]

[[DMDSP]]
DMDSP
^^^^^

A Matlab implementation of the Sparsity-Promoting Dynamic Mode Decomposition
(DMDSP) algorithm. Dynamic Mode Decomposition (DMD) is an effective means for
capturing the essential features of numerically or experimentally generated
snapshots, and its sparsity-promoting variant DMDSP achieves a desirable
tradeoff between the quality of approximation (in the least-squares sense) and
the number of modes that are used to approximate available data. Sparsity is
induced by augmenting the least-squares deviation between the matrix of
snapshots and the linear combination of DMD modes with an additional term that
penalizes the ell_1-norm of the vector of DMD amplitudes. We employ
alternating direction method of multipliers (ADMM) to solve the resulting
convex optimization problem and to efficiently compute the globally optimal
solution.

http://www.ece.umn.edu/users/mihailo//software/dmdsp/[+http://www.ece.umn.edu/users/mihailo//software/dmdsp/+]

http://www.ece.umn.edu/users/mihailo//papers/jovschnicPOF14.pdf[+http://www.ece.umn.edu/users/mihailo//papers/jovschnicPOF14.pdf+]

http://www.ece.umn.edu/users/mihailo//talks/siam13dmdsp.pdf[+http://www.ece.umn.edu/users/mihailo//talks/siam13dmdsp.pdf+]

[[NeedMat]]
NeedMat
^^^^^^^

A Matlab Package that implements fast spherical needlet transforms and fast spherical needlet evaluations.

https://github.com/minjay/NeedMat[+https://github.com/minjay/NeedMat+]

RBFSPHERE
^^^^^^^^^

Radial basis function (RBF) methods are advantageous for a wide-range of applications from analyzing/synthesizing "scattered" data (scalar and vector valued quantities) to numerically solving partial differential equations on geometrically difficult domains.
This is a Matlab package for working with RBFs.

http://math.boisestate.edu/\~wright/montestigliano/index.html[+http://math.boisestate.edu/~wright/montestigliano/index.html+]

[[Spherical-Harmonic-Transform]]
Spherical-Harmonic-Transform
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A collection of MATLAB routines for the Spherical Harmonic Transform and related manipulations in the spherical harmonic spectrum.

https://github.com/polarch/Spherical-Harmonic-Transform[+https://github.com/polarch/Spherical-Harmonic-Transform+]

Maven
~~~~~

Maven is a build automation tool used primarily for Java projects. The word
maven means 'accumulator of knowledge' in Yiddish.[3] Maven addresses two
aspects of building software: First, it describes how software is built, and
second, it describes its dependencies. Contrary to preceding tools like Apache
Ant, it uses conventions for the build procedure, and only exceptions need to
be written down. An XML file describes the software project being built, its
dependencies on other external modules and components, the build order,
directories, and required plug-ins. It comes with pre-defined targets for
performing certain well-defined tasks such as compilation of code and its
packaging. Maven dynamically downloads Java libraries and Maven plug-ins from
one or more repositories such as the Maven 2 Central Repository, and stores
them in a local cache.[4] This local cache of downloaded artifacts can also be
updated with artifacts created by local projects. Public repositories can also
be updated.

http://maven.apache.org/[+http://maven.apache.org/+]

MBDyn
~~~~~

MBDyn is the first and possibly the only free* general purpose Multibody
Dynamics analysis software.
It features the integrated multidisciplinary simulation of multibody,
multiphysics systems, including nonlinear mechanics of rigid and flexible
bodies (geometrically exact & composite-ready beam and shell finite elements,
component mode synthesis elements, lumped elements) subjected to kinematic
constraints, along with smart materials, electric networks, active control,
hydraulic networks, and essential fixed-wing and rotorcraft aerodynamics.

MBDyn simulates the behavior of heterogeneous mechanical, aeroservoelastic
systems based on first principles equations.
It can be easily coupled to external solvers for co-simulation of multiphysics
problems, e.g. Computational Fluid Dynamics (CFD), terradynamics,
block-diagram solvers like Scicos, Scicoslab and Simulink, using a simple C,
Cxx or Python peer-side API.

MBDyn is being actively developed and used in the aerospace (aircraft,
helicopters, tiltrotors, spacecraft), wind energy (wind turbines), automotive
(cars, trucks) and mechatronic fields (industrial robots, parallel robots,
micro aerial vehicles (MAV)) for the analysis and simulation of the dynamics
of complex systems.

https://www.mbdyn.org/[+https://www.mbdyn.org/+]

mdpc
~~~~

Morse decompositions for piecewise constant vector fields.

https://github.com/andrzejszymczak/mdpc[+https://github.com/andrzejszymczak/mdpc+]

media center
~~~~~~~~~~~~

[[Kodi]]
Kodi
^^^^

A free and open source (GPL) software media center for playing videos, music, pictures, games, and more. Kodi runs on Linux, OS X, Windows, iOS, and Android, featuring a 10-foot user interface for use with televisions and remote controls. It allows users to play and view most videos, music, podcasts, and other digital media files from local and network storage media and the internet.  This was formerly known as XBMC.

http://kodi.tv/[+http://kodi.tv/+]

mediagoblin
~~~~~~~~~~~

MediaGoblin is a free software media publishing platform that anyone can run.
You can think of it as a decentralized alternative to Flickr, YouTube,
SoundCloud, etc.

http://mediagoblin.org/[+http://mediagoblin.org/+]

Mediawiki
~~~~~~~~~

MediaWiki is a free software open source wiki package written in PHP,
originally for use on Wikipedia. It is now also used by several other projects
of the non-profit Wikimedia Foundation and by many other wikis, including this
website, the home of MediaWiki.

MediaWiki is an extremely powerful, scalable software and a feature-rich wiki
implementation that uses PHP to process and display data stored in a database,
such as MySQL.
Pages use MediaWiki's wikitext format, so that users without knowledge of
XHTML or CSS can edit them easily.
When a user submits an edit to a page, MediaWiki writes it to the database,
but without deleting the previous versions of the page, thus allowing easy
reverts in case of vandalism or spamming. MediaWiki can manage image and
multimedia files, too, which are stored in the filesystem. For large wikis
with lots of users, MediaWiki supports caching and can be easily coupled with
Squid proxy server software.

http://www.mediawiki.org/wiki/MediaWiki[+http://www.mediawiki.org/wiki/MediaWiki+]

https://en.wikipedia.org/wiki/MediaWiki[+https://en.wikipedia.org/wiki/MediaWiki+]

https://github.com/wikimedia/mediawiki[+https://github.com/wikimedia/mediawiki+]

Collection
^^^^^^^^^^
The Collection extension for MediaWiki allows users to collect articles and
generate downloadable version in different formats (PDF, OpenDocument Text
etc.) for article collections and single articles.

http://www.mediawiki.org/wiki/Extension:Collection[+http://www.mediawiki.org/wiki/Extension:Collection+]

https://mwlib.readthedocs.org/en/latest/collection.html[+https://mwlib.readthedocs.org/en/latest/collection.html+]

mwlib
xxxx+

Provides a library for parsing MediaWiki articles and converting them to
different output formats.
The collection extension is a MediaWiki extensions enabling users to collect
articles and generate PDF files from those.

https://mwlib.readthedocs.org/en/latest/index.html[+https://mwlib.readthedocs.org/en/latest/index.html+]

Kiwix
^^^^^

Kiwix is an offline reader for web content. It's software intended to make
Wikipedia available without using the internet, but it is potentially suitable
for all HTML content. Kiwix supports the ZIM format, a highly compressed open
format with additional meta-data.  The features include a full text search
engine, bookmarks and notes, an HTTP server, PDF/HTML export, a user interface
in more than 10 languages, tabs navigation, and integrated content manager and
downloader, etc.

See also xref:OpenZIM[OpenZIM].

http://www.kiwix.org/wiki/Main_Page[+http://www.kiwix.org/wiki/Main_Page+]

Semantic MediaWiki
^^^^^^^^^^^^^^^^^^

A free, open-source extension to MediaWiki – the wiki software that powers Wikipedia – that lets you store and query data within the wiki's pages.
Semantic MediaWiki is also a full-fledged framework, in conjunction with many spinoff extensions, that can turn a wiki into a powerful and flexible knowledge management system. All data created within SMW can easily be published via the Semantic Web, allowing other systems to use this data seamlessly.

https://semantic-mediawiki.org/[+https://semantic-mediawiki.org/+]

[[MEDSLIK]]
MEDSLIK
~~~~~~~

A model designed to be used to predict the transport and weathering of an oil spill, using a lagrangian representation of the oil slick. 
MEDSLIK-II simulates the transport of the surface slick governed by the water currents and by the wind. Oil particles are also dispersed by turbulent fluctuation components that are parameterized with a random walk scheme. In addition to advective and diffusive displacements, the oil spill particles change due to various physical and chemical processes that transform the oil (evaporation, emulsification, dispersion in water column, adhesion to coast). MEDSLIK-II includes a proper representation of high frequency currents and wind fields in the advective components of the lagrangian trajectory model, the introduction of the Stokes drift velocity and the coupling with the remote-sensing data. 

http://gnoo.bo.ingv.it/MEDSLIKII/[+http://gnoo.bo.ingv.it/MEDSLIKII/+]

http://www.geosci-model-dev.net/6/1851/2013/gmd-6-1851-2013.html[+http://www.geosci-model-dev.net/6/1851/2013/gmd-6-1851-2013.html+]

Medusa
~~~~~~

A  new innovative Python implementation harnessing Google's super fast Dart
Virtual Machine running Python at near native speeds.

http://in.pycon.org/funnel/2014/130-medusa-a-much-faster-python-implementation-based-on-the-dart-virtual-machine[+http://in.pycon.org/funnel/2014/130-medusa-a-much-faster-python-implementation-based-on-the-dart-virtual-machine+]

https://github.com/rahul080327/medusa[+https://github.com/rahul080327/medusa+]

MEMPSODE
~~~~~~~~

A global optimization software tool that integrates two prominent
population-based stochastic algorithms, namely Particle Swarm Optimization and
Differential Evolution, with well established efficient local search
procedures made available via the Merlin optimization environment. The
resulting hybrid algorithms, also referred to as Memetic Algorithms, combine
the space exploration advantage of their global part with the efficiency asset
of the local search, and as expected they have displayed a highly efficient
behavior in solving diverse optimization problems. The proposed software is
carefully parametrized so as to offer complete control to fully exploit the
algorithmic virtues. It is accompanied by comprehensive examples and a large
set of widely used test functions, including tough atomic cluster and protein
conformation problems.

http://www.sciencedirect.com/science/article/pii/S0010465512000136[+http://www.sciencedirect.com/science/article/pii/S0010465512000136+]

[[Mesos]]
Mesos
~~~~~

Mesos is a distributed systems kernel.
It is built using the same principles as the Linux kernel, only at a different level of abstraction. The Mesos kernel runs on every machine and provides applications (e.g., Hadoop, Spark, Kafka, Elastic Search) with API’s for resource management and scheduling across entire datacenter and cloud environments.

Apache Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications, or frameworks. It can run Hadoop, MPI, Hypertable, Spark, and other frameworks on a dynamically shared pool of nodes.

http://mesos.apache.org/[+http://mesos.apache.org/+]

https://github.com/apache/mesos/[+https://github.com/apache/mesos/+]

*Advanced Mesos Course* - http://open.mesosphere.com/intro-course/[+http://open.mesosphere.com/intro-course/+]

metadata systems
~~~~~~~~~~~~~~~~

[[AMGA]]
AMGA
^^^^

A prototype highly performant and distributed metadata server for supplying
the need for metadata catalogues in a grid environment such as used in the HEP
community.
AMGA as a metadata service allows users to attach metadata information to files stored on the Grid, where metadata can be any relationally organized data typically stored in a relational database system (RDBMS). In addition, the metadata in AMGA can also be stored independently of any associated files, which allows AMGA to be used as a general access tool to relational databases on the Grid. AMGA features a simple to learn metadata access language, which has been very useful for the adoption of AMGA in smaller Grid applications, as it considerably lowers the technical hurdle to make use of relational data.

One of the main features of AMGA, and one unique to it, is the possibility to replicate metadata between different AMGA instances allowing the federation of metadata, but also to increase the scalability and improve the access times on a globally deployed Grid.   Performance and efficiency of the access across WANs has been independently targeted by an access protocol optimised for the bulk transfer of metadata across WANs using data streaming.

The AMGA implementation uses streaming to communicate between client and server which shows a very promising performance. To meet the EGEE requirements, we have also implemented an alternative SOAP-based frontend.
The package includes:

* a  streaming and a SOAP front-end;
* an interactive client for the streaming front-end; and
* client APIs for the streaming client (Cxx, Java and Python).

http://amga.web.cern.ch/amga/[+http://amga.web.cern.ch/amga/+]

http://www.digplanet.com/wiki/GLite-AMGA[+http://www.digplanet.com/wiki/GLite-AMGA+]

MeteoIO
~~~~~~~

MeteoIO can be seen as a set of modules that is focused on the handling of
input/output operations (including data preparation) for numerical simulations
in the realm of earth sciences. On the visible side, it offers the following
modules, working on a pre-determined set of meteorological parameters or on
parameters added by the developer:

* a set of plugins for accessing the data (for example, a plugin might be
responsible for fetching the raw data from a given database)
* a set of filters and processing elements for applying transformations to
the data (for example, a filter might remove all data that is out of range)
* a set of resampling algorithms to temporally interpolate the data at the
required timestamp
* a set of parametrizations to generate data/meteorological parameters when
they could not be interpolated
* a set of spatial interpolation algorithms (for example, such an algorithm
might perform Inverse Distance Weighting for filling a grid with spatially
interpolated data)

Each of these steps can be configured and fine tuned according to the needs of
the model and the wishes of the user.

http://models.slf.ch/p/meteoio/[+http://models.slf.ch/p/meteoio/+]

http://www.geosci-model-dev-discuss.net/7/3595/2014/gmdd-7-3595-2014.html[+http://www.geosci-model-dev-discuss.net/7/3595/2014/gmdd-7-3595-2014.html+]

MeteorOS
~~~~~~~~

Meteor is an ultra-simple environment for building modern websites. What once
took weeks, even with the best tools, now takes hours with Meteor.

The web was originally designed to work in the same way that mainframes worked
in the 70s. The application server rendered a screen and sent it over the
network to a dumb terminal. Whenever the user did anything, that server
rerendered a whole new screen. This model served the Web well for over a
decade. It gave rise to LAMP, Rails, Django, PHP.

But the best teams, with the biggest budgets and the longest schedules, now
build applications in JavaScript that run on the client. These apps have
stellar interfaces. They don't reload pages. They are reactive: changes from
any client immediately appear on everyone's screen.

They've built them the hard way. Meteor makes it an order of magnitude
simpler, and a lot more fun. You can build a complete application in a
weekend, or a sufficiently caffeinated hackathon. No longer do you need to
provision server resources, or deploy API endpoints in the cloud, or manage a
database, or wrangle an ORM layer, or swap back and forth between JavaScript
and Ruby, or broadcast data invalidations to clients.


https://www.meteor.com/[+https://www.meteor.com/+]

metos3d
~~~~~~~

The simulation and parameter optimization of coupled ocean circulation and
ecosystem models in three space dimensions is one of the most challenging
tasks in numerical climate research. Here we present a scientific toolkit that
aims at supporting researchers by defining clear coupling interfaces,
providing state-of-the-art numerical methods for simulation, parallelization
and optimization while using only freely available and (to a great extend)
platform-independent software. Besides defining a user-friendly coupling
interface (API) for marine ecosystem or biogeochemical models, we heavily rely
on the Portable, Extensible Toolkit for Scientific computation (PETSc)
developed at Argonne Nat. Lab. for a wide variety of parallel linear and
non-linear solvers and optimizers. We specifically focus on the usage of
matrix-free Newton-Krylov methods for the fast computation of steady periodic
solutions, and make use of the Transport Matrix Method (TMM).

http://arxiv.org/abs/1410.0204[+http://arxiv.org/abs/1410.0204+]

https://github.com/metos3d/metos3d[+https://github.com/metos3d/metos3d+]

http://metos3d.github.io/metos3d/[+http://metos3d.github.io/metos3d/+]

miniMAL
~~~~~~~

A Lisp implemented in < 1 KB of JavaScript with macros, TCO, interop and
exception handling. 

https://github.com/kanaka/miniMAL[+https://github.com/kanaka/miniMAL+]

MODES
~~~~~

Software for the analysis of global dynamical fields in (re)analyses, weather forecasts and climate models.
MODES enables the diagnosis of properties of balanced and inertio-gravity (IG) circulations across many scales. In particular, the IG spectrum, which has only recently become observable, can be studied simultaneously in the mass and wind fields while considering the whole model depth in contrast to the majority of studies. 

http://www.fmf.uni-lj.si/\~zagarn/modes.php[+http://www.fmf.uni-lj.si/~zagarn/modes.php+]

http://www.geosci-model-dev.net/8/1169/2015/gmd-8-1169-2015.html[+http://www.geosci-model-dev.net/8/1169/2015/gmd-8-1169-2015.html+]

modred
~~~~~~

A parallelized Python library for finding modal decompositions and
reduced-order models.
Parallel implementations of the proper orthogonal decomposition (POD),
balanced POD (BPOD), dynamic mode decomposition (DMD), and Petrov-Galerkin
projection are provided, as well as serial implementations of the Observer
Kalman filter Identification method (OKID) and the Eigensystem Realization
Algorithm (ERA). Modred is applicable to a wide range of problems and nearly
any type of data.

http://pythonhosted.org/modred/[+http://pythonhosted.org/modred/+]

module systems
~~~~~~~~~~~~~~

[[Lmod]]
Lmod
^^^^

A xref:Lua[Lua]-based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users' environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.

https://www.tacc.utexas.edu/research-development/tacc-projects/lmod[+https://www.tacc.utexas.edu/research-development/tacc-projects/lmod+]

Mojolicious
~~~~~~~~~~~

A next generation web framework for the Perl programming language. 

http://mojolicio.us/[+http://mojolicio.us/+]

Mondrian
~~~~~~~~

Mondrian is a general purpose statistical data-visualization system. It
features outstanding interactive visualization techniques for data of almost
any kind, and has particular strengths, compared to other tools, for working
with Categorical Data, Geographical Data and LARGE Data.

All plots in Mondrian are fully linked, and offer many interactions and
queries. Any case selected in a plot in Mondrian is highlighted in all other
plots.

Currently implemented plots comprise Histograms, Boxplots y by x,
Scatterplots, Barcharts, Mosaicplots, Missing Value Plots, Parallel
Coordinates/Boxplots, SPLOMs and Maps.

Mondrian works with data in standard tab-delimited or comma-separated ASCII
files and can load data from R workspaces. There is basic support for working
directly on data in Databases (please email for further info).

Mondrian is written in JAVA and is distributed as a native application
(wrapper) for MacOS X and Windows. Linux users need to start the jar-file.

http://rosuda.org/mondrian/[+http://rosuda.org/mondrian/+]

http://stats.math.uni-augsburg.de/Mondrian/[+http://stats.math.uni-augsburg.de/Mondrian/+]

MongoDB
~~~~~~~

MongoDB (from humongous) is a cross-platform document-oriented database.
Classified as a NoSQL database, MongoDB eschews the traditional table-based
relational database structure in favor of JSON-like documents with dynamic
schemas (MongoDB calls the format BSON), making the integration of data in
certain types of applications easier and faster.

http://www.mongodb.org/[+http://www.mongodb.org/+]

mongodb-d4
^^^^^^^^^^

D4 is an automated tool for a generating distributed document database designs
for applications running on MongoDB. This tool specifically targets
applications running highly concurrent workloads, and thus its designs are
tailored to the unique properties of large-scale, Web-based applications. It
can also be used to assist in porting MySQL-based applications to MongoDB.

Using a sample workload trace from a either a document-oriented or relational
database application, D4 will compute the best a database design that
optimizes the throughput and latency of a document DBMS.

https://github.com/cmu-db/mongodb-d4[+https://github.com/cmu-db/mongodb-d4+]

http://database.cs.brown.edu/projects/mongodb/

moodle
~~~~~~

Moodle is a free, online Learning Management system enabling educators to
create their own private website filled with dynamic courses that extend
learning, any time, anywhere.
Whether you're a teacher, student or administrator, Moodle can meet your
needs. Moodle’s extremely customisable core comes with many standard features.

https://moodle.org/[+https://moodle.org/+]

https://moodle.org/plugins/[+https://moodle.org/plugins/+]

[[MOOSE]]
MOOSE
~~~~~

The Multiphysics Object-Oriented Simulation Environment (MOOSE) is a finite-element, multiphysics framework primarily developed by Idaho National Laboratory. It provides a high-level interface to some of the most sophisticated nonlinear solver technology on the planet. MOOSE presents a straightforward API that aligns well with the real-world problems scientists and engineers need to tackle. Every detail about how an engineer interacts with MOOSE has been thought through, from the installation process through running your simulation on state of the art supercomputers, the MOOSE system will accelerate your research.

http://mooseframework.com/[+http://mooseframework.com/+]

http://mooseframework.org/[+http://mooseframework.org/+]

https://github.com/idaholab/moose[+https://github.com/idaholab/moose+]

*Continuous Integration for Concurrent Computational Framework and Application Development* (online article) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.as/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.as/+]

Mopidy
~~~~~~

Mopidy is an extensible music server written in Python.

Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music,
and more. You edit the playlist from any phone, tablet, or computer using a
range of MPD and web clients.

https://www.mopidy.com/[+https://www.mopidy.com/+]

MORSE
~~~~~

MORSE is an generic simulator for academic robotics. It focuses on realistic
3D simulation of small to large environments, indoor or outdoor, with one to
tenths of autonomous robots.

MORSE can be entirely controlled from the command-line. Simulation scenes are
generated from simple Python scripts.

MORSE comes with a set of standard sensors (cameras, laser scanner, GPS,
odometry,...), actuators (speed controllers, high-level waypoints controllers,
generic joint controllers) and robotic bases (quadrotors, ATRV, Pioneer3DX,
generic 4 wheel vehicle, PR2,...). New ones can easily be added.

MORSE rendering is based on the Blender Game Engine. The OpenGL-based Game
Engine supports shaders, provides advanced lightning options, supports
multi-texturing, and use the state-of-the-art Bullet library for physics
simulation.

https://www.openrobots.org/wiki/morse/[+https://www.openrobots.org/wiki/morse/+]

MoviePy
~~~~~~~

MoviePy is a Python module for video editing, which can be used for basic
operations (like cuts, concatenations, title insertions), video compositing
(a.k.a. non-linear editing), video processing, or to create advanced effects.
It can read and write the most common video formats, including GIF.

http://zulko.github.io/moviepy/[+http://zulko.github.io/moviepy/+]

MPDATA
~~~~~~

Comparison among OOP versions of an MPDATA code written using Python, Fortran
and Cxx.

http://arxiv.org/abs/1301.1334[+http://arxiv.org/abs/1301.1334+]

MPI
~~~

The Message Passing Interface...

BDMPI
^^^^^

BDMPI is a message passing library and associated runtime system for
developing out-of-core distributed computing applications for problems whose
aggregate memory requirements exceed the amount of memory that is available on
the underlying computing cluster. BDMPI is based on the Message Passing
Interface (MPI) and provides a subset of MPI's API along with some extensions
that are designed for BDMPI's memory and execution model.

A BDMPI-based application is a standard memory-scalable parallel MPI program
that was developed assuming that the underlying system has enough
computational nodes to allow for the in-memory execution of the computations.
This program is then executed using a sufficiently large number of processes
so that the per-process memory fits within the physical memory available on
the underlying computational node(s). BDMPI maps one or more of these
processes to the computational nodes by relying on the OS's virtual memory
management to accommodate the aggregate amount of memory required by them.
BDMPI prevents memory thrashing by coordinating the execution of these
processes using node-level co-operative multi-tasking that limits the number
of processes that can be running at any given time. This ensures that the
currently running process(es) can establish and retain memory residency and
thus achieve efficient execution. BDMPI exploits the natural blocking points
that exist in MPI programs to transparently schedule the co-operative
execution of the different processes. In addition, BDMPI's implementation of
MPI's communication operations is done so that to maximize the time over which
a process can execute between successive blocking points. This allows it to
amortize the cost of loading data from disk over the maximal amount of
computations that can be performed.

Since BDMPI is based on the standard MPI library, it also provides a framework
that allows the automated out-of-core execution of existing MPI applications.
BDMPI is implemented in such a way so that to be a drop-in replacement of
existing MPI implementations and allow existing codes that utilize the subset
of MPI functions implemented by BDMPI to compile unchanged.

http://glaros.dtc.umn.edu/gkhome/bdmpi/overview[+http://glaros.dtc.umn.edu/gkhome/bdmpi/overview+]

[[DART-MPI]]
DART-MPI
^^^^^^^^

A  runtime environment, which implements the PGAS paradigm on large-scale high-performance computing clusters. A specific feature of our implementation is the use of one-sided communication of the Message Passing Interface (MPI) version 3 (i.e. MPI-3) as the underlying communication substrate.

http://www.dash-project.org/[+http://www.dash-project.org/+]

http://arxiv.org/abs/1507.01773[+http://arxiv.org/abs/1507.01773+]

DataMPI
^^^^^^^

DataMPI is an efficient, flexible, and productive communication library, which
provides a set of key-value pair based communication interfaces that extends
MPI for Big Data. Through utilizing the efficient communication technologies
in the High-Performance Computing area, DataMPI can speedup the emerging data
intensive computing applications. DataMPI takes a step in bridging the two
fields of HPC and Big Data.

DataMPI can support multiple modes for various Big Data Computing
applications, including Common, MapReduce, Streaming, and Iteration. The
current version implements the functionalities and features of the Common
mode, which aims to support the single program, multiple data (SPMD)
applications. The remaining modes will be released in the future.

The current implementation of DataMPI is extending mpiJava. We also integrate
some features from Hadoop under Apache License 2.0. The current evaluations of
DataMPI use MVAPICH2 as the backend. DataMPI also supports other MPI
implementations, such as MPICH2.

http://datampi.org/[+http://datampi.org/+]

http://arxiv.org/abs/1403.3480[+http://arxiv.org/abs/1403.3480+]

[[GridMPI]]
GridMPI
^^^^^^^

An implementation of the MPI (Message Passing Interface) standard designed for high performance computing in the Grid. It establishes a synthesized cluster computer by binding multiple cluster computers distributed geographically. Users are able to seamlessly deploy their application programs from a local system to the Grid environment for processing a very large data set, which is too large to run on a single system. 

GridMPI targets to make global communications efficient, by optimizing the behavior of protocols over the links with non-uniform latency and bandwidth, and also to hide the details of the lower-level network geometry from users. The GridMPI project is working on to provide variations of collective communication algorithms and an abstract layer to hide network geometry, and an interface to the TCP/IP communication layer to make it adaptive to those algorithms. 

PSPacer is a precise software pacer of IP traffic for Linux, which controls IP traffic to regulate bandwidth and smooth bursty traffic. It is implemented as a Linux loadable kernel module, but it controls the traffic at a very high precision (less than a micro-second!) which was only possible by using a special hardware. PSPacer is a standalone module and not bound to GridMPI. Its application varies widely, for example, high-bandwidth TCP/IP streaming, and traffic control of low-bandwidth lines. 

http://aist-itri.github.io/gridmpi/[+http://aist-itri.github.io/gridmpi/+]

MPICH-Madeleine
^^^^^^^^^^^^^^^

MPICH-Madeleine is a free MPICH-based implementation of the MPI standard,
which is a high-level communication interface designed to provide high
performance communications on various network architectures including
supercomputers and clusters of workstations (usually off-the-shelf PC/s
interconnected by high speed links). Nowadays, clusters of workstations become
increasingly popular thanks to the availability of many high speed connection
technologies (Gigabit-Ethernet, Myrinet, GigaNet, SCI). Furthermore,
interconnecting such COW/s to build heterogeneous clusters of clusters is now
a hot issue. Unfortunately, no current MPI implementation supports this kind
of architectures efficiently. Indeed, the only way to handle network
heterogeneity is to use interoperable implementations of MPI: several MPI
implementations (one per cluster) communicate with each other using an
inter-MPI glue. 

Our alternate proposal is to provide a true multi-protocol implementation of
MPI on top of a generic and multi-protocol communication layer called
Madeleine (version 3). Madeleine III is the communication sub-system of the
Parallel Multithreaded Machine runtime environment. 

This project is deprecated in favor of xref:NewMadeleine[NewMadeleine].

http://runtime.bordeaux.inria.fr/mpi/[+http://runtime.bordeaux.inria.fr/mpi/+]

[[MPWide]]
MPWide
^^^^^^

A communication library for message passing across wide area networks.
MPWide has been designed to connect application running on distributed (super)computing resources, and to maximize the communication performance on wide area networks for those without administrative privileges. It can be used to provide message-passing between application, move files, and make very fast connections in client-server environments.

The core MPWide functionalities are provided by the MPWide Cxx API, the communication codebase, and the Socket class.
The Socket class is used to manage and use individual tcp connections, while the role of the communication codebase is to provide the MPWide API functionalities in Cxx, using the Socket class.

MPWide relies on a number of data structures, which are used to make it easier to manage the customized connections between endpoints. The most straightforward way to construct a connection in MPWide is to create a communication path. Each path consists of 1 or more tcpstreams, each of which is used to facilitate actual communications over that path. Using a single tcp stream is sufficient to enable a connection, but in many wide area networks, MPWide will deliver much better performance when multiple streams are used. MPWide supports the presence of multiple paths, and the creation and deletion of paths at runtime. 

MPWide comes with a number of parameters which allow users to optimize the performance of individual paths. Aside from varying the number of streams, users can modify the size of data sent and received per low-level communication call (the chunk size), the tcp window size, and limit the throughput for individual streams by adjusting the communication pacing rate. The number of streams will always need to provided by the user when creating a path, but users can choose to have the other parameters automatically tuned by enabling the MPWide autotuner.

A Python interface to MPWide is available.

https://github.com/djgroen/MPWide[+https://github.com/djgroen/MPWide+]

*MPWide: a light-weight library for efficient message passing over wide area networks* (online paper) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ah/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ah/+]

[[NewMadeleine]]
NewMadeleine
^^^^^^^^^^^^

NewMadeleine is the fourth incarnation of the Madeleine communication
library. The new architecture aims at enabling the use of a much wider range
of communication flow optimization techniques. Its design is entirely modular:
drivers and optimization strategies are dynamically loadable software
components, allowing experimentations with multiple approaches or on multiple
issues with regard to processing communication flows.

The optimizing scheduler SchedOpt targets applications with irregular,
multi-flow communication schemes such as found in the increasingly common
application conglomerates made of multiple programming environments and
coupled pieces of code, for instance. SchedOpt itself is easily extensible
through the concepts of optimization strategies (what to optimize for, what
the optimization goal is) expressed in terms of tactics (how to optimize to
reach the optimization goal). Tactics themselves are made of basic
communication flows operations such as packet merging or reordering.

The communication library is fully multi-threaded through its close
integration with PIOMan. It manages concurrent communication operations from
multiple libraries and from multiple threads. Its MPI implementation Mad-MPI
fully supports the MPI_THREAD_MULTIPLE multi-threading level.
It is available on Infiniband (ibverbs), Myrinet (MX and GM), TCP (sockets)
and legacy SCI and Quadrics QsNet-2.

http://pm2.gforge.inria.fr/newmadeleine/[+http://pm2.gforge.inria.fr/newmadeleine/+]

Open MPI
^^^^^^^^

The Open MPI Project is an open source Message Passing Interface
implementation that is developed and maintained by a consortium of academic,
research, and industry partners. Open MPI is therefore able to combine the
expertise, technologies, and resources from all across the High Performance
Computing community in order to build the best MPI library available. Open MPI
offers advantages for system and software vendors, application developers and
computer science researchers.

http://www.open-mpi.org/[+http://www.open-mpi.org/+]

ORCM
xxxx

The Open Resilient Cluster Manager (ORCM) was originally developed as an
open-source project (under the Open MPI license) by Cisco Systems, Inc to
provide a resilient, 100% uptime run-time environment for enterprise-class
routers. Based on the Open Run-Time Environment (ORTE) embedded in Open MPI,
the system provided launch and execution support for processes executing
within the router itself (e.g., computing routing tables), ensuring that a
minimum number of copies of each program were always present.

ORCM (Open Resilient Cluster Manager) is a derivative from Open MPI
implementation.  It consists of:

* ORCM: Open Resilient Cluster Manager. Provides the following: Resource
Management, Scheduler, Job launcher and Resource Monitoring subsystem.
* ORTE: The Open Run-Time Environment (support for different back-end run-time
systems). Provides the RM messaging interface, RM error management
subsystem, RM routing subsystem and RM resource allocation subsystem.
* OPAL: The Open Portable Access Layer (utility and "glue" code used by ORCM
and ORTE). Provides operating system interfaces.

https://github.com/open-mpi/orcm/wiki[+https://github.com/open-mpi/orcm/wiki+]

http://www.open-mpi.org/projects/orcm/[+http://www.open-mpi.org/projects/orcm/+]

[[MPI-SCVT]]
MPI-SCVT
~~~~~~~~

A new algorithm, featuring overlapping domain decompositions, for the parallel construction of Delaunay and Voronoi tessellations is developed. Overlapping allows for the seamless stitching of the partial pieces of the global Delaunay tessellations constructed by individual processors. The algorithm is then modified, by the addition of stereographic projections, to handle the parallel construction of spherical Delaunay and Voronoi tessellations. The algorithms are then embedded into algorithms for the parallel construction of planar and spherical centroidal Voronoi tessellations that require multiple constructions of Delaunay tessellations. This combination of overlapping domain decompositions with stereographic projections provides a unique algorithm for the construction of spherical meshes that can be used in climate simulations. 

See also xref:STRIPACK[STRIPACK].

http://sourceforge.net/p/mpi-scvt/wiki/Home/[+http://sourceforge.net/p/mpi-scvt/wiki/Home/+]

http://www.geosci-model-dev.net/6/1353/2013/gmd-6-1353-2013.html[+http://www.geosci-model-dev.net/6/1353/2013/gmd-6-1353-2013.html+]

mpld3
~~~~~

The mpld3 project brings together Matplotlib, the popular Python-based
graphing library, and D3js, the popular Javascript library for creating
interactive data visualizations for the web. The result is a simple API for
exporting your matplotlib graphics to HTML code which can be used within the
browser, within standard web pages, blogs, or tools such as the IPython
notebook.

http://mpld3.github.io/[+http://mpld3.github.io/+]

MPPhys
~~~~~~

In a first course to classical mechanics elementary physical processes like
elastic two-body collisions, the mass–spring model, or the gravitational
two-body problem are discussed in detail. The continuation to many-body
systems, however, is deferred to graduate courses although the underlying
equations of motion are essentially the same and although there is a strong
motivation for high-school students in particular because of the use of
particle systems in computer games. The missing link between the simple and
the more complex problem is a basic introduction to solve the equations of
motion numerically which could be illustrated, however, by means of the Euler
method. The many-particle physics simulation package MPPhys offers a platform
to experiment with simple particle simulations. The aim is to give a principle
idea how to implement many-particle simulations and how simulation and
visualization can be combined for interactive visual explorations.

http://www.sciencedirect.com/science/article/pii/S0010465513004116[+http://www.sciencedirect.com/science/article/pii/S0010465513004116+]

[[MPS]]
MPS
~~~

With MPS you can design your own extensible DSLs and start using them right away to build end-user applications. Unique technology of projectional editing allows to overcome the limits of language parsers, and build much richer DSL editors, such as ones with tables and diagrams. Along with the editors, you can write comprehensive generators from your DSL to multiple target languages, be it another MPS DSL, or any of the "base" languages such as Java, C, XML, and other.

http://www.jetbrains.com/mps/[+http://www.jetbrains.com/mps/+]

MPWide
~~~~~~

MPWide is a light-weight communication library for distributed computing. It
is specifically developed to allow message passing over long-distance networks
using path-specific optimizations.

https://github.com/djgroen/MPWide/wiki[+https://github.com/djgroen/MPWide/wiki+]

http://openresearchsoftware.metajnl.com/article/view/jors.ah/[+http://openresearchsoftware.metajnl.com/article/view/jors.ah/+]

http://iopscience.iop.org/1749-4699/4/1/015001[+http://iopscience.iop.org/1749-4699/4/1/015001+]

http://arxiv.org/abs/1311.5740[+http://arxiv.org/abs/1311.5740+]

MSCEER
~~~~~~

Morse-Smale Complex Extraction, Exploration, and Reasoning is a set of tools
and libraries for feature extraction and exploration in scalar fields. MSCEER
computes a gradient-based abstract representation of a scalar field.

http://cedmav.sci.utah.edu/research/project/13-msceer.html[+http://cedmav.sci.utah.edu/research/project/13-msceer.html+]

http://link.springer.com/book/10.1007/978-3-662-44900-4[+http://link.springer.com/book/10.1007/978-3-662-44900-4+]

mscomplex3d
~~~~~~~~~~~

The mscomplex3d consists of two modules for computation and analysis of
Morse-Smale complexes on 3d grids. The first is a command line exec named
mscomplex3d. The second is a python loadable module named pyms3d. 
 The Morse-Smale complex is a topological data structure that partitions
datasets based on the gradients of an input scalar function. See here for a
quick introduction on Morse-Smale complexes. This website presents software
that computes the Morse-Smale complex of scalar functions defined on 3D
Structured Grids and 2D triangle meshes.

https://bitbucket.org/vgl_iisc/mscomplex-3d/wiki/Home[+https://bitbucket.org/vgl_iisc/mscomplex-3d/wiki/Home+]

http://vgl.serc.iisc.ernet.in/mscomplex/primer.html[+http://vgl.serc.iisc.ernet.in/mscomplex/primer.html+]

http://vgl.serc.iisc.ernet.in/mscomplex/[+http://vgl.serc.iisc.ernet.in/mscomplex/+]

https://github.com/nithins/mscomplex-tri[+https://github.com/nithins/mscomplex-tri+]

https://github.com/nithins/mscomplex[+https://github.com/nithins/mscomplex+]

MTT
~~~

MTT comprises a set of tools for modelling dynamic physical systems using the
bond-graph methodology and transforming these models into representations
suitable for analysis, control and simulation.

http://mtt.sourceforge.net/[+http://mtt.sourceforge.net/+]

http://www2.engr.arizona.edu/\~cellier/bg.html[+http://www2.engr.arizona.edu/~cellier/bg.html+]

[[MUMPS]]
MUMPS
~~~~~

The MUltifrontal Massively Parallel Solver (MUMPS) is a package for solving systems of
linear equations of the form Ax=b, where A is a sparse matrix that can be either unsymmetric,
symmetric positive definite, or general symmetric, on distributed memory computers.  MUMPS
implements a direct method based on a multifrontal approach which performs a
Gaussian factorization.

http://mumps.enseeiht.fr/[+http://mumps.enseeiht.fr/+]

[[MurCSS]]
MurCSS
~~~~~~

MurCSS  (Murphy-Epstein decomposition and Continuous Ranked Probability Skill Score) is a tool for standardized evaluation of decadal hindcast-prediction systems
written in Python using xref:CDO[CDO].
It analyzes decadal hindcast experiments in a deterministic and probabilistic way.

https://github.com/illing2005/murcss[+https://github.com/illing2005/murcss+]

https://www-miklip.dkrz.de/about/murcss/[+https://www-miklip.dkrz.de/about/murcss/+]

*MurCSS: A Tool for Standardized Evaluation of Decadal Hindcast Systems* (online article) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bf/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bf/+]

[[MUSCLE2]]
MUSCLE2
~~~~~~~

The Multiscale Coupling Library and Environment is a portable framework to do
multiscale modeling and simulation on distributed computing resources. The
generic coupling mechanism of MUSCLE is suitable for many types of multiscale
applications, notably for multiscale models as defined by the  MAPPER project
or complex automata as defined in the  COAST project. Submodels can be
implemented from scratch, but legacy code can also be used with only minor
adjustments. The runtime environment solves common problems in distributed
computing and couples submodels of a multiscale model, whether they are built
for high-performance supercomputers or for local execution. MUSCLE supports
Java, C, Cxx, Fortran, Python, MATLAB and Scala code, using MPI, xref:OpenMP[OpenMP], or
threads. 

http://apps.man.poznan.pl/trac/muscle[+http://apps.man.poznan.pl/trac/muscle+]

http://www.sciencedirect.com/science/article/pii/S1877750314000465[+http://www.sciencedirect.com/science/article/pii/S1877750314000465+]

music
~~~~~

beets
^^^^^

The purpose of beets is to get your music collection right once and for all.
It catalogs your collection, automatically improving its metadata as it goes
using the MusicBrainz database. Then it provides a bouquet of tools for
manipulating and accessing your music.

Because beets is designed as a library, it can do almost anything you can
imagine for your music collection. Via plugins, beets becomes a panacea:

* Fetch or calculate all the metadata you could possibly need: album art,
lyrics, genres, tempos, ReplayGain levels, or acoustic fingerprints.
* Get metadata from MusicBrainz, Discogs, or Beatport. Or guess metadata using
songs' filenames or their acoustic fingerprints.
* Transcode audio to any format you like.
* Check your library for duplicate tracks and albums or for albums that are
missing tracks.
* Browse your music library graphically through a Web browser and play it in
any browser that supports HTML5 Audio.

http://beets.radbox.org/[+http://beets.radbox.org/+]

Mutil
~~~~~

Copies between local file systems are a daily activity. Files are constantly
being moved to locations accessible by systems with different functions and/or
storage limits, being backed up and restored, or being moved due to upgraded
and/or replaced hardware. Hence, maximizing the performance of copies as well
as checksums that ensure the integrity of copies is desirable to minimize the
turnaround time of user and administrator activities. Modern parallel file
systems provide very high performance for such operations using a variety of
techniques such as striping files across multiple disks to increase aggregate
I/O bandwidth and spreading disks across multiple servers to increase
aggregate interconnect bandwidth.

To achieve peak performance from such systems, it is typically necessary to
utilize multiple concurrent readers/writers from multiple systems to overcome
various single-system limitations such as number of processors and network
bandwidth. The standard cp and md5sum tools of GNU coreutils found on every
modern Unix/Linux system, however, utilize a single execution thread on a
single CPU core of a single system, hence cannot take full advantage of the
increased performance of clustered file systems.

Mutil provides mcp and msum, which are drop-in replacements for cp and md5sum
that utilize multiple types of parallelism to achieve maximum copy and
checksum performance on clustered file systems. Multi-threading is used to
ensure that nodes are kept as busy as possible. Read/write parallelism allows
individual operations of a single copy to be overlapped using asynchronous
I/O. Multi-node cooperation allows different nodes to take part in the same
copy/checksum. Split file processing allows multiple threads to operate
concurrently on the same file. Finally, hash trees allow inherently serial
checksums to be performed in parallel.

http://ti.arc.nasa.gov/opensource/projects/mutil/[+http://ti.arc.nasa.gov/opensource/projects/mutil/+]

Muster
~~~~~~

The Muster library provides implementations of serial and parallel K-Medoids 
clustering algorithms.  It is intended as a general framework for parallel 
cluster analysis, particularly for performance data analysis on systems with 
very large numbers of processes.

The parallel implementations in the Muster are designed to perform well even
in environments where the data to be clustered is entirely distributed.  For
example, many performance tools need to analyze one data element from each
process in a system.  To analyze this data efficiently, clustering algorithms 
that move as little data as possible are required.  In Muster, we exploit 
sampled clustering algorithms to realize this efficiency.

The parallel algorithms in Muster are implemented using the Message Passing 
Interface (MPI), making them suitable for use on many of the world's largest 
supercomputers.  They should, however, also run efficiently on your laptop.

https://github.com/scalability-llnl/muster[+https://github.com/scalability-llnl/muster+]

[[NN]]
////
NNNN
////

nami
~~~~

Parallel wavelet compression.

https://github.com/scalability-llnl/nami[+https://github.com/scalability-llnl/nami+]

[[Nanocubes]]
Nanocubes
~~~~~~~~~

Nanocubes provides you with real-time visualization of large datasets. Slice and dice your data with respect to space, time, or some of your data attributes, and view the results in real-time on a web browser over heatmaps, bar charts, and histograms. We've used it for tens of billions of data points.

The main nanocubes program is a command-line utility that processes your data and starts a web server to answer query requests. We provide Javascript APIs for visualization. But nanocubes can be used for fast analysis of your data as well: you can think of it as a very fast (if somehow limited) database query engine.

Nanocubes build on data cube technology. Until recently, data cubes took a very large amount of space. This means they could not be stored in main memory, so their computation and access for large datasets did not mix well with interactive visualization. Our main innovation is an algorithm for hierarchical data cubes that has very modest memory requirements.

http://www.nanocubes.net/[+http://www.nanocubes.net/+]

https://github.com/laurolins/nanocube[+https://github.com/laurolins/nanocube+]

National Data Service
~~~~~~~~~~~~~~~~~~~~~

The National Data Service is an emerging vision of how scientists and
researchers across all disciplines can find, reuse, and publish data. It is an
international federation of data providers, data aggregators,
community-specific federations, publishers, and cyberinfrastructure providers.
It builds on the data archiving and sharing efforts under way within specific
communities and links them together with a common set of tools.

http://www.nationaldataservice.org/[+http://www.nationaldataservice.org/+]

NavPy
~~~~~

Navigation and estimation tools written in Python.

https://github.com/NavPy[+https://github.com/NavPy+]

NCAS
~~~~

Parallel computation is widely employed in scientific researches, engineering
activities and product development. Parallel program writing itself is not
always a simple task depending on problems solved. Large-scale scientific
computing, huge data analyses and precise visualizations, for example, would
require parallel computations, and the parallel computing needs the
parallelization techniques. In this Chapter a parallel program generation
support is discussed, and a computer-assisted parallel program generation
system P-NCAS is introduced. Computer assisted problem solving is one of key
methods to promote innovations in science and engineering, and contributes to
enrich our society and our life toward a programming-free environment in
computing science. Problem solving environments (PSE) research activities had
started to enhance the programming power in 1970's. The P-NCAS is one of the
PSEs; The PSE concept provides an integrated human-friendly computational
software and hardware system to solve a target class of problems.

http://www.ee.utsunomiya-u.ac.jp/\~kawatalab/pse/ncas.html[+http://www.ee.utsunomiya-u.ac.jp/~kawatalab/pse/ncas.html+]

http://arxiv.org/abs/1503.04501[+http://arxiv.org/abs/1503.04501+]

NcSOS
~~~~~

NcSOS adds an OGC SOS service to datasets in your existing xref:THREDDS[THREDDS] server. It
complies with the IOOS SWE Milestone 1.0 templates and requires your datasets
be in any of the CF 1.6 Discrete Sampling Geometries.

NcSOS acts like other THREDDS services (such an OPeNDAP and WMS) where as
there are individual service endpoints for each dataset. It is best to
aggregate your files and enable the NcSOS service on top of the aggregation.
i.e. The NcML aggregate of hourly files from an individual station would be a
good candidate to serve with NcSOS. Serving the individual hourly files with
NcSOS would not be as beneficial.

You will need a working THREDDS installation of a least version 4.3.16 to run
NcSOS.

https://github.com/asascience-open/ncSOS[+https://github.com/asascience-open/ncSOS+]

NDL
~~~

The numerical differentiation library (NDL) used for the numerical estimation
of first and second order partial derivatives of a function by finite
differencing. In this version we have restructured the serial implementation
of the code so as to achieve optimal task-based parallelization. The pure
shared-memory parallelization of the library has been based on the lightweight
xref:OpenMP[OpenMP] tasking model allowing for the full extraction of the available
parallelism and efficient scheduling of multiple concurrent library calls. On
multicore clusters, parallelism is exploited by means of TORC, an MPI-based
multi-threaded tasking library. The new MPI implementation of NDL provides
optimal performance in terms of function calls and, furthermore, supports
asynchronous execution of multiple library calls within legacy MPI programs.
In addition, a Python interface has been implemented for all cases, exporting
the functionality of our library to sequential Python codes.

http://www.sciencedirect.com/science/article/pii/S0010465514001258[+http://www.sciencedirect.com/science/article/pii/S0010465514001258+]

Neko
~~~~

Neko is an high-level dynamicly typed programming language. It can be used as
an embedded scripting language. It has been designed to provide a common
runtime for several different languages. Learning and using Neko is very easy.
You can easily extend the language with C libraries. You can also write
generators from your own language to Neko and then use the Neko Runtime to
compile, run, and access existing libraries. 

http://nekovm.org/[+http://nekovm.org/+]

Neo
~~~

Neo is minimal and fast Go Web Framework with extremely simple API.

During development you will enjoy in automatic reruning and recompiling your
Neo application when source changes.

Build your Neo Application in few lines of code.

http://ivpusic.github.io/neo/[+http://ivpusic.github.io/neo/+]

Neo4j
~~~~~

An open-source NoSQL graph database implemented in Java and Scala. With
development starting in 2003, it has been publicly available since 2007. The
source code and issue tracking are available on GitHub, with support readily
available on Stack Overflow and the Neo4j Google group.

Neo4j implements the Property Graph Model efficiently down to the storage
level. As opposed to graph processing or in-memory libraries, Neo4j provides
full database characteristics including ACID transaction compliance, cluster
support, and runtime failover, making it suitable to use graph data in
production scenarios.

Neo4j’s free and open-source Community edition is a high-performance, fully
ACID-transactional database. The Community edition includes (but is not
limited to) all the functionality described previously in this section.

http://neo4j.com/[+http://neo4j.com/+]

GraphGist
^^^^^^^^^

GraphGists are an easy way to create and share documents containing not just
prose, structure and pictures but most importantly example graph models and
use-cases expressed in Neo4j’s query language Cypher.
These documents are written in AsciiDoc — the simple, textual markup
language — and rendered in your browser as rich and interactive web pages that
you can quickly evolve from describing simple howtos or questions to providing
an extensive use-case specification.

http://gist.neo4j.org/[+http://gist.neo4j.org/+]

NESToolbox
~~~~~~~~~~

The NESToolbox is a collection of algorithms to perform similarity estimation
for irregularly sampled time series as they arise for example in the
geosciences. It is implemented as a toolbox for the widely used software
MATLAB and the freely available open-source software OCTAVE. 

The installation of the Python portation is simple: just copy the nest.py in
your working directory.

http://tocsy.pik-potsdam.de/nest.php[+http://tocsy.pik-potsdam.de/nest.php+]

http://www.clim-past.net/10/107/2014/cp-10-107-2014.html[+http://www.clim-past.net/10/107/2014/cp-10-107-2014.html+]

[[netCDF]]
[[NetCDF]]
NetCDF
~~~~~~

https://github.com/Unidata/netcdf-c[+https://github.com/Unidata/netcdf-c+]

https://github.com/Unidata/netcdf-fortran[+https://github.com/Unidata/netcdf-fortran+]

https://github.com/Unidata/netcdf-cxx4[+https://github.com/Unidata/netcdf-cxx4+]

CfRadial
^^^^^^^^

CF-compliant NetCDF for radial data.

http://www.ral.ucar.edu/projects/titan/docs/radial_formats/index.html[+http://www.ral.ucar.edu/projects/titan/docs/radial_formats/index.html+]

[[CMOR2]]
CMOR2
^^^^^

The "Climate Model Output Rewriter" (CMOR, pronounced "Seymour") comprises a
set of C-based functions, with bindings to both Python and FORTRAN 90, that
can be used to produce CF-compliant netCDF files that fulfill the requirements
of many of the climate community's standard model experiments. These
experiments are collectively referred to as MIP's and include, for example,
AMIP, CMIP, CFMIP, PMIP, APE, and IPCC scenario runs. The output resulting
from CMOR is "self-describing" and facilitates analysis of results across
models.

Much of the metadata written to the output files is defined in MIP-specific
tables, typically made available from each MIP's web site. CMOR relies on
these tables to provide much of the metadata that is needed in the MIP
context, thereby reducing the programming effort required of the individual
MIP contributors.

http://pcmdi.github.io/cmor-site/[+http://pcmdi.github.io/cmor-site/+]

netcdf-fortran
^^^^^^^^^^^^^^

The NetCDF Fortran libraries.

https://github.com/Unidata/netcdf-fortran[+https://github.com/Unidata/netcdf-fortran+]

obs4MIPS
xxxxxxxx

The software package to be disclosed, obs4MIPS.py, is a front end to an
existing free software package, CMOR2 (Climate Model Output Rewriter), written
by Lawrence Livermore National Laboratory (LLNL), and reads in a multitude of
standard data formats, such as netcdf3, netcdf4, Grads control files, Matlab
data files or a list of netcdf files, and converts the data into the CMIP5
data format to allow publication on the Earth System Grid Federation (ESGF)
data node.

http://opensource.gsfc.nasa.gov/projects/obs4mips/index.php[+http://opensource.gsfc.nasa.gov/projects/obs4mips/index.php+]


h5netcdf
^^^^^^^^

Pythonic interface to netCDF4 via h5py.

https://github.com/shoyer/h5netcdf[+https://github.com/shoyer/h5netcdf+]

[[ncio]]
ncio
^^^^

The NCIO (NetCDF Input/Output) module has been designed as an interface to the NetCDF library with simplicity and ease of use in mind. While this implies that some NetCDF functionality is masked from the user, the subroutines provided here are adequate for basic serial reading and writing tasks of up to 6-D data arrays along with corresponding data attributes.

https://github.com/alex-robinson/ncio[+https://github.com/alex-robinson/ncio+]

http://www.geosci-model-dev.net/8/1877/2015/gmd-8-1877-2015.html[+http://www.geosci-model-dev.net/8/1877/2015/gmd-8-1877-2015.html+]

[[NCO]]
NCO
^^^

The NCO toolkit manipulates and analyzes data stored in netCDF-accessible formats, including DAP, HDF4, and HDF5. It exploits the geophysical expressivity of many CF (Climate & Forecast) metadata conventions, the flexible description of physical dimensions translated by UDUnits, the network transparency of OPeNDAP, the storage features (e.g., compression, chunking, groups) of HDF (the Hierarchical Data Format), and many powerful mathematical and statistical algorithms of GSL (the GNU Scientific Library).

The netCDF Operators (NCO) comprise a dozen standalone, command-line programs that take netCDF, HDF, and/or DAP files as input, then operate (e.g., derive new data, compute statistics, print, hyperslab, manipulate metadata) and output the results to screen or files in text, binary, or netCDF formats. NCO aids analysis of gridded scientific data. The shell-command style of NCO allows users to manipulate and analyze files interactively, or with expressive scripts that avoid some overhead of higher-level programming environments. 

http://nco.sourceforge.net/[+http://nco.sourceforge.net/+]

https://github.com/czender/nco[+https://github.com/czender/nco+]

[[NetCDF_Java]]
NetCDF Java
^^^^^^^^^^^

The NetCDF-Java library implements a Common Data Model (CDM), a generalization of the NetCDF, OpenDAP and HDF5 data models. The library is a prototype for the NetCDF-4 project, which provides a C language API for the "data access layer" of the CDM, on top of the HDF5 file format. The NetCDF-Java library is a 100% Java framework for reading netCDF and other file formats into the CDM, as well as writing to the netCDF-3 file format. Writing to the netCDF-4 file format requires installing the netCDF C library. The NetCDF-Java library also implements NcML, which allows you to add metadata to CDM datasets, as well as to create virtual datasets through aggregation. The xref:THREDDS[THREDDS] Data Server (TDS) is built on top of the NetCDF-Java library.

http://www.unidata.ucar.edu/software/thredds/current/netcdf-java/[+http://www.unidata.ucar.edu/software/thredds/current/netcdf-java/+]

http://www.unidata.ucar.edu/software/thredds/current/netcdf-java/CDM/index.html[+http://www.unidata.ucar.edu/software/thredds/current/netcdf-java/CDM/index.html+]

netcdf4-python
^^^^^^^^^^^^^^

Python/numpy interface to the netCDF C library.

https://github.com/Unidata/netcdf4-python[+https://github.com/Unidata/netcdf4-python+]

http://unidata.github.io/netcdf4-python/[+http://unidata.github.io/netcdf4-python/+]

*netCDF4 Module* - http://unidata.github.io/netcdf4-python/[+http://unidata.github.io/netcdf4-python/+]

*Reading NetCDF Data* (notebook) - https://github.com/Unidata/unidata-python-workshop/blob/master/reading_netCDF.ipynb[+https://github.com/Unidata/unidata-python-workshop/blob/master/reading_netCDF.ipynb+]

netCDF4p
xxxxxxxx

Wrapper around python-netCDF4 that allows Coordinate subscripting, similar to NCL.

https://github.com/mathause/netCDF4p[+https://github.com/mathause/netCDF4p+]

[[PnetCDF]]
PnetCDF
^^^^^^^

A  library providing high-performance parallel I/O while still maintaining file-format compatibility with  Unidata's NetCDF, specifically the formats of CDF-1 and CDF-2. Although NetCDF supports parallel I/O starting from version 4, the files must be in HDF5 format. PnetCDF is currently the only choice for carrying out parallel I/O on files that are in classic formats (CDF-1 and 2). 
In addition, PnetCDF supports the CDF-5 file format, an extension of CDF-2, that supports more data types and allows users to define large dimensions, attributes, and variables.

http://trac.mcs.anl.gov/projects/parallel-netcdf[+http://trac.mcs.anl.gov/projects/parallel-netcdf+]

http://www.geosci-model-dev.net/8/1033/2015/gmd-8-1033-2015.html[+http://www.geosci-model-dev.net/8/1033/2015/gmd-8-1033-2015.html+]

PyReshaper
^^^^^^^^^^

A for converting NetCDF files from time-slice (history) format to time-series (single-variable).
This performs time-slice to time-series convertion of NetCDF files, compliant with the CF 1.6 Conventions. The PyReshaper package is designed to run in parallel to maximize performance, with the parallelism implemented over variables (i.e., task parallelism). This means that the maximum parallelism achieveable for a given operation is one core/processor per variables in the time-slice NetCDF files.

https://github.com/NCAR-CISL-ASAP/PyReshaper[+https://github.com/NCAR-CISL-ASAP/PyReshaper+]

pysgrid
^^^^^^^

A Python tools for staggered grids.

https://github.com/sgrid/pysgrid[+https://github.com/sgrid/pysgrid+]

pyugrid
^^^^^^^

A Python API to utilize data written using the netcdf unstructured grid conventions.

https://github.com/pyugrid/pyugrid[+https://github.com/pyugrid/pyugrid+]

https://github.com/ugrid-conventions/ugrid-conventions[+https://github.com/ugrid-conventions/ugrid-conventions+]

[[Rosetta]]
Rosetta
^^^^^^^

A web-based service that provides an easy, wizard-based interface for data collectors to transform their datalogger generated ASCII output into Climate and Forecast (CF) compliant netCDF files, complete with metadata describing what data are contained in the file, the instruments used to collect the data, and other critical information that otherwise may be lost in one of many dreaded README files.

http://www.unidata.ucar.edu/software/rosetta/[+http://www.unidata.ucar.edu/software/rosetta/+]

https://github.com/Unidata/rosetta[+https://github.com/Unidata/rosetta+]

NotebookCloud
~~~~~~~~~~~~~

Running IPython notebook servers on Amazon's EC2.

https://chrome.google.com/webstore/detail/notebookcloud/afepkpkdjhebipfagcfkeganflaannla?hl=en-GB[+https://chrome.google.com/webstore/detail/notebookcloud/afepkpkdjhebipfagcfkeganflaannla?hl=en-GB+]

NPiler
~~~~~~

We introduce the very first NPU compilation workflow, called NPiler, which
automatically converts annotated regions of imperative code to a neural
network representation. First, the programmer annotates the regions of
imperative code which he/she wants to transform to a neural representation.
NPiler accepts inputs from the programmer to train the network. During this
step, NPiler automatically observes the input and output pairs to the
annotated regions to collect training and testing data. Then, NPiler trains
each possible NPU topology given constraints provide by the programmer. The
outcome of this exploration provides the best possible NPU topology in terms
of minimum root mean square error (RMSE) on test data. Finally, our compiler
replaces the annotated regions with the final neural network representation.
We use FANN library to execute the neural network representation. We released
NPiler with seven representative benchmarks from diverse domains to evaluate
our NPU compilation workflow. 

https://bitbucket.org/act-lab/axbench[+https://bitbucket.org/act-lab/axbench+]

http://act-lab.org/artifacts/npiler/[+http://act-lab.org/artifacts/npiler/+]

Numpy
~~~~~

[[ND4J]]
ND4J
^^^^

A scientific computing library for the JVM. It is meant to be used in production environments rather than as a research tool, which means routines are designed to run fast with minimum RAM requirements.
A usability gap has separated Java, Scala and Clojure programmers from the most powerful tools in data analysis, like NumPy or Matlab. With ND4J, intuitive scientific computing tools once limited to the Python community are now open source, distributed and integrated with GPUs on the JVM.

http://nd4j.org/[+http://nd4j.org/+]

[[NO]]
////
NOOO
////

OCCI
~~~~

The Open Cloud Computing Interface comprises a set of open community-lead
specifications delivered through the Open Grid Forum. OCCI is a Protocol and
API for all kinds of Management tasks. OCCI was originally initiated to create
a remote management API for IaaS model based Services, allowing for the
development of interoperable tools for common tasks including deployment,
autonomic scaling and monitoring. It has since evolved into a flexible API
with a strong focus on integration, portability, interoperability and
innovation while still offering a high degree of extensibility. The current
release of the Open Cloud Computing Interface is suitable to serve many other
models in addition to IaaS, including e.g. PaaS and SaaS. 

http://occi-wg.org/[+http://occi-wg.org/+]

http://occi-wg.org/community/implementations/[+http://occi-wg.org/community/implementations/+]

ocl-icd
~~~~~~~

OpenCL implementations are provided as ICD (Installable Client Driver). An
OpenCL program can use several ICD thanks to the use of an ICD Loader as
provided by this project. This free ICD Loader can load any (free or non free)
ICD.

This package aims at creating an Open Source alternative to vendor specific
OpenCL ICD loaders.
The main difficulties to create such software is that the order of
function pointers in a structure is not publicy available.
This software maintains a YAML database of all known and guessed
entries.
This package also delivers a skeleton of bindings to incorporate inside an
OpenCL implementation to give it ICD functionalities.

https://forge.imag.fr/projects/ocl-icd/[+https://forge.imag.fr/projects/ocl-icd/+]

OCR
~~~

The Open Community Runtime project is creating an application building
framework that explores new methods of high-core-count programming. The
initial focus is on HPC applications. Its goal is to create a tool that helps
app developers improve the power efficiency, programmability, and reliability
of their work while maintaining app performance.

OCR will help the app developer with the complex process of writing multi-core
apps create by masking the effort to manage event-driven tasks, events (which
embody dataflow and code flow dependencies), memory data blocks (with semantic
annotations for runtime use), machine description facilities, and more.

This is a large open source project distributed under the GPL-2.0+ open source
license. With a mature and established codebase containing almost 8 million
lines of code, Linux ACPI is written largely in C. OCR was originally unveiled
at Supercomputing Conference 2012 (SC12) with a major new release (v0.8)
introduced at Supercomputing 2013 (SC13). Community participation is
encouraged, both for runtime enhancement as well as exploration of
algorithm/application decomposition for new programming models.

https://xstack.exascale-tech.com/git/public/xstack.git[+https://xstack.exascale-tech.com/git/public/xstack.git+]

https://01.org/open-community-runtime[+https://01.org/open-community-runtime+]

https://github.com/01org/ocr[+https://github.com/01org/ocr+]

Octave
~~~~~~

GNU Octave is a high-level interpreted language, primarily intended for numerical computations. It provides capabilities for the numerical solution of linear and nonlinear problems, and for performing other numerical experiments. It also provides extensive graphics capabilities for data visualization and manipulation. Octave is normally used through its interactive command line interface, but it can also be used to write non-interactive programs. The Octave language is quite similar to Matlab so that most programs are easily portable. 
http://www.gnu.org/software/octave/[+http://www.gnu.org/software/octave/+]

octavemagic
^^^^^^^^^^^

The octavemagic extension provides the ability to interact with Octave. It is
provided by the oct2py package, which may be installed using pip or
easy_install.

https://pypi.python.org/pypi/oct2py[+https://pypi.python.org/pypi/oct2py+]

http://nbviewer.ipython.org/github/blink1073/oct2py/blob/master/example/octavemagic_extension.ipynb[+http://nbviewer.ipython.org/github/blink1073/oct2py/blob/master/example/octavemagic_extension.ipynb+]

OData
~~~~~

OData (Open Data Protocol) is an OASIS standard that defines the best practice
for building and consuming RESTful APIs. OData helps you focus on your
business logic while building RESTful APIs without having to worry about the
approaches to define request and response headers, status codes, HTTP methods,
URL conventions, media types, payload formats and query options etc. OData
also guides you about tracking changes, defining functions/actions for
reusable procedures and sending asynchronous/batch requests etc. Additionally,
OData provides facility for extension to fulfil any custom needs of your
RESTful APIs.
OData RESTful APIs are easy to consume. The OData metadata, a machine-readable
description of the data model of the APIs, enables the creation of powerful
generic client proxies and tools. Some of them can help you interact with
OData even without knowing anything about the protocol.

http://www.odata.org/[+http://www.odata.org/+]

ODataPy
^^^^^^^

ODataPy is an open-source Python library that implements the Open Data
Protocol (OData). It supports the OData protocol version 4.0. It is built on
top of ODataCpp using language binding. It is under development and currently
serves only parts of client and client side proxy generation (code gen)
aspects of OData.

https://github.com/OData/odatapy-client[+https://github.com/OData/odatapy-client+]

odataserver
^^^^^^^^^^^

Odata Server with support for MySQL and for BLOBs managed by Leveled.

https://github.com/gizur/odataserver/[+https://github.com/gizur/odataserver/+]

OFED
~~~~

The OpenFabrics Enterprise Distribution (OFED™) is open-source software for
RDMA and kernel bypass applications. OFED is used in business, research and
scientific environments that require highly efficient networks, storage
connectivity and parallel computing. The software provides high performance
computing sites and enterprise data centers with flexibility and investment
protection as computing evolves towards applications that require extreme
speeds, massive scalability and utility-class reliability.

OFED includes kernel-level drivers, channel-oriented RDMA and send/receive
operations, kernel bypasses of the operating system, both kernel and
user-level application programming interface (API) and services for parallel
message passing (MPI), sockets data exchange (e.g., RDS, SDP), NAS and SAN
storage (e.g. iSER, NFS-RDMA, SRP) and file system/database systems.

The network and fabric technologies that provide RDMA performance with OFED
include: legacy 10 Gigabit Ethernet, iWARP for Ethernet, RDMA over Converged
Ethernet (RoCE), and 10/20/40 Gigabit InfiniBand.

https://www.openfabrics.org/index.php/ofed-for-linux-ofed-for-windows/ofed-overview.html[+https://www.openfabrics.org/index.php/ofed-for-linux-ofed-for-windows/ofed-overview.html+]

OFF
~~~

OFF, an open source (free software) code for performing fluid dynamics
simulations, is presented. The aim of OFF is to solve, numerically, the
unsteady (and steady) compressible Navier–Stokes equations of fluid dynamics
by means of finite volume techniques: the research background is mainly
focused on high-order (WENO) schemes for multi-fluids, multi-phase flows over
complex geometries. To this purpose a highly modular, object-oriented
application program interface (API) has been developed. In particular, the
concepts of data encapsulation and inheritance available within Fortran
language (from standard 2003) have been stressed in order to represent each
fluid dynamics “entity” (e.g. the conservative variables of a finite volume,
its geometry, etc…) by a single object so that a large variety of
computational libraries can be easily (and efficiently) developed upon these
objects. 

http://www.sciencedirect.com/science/article/pii/S0010465514001283[+http://www.sciencedirect.com/science/article/pii/S0010465514001283+]

http://szaghi.github.io/OFF/index.html[+http://szaghi.github.io/OFF/index.html+]

OGC
~~~

See also xref:Leaflet[Leaflet] and xref:MapFish[MapFish].

Sensor Web
^^^^^^^^^^

http://en.wikipedia.org/wiki/Sensor_web[+http://en.wikipedia.org/wiki/Sensor_web+]

http://www.ogcnetwork.net/taxonomy/term/152[+http://www.ogcnetwork.net/taxonomy/term/152+]

http://www.ogcnetwork.net/swe_software[+http://www.ogcnetwork.net/swe_software+]

52N SOS
^^^^^^^

A reference implementation of the OGC Sensor Observation Service specification
(version 2.0).

https://github.com/52North/SOS[+https://github.com/52North/SOS+]

COWS
^^^^

The CEDA OGC Web Services framework (COWS) is a Python software framework
developed at the Centre of Environmental Data Archival for implementing Open
Geospacial Consortium web service standards.

http://cows.ceda.ac.uk/[+http://cows.ceda.ac.uk/+]

GeoJModelBuilder
^^^^^^^^^^^^^^^^

GeoJModelBuilder couples geosprocessing Web services, NASA World Wind and
Sensor Web services to support geoprocessing modeling and environmental
monitoring.The main goal of GeoJModelBuilder is to bring an easy-to-use tool
to the geoscientific community.

The tool can allow users to drag and drop various geospatial services to
visually generate workflows and interact with the workflows in a virtual globe
environment. It also allows users to audit trails of workflow executions,
check the provenance of data products, and support scientific reproducibility.

The programming language used for the development is Java due to its
platform-independent feature. The tool can be operated on any operating
systems such as Windows or Unix/Linux that supports Java.

http://sourceforge.net/projects/geopw/[+http://sourceforge.net/projects/geopw/+]

http://sourceforge.net/p/geopw/wiki/home/[+http://sourceforge.net/p/geopw/wiki/home/+]

http://geopw.whu.edu.cn/geopw.html[+http://geopw.whu.edu.cn/geopw.html+]

GeoNetwork
^^^^^^^^^^

A catalog application to manage spatially referenced resources. It provides
powerful metadata editing and search functions as well as an embedded
interactive web map viewer.
GeoNetwork has been developed to connect spatial information communities and
their data using a modern architecture, which is at the same time powerful and
low cost, based on the principles of Free and Open Source Software (FOSS) and
International and Open Standards for services and protocols (a.o. from
ISO/TC211 and OGC).

The software provides an easy to use web interface to search geospatial data
across multiple catalogs, combine distributed map services in the embedded map
viewer, publish geospatial data using the online metadata editing tools and
optionally the embedded GeoServer map server.

You will find support for a range of standards. Metadata standards
(ISO19115/ISO19119/ISO19110 following ISO19139, FGDC and Dublin Core), Catalog
interfaces (OGC-CSW2.0.2 ISO profile client and server, OAI-PMH client and
server, GeoRSS server, GEO OpenSearch server, WebDAV harvesting, GeoNetwork to
GeoNetwork harvesting support) and Map Services interfaces (OGC-WMS, WFS, WCS,
KML and others) through the embedded GeoServer map server.

http://geonetwork-opensource.org/[+http://geonetwork-opensource.org/+]

GeoNode
^^^^^^^

GeoNode is a web-based application and platform for developing geospatial
information systems (GIS) and for deploying spatial data infrastructures
(SDI).
Data management tools built into GeoNode allow for integrated creation of
data, metadata, and map visualizations. Each dataset in the system can be
shared publicly or restricted to allow access to only specific users. Social
features like user profiles and commenting and rating systems allow for the
development of communities around each platform to facilitate the use,
management, and quality control of the data the GeoNode instance contains.

http://geonode.org/[+http://geonode.org/+]

GeoServer
~~~~~~~~~

GeoServer is an open source server for sharing geospatial data.  Designed for interoperability, it publishes data from any major spatial data source using open standards. 

 GeoServer is a OGC compliant implementation of a number of open standards such as Web Feature Service (WFS), Web Map Service (WMS), and Web Coverage Service (WCS).

Additional formats and publication options are available including Web Map Tile Service (WMTS) and extensions for Catalogue Service (CSW) and Web Processing Service (WPS). 

http://geoserver.org/[+http://geoserver.org/+]

GeoWebCache
^^^^^^^^^^^

GeoWebCache is a Java web application used to cache map tiles coming from a
variety of sources such as OGC Web Map Service (WMS). It implements various
service interfaces (such as WMS-C, WMTS, TMS, Google Maps KML, Virtual Earth)
in order to accelerate and optimize map image delivery. It can also recombine
tiles to work with regular WMS clients.

http://geowebcache.org/[+http://geowebcache.org/+]

https://github.com/justb4/geowebcache[+https://github.com/justb4/geowebcache+]

[[i52n-SOS]]
i52n-SOS
^^^^^^^^

An IOOS customized build of the 52°North Sensor Observation Service (SOS).
This extends the stock upstream 52°North (52n) SOS with IOOS specific encoding formats, test data, and more.
The custom encoding formats include:

* enhanced GetCapabilitiesResponse (extra metadata)
* enhanced SensorML (extra metadata and network/station/sensor hierarchies)
* O&M and SWE (IOOS m1.0 SOS format)
* netCDF (CF 1.6/ACDD 1.1/NODC 1.0/IOOS 1.0 conventions) 

http://ioos.github.io/i52n-sos/[+http://ioos.github.io/i52n-sos/+]

*IOOS SOS v1.0 WSDD* - http://ioos.github.io/sos-guidelines/doc/wsdd/sos_wsdd_github_notoc/[+http://ioos.github.io/sos-guidelines/doc/wsdd/sos_wsdd_github_notoc/+]

[[ioos_sos_parser]]
ioos_sos_parser
^^^^^^^^^^^^^^^

A XML Parser library for Sensor Web Enablement (SWE) Common Data Model (CDM).

https://github.com/asascience-open/ioos_sos_parser[+https://github.com/asascience-open/ioos_sos_parser+]

istSOS
^^^^^^

An OGC SOS server implementation written in Python. istSOS allows for managing
and dispatch observations from monitoring sensors according to the Sensor
Observation Service standard.
The project provides also a Graphical user Interface that allows for easing
the daily operations and a RESTful Web api for automatizing administration
procedures.

http://sourceforge.net/projects/istsos/[+http://sourceforge.net/projects/istsos/+]

[[OGCServer]]
OGCServer
^^^^^^^^^

Compliant WMS server written in Python and using Mapnik Cxx library.

https://github.com/mapnik/OGCServer/wiki[+https://github.com/mapnik/OGCServer/wiki+]

OWSLib
^^^^^^

OWSLib is a Python package for client programming with Open Geospatial
Consortium (OGC) web service (hence OWS) interface standards, and their
related content models.

http://geopython.github.io/OWSLib/[+http://geopython.github.io/OWSLib/+]

http://nbviewer.ipython.org/github/EODP-NZ/eib-ows-examples/blob/master/python/NIWA%20OWS%20Examples%20WMS.ipynb[+http://nbviewer.ipython.org/github/EODP-NZ/eib-ows-examples/blob/master/python/NIWA%20OWS%20Examples%20WMS.ipynb+]

http://nbviewer.ipython.org/github/rsignell-usgs/notebook/blob/master/wms_sample.ipynb[+http://nbviewer.ipython.org/github/rsignell-usgs/notebook/blob/master/wms_sample.ipynb+]

PyCSW
^^^^^

An OGC CSW server implementation written in Python that allows for the
publishing and discovery of geospatial metadata, providing a standards-based
metadata and catalogue component of spatial data infrastructures.

http://pycsw.org/[+http://pycsw.org/+]

http://geopython.github.io/pycsw-workshop/[+http://geopython.github.io/pycsw-workshop/+]

http://geopython.github.io/[+http://geopython.github.io/+]

pyoos
^^^^^

Python library for collecting Met/Ocean observations.
Pyoos attempts to fill the need for a high level data collection library for
met/ocean data publically available through many different websites and
webservices.

Pyoos will collect and parse the following data services into the xref:Paegan[Paegan]
Discrete Geometry CDM:

* IOOS SWE SOS 1.0 Services
** ex. NcSOS instance: sos.maracoos.org/stable/sos/wflow700-agg.ncml
** ex. IOOS 52N instance: ioossos.axiomalaska.com/52n-sos-ioos-stable
* NERRS Observations - SOAP
* http://sdf.ndbc.noaa.gov/sos/[NDBC SOS]
* http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/[CO-OPS SOS]
* http://www.epa.gov/storet/[STORET Water Quality] - WqxOutbound via REST
* http://waterdata.usgs.gov/nwis/qw[USGS NWIS Water Quality] - WqxOutbound via REST (waterqualitydata.us)
* http://help.waterdata.usgs.gov/faq/automated-retrievals[USGS Instantaneous Values] - WaterML via REST
* http://graphical.weather.gov/xml/rest.php[NWS AWC Observations] - XML via REST (http://www.aviationweather.gov)
* http://www.nws.noaa.gov/oh/hads/[HADS] - limited to 7 day rolling window
of data)

https://github.com/ioos/pyoos[+https://github.com/ioos/pyoos+]

PySOS
^^^^^
PySOS, a python-based implementation of the OGC SOS standard. PySOS is a
lightweight set of scripts that work in conjunction with a web server to serve
data from a relational database.

http://sourceforge.net/projects/pysos/[+http://sourceforge.net/projects/pysos/+]

http://sourceforge.net/p/pysos/code/HEAD/tree/[+http://sourceforge.net/p/pysos/code/HEAD/tree/+]

python-sld
^^^^^^^^^^

The OGC Styled Layer Descriptor (SLD) profile of the WMS standard defines
encoding that extends the WMS standard to allow user-defined symbolization and
coloring of geographic feature and coverage data.  It  addresses the need for
users and software to be able to control the visual portrayal of the
geospatial data. The ability to define styling rules requires a styling
language that the client and server can both understand. 

This is a Python library for reading, writing, and manipulating SLD files.

https://crate.io/packages/python-sld/1.0.10/[+https://crate.io/packages/python-sld/1.0.10/+]

http://www.opengeospatial.org/standards/sld[+http://www.opengeospatial.org/standards/sld+]

PyWPS
^^^^^

An implementation of the Web processing Service standard from Open Geospatial
Consortium.
PyWPS offers an environment for programming own processes (geofunctions or
models) which can be accessed from the public. The main advantage of PyWPS is,
that it has been written with native support for GRASS GIS. Access to GRASS
modules via web interface should be as easy as possible.

http://pywps.wald.intevation.org/[+http://pywps.wald.intevation.org/+]

http://pywps.wald.intevation.org/documentation/course/[+http://pywps.wald.intevation.org/documentation/course/+]

sos4R
^^^^^

A client for Sensor Observation Services (SOS) as specified by the Open
Geospatial Consortium (OGC). It allows users to retrieve metadata from SOS web
services and to interactively create requests for near real-time observation
data based on the available sensors, phenomena, observations et cetera using
thematic, temporal and spatial filtering.

http://cran.r-project.org/web/packages/sos4R/index.html[+http://cran.r-project.org/web/packages/sos4R/index.html+]

SOS.js
^^^^^^

SOS.js is a JavaScript library to browse, visualise, and access, data from an
Open Geospatial Consortium (OGC) Sensor Observation Service (SOS).
The library consists of a number of modules, which along with their
dependencies build a layered abstraction for communicating with a SOS.

The core module - SOS.js, contains a number of objects that encapsulate core
concepts of SOS, such as managing the service connection parameters, the
service’s capabilities document, methods to access the service’s Features of
Interest (FOIs), offerings, observed properties etc. It also contains various
utility functions, available as methods of the SOS.Utils object. 
This module is built on top of OpenLayers, for low-level SOS request/response
handling.

The user interface module - SOS.Ui.js, contains the UI components of the
library. These components can be used standalone, but are also brought
together in the default SOS.App object as a (somewhat) generic web
application. This module is built on top of OpenLayers which provides simple
mapping for discovery; jQuery for the UI and plumbing; and flot, which is a
jQuery plugin, for the plotting.

https://github.com/52North/sos-js[+https://github.com/52North/sos-js+]

http://sosjs.readthedocs.org/en/latest/[+http://sosjs.readthedocs.org/en/latest/+]

sospilot
^^^^^^^^

Sensor Observation Service (SOS) and data management.

https://github.com/Geonovum/sospilot[+https://github.com/Geonovum/sospilot+]

http://sospilot.readthedocs.org/en/latest/[+http://sospilot.readthedocs.org/en/latest/+]

[[SpatiaLite]]
SpatiaLite
^^^^^^^^^^

A library extending the basic SQLite core in order to get a full fledged
Spatial DBMS, really simple and lightweight, but mostly OGC-SFS compliant.

http://www.gaia-gis.it/gaia-sins/[+http://www.gaia-gis.it/gaia-sins/+]

spatialite-tools
xxxxxxxxxxxxxxxx

A collection of open source Command Line Interface (CLI) tools supporting
SpatiaLite.

https://www.gaia-gis.it/fossil/spatialite-tools/index[+https://www.gaia-gis.it/fossil/spatialite-tools/index+]

webEOM
^^^^^^

The web-based Earth Observation Monitor (webEOM) provides easy access and
visualization for spatial time-series data. It is based on a spatial data
infrastructure containing a Metadata catalogue, visualization and download
services as well as processing services. These services are compliant to
specifications of the Open Geospatial Consortium (OGC).

webEOM is designed for an easy usage. Time-series plots can be generated
within a few clicks without data processing needs by the user. Further
developments are planned for 2014, e.g. users will have the possibilities to
generate further plots specified by individual parameters and users can
specifiy monitoring parameters for individual areas and datasets.

http://earth-observation-monitor.net/software.php[+http://earth-observation-monitor.net/software.php+]

pyEOM
xxxx+

Data download from multiple data providers as well as data integration and
provision with OGC compliant web services are implemented in Python
programming language. 

https://github.com/jonas-eberle/pyEOM[+https://github.com/jonas-eberle/pyEOM+]

http://earth-observation-monitor.net/software.php[+http://earth-observation-monitor.net/software.php+]

OmicABEL
~~~~~~~~

The OmicABEL (pronounced as "amicable") package allows rapid mixed-model based
genome-wide association analysis; it efficiently handles large datasets, and
both single trait and multiple trait ("omics") analysis.

CLAK-GWAS is a software for performing Genome-Wide Association Studies
(GWAS). It provides a high-performance implementation of two 
algorithms, CLAK-Chol and CLAK-Eig, for GWAS involving single and 
multiple phenotypes, respectively. 

https://github.com/lucasb-eyer/OmicABEL[+https://github.com/lucasb-eyer/OmicABEL+]

http://arxiv.org/abs/1404.3406[+http://arxiv.org/abs/1404.3406+]

http://www.genabel.org/packages/OmicABEL[+http://www.genabel.org/packages/OmicABEL+]

Omni
~~~~

Omni compiler is a collection of programs and libraries that allow users to
build code transformation compilers. Omni Compiler is to translate C and
Fortran programs with XcalableMP and/or OpenACC directives into parallel code
suitable for compiling with a native compiler linked with the Omni Compiler
runtime library. 

Omni compiler consists of following components:

* XcalableMP -
XcalableMP is a directive-based language extension of C and Fortran for
distributed memory systems. XcalableMP allows users to develop a parallel
application and to tune its performance with minimal and simple notation. 
* OpenACC -
OpenACC is a directive-based programming interface for accelerators such
as GPGPU. OpenACC allows users to express the offloading of data and
computations to accelerators to simplify the porting process for legacy
CPU-based applications. 
* XcodeML -
XcodeML is an intermediate code written in XML for C and Fortran
languages. 

http://omni-compiler.org/[+http://omni-compiler.org/+]

OMP2HMPP
~~~~~~~~

OMP2HMPP a tool that, automatically translates a high-level C source
code into HMPP. The generated version rarely will differs from a
hand-coded HMPP version, and will provide an important speedup, near 113%,
that could be later improved by hand-coded xref:CUDA[CUDA].

https://github.com/sdruix/OMP2HMPP[+https://github.com/sdruix/OMP2HMPP+]

http://arxiv.org/abs/1407.6932[+http://arxiv.org/abs/1407.6932+]

http://en.wikipedia.org/wiki/OpenHMPP[+http://en.wikipedia.org/wiki/OpenHMPP+]

OMP2MPI
~~~~~~~

OMP2MPI automatically generates MPI source code from xref:OpenMP[OpenMP]. Allowing that
the program exploits non shared-memory architectures such as cluster, or
Network-on-Chip based(NoC-based) Multiprocessors-System-onChip (MPSoC).
OMP2MPI gives a solution that allow further optimization by an expert that
want to achieve better results. Tested set of problems obtains in most of
cases with more than 20x of speedup for 64 cores compared to the sequential
version and an average speedup over 4x compared to OpenMP.

https://github.com/sdruix/OMP2MPI[+https://github.com/sdruix/OMP2MPI+]

http://arxiv.org/abs/1502.02921[+http://arxiv.org/abs/1502.02921+]

[[OmpSs]]
OmpSs
~~~~~

OmpSs is an effort to integrate features from the StarSs programming model
developed by BSC into a single programming model. In particular, our objective
is to extend xref:OpenMP[OpenMP] with new directives to support asynchronous parallelism
and heterogeneity (devices like GPUs). However, it can also be understood as
new directives extending other accelerator based APIs like xref:CUDA[CUDA]
or xref:OpenCL[OpenCL]. Our
OmpSs environment is built on top of our Mercurium compiler and Nanosxx
runtime system.

Asynchronous parallelism is enabled in OmpSs by the use data-dependencies
between the different tasks of the program. To support heterogeneity a new
construct is introduced: the target construct.

http://pm.bsc.es/ompss[+http://pm.bsc.es/ompss+]

http://www.ecmwf.int/sites/default/files/HPC-WS-Markomanolis.pdf[+http://www.ecmwf.int/sites/default/files/HPC-WS-Markomanolis.pdf+]

DLB
^^^

DLB is a dynamic library designed to speed up hybrid applications by
improving its load balance.
DLB will redistribute the computational resources of the second level of
parallelism to improve the load balance of the outer level of parallelism.

https://pm.bsc.es/projects/dlb[+https://pm.bsc.es/projects/dlb+]

Nanos
^^^^^

A runtime designed to serve as runtime support in parallel environments. It is
mainly used to support  OmpSs, a extension to xref:OpenMP[OpenMP] developed at BSC. It also
has modules to support  OpenMP and  Chapel.
Nanospp provides services to support task parallelism using synchronizations
based on data-dependencies. Data parallelism is also supported by means of
services mapped on top of its task support. Task are implemented as user-level
threads when possible.
It also provides support for maintaining coherence across different address
spaces (such as with GPUs or cluster nodes). It provides software directory
and cache modules to this end.

http://pm.bsc.es/nanox[+http://pm.bsc.es/nanox+]

Mercurium
^^^^^^^^^

Mercurium is a source-to-source compilation infrastructure aimed at fast
prototyping. Current supported languages are C, Cxx. Mercurium is mainly used
in Nanos environment to implement xref:OpenMP[OpenMP] but since it is quite extensible it
has been used to implement other programming models or compiler
transformations, examples include Cell Superscalar, Software Transactional
Memory, Distributed Shared Memory or the ACOTES project, just to name a few.

Extending Mercurium is achieved using a plugin architecture, where plugins
represent several phases of the compiler. These plugins are written in Cxx and
dynamically loaded by the compiler according to the chosen configuration. Code
transformations are implemented in terms of source code (there is no need to
modify or know the internal syntactic representation of the compiler).

http://pm.bsc.es/mcxx[+http://pm.bsc.es/mcxx+]

https://pm.bsc.es/projects/mcxx[+https://pm.bsc.es/projects/mcxx+]

OP2
~~~

An open-source framework for the execution of unstructured grid applications
on clusters of GPUs or multi-core CPUs.
P2 is an API with associated libraries and preprocessors to generate
parallel executables for applications on unstructured grids.

The OP2 project is developing an open-source framework for the execution of
unstructured grid applications on clusters of GPUs or multi-core CPUs.
Although OP2 is designed to look like a conventional library, the
implementation uses source-source translation to generate the appropriate
back-end code for the different target platforms.

http://www.oerc.ox.ac.uk/projects/op2[+http://www.oerc.ox.ac.uk/projects/op2+]

https://github.com/OP2/OP2-Common[+https://github.com/OP2/OP2-Common+]

http://www.oerc.ox.ac.uk/research/op2[+http://www.oerc.ox.ac.uk/research/op2+]

PyOP2
^^^^^

Framework for performance-portable parallel computations on unstructured
meshes.

https://github.com/OP2/PyOP2[+https://github.com/OP2/PyOP2+]


OpenACC
~~~~~~~

The OpenACC Application Program Interface describes a collection of compiler
directives to specify loops and regions of code in standard C, Cxx and Fortran
to be offloaded from a host CPU to an attached accelerator. OpenACC is
designed for portability across operating systems, host CPUs, and a wide range
of accelerators, including APUs, GPUs, and many-core coprocessors.
The directives and programming model defined in the OpenACC API document allow
programmers to create high-level host+accelerator programs without the need to
explicitly initialize the accelerator, manage data or program transfers
between the host and accelerator, or initiate accelerator startup and
shutdown.

All of these details are implicit in the programming model and are managed by
the OpenACC API-enabled compilers and runtimes. The programming model allows
the programmer to augment information available to the compilers, including
specification of data local to an accelerator, guidance on mapping of loops
onto an accelerator, and similar performance-related details.

OpenACC in GCC
^^^^^^^^^^^^^^

This page contains information on GCC's implementation of the OpenACC
specification and related functionality. 

https://gcc.gnu.org/wiki/OpenACC[+https://gcc.gnu.org/wiki/OpenACC+]

KernelGen (LLVM)
^^^^^^^^^^^^^^^^

A prototype of auto-parallelizing Fortran/C compiler for NVIDIA GPUs,
targeting numerical modelling code.

https://hpcforge.org/plugins/mediawiki/wiki/kernelgen/index.php/Main_Page[+https://hpcforge.org/plugins/mediawiki/wiki/kernelgen/index.php/Main_Page+]

http://hpcforge.org/projects/kernelgen/[+http://hpcforge.org/projects/kernelgen/+]

OpenAlea
~~~~~~~~

OpenAlea is an open source project primarily aimed at the plant research
community. It is a distributed collaborative effort to develop Python
libraries and tools that address the needs of current and future works in
Plant Architecture modeling. OpenAlea includes modules to analyse, visualize
and model the functioning and growth of plant architecture. 

http://openalea.gforge.inria.fr/dokuwiki/doku.php[+http://openalea.gforge.inria.fr/dokuwiki/doku.php+]

https://gforge.inria.fr/projects/openalea/[+https://gforge.inria.fr/projects/openalea/+]

OpenARC
~~~~~~~

OpenARC is a new, open source compiler framework, which provides extensible
environment, where various performance optimizations, traceability mechanisms,
fault tolerance techniques, etc., can be built for better
debuggability/performance/resilience on the complex accelerator computing.

http://ft.ornl.gov/research/openarc[+http://ft.ornl.gov/research/openarc+]

http://cetus.ecn.purdue.edu/[+http://cetus.ecn.purdue.edu/+]

http://ft.ornl.gov/research/openarc[+http://ft.ornl.gov/research/openarc+]

Open CASCADE
~~~~~~~~~~~~

Open CASCADE Technology is a software development kit (SDK) intended for
development of applications dealing with 3D CAD data, freely available in open
source. It includes a set of Cxx class libraries providing services for 3D
surface and solid modeling, visualization, data exchange and rapid application
development.

http://www.opencascade.org/[+http://www.opencascade.org/+]

OpenCL
~~~~~~

OpenCL™ is the first open, royalty-free standard for cross-platform, parallel
programming of modern processors found in personal computers, servers and
handheld/embedded devices. OpenCL (Open Computing Language) greatly improves
speed and responsiveness for a wide spectrum of applications in numerous
market categories from gaming and entertainment to scientific and medical
software.

https://www.khronos.org/opencl/[+https://www.khronos.org/opencl/+]

[[EasyOpenCL]]
EasyOpenCL
^^^^^^^^^^

The easiest way to get started with OpenCL.

https://github.com/Gladdy/EasyOpenCL[+https://github.com/Gladdy/EasyOpenCL+]

Intel OpenCL
^^^^^^^^^^^^

Intel® Code Builder for OpenCL™ API is a comprehensive environment for OpenCL
software development on Intel Architecture processors and Intel Xeon Phi™
coprocessors. The Code Builder comprises the Intel implementation of the
OpenCL standard and a set of tools for OpenCL application development on
Linux* operating systems.

https://software.intel.com/en-us/articles/standalone-code-builder-release-notes[+https://software.intel.com/en-us/articles/standalone-code-builder-release-notes+]

https://software.intel.com/en-us/articles/intel-code-builder-for-opencl-api[+https://software.intel.com/en-us/articles/intel-code-builder-for-opencl-api+]

[[opencl4py]]
opencl4py
^^^^^^^^^

Python cffi OpenCL bindings and helper classes.

https://github.com/Samsung/opencl4py[+https://github.com/Samsung/opencl4py+]

pocl
^^^^

Portable Computing Language (pocl) aims to become a MIT-licensed open source
implementation of the OpenCL standard which can be easily adapted for new
targets and devices, both for homogeneous CPU and heterogenous
GPUs/accelerators.

pocl uses Clang as an OpenCL C frontend and LLVM for the kernel compiler
implementation, and as a portability layer. Thus, if your desired target has
an LLVM backend, it should be able to get OpenCL support easily by using pocl.

The goal is to accomplish improved performance portability using a kernel
compiler that can generate multi-work-item work-group functions that exploit
various types of parallel hardware resources: VLIW, superscalar, SIMD, SIMT,
multicore, multithread ...

Additional purpose of the project is to serve as a research platform for
issues in parallel programming on heterogeneous platforms.

http://portablecl.org/[+http://portablecl.org/+]

http://pocl.sourceforge.net/[+http://pocl.sourceforge.net/+]

OpenClimateGIS
~~~~~~~~~~~~~~

OpenClimateGIS (OCGIS) is a Python package designed for geospatial
manipulation, subsetting, computation, and translation of climate datasets
stored in local NetCDF files or files served through xref:THREDDS[THREDDS] data servers.
OpenClimateGIS has a straightforward, request-based API that is simple to use
yet complex enough to perform a variety of computational tasks. The software
is built entirely from open source packages. ClimateTranslator is a new web
interface to the OpenClimateGIS functionality.

https://www.earthsystemcog.org/projects/openclimategis/[+https://www.earthsystemcog.org/projects/openclimategis/+]

OpenCMISS
~~~~~~~~~

OpenCMISS libraries and applications provide the foundation for developing
computational modelling and visualisation software, particularly targeting
bioengineering.

http://physiomeproject.org/software/opencmiss[+http://physiomeproject.org/software/opencmiss+]

OpenCores
~~~~~~~~~

OpenCores is an open source hardware community developing
digital open source hardware through electronic design automation, with a
similar ethos to the free software movement. OpenCores hopes to eliminate
redundant design work and slash development costs.

http://opencores.org/[+http://opencores.org/+]

http://en.wikipedia.org/wiki/OpenCores[+http://en.wikipedia.org/wiki/OpenCores+]

http://en.wikipedia.org/wiki/OpenRISC[+http://en.wikipedia.org/wiki/OpenRISC+]

http://www.embecosm.com/appnotes/ean2/embecosm-or1k-setup-ean2-issue-3.html[+http://www.embecosm.com/appnotes/ean2/embecosm-or1k-setup-ean2-issue-3.html+]

OPeNDAP
~~~~~~~

OPeNDAP stands for "Open-source Project for a Network Data Access Protocol"
OPeNDAP is both the name of a non-profit organization and the commonly-used
name of a protocol which the OPeNDAP organization has developed. The DAP2
protocol provides a discipline-neutral means of requesting and providing data
across the World Wide Web. The goal is to allow end users, whoever they may
be, to access immediately whatever data they require in a form they can use,
all while using applications they already possess and are familiar with. In
the field of oceanography, OPeNDAP has already helped the research community
make significant progress towards this end. Ultimately, it is hoped, OPeNDAP
will be a fundamental component of systems which provide machine-to-machine
interoperability with semantic meaning in a highly distributed environment of
heterogeneous datasets. The OPeNDAP organization exists to develop, implement,
and promulgate the OPeNDAP protocol. It presents the results of its work
freely to the public with the hope that it will be of service in many
disciplines and facilitate sharing of and access to their data streams.

http://www.opendap.org/[+http://www.opendap.org/+]

Pydap
^^^^^

Pydap is a pure Python library implementing the Data Access Protocol, also
known as DODS or OPeNDAP. You can use Pydap as a client to access hundreds of
scientific datasets in a transparent and efficient way through the internet;
or as a server to easily distribute your data from a variety of formats.

Pydap includes several handlers, i.e. special Python modules that convert
between a given data format and the data model used by Pydap (defined in the
pydap.model module). They are necessary in order to Pydap be able to actually
serve a dataset. There are handlers for NetCDF, HDF 4 & 5, Matlab, relational
databases, Grib 1 & 2, CSV, Seabird CTD files, and a few more.

http://www.pydap.org/[+http://www.pydap.org/+]

OpenDSA
~~~~~~~

The goal of the OpenDSA project is to create open-source courseware for use in
Data Structures and Algorithms courses, that deeply integrates
textbook-quality content with algorithm visualizations and interactive,
automatically assessed exercises.

https://github.com/OpenDSA/OpenDSA[+https://github.com/OpenDSA/OpenDSA+]

http://algoviz.org/OpenDSA/[+http://algoviz.org/OpenDSA/+]

http://www.sciencedirect.com/science/article/pii/S016764231300333X[+http://www.sciencedirect.com/science/article/pii/S016764231300333X+]

OpenFL
~~~~~~

OpenFL is a free and open source software framework and platform for the
creation of multi-platform applications and video games. OpenFL programs
are written in a single language (Haxe) and may be published to Flash movies,
or standalone applications for Microsoft Windows, Mac OS X, Linux, iOS,
Android, BlackBerry OS, Firefox OS, HTML5 and Tizen.

OpenFL is designed to mimic Adobe Flash Player, and provides much of the same
functionality and API. SWF files created with Adobe Flash Professional
or other authoring tools may be used in OpenFL programs.

http://www.openfl.org/[+http://www.openfl.org/+]

[[OpenMP]]
OpenMP
~~~~~~

OpenMP (Open Multi-Processing) is an API that supports multi-platform shared memory multiprocessing programming in C, Cxx, and Fortran,[4] on most processor architectures and operating systems.
It consists of a set of compiler directives, library routines, and environment variables that influence run-time behavior.

OpenMP is an implementation of multithreading, a method of parallelizing whereby a master thread (a series of instructions executed consecutively) forks a specified number of slave threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.

http://openmp.org/wp/[+http://openmp.org/wp/+]

https://en.wikipedia.org/wiki/OpenMP[+https://en.wikipedia.org/wiki/OpenMP+]

*OpenMP Tutorial at LLNL* - https://computing.llnl.gov/tutorials/openMP/[+https://computing.llnl.gov/tutorials/openMP/+]

*IWOMP (International Workshop on OpenMP)* - http://www.iwomp.org/[+http://www.iwomp.org/+]

*FORTRAN90 Examples of Parallel Programming with OpenMP* - http://people.sc.fsu.edu/\~jburkardt/f_src/openmp/openmp.html[+http://people.sc.fsu.edu/~jburkardt/f_src/openmp/openmp.html+]

*GCC OpenMP Wiki* - https://gcc.gnu.org/wiki/openmp[+https://gcc.gnu.org/wiki/openmp+]

See also xref:OmpSs[OmpSs].

ForestGOMP
^^^^^^^^^^

ForestGOMP is an xref:OpenMP[OpenMP] runtime compatible with GCC 4.2, offering a structured
way to efficiently execute OpenMP applications onto hierarchical (NUMA)
architectures.

http://runtime.bordeaux.inria.fr/forestgomp/[+http://runtime.bordeaux.inria.fr/forestgomp/+]

OpenNL
~~~~~~

OpenNL (Open Numerical Library) is a library for solving sparse linear
systems, especially designed for the Computer Graphics community. The goal for
OpenNL is to be as small as possible, while offering the subset of
functionalities required by this application field. The Makefiles of OpenNL
can generate a single .c + .h file, very easy to integrate in other projects.
The distribution includes an implementation of our Least Squares Conformal
Maps parameterization method.
It includes support for xref:CUDA[CUDA] and Fermi architecture (Concurrent Number
Cruncher and Nathan Bell's ELL formats.

http://alice.loria.fr/index.php/software/4-library/23-opennl.html[+http://alice.loria.fr/index.php/software/4-library/23-opennl.html+]

OpenSim
~~~~~~~

Description: OpenSim is a freely available, user extensible software system
that lets users develop models of musculoskeletal structures and create
dynamic simulations of movement. 

OpenSim 3.2 includes an improved scripting interface, accessible through the
Graphical User Interface (GUI), Matlab, and now Python. We also added new
visualization capabilities and usability improvements in the OpenSim
application. 

http://opensim.stanford.edu/[+http://opensim.stanford.edu/+]

https://github.com/opensim-org[+https://github.com/opensim-org+]

https://simtk.org/home/opensim[+https://simtk.org/home/opensim+]

OpenStack
~~~~~~~~~

OpenStack is a cloud operating system that controls large pools of compute,
storage, and networking resources throughout a datacenter, all managed through
a dashboard that gives administrators control while empowering their users to
provision resources through a web interface. 

OpenStack is a free and open-source cloud computing software platform.[2]
Users primarily deploy it as an infrastructure as a service (IaaS) solution.
The technology consists of a series of interrelated projects that control
pools of processing, storage, and networking resources throughout a data
center—which users manage through a web-based dashboard, command-line tools,
or a RESTful API. OpenStack.org released it under the terms of the Apache
License.

https://wiki.openstack.org/wiki/Main_Page[+https://wiki.openstack.org/wiki/Main_Page+]

Neutron
^^^^^^^

Neutron is an OpenStack project to provide "networking as a service" between
interface devices (e.g., vNICs) managed by other Openstack services (e.g.,
nova). 

https://wiki.openstack.org/wiki/Neutron[+https://wiki.openstack.org/wiki/Neutron+]

OpenUH
~~~~~~

OpenUH is an open source, optimizing compiler suite for C, Cxx and Fortran,
based on Open64. It supports a variety of architectures including x86-64,
IA-32, IA-64, MIPS, and PTX. 

OpenUH extends the Open64 OpenMP implementation by adding support for nested
parallelism and the tasking features introduced in OpenMP 3.0. The OpenMP
runtime library that comes with OpenUH supports several task scheduling
strategies, enables selection of more scalable barrier algorithms, and
provides an implementation of the OpenMP Collector API for interaction with
performance collection tools (including DARWIN). The OpenMP implementation has
been successfully tested using a number of applications and validated with the
NAS Parallel Benchmarks (NPB) and our OpenMP Validation Suite, developed in
collaboration with the High Performance Computing Center Stuttgart (HLRS) from
the University of Stuttgart. OpenUH also provides support for Fortran
coarrays, an extension that has been adopted in the Fortran 2008 standard.
With the use of coarrays, a programmer can easily write parallel Fortran
programs for a variety of parallel systems. The OpenUH CAF implementation can
work in conjunction with either the GASNet or ARMCI runtime libraries,
open-source projects which are freely downloadable online. 

To achieve portability, OpenUH is able to emit optimized C or Fortran 77 code
that may be compiled by a native compiler on other platforms. The supporting
runtime libraries are also portable - the OpenMP runtime library is based on
the portable Pthreads interface while the Coarray Fortran runtime library is
based on the portable GASNet (or, optionally, ARMCI) communications
interfaces. 

https://github.com/uhhpctools/openuh[+https://github.com/uhhpctools/openuh+]

OpenVMS
~~~~~~~

ftp://ftp.hp.com/pub/openvms/[+ftp://ftp.hp.com/pub/openvms/+]

ftp://ftp.hp.com/pub/openvms/openvmsft/jedi/[+ftp://ftp.hp.com/pub/openvms/openvmsft/jedi/+]

[[OpenZIM]]
OpenZIM
~~~~~~~

The openZIM project proposes offline storage solutions for content coming from
the Web. The project has two different targets:

* Definition of the ZIM file format: an open and standardized file format,
* Implementation of the zimlib: an open source (GPLv2) implementation of the
ZIM file format.

See also xref:Kiwix[Kiwix] and xref:Internet-in-a-Box[Internet-in-a-Box].

http://www.openzim.org/wiki/Main_Page[+http://www.openzim.org/wiki/Main_Page+]

[[operating_systems]]
operating systems
~~~~~~~~~~~~~~~~~

[[Barrelfish]]
Barrelfish
^^^^^^^^^^

A new research operating system being built from scratch.
We are exploring how to structure an OS for future multi- and many-core
systems. We are motivated by two closely related trends in hardware design:
first, the rapidly growing number of cores, which leads to a scalability
challenge, and second, the increasing diversity in computer hardware,
requiring the OS to manage and exploit heterogeneous hardware resources.

Barrelfish is “multikernel” operating system [3]: it consists of a small
kernel running on each core (one
kernel per core), and while rest of the OS is structured as a distributed
system of single-core processes
atop these kernels. Kernels share no memory, even on a machine with
cache-coherent shared RAM,
and the rest of the OS does not use shared memory except for transferring
messages and data between
cores, and booting other cores. Applications can use multiple cores and share
address spaces (and
therefore cache-coherent shared memory) between cores, but this facility is
provided by user-space
runtime libraries.

http://www.barrelfish.org/[+http://www.barrelfish.org/+]

http://wiki.barrelfish.org/[+http://wiki.barrelfish.org/+]

https://github.com/daleooo/barrelfish[+https://github.com/daleooo/barrelfish+]

[[node9]]
node9
^^^^^

A portable hybrid distributed OS based on Inferno, LuaJIT and Libuv.
Node9 is a hosted 64-bit operating system based on Bell Lab's Inferno OS, but
using the Lua scripting language instead of Limbo and the LuaJIT high
peformance virtual machine instead of the DIS virtual machine.  It also uses
the libuv I/O library for maximum portability, efficient event processing and
thread management.

Node9 embraces a highly interactive programming environment optimized for the
needs of distributed computing based on the Plan9/Inferno 9p resource sharing
protocol, per-process namespace security and application message channels.

https://github.com/jvburnes/node9[+https://github.com/jvburnes/node9+]

https://github.com/jvburnes/node9/blob/master/doc/node9-hackers-guide.txt[+https://github.com/jvburnes/node9/blob/master/doc/node9-hackers-guide.txt+]

[[Qubes]]
Qubes
^^^^^

Qubes is a security-oriented, open-source operating system for personal computers. It uses virtualization to implement security by compartmentalization and supports both Linux and Windows virtual environments. Qubes 3.0 introduces the Hypervisor Abstraction Layer (HAL), which renders Qubes independent of its underlying virtualization system.

https://www.qubes-os.org/[+https://www.qubes-os.org/+]

http://northox.github.io/qubes-rumprun/[+http://northox.github.io/qubes-rumprun/+]

[[RancherOS]]
RancherOS
^^^^^^^^^

A 20mb Linux distro that runs the entire OS as Docker containers.

http://rancher.com/rancher-os/[+http://rancher.com/rancher-os/+]

PerspicuOS
^^^^^^^^^^

PerspicuOS is a prototype operating system that realizes the Nested Kernel, a
new operating architecture that restricts access to a device's memory
management unit so that it can then perform memory isolation within the
kernel. The key challenge that PerspicuOS addresses is how to virtualize the
MMU on real hardware, AMD64, in a real operating system, FreeBSD 9.0, while
not assuming any hardware privilege separation or kernel integrity properties
such as control flow integrity.

http://nestedkernel.org/PerspicuOS/[+http://nestedkernel.org/PerspicuOS/+]

Inferno
^^^^^^^

A distributed operating system, originally developed at Bell Labs, but now
developed and maintained by Vita Nuova® as Free Software. Applications written
in Inferno's concurrent programming language, Limbo, are compiled to its
portable virtual machine code (Dis), to run anywhere on a network in the
portable environment that Inferno provides. Unusually, that environment looks
and acts like a complete operating system.

The use of a high-level language and virtual machine is sensible but mundane.
The interesting thing is the system's representation of services and
resources. They are represented in a file-like name hiearchy. Programs access
them using only the file operations open, read/write, and close. The 'files'
may of course represent stored data, but may also be devices, network and
protocol interfaces, dynamic data sources, and services. The approach unifies
and provides basic naming, structuring, and access control mechanisms for all
system resources. A single file-service protocol (the same as Plan 9's 9P)
makes all those resources available for import or export throughout the
network in a uniform way, independent of location. An application simply
attaches the resources it needs to its own per-process name hierarchy ('name
space').

The system can be used to build portable client and server applications. It
makes it straightforward to build lean applications that share all manner of
resources over a network, without the cruft of much of the 'Grid' software one
sees.

http://code.google.com/p/inferno-os/[+http://code.google.com/p/inferno-os/+]

http://www.vitanuova.com/[+http://www.vitanuova.com/+]

Sortix
^^^^^^

Sortix is a small self-hosting Unix-like operating system developed since 2011
aiming to be a clean and modern POSIX implementation. There's a lot of
technical debt that needs to be paid, but it's getting better. Traditional
design mistakes are avoided or aggressively deprecated by updating the base
system and ports as needed. The Sortix kernel, standard libraries, and most
utilities were written entirely from scratch. The system is halfway through
becoming multi-user and while security vulnerabilities are recognized as bugs,
it should be considered insecure at this time.

https://sortix.org/[+https://sortix.org/+]

TempleOS
^^^^^^^^

TempleOS is somewhat of a legend in the operating system community. Its sole author, Terry A. Davis, has spent the past 12 years attempting to create a new operating from scratch.

http://www.templeos.org/[+http://www.templeos.org/+]

http://www.codersnotes.com/notes/a-constructive-look-at-templeos[+http://www.codersnotes.com/notes/a-constructive-look-at-templeos+]

OrangeFS
~~~~~~~~

Orange File System is a branch of the Parallel Virtual File System. Like PVFS,
Orange is a parallel file system designed for use on high end computing (HEC)
systems that provides very high performance access to disk storage for
parallel applications. OrangeFS is different from PVFS in that we have
developed features for OrangeFS that are not presently available in the PVFS
main distribution. While PVFS development tends to focus on specific very
large systems, Orange considers a number of areas that have not been well
supported by PVFS in the past.

OrangeFS is presently integrated with ROMIO through MPICH2, and includes
FUSE support.  It has also been integrated with pNFS.

http://orangefs.org/[+http://orangefs.org/+]

Orcc
~~~~

Orcc is an open-source Integrated Development Environment based on Eclipse and
dedicated to dataflow programming. The primary purpose of Orcc is to provide
developers with a compiler infrastructure to allow software/hardware code to
be generated from dataflow descriptions. Orcc does not generate assembly or
executable code directly, rather it generates source code that must be
compiled by another tool.

Orcc also brings a complete Java-based simulator which allows developers to
quickly test their applications without taking in consideration low-level
details relative to the target platform. The simulator can be launched
directly from eclipse to execute any RVC-CAL application. Indeed, the
simulator simply interprets our intermediate representation of networks and
actors, but it is however able to perform all basic interactions required to
perform a functional validation, such as displaying text, images or videos to
the screen.

http://orcc.sourceforge.net/[+http://orcc.sourceforge.net/+]

https://github.com/orcc/orcc[+https://github.com/orcc/orcc+]

orc-apps
^^^^^^^^

Orc-apps is a library of open-source applications described in a dynamic
dataflow programming way, using the RVC-CAL and FNL languages. The
applications are fully compliant with the Orcc toolset.

https://github.com/orcc/orc-apps[+https://github.com/orcc/orc-apps+]

ORCM
~~~~

ORCM was originally developed as an open-source project (under the Open MPI
license) by Cisco Systems, Inc to provide a resilient, 100% uptime run-time
environment for enterprise-class routers. Based on the Open Run-Time
Environment (ORTE) embedded in Open MPI, the system provided launch and
execution support for processes executing within the router itself (e.g.,
computing routing tables), ensuring that a minimum number of copies of each
program were always present. Failed processes were relocated based on the
concept of fault groups - i.e., the grouping of nodes with common failure
modes. Thus, ORCM attempted to avoid cascade failures by ensuring that
processes were not relocated onto nodes with a high probability of failing in
the immediate future.

The Cisco implementation naturally required a significant amount of
monitoring, and included the notion of fault prediction as a means of taking
pre-emptive action to relocate processes prior to their node failing. This was
facilitated using an analytics framework that allowed users to chain various
analysis modules in the data pipeline so as to perform in-flight data
reduction.

Subsequently, ORCM was extended by Greenplum to serve as a scalable monitoring
system for Hadoop clusters. While ORCM itself had run on quite a few "nodes"
in the Cisco router, and its base ORTE platform has been used for years on
very large clusters involving many thousands of nodes, this was the first time
the ORCM/ORTE platform had been used solely as a system state-of-health
monitor with no responsibility for process launch or monitoring. Instead, ORCM
was asked to provide a resilient, scalable monitoring capability that tracked
process resource utilization and node state-of-health, collecting all the data
in a database for subsequent analysis. Sampling rates were low enough that
in-flight data reduction was not required, nor was fault prediction considered
to be of value in the Hadoop paradigm.

http://www.open-mpi.org/projects/orcm/[+http://www.open-mpi.org/projects/orcm/+]

Ori
~~~

Ori is a distributed file system built for offline operation and empowers the
user with control over synchronization operations and conflict resolution. We
provide history through light weight snapshots and allow users to verify the
history has not been tampered with. Through the use of replication instances
can be resilient and recover damaged data from other nodes.

http://ori.scs.stanford.edu/[+http://ori.scs.stanford.edu/+]

Orio
~~~~

An open-source extensible framework for the definition of domain-specific
languages and generation of optimized (C, Fortran, CUDA, OpenCL) code for
multiple architecture targets (e.g., CPUs, NVIDIA and AMD GPUs, Intel Phi),
including support for empirical autotuning of the generated code.

Orio is a Python framework for transformation and automatically tuning the
performance of codes written in different source and target languages,
including transformations from a number of simple languages (e.g., a
restricted subset of C) to C, Fortran, CUDA, and OpenCL targets. The tool
generates many tuned versions of the same operation using different
optimization parameters, and performs an empirical search for selecting the
best among multiple optimized code variants.

http://brnorris03.github.io/Orio/[+http://brnorris03.github.io/Orio/+]

https://github.com/brnorris03/Orio[+https://github.com/brnorris03/Orio+]

ORSA
~~~~

ORSA is an interactive tool for scientific grade Celestial Mechanics
computations. Asteroids, comets, artificial satellites, Solar and extra-Solar
planetary systems can be accurately reproduced, simulated, and analyzed. 
One of the main goals is to create a common infrastructure among the existing
celestial mechanics programs and standards.
The features include:

* accurate numerical algorithms 
* use of JPL ephemeris files for accurate planets positions 
* Qt-based graphical user interface 
* advanced 2D plotting tool and 3D OpenGL viewer 
* import asteroids and comets from all the known databases (MPC, JPL, Lowell,
AstDyS, and NEODyS) 
* integrated download tool to update databases 
* stand alone numerical library liborsa

http://orsa.sourceforge.net/[+http://orsa.sourceforge.net/+]

RendezvousWithVesta
^^^^^^^^^^^^^^^^^^^

RendezvousWithVesta is a graphical software tool developed by Pasquale
Tricarico at the Planetary Science Institute, in support to the NASA DAWN
mission. It allows to accurately simulate the dynamics of a spacecraft
orbiting the asteroid (4) Vesta.  The motivations for developing this
tool are (1) understand how the physical parameters of Vesta affect the
stability of low polar orbits; (2) understand how the physical parameters of
Vesta and the orbital elements of DAWN affect the coverage of Vesta's surface;
and (3) provide a fast and reliable tool for the generation of orbits suitable
for input in the Science Opportunity Analyzer (SOA) tool.

The features include:

* validated numerical algorithms, tested on NEAR mission data, and capable of
accurately reproducing NEAR's orbit around Eros;
* complete control over Vesta's physical properties: mass, mass distribution
model, shape model, rotation period, and pole ecliptic latitude and
longitude;
* control over DAWN's initial orbit around Vesta: epoch, radius, equatorial
(Vesta's equator) inclination, phase angle;
* export simulations as SPICE kernel files and as ASCII data files;
* 3D graphical visualization of the numerical simulation, including the ground
tracking of DAWN over Vesta's surface;
* 2D plot of the altitude of the spacecraft and of the Vesta profile at nadir;
and;
* completely open source and part of the ORSA framework.

http://orsa.sourceforge.net/RendezvousWithVesta/[+http://orsa.sourceforge.net/RendezvousWithVesta/+]

SurfaceCoverage
^^^^^^^^^^^^^^^^

SurfaceCoverage is a graphical software tool developed by Pasquale Tricarico
at the Planetary Science Institute, in support to the NASA Dawn mission. It
allows to estimate the coverage of the surface of asteroid (4) Vesta by the
Dawn spacecraft in a wide range of configurations.

http://orsa.sourceforge.net/SurfaceCoverage/[+http://orsa.sourceforge.net/SurfaceCoverage/+]

[[OSKI]]
OSKI
~~~~

The Optimized Sparse Kernel Interface (OSKI) Library is a collection of low-level C primitives that provide automatically tuned computational kernels on sparse matrices, for use in solver libraries and applications. OSKI has a xref:BLAS[BLAS]-style interface, providing basic kernels like sparse matrix-vector multiply and sparse triangular solve, among others.
The current implementation targets cache-based superscalar uniprocessor machines, though we are developing extensions for vector architectures, SMPs, and large-scale distributed memory machines. 

http://bebop.cs.berkeley.edu/oski/about.html[+http://bebop.cs.berkeley.edu/oski/about.html+]

OSTree
~~~~~~

OSTree is a tool for managing bootable, immutable, versioned filesystem trees.
It is not a package system; nor is it a tool for managing full disk images.
Instead, it sits between those levels, offering a blend of the advantages (and
disadvantages) of both. 

You can use any build system you like to place content into it on a build
server, then export an OSTree repository via static HTTP. On each client
system, "ostree admin upgrade" can incrementally replicate that content,
creating a new root for the next reboot. This provides fully atomic upgrades.
Any changes made to /etc are propagated forwards, and all local state in /var
is shared.

A key goal of the project is to complement existing package systems like RPM
and Debian packages, and help further their evolution. In particular for
example, RPM-OSTree (linked below) has as a goal a hybrid tree/package model,
where you replicate a base tree via OSTree, and then add packages on top. 

https://wiki.gnome.org/Projects/OSTree[+https://wiki.gnome.org/Projects/OSTree+]

https://lwn.net/Articles/581811/[+https://lwn.net/Articles/581811/+]

Overview
~~~~~~~~

An open source document mining platform.
Read and analyze thousands of documents super quickly. Full text search, topic
modeling, coding and tagging, visualizations and more. All in an easy-to use,
visual workflow.

https://github.com/overview[+https://github.com/overview+]

https://github.com/overview/overview-server/wiki/Installing-and-Running-Overview[+https://github.com/overview/overview-server/wiki/Installing-and-Running-Overview+]

https://www.overviewproject.org/[+https://www.overviewproject.org/+]

ownCloud
~~~~~~~~

ownCloud provides access to your data through a web interface or WebDAV while
providing a platform to view, sync and share across devices easily—all under
your control. ownCloud’s open architecture is extensible via a simple but
powerful API for applications and plugins and works with any storage. 

https://owncloud.org/[+https://owncloud.org/+]

[[NP]]
////
NPPP
////

PaCaL
~~~~~

A Python package for arithmetical computations on random variables. The
package is capable of performing the four arithmetic operations: addition,
subtraction, multiplication and division, as well as computing many standard
functions of random variables. Summary statistics, random number generation,
plots, and histograms of the resulting distributions can easily be obtained
and distribution parameter ﬁtting is also available. The operations are
performed numerically and their results interpolated allowing for arbitrary
arithmetic operations on random variables following practically any
probability distribution encountered in practice. The package is easy to use,
as operations on random variables are performed just as they are on standard
Python variables. Independence of random variables is, by default, assumed on
each step but some computations on dependent random variables are also
possible. We demonstrate on several examples that the results are very
accurate, often close to machine precision. Practical applications include
statistics, physical measurements or estimation of error distributions in
scientiﬁc computations.

http://www.jstatsoft.org/v57/i10[+http://www.jstatsoft.org/v57/i10+]

http://pacal.sourceforge.net/[+http://pacal.sourceforge.net/+]

PadicoTM
~~~~~~~~

PadicoTM is the runtime infrastructure for the Padico software environment for
computational grids.
It is composed of a core which provides a high-performance framework for
networking and multi-threading, and services plugged into the core.
High-performance communications and threads are obtained thanks to Marcel and
Madeleine, provided by the PM2 software suite. The PadicoTM core aims at
making the different services running at the same time run in a cooperative
way rather than competitive. 

PadicoTM exhibits standard interface (VIO: virtual sockets; Circuit:
Madeleine-like API; etc.) usable by various middleware systems. Thanks to
symbol interception by PadicoTM, middleware is unmodified and utilizes
PadicoTM communication methods seamlessly. The middleware systems available
over PadicoTM are:

* CORBA implementations: omniORB and Mico
* the MPI implementations xref:NewMadeleine[NewMadeleine] and xref:GridMPI[GridMPI]
* a Java Virtual Machine based on Kaffe
* the gSOAP SOAP/Web services development toolkit
* an implementation of the JXTA P2P specifications called 
https://java.net/projects/jxta-c[JXTA-C]

[[Paegan]]
Paegan
~~~~~~

A Python Common Data Model (CDM) for met/ocean data.
Paegan attempts to fill the need for a high level common data model (CDM) library for array based met/ocean data stored in netCDF files or distributed over OPeNDAP.

https://github.com/asascience-open/paegan[+https://github.com/asascience-open/paegan+]

Pandas
~~~~~~

An open source, BSD-licensed library providing high-performance, easy-to-use
data structures and data analysis tools for the Python programming language.
Pandas is a Python package providing fast, flexible, and expressive data
structures designed to make working with “relational” or “labeled” data both
easy and intuitive. It aims to be the fundamental high-level building block
for doing practical, real world data analysis in Python. Additionally, it has
the broader goal of becoming the most powerful and flexible open source data
analysis / manipulation tool available in any language.

The two primary data structures of pandas, Series (1-dimensional) and
DataFrame (2-dimensional), handle the vast majority of typical use cases in
finance, statistics, social science, and many areas of engineering. For R
users, DataFrame provides everything that R’s data.frame provides and much
more. pandas is built on top of NumPy and is intended to integrate well within
a scientific computing environment with many other 3rd party libraries.

http://pandas.pydata.org/[+http://pandas.pydata.org/+]

GeoCoon
^^^^^^^

GeoCoon is GIS data analysis Python library, which integrates Pandas data
frames with Shapely GIS geometries.
The library provides means to load GIS data in a form of Shapely objects into
Pandas data frame and analyze data using Pandas idioms. It allows to access
attributes and call methods on Shapely geometries in a vectorized manner.

GeoCoon supports:

* Point, line string and polygon geometries.
* Vectorized GIS object attribute access and method execution
* Pandas data selection and split-apply-combine idioms.
* SQL/MM databases, i.e. PostgreSQL with PostGIS extension
* Multiple geometry columns in a data frame

http://wrobell.it-zone.org/geocoon/[+http://wrobell.it-zone.org/geocoon/+]

[[pandashells]]
pandashells
^^^^^^^^^^^

A set of command-line tools for working with tabular data.
Pandashells is an attempt to marry the expressive, concise workflow of the shell pipeline with the statistical and visualization tools of the python data-stack.

https://github.com/robdmc/pandashells[+https://github.com/robdmc/pandashells+]

PaPy
~~~~

PaPy, which stands for parallel pipelines in Python, is a highly flexible
framework that enables the construction of robust, scalable workflows for
either generating or processing voluminous datasets. A workflow is created
from user-written Python functions (nodes) connected by 'pipes' (edges) into a
directed acyclic graph. These functions are arbitrarily definable, and can
make use of any Python modules or external binaries. Given a user-defined
http://www.peervpn.net/[+http://www.peervpn.net/+]

[[ParaView]]
ParaView
~~~~~~~~

An open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView’s batch processing capabilities.

The ParaView code base is designed in such a way that all of its components can be reused to quickly develop vertical applications. This flexibility allows ParaView developers to quickly develop applications that have specific functionality for a specific problem domain. ParaView runs on distributed and shared memory parallel and single processor systems.Under the hood, ParaView uses the Visualization Toolkit (VTK) as the data processing and rendering engine and has a user interface written using Qt.

http://www.paraview.org/[+http://www.paraview.org/+]

https://github.com/Kitware/ParaView[+https://github.com/Kitware/ParaView+]

*ParaView Wiki* - http://www.paraview.org/Wiki/ParaView[+http://www.paraview.org/Wiki/ParaView+]

*Using ParaView to Visualize Scientific Data* (online tutorial) - http://www.bu.edu/tech/support/research/training-consulting/online-tutorials/paraview/[+http://www.bu.edu/tech/support/research/training-consulting/online-tutorials/paraview/+]

[[pv_atmos]]
pv_atmos
^^^^^^^^

Python scripting for scientific visualization software ParaView applied to
atmospheric xref:NetCDF[NetCDF] data.
Historically, pv_atmos has been developed to work with geophysical, and in particular, atmospheric model data (hence the name). However, pv_atmos has evolved into a very general package, and contains routines for visualizing netCDF data, and the capability to show arbitrary axes and labels in a large variety of geometries (linear and logarithmic axes, spherical geometry).

https://github.com/mjucker/pv_atmos[+https://github.com/mjucker/pv_atmos+]

*Scientific Visualisation of Atmospheric Data with ParaView* (online paper) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.al/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.al/+]

[[PARDISO]]
PARDISO
~~~~~~~

A  thread-safe, high-performance, robust, memory efficient and easy to use software for solving large sparse symmetric and unsymmetric linear systems of equations on shared-memory and distributed-memory multiprocessors.  

Features of the library version: Unsymmetric, structurally symmetric or symmetric systems, real or complex, positive definite or indefinite, hermitian. LU with complete pivoting. Parallel on SMPs and Cluster of SMPs. Automatic combination of iterative and direct solver algorithms to accelerate the solution process for very large three-dimensional systems. 

http://www.pardiso-project.org/[+http://www.pardiso-project.org/+]

PDF
~~~

Manipulating, storing and transmogrifying PDF files.

i-librarian
^^^^^^^^^^^

A collaborative PDF manager, which enables researchers, scholars, or students
to create an annotated collection of PDF articles.

https://github.com/mkucej/i-librarian[+https://github.com/mkucej/i-librarian+]

http://i-librarian.net/[+http://i-librarian.net/+]

[[PeachPy]]
PeachPy
~~~~~~~

A Python framework for writing high-performance assembly kernels.
PeachPy aims to simplify writing optimized assembly kernels while preserving all optimization opportunities of traditional assembly.

https://github.com/Maratyszcza/PeachPy[+https://github.com/Maratyszcza/PeachPy+]

PEGASUS
~~~~~~~

PEGASUS is a Peta-scale graph mining system, fully written in Java. It runs in
parallel, distributed manner on top of Hadoop. Hadoop is a cloud computing
platfrom, as well as an open source implementation of MapReduce framework
which was originally designed for web-scale data processing by Google.

Existing works on graph mining has limited scalability: usually, the maximum
graph size is order of millions. PEGASUS breaks the limit by scaling up the
algorithms to billion-scale graphs. The breakthrough was possible by the
careful algorithm design and implementation for Hadoop, a massive cloud
computing platform.

http://www.cs.cmu.edu/\~pegasus/[+http://www.cs.cmu.edu/~pegasus/+]

Pelican
~~~~~~~

Pelican is a static site generator, written in Python.
The features include:

* Write your content in reStructuredText, Markdown, or xref:ASCIIDOC[AsciiDoc] formats
* Completely static output is easy to host anywhere
* Themes that can be customized via Jinja templates
* Publish content in multiple languages
* Atom/RSS feeds
* Code syntax highlighting
* Import from WordPress, Dotclear, RSS feeds, and other services
* Modular plugin system and corresponding plugin repository

http://blog.getpelican.com/[+http://blog.getpelican.com/+]

https://github.com/getpelican/pelican[+https://github.com/getpelican/pelican+]

Plugin Repository -
https://github.com/getpelican/pelican-plugins[+https://github.com/getpelican/pelican-plugins+]

External Plugins -
https://github.com/getpelican/pelican/wiki/Externally-hosted-plugins-and-tools[+https://github.com/getpelican/pelican/wiki/Externally-hosted-plugins-and-tools+]

perl
~~~~

The pathologically eclectic rubbish lister ain't dead yet.

Lingua::Romana::Perligata
^^^^^^^^^^^^^^^^^^^^^^^^^

A Perl module that makes it possible to write Perl programs in Latin.

http://www.csse.monash.edu.au/\~damian/papers/HTML/Perligata.html[+http://www.csse.monash.edu.au/~damian/papers/HTML/Perligata.html

PetIGA
~~~~~~

This software framework implements a NURBS-based Galerkin finite element
method (FEM), popularly known as isogeometric analysis (IGA). It is heavily
based on PETSc, the Portable, Extensible Toolkit for Scientific Computation.
PETSc is a collection of algorithms and data structures for the solution of
scientific problems, particularly those modeled by partial differential
equations (PDEs). PETSc is written to be applicable to a range of problem
sizes, including large-scale simulations where high performance parallel is a
must. PetIGA can be thought of as an extension of PETSc, which adds the NURBS
discretization capability and the integration of forms. The PetIGA framework
is intended for researchers in the numeric solution of PDEs who have
applications which require extensive computational resources.

https://bitbucket.org/dalcinl/petiga/[+https://bitbucket.org/dalcinl/petiga/+]

http://arxiv.org/abs/1305.4452[+http://arxiv.org/abs/1305.4452+]

petools
~~~~~~~

A toolkit for development of scientific applications related to processing
observational data.
It includes:

* Fast linear algebra routines, including one of the fastest subroutine for
inversion of a square symmetric positively determined matrix in the upper
triangular representation.
* Graphic library DiaGI (Dialog Graphic Interface) which makes a plot of
one-dimensional function(s) from one call and allows a user to adjust
parameters of the plot interactively.
* Routine MatView which displays a portion of a big matrix on the screen and
allows a user to change the boundaries of the displayed area interactively.
* A set of routines for manipulation with splines; various routines for
multi-dimensional B-spline transform, etc. 
* Various routines for least squares, regression computation, error handler,
interface to a low level I/O, date transformation etc. 

See xref:fourpack[fourpack].

http://astrogeo.org/petools/[+http://astrogeo.org/petools/+]

Petuum
~~~~~~

Petuum is a distributed machine learning framework. It aims to provide a
generic algorithmic and systems interface to large scale machine learning, and
takes care of difficult systems "plumbing work" and algorithmic acceleration,
while simplifying the distributed implementation of ML programs - allowing you
to focus on model perfection and Big Data Analytics. Petuum runs efficiently
at scale on research clusters and cloud compute like Amazon EC2 and Google
GCE. 

http://petuum.github.io/[+http://petuum.github.io/+]

http://arxiv.org/abs/1412.1576[+http://arxiv.org/abs/1412.1576+]

phcpy
^^^^^

This documentation describes a collection of Python modules to compute
solutions of polynomial systems using PHCpack. 

http://arxiv.org/abs/1310.0056[+http://arxiv.org/abs/1310.0056+]

http://homepages.math.uic.edu/\~jan/phcpy_doc_html/index.html[+http://homepages.math.uic.edu/~jan/phcpy_doc_html/index.html+]

PHG
~~~

A toolbox for developing parallel adaptive finite element programs.
PHG deals with conforming tetrahedral meshes and uses bisection for adaptive
local mesh refinement and MPI for message passing. PHG has an object oriented
design which hides parallelization details and provides common operations on
meshes and finite element functions in an abstract way, allowing the users to
concentrate on their numerical algorithms.

PHG has a set of rich and easy to use interfaces to other packages,
including ParMETIS, PETSc, Hypre, xref:SuperLU[SuperLU], xref:MUMPS[MUMPS], xref:Trilinos[Trilinos], PARPACK, JDBSYM,
LOBPCG.

http://lsec.cc.ac.cn/phg/index_en.htm[+http://lsec.cc.ac.cn/phg/index_en.htm+]

[[PHIST]]
PHIST
~~~~~

The Pipelined Hybrid-parallel Iterative Solver Toolkit (PHIST) is what its name
implies.
It contains:

* an abstract kernel interface layer defining the basic operations commonly used in iterative linear algebra solvers;
* implementations of the interface using a built-in sample implementation using MPI and
OpenMP, xref:GHOST[GHOST] and xref:Trilinos[Trilinos]; and
* various algorithms implemented using the interface layer.

https://bitbucket.org/essex/phist[+https://bitbucket.org/essex/phist+]

Pizco
~~~~~

Pizco is Python module/package that allows python objects to communicate via
ZMQ. Objects can be exposed to other process in the same computer or over the
network, allowing clear separation of concerns, resources and permissions.

As ZMQ is used as the transport layer, communication is fast and efficient,
and different protocols are supported. It has a complete test coverage. It
runs in Python 3.2+ and requires PyZMQ. It is licensed under BSD.

https://pizco.readthedocs.org/en/latest/[+https://pizco.readthedocs.org/en/latest/+]

https://github.com/hgrecco/pizco[+https://github.com/hgrecco/pizco+]

[[Platform_MPI]]
Platform MPI
~~~~~~~~~~~~

IBM® Platform MPI Community Edition is a no-charge community edition of IBM
Platform MPI supporting the core MPI features. It is available for download,
deployment, and redistribution at no charge. This edition is simple, flexible,
powerful, and reliable; easy to install, embed, deploy; embodies core
capabilities of Platform MPI for Linux® and Windows®; and provides an optional
low cost offering that includes higher rank counts, 24/7 IBM customer support,
fix packs, and upgrade protection.

http://www.ibm.com/developerworks/downloads/im/mpi/index.html[+http://www.ibm.com/developerworks/downloads/im/mpi/index.html+]

PLUTO
~~~~~

PLUTO is an automatic parallelization tool based on the polyhedral model. The
polyhedral model for compiler optimization provides an abstraction to perform
high-level transformations such as loop-nest optimization and parallelization
on affine loop nests. Pluto transforms C programs from source to source for
coarse-grained parallelism and data locality simultaneously. The core
transformation framework mainly works by finding affine transformations for
efficient tiling and fusion, but not limited to those.
OpenMP parallel code for multicores can be automatically generated from
sequential C program sections. Outer, inner, or pipelined parallelization is
achieved (purely with OpenMP pragrams), besides register tiling and making
code amenable to auto-vectorization.

Though the tool is fully automatic (C to OpenMP C), a number of options are
provided (both command-line and through meta files) to tune aspects like tile
sizes, unroll factors, and outer loop fusion structure. Cloog-ISL is used for
code generation.

http://pluto-compiler.sourceforge.net/[+http://pluto-compiler.sourceforge.net/+]

http://www.ece.lsu.edu/jxr/pluto/index.html[+http://www.ece.lsu.edu/jxr/pluto/index.html+]

plyr
~~~~

A simple, accessible HTML5 media player. 

https://github.com/selz/plyr[+https://github.com/selz/plyr+]

PM2
~~~

A low-level generic runtime system which integrates multithreading
management and a high performance multi-cluster communication library.
PM2 is an umbrella software suite for high-performance runtime systems.
Modules may be installed and used together or separately.
The modules are:

* xref:NewMadeleine[NewMadeleine] - a high performance communication
library for clusters
* xref:PIOMan[PIOMan] - a generic I/O manager designed to deal with
interactions between communication and multithreading
* xref:PadicoTM[PadicoTM] - a component-based high performance communication
framework for grid computing that enables a wide variety of middleware
systems, e.g. MPI, CORBA, Java RMI, ICE, SOAP, etc.
* xref:Marcel[Marcel] - a thread library developed to meet the needs
of the PM2 multithreaded environment

http://gforge.inria.fr/projects/pm2/[+http://gforge.inria.fr/projects/pm2/+]

PMIx
~~~~

The Process Management Interface (PMI) has been used for quite some time as a
means of exchanging wireup information needed for interprocess communication.
Two versions (PMI-1 and PMI-2) have been released as part of the MPICH effort.
While PMI-2 demonstrates better scaling properties than its PMI-1 predecessor,
attaining rapid launch and wireup of the roughly 1M processes executing across
100k nodes expected for exascale operations remains challenging.

PMI Exascale (PMIx) represents an attempt to resolve these questions by
providing an extended version of the PMI standard specifically designed to
support clusters up to and including exascale sizes. The overall objective of
the project is not to branch the existing pseudo-standard definitions - in
fact, PMIx fully supports both of the existing PMI-1 and PMI-2 APIs - but
rather to (a) augment and extend those APIs to eliminate some current
restrictions that impact scalability, and (b) provide a reference
implementation of the PMI-server that demonstrates the desired level of
scalability. 

http://www.open-mpi.org/projects/pmix/[+http://www.open-mpi.org/projects/pmix/+]

Pochoir
~~~~~~~

Pochoir (pronounced "PO-shwar") is a compiler and runtime system for
implementing stencil computations on multicore processors. A stencil defines
the value of a grid point in a d-dimensional spatial grid at time t as a
function of neighboring grid points at recent times before t. A stencil
computation computes the stencil for each grid point over many time steps.
Using Pochoir, a user specifies a computing kernel and boundary conditions
using a simple stencil language embedded in Cxx. The Pochoir compiler produces
cache-efficient multithreaded Cxx code that can be compiled with the Intel
12.0 compiler for Cxx with the Cilk multithreading extensions, which is
available as part of the Intel Parallel Computer suite. The Pochoir package
contains two main components: a Cxx template library for debugging and testing
Pochoir compliance and a domain-specific compiler written in Haskell that
produces highly optimized code.

http://groups.csail.mit.edu/sct/wiki/index.php?title=The_Pochoir_Project[+http://groups.csail.mit.edu/sct/wiki/index.php?title=The_Pochoir_Project+]

https://github.com/Pochoir/Pochoir[+https://github.com/Pochoir/Pochoir+]

Poincare
~~~~~~~~

The Poincaré code is a Maple project package that aims to gather significant
computer algebra normal form (and subsequent reduction) methods for handling
nonlinear ordinary differential equations. As a first version, a set of
fourteen easy-to-use Maple  commands is introduced for symbolic creation of
(improved variants of Poincaré’s) normal forms as well as their associated
normalizing transformations. The software is the implementation by the authors
of carefully studied and followed up selected normal form procedures from the
literature, including some authors’ contributions to the subject. As can be
seen, joint-normal-form programs involving Lie-point symmetries are of special
interest and are published in CPC Program Library for the first time,
Hamiltonian variants being also very useful as they lead to encouraging
results when applied, for example, to models from computational physics like
Hénon–Heiles.

http://www.sciencedirect.com/science/article/pii/S0010465513001380[+http://www.sciencedirect.com/science/article/pii/S0010465513001380+]

Polly
~~~~~

Polly is a high-level loop and data-locality optimizer and optimization
infrastructure for LLVM. It uses an abstract mathematical representation based
on integer polyhedra to analyze and optimize the memory access pattern of a
program. We currently perform classical loop transformations, especially
tiling and loop fusion to improve data-locality. Polly can also exploit OpenMP
level parallelism, expose SIMDization opportunities. Work has also be done in
the area of automatic GPU code generation.

http://polly.llvm.org/[+http://polly.llvm.org/+]

polymake
~~~~~~~~

A tool to study the combinatorics and the geometry of convex polytopes and
polyhedra. It is also capable of dealing with simplicial complexes, matroids,
polyhedral fans, graphs, tropical objects, and other objects.

http://www.polymake.org/doku.php[+http://www.polymake.org/doku.php+]

https://github.com/polymake[+https://github.com/polymake+]

http://arxiv.org/abs/1408.4653[+http://arxiv.org/abs/1408.4653+]

http://arxiv.org/abs/math/0507273[+http://arxiv.org/abs/math/0507273+]

Pomegranate
~~~~~~~~~~~

Pomegranate is an open source Python application that implements the open
Webification (w10n) Science API for major scientific data stores (HDF, NetCDF,
etc.). It makes file inner components, such attributes and data arrays,
directly addressable and accessible via well-defined and meaningful URLs.

Data exposed by w10n-sci API is readily consumable by any HTTP client. It can
be as simple as a command line like curl or wget, or as advanced as a
full-fledged HTML5 web application such as REX.

Pomegranate has been included in Taiga, a turnkey software tool that
simplifies the use of scientific data.

It can be installed as a command line tool and/or a ReSTful web service. 

Source code is available at Open Channel Software. However, please note that
Pomegranate alone won't be enough to establish a w10n-sci service. What you
really need is this instruction service-setup.txt, that details the steps
necessary to build, install and configure for a complete service. Or rather
use a turnkey solution like Taiga, so that you can be up and running in
minutes. 

http://pomegranate.nasa.gov/[+http://pomegranate.nasa.gov/+]

PostgreSQL
~~~~~~~~~~

PostgREST
^^^^^^^^^

Serves a fully RESTful API from any existing PostgreSQL database. It provides a cleaner, more standards-compliant, faster API than you are likely to write from scratch.

https://github.com/begriffs/postgrest[+https://github.com/begriffs/postgrest+]

ppohDEM
~~~~~~~

We investigate performance improvements for the discrete element method (DEM)
used in ppohDEM. First, we use OpenMP and MPI to parallelize DEM for efficient
operation on many types of memory, including shared memory, and at any scale,
from small PC clusters to supercomputers. We also describe a new algorithm for
the descending storage method (DSM) based on a sort technique that makes
creation of contact candidate pair lists more efficient. Finally, we measure
the performance of ppohDEM using the proposed improvements, and confirm that
computational time is significantly reduced. We also show that the parallel
performance of ppohDEM can be improved by reducing the number of OpenMP
threads per MPI process.

http://www.sciencedirect.com/science/article/pii/S0010465514000472[+http://www.sciencedirect.com/science/article/pii/S0010465514000472+]

precimonious
~~~~~~~~~~~~

Precimonious employs a dynamic program analysis technique to find a lower
floating-point precision that can be used in any part of a program.
Precimonious performs a search on the program variables trying to lower their
precision subject to accuracy constraints and performance goals. The tool then
recommends a type instantiation for these variables using less precision while
producing an accurate enough answer without causing exceptions.

https://github.com/corvette-berkeley/precimonious[+https://github.com/corvette-berkeley/precimonious+]

http://www.davidhbailey.com/dhbpapers/precimonious.pdf[+http://www.davidhbailey.com/dhbpapers/precimonious.pdf+]

PREESM
~~~~~~

PREESM is an open source rapid prototyping tool. It simulates signal
processing applications and generates code for heterogeneous multi/many-core
embedded systems. Its dataflow language eases the description of parallel
signal processing applications.

The PREESM tool inputs are an algorithm graph, an architecture graph, and a
scenario which is a set of parameters and constraints that specify the
conditions under which the deployment will run. The chosen type of algorithm
graph is a parameterized and hierarchical extension of Synchronous Dataflow
(SDF) graphs named PiSDF. The architecture graph is named System-Level
Architecture Model (S-LAM). From these inputs, PREESM maps and schedules
automatically the code over the multiple processing elements and generates
multi-core code.

PREESM is an Eclipse plug-in.

http://preesm.sourceforge.net/website/[+http://preesm.sourceforge.net/website/+]

premake
~~~~~~~

Describe your software project just once, using Premake's simple and easy to
read syntax, and build it everywhere.

Generate project files for Visual Studio, GNU Make, Xcode,
xref:Code::Blocks[Code:Blocks], and
more across Windows, Mac OS X, and Linux. Use the full featured Lua scripting
engine to make build configuration tasks a breeze.

http://premake.bitbucket.org/[+http://premake.bitbucket.org/+]

PRIMME
~~~~~~

PRIMME is a C library to find a number of eigenvalues and their corresponding
eigenvectors of a
Real Symmetric, or Complex Hermitian matrix A.
Symmetric and Hermitian eigenvalue problems enjoy a remarkable theoretical
structure that allows for efficient and stable algorithms for obtaining a few
required eigenpairs. This is probably one of the reasons that enabled
applications requiring the solution of symmetric eigenproblems to push their
accuracy and thus computational demands to unprecedented levels. Materials
science, structural engineering, and some QCD applications routinely compute
eigenvalues of matrices of dimension more than a million; and often much more
than that! Typically, with increasing dimension comes increased ill
conditioning, and thus the use of preconditioning becomes essential.

http://www.cs.wm.edu/~andreas/software/[+http://www.cs.wm.edu/~andreas/software/+]

[[Processing]]
Processing
~~~~~~~~~~

A programming language, development environment, and online community. Since
2001, Processing has promoted software literacy within the visual arts and
visual literacy within technology. Initially created to serve as a software
sketchbook and to teach computer programming fundamentals within a visual
context, Processing evolved into a development tool for professionals.

Processing continues to be an alternative to proprietary software tools with
restrictive and expensive licenses, making it accessible to schools and
individual students. Its open source status encourages the community
participation and collaboration that is vital to Processing’s growth.
Contributors share programs, contribute code, and build libraries, tools, and
modes to extend the possibilities of the software. The Processing community
has written more than a hundred libraries to facilitate computer vision, data
visualization, music composition, networking, 3D file exporting, and
programming electronics.

http://processing.org/[+http://processing.org/+]

https://github.com/shiffman/The-Nature-of-Code-Examples[+https://github.com/shiffman/The-Nature-of-Code-Examples+]

*Processing on the Raspberry Pi 2* - https://github.com/processing/processing/wiki/Raspberry-Pi[+https://github.com/processing/processing/wiki/Raspberry-Pi+]

HYPE_Processing
^^^^^^^^^^^^^^^

A collection of classes that performs the heavy lifting for you by writing a
minimal amount of code. This library is compatible with both Processing and
Processing.js

https://github.com/hype/HYPE_Processing[+https://github.com/hype/HYPE_Processing+]

http://www.hypeframework.org/[+http://www.hypeframework.org/+]

[[MassivePixelEnvironment]]
MassivePixelEnvironment
^^^^^^^^^^^^^^^^^^^^^^^

A xref:Processing[Processing] library easily extending sketches to distributed display environments.

http://tacc.github.io/MassivePixelEnvironment/[+http://tacc.github.io/MassivePixelEnvironment/+]

https://github.com/TACC/MassivePixelEnvironment[+https://github.com/TACC/MassivePixelEnvironment+]

Processing.js
^^^^^^^^^^^^^

Processing.js is the sister project of the popular Processing visual
programming language, designed for the web. Processing.js makes your data
visualizations, digital art, interactive animations, educational graphs, video
games, etc. work using web standards and without any plug-ins. You write code
using the Processing language, include it in your web page, and Processing.js
does the rest.

http://processingjs.org/[+http://processingjs.org/+]

pyprocessing
^^^^^^^^^^^^

This project provides a Python package that creates an environment for
graphics applications that closely resembles that of the Processing system.
The project mission is to implement Processing's friendly graphics functions
and interaction model in Python. Not all of Processing is to be ported,
though, since Python itself already provides alternatives for many features of
Processing, such as XML parsing. 
The pyprocessing backend is built upon OpenGL and
xref:Pyglet[Pyglet], which provide the
actual graphics rendering. Since these are multiplatform, so is pyprocessing. 

http://code.google.com/p/pyprocessing/[+http://code.google.com/p/pyprocessing/+]

toxiclibs
^^^^^^^^^

An independent, open source library collection for computational design tasks
with Java & Processing.
The classes are purposefully kept fairly generic in order to maximize re-use
in different contexts ranging from generative design, animation,
interaction/interface design, data visualization to architecture and digital
fabrication, use as teaching tool and more.

http://toxiclibs.org/[+http://toxiclibs.org/+]

[[programming_language]]
programming language
~~~~~~~~~~~~~~~~~~~~

Languages of special interest, i.e. that your author finds shiny.

babar
^^^^^

A little language for machines with Speech Acts inspired by Elephant 2000. The
parser uses the wonderful Clojure Instaparse library. The language aims to
have syntactically sugared "speech acts" that the machine uses as inputs and
outputs. The language also supports beliefs and goals from McCarthy's paper,
Ascribing Mental Qualities to Machines.

https://github.com/gigasquid/babar[+https://github.com/gigasquid/babar+]

Bloom
^^^^^

Bloom is a programming language for the cloud and other distributed computing systems. BOOM is the research project at UC Berkeley that is developing Bloom, as part of a larger agenda to make it easy to build distributed software systems.
Bloom removes traditional mismatches between distributed software and platforms, enabling powerful coding and code analysis without resorting to exotic syntax.

Bloom was designed to match–and exploit–the disorderly reality of distributed systems.  Bloom programmers write programs made up of unordered collections of statements, and are given constructs to impose order when needed.  The standard data structures in Bloom are disorderly collections, rather than scalar variables and structures. These data structures reflect the realities of non-deterministic ordering inherent in distributed systems. Bloom provides simple, familiar syntax for manipulating these structures.

http://www.bloom-lang.net/[+http://www.bloom-lang.net/+]

https://github.com/bloom-lang/bud/blob/master/docs/getstarted.md[+https://github.com/bloom-lang/bud/blob/master/docs/getstarted.md+]

D Language
^^^^^^^^^^

The D programming language is an object-oriented, imperative, multi-paradigm
system programming language.
hough it originated as a re-engineering of Cxx, D is a distinct language,
having redesigned some core Cxx features while also taking inspiration from
other languages, notably Java, Python, Ruby, C#, and Eiffel.
D's design goals attempt to combine the performance and safety of compiled
languages with the expressive power of modern dynamic languages. Idiomatic D
code is commonly as fast as equivalent Cxx code, while being shorter
and memory safe.
Type inference, automatic memory management and syntactic sugar for common
types allow faster development, while bounds checking, design by contract
features and a concurrency-aware type system help reduce the occurrence of
bugs.

D supports five main programming paradigms—imperative, object-oriented,
metaprogramming, functional and concurrent (Actor model).
C's application binary interface (ABI) is supported as well as all of C's
fundamental and derived types, enabling direct access to existing C code and
libraries. D bindings are available for many popular C libraries. C's standard
library is part of standard D.

http://dlang.org/[+http://dlang.org/+]

http://en.wikipedia.org/wiki/D_%28programming_language%29[+http://en.wikipedia.org/wiki/D_%28programming_language%29+]

DRAKON
^^^^^^

DRAKON is a visual language for specifications from the Russian space program.
DRAKON is used for capturing requirements and building software that controls
spacecraft.
The rules of DRAKON are optimized to ensure easy understanding by human
beings.

DRAKON Editor is a free tool for authoring DRAKON flowcharts. It also supports
sequence diagrams, entity-relationship and class diagrams.
The user interface of DRAKON Editor is extremely simple and straightforward.
Software developers can build real programs with DRAKON Editor. Source code
can be generated in several programming languages, including Java,
Processing.org, D, C#, C/Cxx (with Qt support), Python, Tcl, Javascript, Lua,
Erlang, AutoHotkey and Verilog.

http://drakon-editor.sourceforge.net/[+http://drakon-editor.sourceforge.net/+]

Eon
^^^

Eon is the first energy-aware programming language. It is a declarative
coordination language and runtime system designed to simplify the development
of perpetual systems by separating program logic from energy management. Using
Eon, the system designer describes program operation as well as how the
program can be adjusted in order to conserve energy. During operation the Eon
runtime system automatically adjust the program in order to sustain operation
based on online measurements of energy harvest and per-task energy
consumption. 

http://www.cs.dartmouth.edu/\~sorber/eon/index.html[+http://www.cs.dartmouth.edu/~sorber/eon/index.html+]

http://people.cs.clemson.edu/\~jsorber/papers/sorber07sensys.pdf[+http://people.cs.clemson.edu/~jsorber/papers/sorber07sensys.pdf+]

Escher
^^^^^^

A language for connecting technologies using pure metaphors.

http://gocircuit.github.io/escher/[+http://gocircuit.github.io/escher/+]

Esterel
^^^^^^^

Esterel is a programming language dedicated to control-dominated reactive
systems, such as control circuits, embedded systems, human-machine interface,
or communication protocols.

http://www-sop.inria.fr/meije/esterel/esterel-eng.html[+http://www-sop.inria.fr/meije/esterel/esterel-eng.html+]

http://www.esterel.org/[+http://www.esterel.org/+]

http://en.wikipedia.org/wiki/Esterel[+http://en.wikipedia.org/wiki/Esterel+]

Factor
^^^^^^

The Factor programming language combines powerful language features with a
full-featured library. The implementation is fully compiled for performance,
while still supporting interactive development. Factor applications are
portable between all common platforms. Factor can deploy stand-alone
applications on all platforms.

Factor belongs to the family of concatenative languages: this means that, at
the lowest level, a Factor program is a series of words (functions) that
manipulate a stack of references to dynamically-typed values. This gives the
language a powerful foundation which allows many abstractions and paradigms to
be built on top.

http://factorcode.org/[+http://factorcode.org/+]

Halide
^^^^^^

Halide is a new programming language designed to make it easier to write
high-performance image processing code on modern machines. Its current front
end is embedded in Cxx. Compiler targets include x86/SSE, ARM v7/NEON, CUDA,
Native Client, and OpenCL.

http://halide-lang.org/[+http://halide-lang.org/+]

https://github.com/halide/Halide[+https://github.com/halide/Halide+]

HANSEI
^^^^^^

HANSEI is the the embedded domain-specific language for probabilistic
programming: for writing potentially infinite discrete-distribution models and
performing exact inference, importance sampling and inference of inference.

HANSEI is an ordinary OCaml library, with probability distributions
represented as ordinary OCaml programs. Delimited continuations let us reify
non-deterministic programs as lazy search trees, which we may then traverse,
explore, or sample. Thus an inference procedure and a model invoke each other
as co-routines. Thanks to the delimited control, deterministic expressions
look exactly like ordinary OCaml expressions, and are evaluated as such,
without any overhead.

http://okmij.org/ftp/kakuritu/index.html[+http://okmij.org/ftp/kakuritu/index.html+]

HIPAcc
^^^^^^

The Heterogeneous Image Processing Acceleration Framework
allow the design of image processing kernels and algorithms in a
domain-specific language (DSL). From this high-level description, low-level
target code for GPU accelerators is generated using source-to-source
translation. As back ends, the framework supports CUDA, OpenCL, and
Renderscript.

http://hipacc-lang.org/[+http://hipacc-lang.org/+]

https://github.com/hipacc[+https://github.com/hipacc+]

J Language
^^^^^^^^^^

The J programming language, developed in the early 1990s by Kenneth E. Iverson and Roger Hui, is a synthesis of APL (also by Iverson) and the FP and FL function-level languages created by John Backus.
J is a very terse array programming language, and is most suited to mathematical and statistical programming, especially when performing operations on matrices.

http://www.jsoftware.com/jwiki/FrontPage[+http://www.jsoftware.com/jwiki/FrontPage+]

http://www.jsoftware.com/jwiki/Guides/Getting%20Started[+http://www.jsoftware.com/jwiki/Guides/Getting%20Started+]

http://www.jsoftware.com/help/learning/contents.htm[+http://www.jsoftware.com/help/learning/contents.htm+]

https://scottlocklin.wordpress.com/2013/07/28/ruins-of-forgotten-empires-apl-languages/[+https://scottlocklin.wordpress.com/2013/07/28/ruins-of-forgotten-empires-apl-languages/+]

http://tryapl.org/[+http://tryapl.org/+]

Jolie
^^^^^

Jolie is an open-source[1] programming language for developing distributed
applications based on microservices. In the programming paradigm proposed with
Jolie, each program is a service that can communicate with other programs by
sending and receiving messages over a network. Jolie supports an abstraction
layer that allows services to communicate different mediums, ranging from
TCP/IP sockets to local in-memory communications between processes.
Jolie is currently supported by an interpreter implemented in the Java
language, which can be run in multiple operating systems.
Since it supports the orchestration of Web Services, Jolie is an alternative
to XML-based orchestration languages such as WS-BPEL as it offers a concise
(C-like) syntax for accessing XML-like data structures.

http://www.jolie-lang.org/index.html[+http://www.jolie-lang.org/index.html+]

http://en.wikipedia.org/wiki/Jolie_%28programming_language%29[+http://en.wikipedia.org/wiki/Jolie_%28programming_language%29+]

KernelGenius
^^^^^^^^^^^^

Language and compiler for image processing graphs (specific language + C) into
a single merged OpenCL kernel tuned for the target many-core architecture.

https://github.com/tlepley/KernelGenius[+https://github.com/tlepley/KernelGenius+]

LLJS
^^^^

LLJS is a typed dialect of JavaScript that offers a C-like type system with
manual memory management. It compiles to JavaScript and lets you write
memory-efficient and GC pause-free code less painfully, in short, LLJS is the
bastard child of JavaScript and C. LLJS is early research prototype work, so
don't expect anything rock solid just yet. The research goal here is to
explore low-level statically typed features in a high-level dynamically typed
language. Think of it as inline assembly in C, or the unsafe keyword in C#.
It's not pretty, but it gets the job done. 

http://lljs.org/[+http://lljs.org/+]

MLton
^^^^^

MLton is an open source, whole-program optimizing compiler for the Standard ML
(SML) programming language.
MLton aims to produce fast executables, and to encourage rapid prototyping and
modular programming by eliminating performance penalties often associated with
the use of high-level language features.

MLton development began in 1997, and continues with a worldwide
community of developers and users, who have helped to port MLton to a number
of platforms. As a whole-program compiler Mlton is notable amongst SML
environments such as Standard ML of New Jersey (SML/NJ) for lacking an
interactive top level, common among most SML implementations. MLton also
includes several libraries in addition to the SML Basis Library as well as
features to aid in porting code from SML/NJ, one of the more popular SML
implementations. MLton also aims to make programming in the
large more feasible through the use of the MLBasis system simplifying
modularity and managing of namespaces in larger pieces of code.

https://github.com/MLton[+https://github.com/MLton+]

http://www.mlton.org/[+http://www.mlton.org/+]

http://matt.might.net/articles/best-programming-languages/[+http://matt.might.net/articles/best-programming-languages/+]

Mozart
^^^^^^

The Mozart Programming System combines ongoing research in programming
language design and implementation, constraint logic programming, distributed
computing, and human-computer interfaces. Mozart implements the Oz language
and provides both expressive power and advanced functionality.
Mozart excels in creating distributed, concurrent applications, because it
makes a network fully transparent. It supports GUI applications through Tcl/Tk
integration, because it runs applications in a virtual machine: applications
can be developed once and run on many different platforms.

The PLDC Research Group at UCL is proud to announce the first release of
Mozart 2. This release contains a completely redesigned 64-bit virtual machine
(compatible with 32-bit and 64-bit processors), and adds an extension
interface to the virtual machine to allow language extensions defined within
Oz. The PLDC Research Group will use Mozart 2 for future programming education
and future research in programming language design and implementation.

The first release of Mozart 2 does not provide support for constraints or
distributed programming. We plan for successive releases to support constraint
programming with an interface to the Gecode system, and to support distributed
programming with a peer-to-peer transactional storage and extensions for
network-transparent and synchronization-free programming.

http://mozart.github.io/[+http://mozart.github.io/+]

https://github.com/mozart[+https://github.com/mozart+]

http://arxiv.org/abs/cs/0208029[+http://arxiv.org/abs/cs/0208029+]

Nim
^^^

A compiled, garbage-collected systems programming language which has an
excellent productivity/performance ratio. Nim's design focuses on efficiency,
expressiveness, elegance (in the order of priority).

Nim (formerly known as "Nimrod") is a statically typed, imperative programming
language that tries to give the programmer ultimate power without compromises
on runtime efficiency. This means it focuses on compile-time mechanisms in all
their various forms.

Beneath a nice infix/indentation based syntax with a powerful (AST based,
hygienic) macro system lies a semantic model that supports a soft realtime GC
on thread local heaps. Asynchronous message passing is used between threads,
so no "stop the world" mechanism is necessary. An unsafe shared memory heap is
also provided for the increased efficiency that results from that model.

https://github.com/Araq/Nim[+https://github.com/Araq/Nim+]

http://nim-lang.org/[+http://nim-lang.org/+]

https://github.com/rspeer/wiki2text[+https://github.com/rspeer/wiki2text+]

http://rnduja.github.io/2015/10/21/scientific-nim/[http://rnduja.github.io/2015/10/21/scientific-nim/]

Oberon
^^^^^^

Oberon is a general-purpose programming language created in 1986 by Professor
Niklaus Wirth and the latest member of the Wirthian family of ALGOL-like
languages (Euler, Algol-W, Pascal, Modula, and Modula-2). Oberon was the
result of a concentrated effort to increase the power of Modula-2, the direct
successor of Pascal, and simultaneously to reduce its complexity. Its
principal new feature is the concept of type extension of record types:[1] It
permits the construction of new data types on the basis of existing ones and
to relate them, deviating from the dogma of strictly static data typing.

The new System since 2008 is now called A2.
A2 is the name of a modern integrated software environment. It is a
single-user, multi-tasking system that runs on bare hardware or on top of a
host operating system. 

http://www.oberon.ethz.ch/[+http://www.oberon.ethz.ch/+]

http://en.wikipedia.org/wiki/Oberon_%28programming_language%29[+http://en.wikipedia.org/wiki/Oberon_%28programming_language%29+]

OCaml
^^^^^

OCaml is the main implementation of the Caml programming language, created by
Xavier Leroy, Jérôme Vouillon, Damien Doligez, Didier Rémy and others in 1996.
OCaml extends the core Caml language with object-oriented constructs.

OCaml's toolset includes an interactive top level interpreter, a bytecode
compiler, and an optimizing native code compiler. It has a large standard
library that makes it useful for many of the same applications as Python or
Perl, as well as robust modular and object-oriented programming constructs
that make it applicable for large-scale software engineering. OCaml is the
successor to Caml Light. The acronym CAML originally stood for Categorical
Abstract Machine Language, although OCaml abandons this abstract machine.[1]

OCaml is a free open source project managed and principally maintained by
INRIA. In recent years, many new languages have drawn elements from OCaml,
most notably Fsharp and Scala.

http://ocaml.org/[+http://ocaml.org/+]

http://forge.ocamlcore.org/[+http://forge.ocamlcore.org/+]

http://opam.ocaml.org/packages/[+http://opam.ocaml.org/packages/+]

http://planet.ocamlcore.org/[+http://planet.ocamlcore.org/+]

http://en.wikipedia.org/wiki/OCaml[+http://en.wikipedia.org/wiki/OCaml+]

Picat
^^^^^

Picat is a simple, and yet powerful, logic-based multi-paradigm programming
language aimed for general-purpose applications. Picat is a rule-based
language, in which predicates, functions, and actors are defined with
pattern-matching rules. Picat incorporates many declarative language features
for better productivity of software development, including explicit
non-determinism, explicit unification, functions, list comprehensions,
constraints, and tabling. Picat also provides imperative language constructs,
such as assignments and loops, for programming everyday things. The Picat
implementation, which is based on a well-designed virtual machine and
incorporates a memory manager that garbage-collects and expands the stacks and
data areas when needed, is efficient and scalable. Picat can be used for not
only symbolic computations, which is a traditional application domain of
declarative languages, but also for scripting and modeling tasks.

http://picat-lang.org/[+http://picat-lang.org/+]

[[Push]]
Push
^^^^

Push is a programming language designed for evolutionary computation, to be used as the programming language within which evolving programs are expressed. 

http://faculty.hampshire.edu/lspector/push.html[+http://faculty.hampshire.edu/lspector/push.html+]
https://github.com/lspector/Clojush[+https://github.com/lspector/Clojush+]

Pyret
^^^^^

A programming language designed to serve as an outstanding choice for
programming education while exploring the confluence of scripting and
functional programming.  Pyret has Python-inspired syntax for functions,
lists, and operators. Iteration constructs are designed to be evocative of
those in other languages.

http://www.pyret.org/[+http://www.pyret.org/+]

https://github.com/brownplt/pyret-lang/[+https://github.com/brownplt/pyret-lang/+]

[[Racket]]
Racket
^^^^^^

A full-spectrum programming language. It goes beyond Lisp and Scheme with
dialects that support objects, types, laziness, and more. Racket enables
programmers to link components written in different dialects, and it empowers
programmers to create new, project-specific dialects. Racket's libraries
support applications from web servers and databases to GUIs and charts.

http://racket-lang.org/[+http://racket-lang.org/+]

https://news.ycombinator.com/item?id=9261073[+https://news.ycombinator.com/item?id=9261073+]

*Creating Languages in Racket* - http://queue.acm.org/detail.cfm?id=2068896[+http://queue.acm.org/detail.cfm?id=2068896+]

*Fudging Up a Racket* - https://www.hashcollision.org/brainfudge/[+https://www.hashcollision.org/brainfudge/+]

*Python: The Full Monty* - http://cs.brown.edu/\~sk/Publications/Papers/Published/pmmwplck-python-full-monty/[+http://cs.brown.edu/~sk/Publications/Papers/Published/pmmwplck-python-full-monty/+]

Pollen
xxxxxx

A book-publishing system written in Racket.
If you think documents should be programmable, you’ll love it.
If not, you can move along.
Pollen gives you access to a full programming language (Racket) with a
text-based syntax that makes it easy to embed code within your documents.

https://github.com/mbutterick/pollen/tree/master[+https://github.com/mbutterick/pollen/tree/master+]

http://pollenpub.com/[+http://pollenpub.com/+]

Scribble
xxxxxxxx

Scribble is a collection of tools for creating prose documents—papers, books,
library documentation, etc.—in HTML or PDF (via Latex) form. More generally,
Scribble helps you write programs that are rich in textual content, whether
the content is prose to be typeset or any other form of text to be generated
programmatically.

SAC
^^^

SAC (Single Assignment C) is a strict purely functional programming language
whose design is focussed on the needs of numerical applications. Particular
emphasis is laid on efficient support for array processing. Efficiency
concerns are essentially twofold. On the one hand, efficiency in program
development is to be improved by the opportunity to specify array operations
on a high level of abstraction. On the other hand, efficiency in program
execution, i.e. the runtime performance of programs both in time and memory
consumption, is still to be achieved by sophisticated compilation schemes.
Only as far as the latter succeeds, the high-level style of specifications can
actually be called useful. 

In order to overcome the acceptance problems encountered by other functional
or array based languages intended for numerical / array intensive
applications, e.g. Sisal, Nesl, Nial, APL, J, or K, particular regard is paid
to ease the transition from a C / Fortran like programming environment to SAC. 

http://www.sac-home.org/[+http://www.sac-home.org/+]

Shen
^^^^

Shen is a portable functional programming language that offers
pattern matching, lambda calculus consistency, macros,
optional lazy evaluation, static type checking,
an integrated fully functional Prolog,
and an inbuilt compiler-compiler.

Shen has one of the most powerful type systems within functional programming.
Shen runs under a reduced instruction Lisp and is designed for portability.
The word ‘Shen’ is Chinese for 'spirit' and our motto reflects our desire to
liberate our work to live under many platforms. Shen is under BSD and
currently runs under CLisp and SBCL, Clojure, Scheme, Ruby, Python, the JVM
and Javascript. 

http://www.shenlanguage.org/[+http://www.shenlanguage.org/+]

Twelf
^^^^^

Twelf is a language used to specify, implement, and prove properties of
deductive systems such as programming languages and logics.
Twelf is a piece of computer software, and it is also a computer language
understood by the Twelf software.
C code and Java code describe programs, HTML code describes graphical web
pages, and Twelf code describes logical systems.

The reason someone might want to use Twelf code to describe a logical system
is that once they’ve described it, they can write more Twelf code that uses
that logical system. You could use Twelf to write out a statement about basic
arithmetic (for instance, “if a + b = c, then b + a = c”), and then use Twelf
to write out a justification of why that statement is true (i.e. a proof).
When you do so, Twelf will check your proof, making sure that what you said
actually is true! 

It turns out that while basic arithmetic, set theory, and interesting logics
are logical systems, programming languages are also logical systems - and
Twelf has a couple of unique features that make it a great tool to use when
the logical systems you are working with are programming languages.

http://twelf.org/wiki/Main_Page[+http://twelf.org/wiki/Main_Page+]

PSPFFT
~~~~~~

We describe an implementation to solve Poissonʼs equation for an isolated
system on a unigrid mesh using FFTs. The method solves the equation globally
on mesh blocks distributed across multiple processes on a distributed-memory
parallel computer. Test results to demonstrate the convergence and scaling
properties of the implementation are presented. The solver is offered to
interested users as the library PSPFFT.

http://www.sciencedirect.com/science/article/pii/S0010465511001809[+http://www.sciencedirect.com/science/article/pii/S0010465511001809+]

PULSAR
~~~~~~

The Parallel Ultra-Light Systolic Array Runtime (PULSAR), now in version 2.0,
is a complete programming platform for large-scale distributed memory systems
with multicore processors and hardware accelerators. PULSAR provides a simple
abstraction layer over multithreading, message-passing, and multi-GPU,
multi-stream programming. PULSAR offers a general-purpose programming model,
suitable for a wide range of scientific and engineering applications.

This simple programming model allows the user to define the computation in the
form of a Virtual Systolic Array (VSA), which is a set of Virtual Data
Processors (VDPs), and is connected with data channels. This programming model
is also accessible to the user through a very small and simple Application
Programming Interface (API), and all the complexity of executing the workload
on a large-scale system is hidden in the runtime implementation.

The runtime supports distributed memory systems with multicore processors and
relies on POSIX Threads (a.k.a. Pthreads) for intra-node multithreading, and
on the Message Passing Interface (MPI) for inter-node communication. The
runtime also supports multiple Nvidia GPU accelerators, in each distributed
memory node, using the Compute Unified Device Architecture (CUDA) platform.

http://icl.utk.edu/pulsar/[+http://icl.utk.edu/pulsar/+]

[[Pure_Data]]
Pure Data
~~~~~~~~~

Pure Data (aka Pd) is an open source visual programming language. Pd enables
musicians, visual artists, performers, researchers, and developers to create
software graphically, without writing lines of code. Pd is used to process and
generate sound, video, 2D/3D graphics, and interface sensors, input devices,
and MIDI. Pd can easily work over local and remote networks to integrate
wearable technology, motor systems, lighting rigs, and other equipment. Pd is
suitable for learning basic multimedia processing and visual programming
methods as well as for realizing complex systems for large-scale projects.

Pd is a so-called data flow programming language, where software called
patches are developed graphically. Algorithmic functions are represented by
objects, placed on a screen called canvas. Objects are connected together with
cords, and data flows from one object to another through this cords. Each
object performs a specific task, from very low level mathematic operations to
complex audio or video functions such as reverberation, fft transform, or
video decoding.

http://puredata.info/[+http://puredata.info/+]

Pd-L2Ork
^^^^^^^^

Linux-centric monolithic distribution based on pd-extended with focus on
solid/stable core, enhancements, and usability features including infinite
undo, gui-based iemgui object editing, accelerated visual editor and gui
operations, improved appearance, K12 education mode, and more. The
distribution is developed for and maintained by Virginia Tech's Linux Laptop
Orchestra (L2Ork).

http://puredata.info/downloads/Pd-L2Ork[+http://puredata.info/downloads/Pd-L2Ork+]

Pushpin
~~~~~~~

Pushpin is a new way to build realtime HTTP and WebSocket services.

http://pushpin.org/[+http://pushpin.org/+]

PyAOS
~~~~~

A collection of standard atmospheric and oceanic sciences routines.

https://github.com/PyAOS/aoslib[+https://github.com/PyAOS/aoslib+]

pycaml
~~~~~~

A library which follows the Python/C API as closely as possible, while
providing equivalent functionality for objective caml. This is built against
python 2.x and Ocaml 3.04.

It is intended to allow users to build native ocaml libraries and use them
from python, and alternately, in order to allow ocaml users to benefit from
linkable libraries provided for python.

http://pycaml.sourceforge.net/[+http://pycaml.sourceforge.net/+]

https://bitbucket.org/chemoelectric/pycaml[+https://bitbucket.org/chemoelectric/pycaml+]

https://github.com/chemoelectric/pycaml[+https://github.com/chemoelectric/pycaml+]

pyDatalog
~~~~~~~~~

pyDatalog adds the logic programming paradigm to Python's extensive toolbox,
in a pythonic way.  

Logic programmers can now use the extensive standard library of Python, and
Python programmers can now express complex algorithms quickly.

Datalog is a truly declarative language derived from Prolog, with strong
academic foundations.  Datalog excels at managing complexity.  Datalog
programs are shorter than their Python equivalent, and Datalog statements can
be specified in any order, as simply as formula in a spreadsheet. 

https://sites.google.com/site/pydatalog/[+https://sites.google.com/site/pydatalog/+]

http://en.wikipedia.org/wiki/Datalog[+http://en.wikipedia.org/wiki/Datalog+]

Pydgin
~~~~~~

Pydgin provides a collection of classes and functions which act as an embedded
architectural description language (embedded-ADL) for concisely describing the
behavior of instruction set simulators (ISS). An ISS described in Pydgin can
be directly executed in a Python interpreter for rapid prototyping and
debugging, or alternatively can be used to automatically generate a
performant, JIT-optimizing C executable more suitable for application
development.

Automatic generation of JIT-enabled ISS from Pydgin is enabled by the RPython
Translation Toolchain, an open-source tool used by developers of the PyPy
JIT-optimizing Python interpreter.

An ISS described in Pydgin implements an interpretive simulator which can be
directly executed in a Python interpreter for rapid prototyping and debugging.
However, Pydgin ISS can also be automatically translated into a C executable
implementing a JIT-enabled interpretive simulator, providing a
high-performance implementation suitable for application development.
Generated Pydgin executables provide significant performance benefits in two
ways. First, the compiled C implementation enables much more efficient
execution of instruction-by-instruction interpretive simulation than the
original Python implementation. Second, the generated executable provides a
trace-JIT to dynamically compile frequently interpreted hot loops into
optimized assembly.

https://github.com/cornell-brg/pydgin[+https://github.com/cornell-brg/pydgin+]

http://morepypy.blogspot.com/2015/03/pydgin-using-rpython-to-generate-fast.html[+http://morepypy.blogspot.com/2015/03/pydgin-using-rpython-to-generate-fast.html+]

PyDOM
~~~~~

PyDom is a Python package which implements various diagnostics for  NEMO model
output. 

http://servforge.legi.grenoble-inp.fr/projects/PyDom[+http://servforge.legi.grenoble-inp.fr/projects/PyDom+]

PyFR
~~~~

PyFR is an open-source Python based framework for solving advection-diffusion
type problems on streaming architectures using the Flux Reconstruction
approach of Huynh. The framework is designed to solve a range of governing
systems on mixed unstructured grids containing various element types. It is
also designed to target a range of hardware platforms via use of an in-built
domain specific language derived from the Mako templating engine.

http://www.pyfr.org/[+http://www.pyfr.org/+]

https://github.com/vincentlab/PyFR[+https://github.com/vincentlab/PyFR+]

http://arxiv.org/abs/1312.1638[+http://arxiv.org/abs/1312.1638+]

http://arxiv.org/abs/1409.0405[+http://arxiv.org/abs/1409.0405+]

http://www.sciencedirect.com/science/article/pii/S0010465514002549[+http://www.sciencedirect.com/science/article/pii/S0010465514002549+]

[[Pyglet]]
Pyglet
~~~~~~

A cross-platform windowing and multimedia library for Python. 
Pyglet provides an object-oriented programming interface for developing games
and other visually-rich applications for Windows, Mac OS X and Linux.
Features include:

* No external dependencies or installation requirements. For most application
and game requirements, pyglet needs nothing else besides Python, simplifying
distribution and installation.

* Take advantage of multiple windows and multi-monitor desktops. pyglet allows
you to use as many windows as you need, and is fully aware of multi-monitor
setups for use with fullscreen games.

* Load images, sound, music and video in almost any format. pyglet can
optionally use AVbin to play back audio formats such as MP3, OGG/Vorbis and
WMA, and video formats such as DivX, MPEG-2, H.264, WMV and Xvid.

http://www.pyglet.org/[+http://www.pyglet.org/+]

pygeoif
~~~~~~~

PyGeoIf provides a GeoJSON-like protocol for geo-spatial (GIS) vector data.
When you want to write your own geospatilal library with support for this
protocol you may use pygeoif as a starting point and build your functionality
on top of it.

You may think of pygeoif as a 'shapely ultralight' which lets you construct
geometries and perform very basic operations like reading and writing
geometries from/to WKT, constructing line strings out of points, polygons from
linear rings, multi polygons from polygons, etc. It was inspired by shapely
and implements the geometries in a way that when you are familiar with shapely
you feel right at home with pygeoif.
It was written to provide clean and python only geometries for
xref:fastkml[fastkml].

https://github.com/cleder/pygeoif/[+https://github.com/cleder/pygeoif/+]

pyimpute
~~~~~~~~

Utilities for applying scikit-learn to spatial datasets.

https://pypi.python.org/pypi/pyimpute/0.0.2[+https://pypi.python.org/pypi/pyimpute/0.0.2+]

[[PyKE]]
PyKE
~~~~

The Kepler archive contains time-series data that have been calibrated and
reduced from detector pixels. This pipelined reduction includes the removal of
time-series trends systematic to the spacecraft and its environment rather
than the targets. For every target there is a level of subjectivity required
to reduce systematics. Differing scientific goals are likely to have differing
requirements for systematic mitigation. Systematic reduction in the Kepler
pipeline is optimized to yield the highest number of potentially-detectable
exoplanet transits from a sample of 200,000 stars. PyKE, on the other hand, is
a group of python tasks developed for the reduction and analysis of Kepler
pixel-level data and Simple Aperture Photometry (SAP) data of individual
targets with individual characteristics. PyKE was developed to provide
alternative data reduction, tunable to the user's specific science goals. The
main purposes of these tasks are to i) re-extract light curves from
manually-chosen pixel apertures and ii) cotrend and/or detrend the data in
order to reduce or remove systematic noise structure using methods tun-able to
user and target-specific requirements. Tasks to perform data analysis
developed for the author's science programs are also included. PyKE is an open
source project. Contributions of new tasks or enhanced functionality of
existing tasks by the community are welcome.

PyKE is a python-based PyRAF package which can also be executed without PyRAF
on the command line of a shell.

http://keplergo.arc.nasa.gov/PyKE.shtml[+http://keplergo.arc.nasa.gov/PyKE.shtml+]

Pynamic
~~~~~~~

Pynamic is a benchmark designed to test a system's ability to handle the
Dynamic Linking and Loading requirements of Python-based scientific
applications. We developed this benchmark to represent a newly emerging class
of DLL behaviors.  Pynamic builds on pyMPI, an MPI extension to Python.  Our
augmentation includes a code generator that automatically generates Python
C-extension dummy codes and a glue layer that facilitates linking and loading
of the generated dynamic modules into the resulting pyMPI.  Pynamic is
configurable, enabling it to model the static properties of a specific code.
It does not, however, model any significant computations of the target and
hence it is not subjected to the same level of control as the target code. In
fact, we encourage HPC computer vendors and tool developers to add it to their
test suites. This benchmark provides an effective test of the compiler, the
linker, the loader, the OS kernel and other runtime systems of a high
performance computing (HPC) system to handle an important aspect of modern
scientific computing applications. In addition, the benchmark serves as a
stress test case for code development tools. Although Python has recently
gained popularity in the HPC community, its heavy use of DLL operations has
hindered certain HPC code development tools, notably parallel debuggers, from
performing optimally. 

The heart of Pynamic is a Python script that generates C files and compiles
them into shared object libraries. Each library contains a Python callable
entry function as well as a number of utility functions. The user can also
enable cross library function calls with a command line argument. The Pynamic
configure script then links these libraries into the pynamic-pyMPI executable
and creates a driver script to exercise the functions in the generated
libraries. The user can specify the number of libraries to create, as well as
the average number of utility functions per library, thus tailoring the
benchmark to match some application of interest. Pynamic introduces randomness
in the number of functions per module and the function signatures, thus
ensuring some heterogeneity of the libraries and functions. 

https://codesign.llnl.gov/pynamic.php[+https://codesign.llnl.gov/pynamic.php+]

https://github.com/scalability-llnl/pynamic[+https://github.com/scalability-llnl/pynamic+]

[[PyNIO]]
PyNIO
~~~~~

A Python package that allows read and/or write access to a variety of data formats using an interface modeled on netCDF. PyNIO is composed of a C library called libnio along with a Python module based on and with an interface similar to the Scientific.IO.NetCDF module written by Konrad Hinsen. The C library contains the same data I/O code used in xref:NCL[NCL], a scripting language developed for analysis and visualization of geo-scientific data.

http://www.pyngl.ucar.edu/Nio.shtml[+http://www.pyngl.ucar.edu/Nio.shtml+]

[[pyparty]]
pyparty
~~~~~~~

A library for drawing, labeling, patterning and manipulating particles in 2d images. 
Pyparty was built on top of xref:scikit-image[scikit-image].
It emerged as a means to abstract the concept of particles (i.e. image blobs) into custom data structures for intuitive manipulation and characterization whilst preserving the image API. In addition to integrating new particle constructs with the existing array and image processing functions, pyparty extends scikit-image’s rasterization toolbox with new particle types, patterning, and an interface to Matplotlib patch objects for vectorized particle renderings. Thus, pyparty leverages the conventional imaging pipeline at both ends; it provides a tool set for artificial image composition, and a framework for particle post-processing.

https://github.com/hugadams/pyparty[+https://github.com/hugadams/pyparty+]

*pyparty: Intuitive Particle Processing in Python* (online article) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bh/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bh/+]

[[PyRDM]]
PyRDM
~~~~~

A Python-based library for automating the management and online publication of scientific software and data.
PyRDM aims to automate the process of sharing the software and data via online, citable repositories such as Figshare.

PyRDM provides the capability to integrate research data management into the workflows of scientific software packages, so that the software and data can be curated at the push of a button. The functionality of PyRDM is encapsulated in several Python modules which, in short, facilitate the automated publication of scientific software and data via online, citable repositories. Specifically, the Figshare online repository service is used to store the software and data, and provides DOIs for both resources so they can be appended to any provenance metadata. Not only does this allow a specific version of the software or data to be properly cited in a journal publication, it can also reduce data duplication by enabling the sharing of data with colleagues around the world. 

https://github.com/pyrdm/pyrdm/[+https://github.com/pyrdm/pyrdm/+]

*PyRDM: A Python-based library for automating the management and online publication of scientific software and data* (online article) - http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bj/[+http://openresearchsoftware.metajnl.com/articles/10.5334/jors.bj/+]

[[Python]]
Python
~~~~~~

Various implementations of the language standard known as Python.

[[Jython]]
Jython
^^^^^^

An implementation of the Python programming language which is designed to run
on the Java(tm) Platform. It consists of a compiler to compile Python source
code down to Java bytecodes which can run directly on a JVM, a set of support
libraries which are used by the compiled Java bytecodes, and extra support to
make it trivial to use Java packages from within Jython.

Jython is an implementation of the Python language for the Java platform.
Jython 2.5 implements the same language as CPython 2.5, and nearly all of the
Core Python standard library modules. (CPython is the C implementation of the
Python language.)

http://www.jython.org/[+http://www.jython.org/+]

[[PyPy]]
PyPy
^^^^

PyPy is a replacement for CPython. It is built using the RPython language that
was co-developed with it. The main reason to use it instead of CPython is
speed: it runs generally faster.
PyPy 2.5 implements Python 2.7.8 and runs on Intel x86 (IA-32) , x86_64 and
ARM platforms.
It supports all of the core language, passing the Python test suite (with
minor modifications that were already accepted in the main python in newer
versions). It supports most of the commonly used Python standard library
modules.

Our main executable comes with a Just-in-Time compiler. It is really fast in
running most benchmarks – including very large and complicated Python
applications, not just 10-liners.
The case where PyPy works best is when executing long-running programs where a
significant fraction of the time is spent executing Python code.

http://pypy.org/[+http://pypy.org/+]

Stackless
^^^^^^^^^

Stackless Python is an enhanced version of the Python programming language. It
allows programmers to reap the benefits of thread-based programming without
the performance and complexity problems associated with conventional threads.
The microthreads that Stackless adds to Python are a cheap and lightweight
convenience.

Stackless provides the tools to model concurrency more easily than you can
currently do in most conventional languages.
With stackless, you get concurrency in addition to all of the advantages of
python itself, in an environment that you are (hopefully) already familiar
with.

http://www.stackless.com/[+http://www.stackless.com/+]

http://www.grant-olson.net/files/why_stackless.html[+http://www.grant-olson.net/files/why_stackless.html+]


Python-jrpc
~~~~~~~~~~~

A Python remote procedure call framework that uses JSON RPC v2.0.
Python-JRPC allows programmers to create powerful client/server programs with
very little code.

the FastHCS algorithm, we carry out an extensive simulation study and four
real data applications, the results of which show that FastHCS is
systematically more robust to outliers than its competitors. 

http://arxiv.org/abs/1402.3514[+http://arxiv.org/abs/1402.3514+]

http://cran.r-project.org/web/packages/FastHCS/index.html[+http://cran.r-project.org/web/packages/FastHCS/index.html+]

[[Pythran]]
Pythran
~~~~~~~

A Python to Cxx compiler for a subset of the Python language.
It takes a python module annotated with a few interface description and turns
it into a native python module with the same interface, but (hopefully)
faster.
It is meant to efficiently compile scientific programs, and takes advantage of
multi-cores and SIMD instruction units.

http://pythonhosted.org/pythran/[+http://pythonhosted.org/pythran/+]

https://github.com/serge-sans-paille/pythran[+https://github.com/serge-sans-paille/pythran+]

http://serge-sans-paille.github.io/talks/pythran-2014-07-15.html[+http://serge-sans-paille.github.io/talks/pythran-2014-07-15.html+]

https://conference.scipy.org/scipy2013/presentation_detail.php?id=136[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=136+]

[[pyunicorn]]
pyunicorn
~~~~~~~~~

The Pythonic unified complex network and recurrence analysis toolbox is an
open source software package for applying and combining modern methods of data analysis and modeling from complex network theory and nonlinear time series analysis.
pyunicorn is a fully object-oriented and easily parallelizable package written in the language Python. It allows for the construction of functional networks such as climate networks in climatology or functional brain networks in neuroscience representing the structure of statistical interrelationships in large data sets of time series and, subsequently, investigating this structure using advanced methods of complex network theory such as measures and models for spatial networks, networks of interacting networks, node-weighted statistics or network surrogates. Additionally, pyunicorn provides insights into the nonlinear dynamics of complex systems as recorded in uni- and multivariate time series from a non-traditional perspective by means of recurrence quantification analysis (RQA), recurrence networks, visibility graphs and construction of surrogate time series.

http://tocsy.pik-potsdam.de/pyunicorn.php[+http://tocsy.pik-potsdam.de/pyunicorn.php+]

*Unified functional network and nonlinear time series analysis for complex systems science: The pyunicorn package* - http://arxiv.org/abs/1507.01571[+http://arxiv.org/abs/1507.01571+]

*Complex network based techniques to identify extreme events and (sudden) transitions in spatio-temporal systems* - http://arxiv.org/abs/1507.03778[+http://arxiv.org/abs/1507.03778+]

[[NQ]]
////
NQQQ
////

QGIS
~~~~

~~~~

A cross-platform free and open-source desktop geographic information system
(GIS) application that provides data viewing, editing, and analysis
capabilities.
Similar to other software GIS systems QGIS allows users to create maps with
many layers using different map projections. Maps can be assembled in
different formats and for different uses. QGIS allows maps to be composed
of raster or vector layers. Typical for this kind of software the vector data
is stored as either point, line, or polygon-feature. Different kinds of raster
images are supported and the software can perform georeferencing of images.

QGIS provides integration with other open source GIS packages, including
PostGIS, GRASS, and MapServer to give users extensive functionality.[2]
Plugins, written in Python or Cxx, extend the capabilities of QGIS. There are
plugins to geocode using the Google Geocoding API, perform geoprocessing
(fTools) similar to the standard tools found in ArcGIS, interface with
PostgreSQL/PostGIS, SpatiaLite and MySQL databases.

http://www2.qgis.org/en/site/[+http://www2.qgis.org/en/site/+]

PyQGIS
^^^^^^

Python bindings for QGIS that depend on SIP and PyQt4.

http://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/[+http://docs.qgis.org/testing/en/docs/pyqgis_developer_cookbook/+]

[[qtcm]]
qtcm
~~~~

The single-baroclinic mode Neelin-Zeng Quasi-Equilibrium Tropical Circulation Model (QTCM1) is a primitive equation-based intermediate-level atmospheric model that focuses on simulating the tropical atmosphere. The qtcm package is an implementation of the Neelin-Zeng QTCM1 in a Python object-oriented environment using the f2py Fortran wrapping tool. The result is a modeling package where order and choice of subroutine execution can be altered at runtime, model analysis and visualization can also be integrated with model execution at runtime, while retaining the computationally light footprint of the original intermediate-level model. 

http://www.johnny-lin.com/py_pkgs/qtcm/[+http://www.johnny-lin.com/py_pkgs/qtcm/+]

http://www.geosci-model-dev.net/2/1/2009/gmd-2-1-2009.html[+http://www.geosci-model-dev.net/2/1/2009/gmd-2-1-2009.html+]

[[NR]]
////
NRRR
////

R Language
~~~~~~~~~~

A free software environment for statistical computing and graphics. 

https://www.r-project.org/[+https://www.r-project.org/+]

*Tufte in R* - http://motioninsocial.com/tufte/[+http://motioninsocial.com/tufte/+]

Revolution R
^^^^^^^^^^^^

An enhanced distribution of R from Revolution Analytics. RRO 3.2.1 is based on version 3.2.1 of the statistical software R and includes additional capabilities for improved performance, reproducibility and platform support.

https://mran.revolutionanalytics.com/rro/[+https://mran.revolutionanalytics.com/rro/+]

[[FactoMineR]]
FactoMineR
^^^^^^^^^^

Exploratory data analysis methods such as principal component methods and
clustering.

http://factominer.free.fr/[+http://factominer.free.fr/+]

http://cran.r-project.org/web/packages/FactoMineR/index.html[+http://cran.r-project.org/web/packages/FactoMineR/index.html+]

Factoshiny
xxxxxxxxxx

Perform factorial analysis with a menu and draw graphs interactively thanks to
FactoMineR and a Shiny application.

http://cran.r-project.org/web/packages/Factoshiny/index.html[+http://cran.r-project.org/web/packages/Factoshiny/index.html+]

multitaper
^^^^^^^^^^

R package implementing multitaper spectral estimation techniques used in time
series analysis. This version may be slightly more updated than the one on
CRAN.

https://github.com/wesleyburr/multitaper[+https://github.com/wesleyburr/multitaper+]

http://cran.r-project.org/web/packages/multitaper/index.html[+http://cran.r-project.org/web/packages/multitaper/index.html+]

NMF
^^^

This package provides a framework to perform Non-negative Matrix Factorization
(NMF). It implements a set of already published algorithms and seeding
methods, and provides a framework to test, develop and plug new/custom
algorithms. Most of the built-in algorithms have been optimized in Cxx, and
the main interface function provides an easy way of performing parallel
computations on multicore machines.

http://cran.r-project.org/web/packages/NMF/index.html[+http://cran.r-project.org/web/packages/NMF/index.html+]

oce
^^^

Supports the analysis of Oceanographic data, including ADP measurements, CTD
measurements, sectional data, sea-level time series, coastline files, etc.
Provides functions for calculating seawater properties such as potential
temperature and density, as well as derived properties such as buoyancy
frequency and dynamic height.

http://cran.r-project.org/web/packages/oce/index.html[+http://cran.r-project.org/web/packages/oce/index.html+]

OceanView
^^^^^^^^^

Functions for transforming and viewing 2-D and 3-D (oceanographic) data and
model output.

http://cran.r-project.org/web/packages/OceanView/index.html[+http://cran.r-project.org/web/packages/OceanView/index.html+]

OpenCPU
^^^^^^^

OpenCPU is a system for embedded scientific computing and reproducible
research. The OpenCPU server provides a reliable and interoperable HTTP API
for data analysis based on R. You can either use the public servers or host
your own.
The OpenCPU JavaScript client library provides the most seamless integration
of R and JavaScript available today. Enjoy simple RPC and data I/O through
standard Ajax techniques.
No need to learn crazy widgets or obscure framworks. The OpenCPU API is a
clean and simple interface to R, nothing more nothing less. It is compatible
with any language or framework that speaks HTTP. 

https://www.opencpu.org/[+https://www.opencpu.org/+]

https://github.com/jeroenooms/opencpu[+https://github.com/jeroenooms/opencpu+]

http://arxiv.org/abs/1406.4806[+http://arxiv.org/abs/1406.4806+]

plotKML
^^^^^^^

A suite of functions for converting sp-class objects into KML or KMZ documents
for use in Google Earth.  
Visualization of spatial and spatio-temporal objects in Google Earth

http://plotkml.r-forge.r-project.org/[+http://plotkml.r-forge.r-project.org/+]

http://cran.r-project.org/web/packages/plotKML/index.html[+http://cran.r-project.org/web/packages/plotKML/index.html+]

http://www.jstatsoft.org/v63/i05[+http://www.jstatsoft.org/v63/i05+]

[[PTAk]]
PTAk
^^^^

A multiway method to decompose a tensor (array) of any order, as a
generalisation of SVD also supporting non-identity metrics and penalisations.
2-way SVD with these extensions is also available. The package includes also
some other multiway methods: PCAn (Tucker-n) and PARAFAC/CANDECOMP with these
extensions.

http://cran.r-project.org/web/packages/PTAk/[+http://cran.r-project.org/web/packages/PTAk/+]

http://www.jstatsoft.org/v34/i10/[+http://www.jstatsoft.org/v34/i10/+]

r-hht
^^^^^

This package builds on the EMD package to provide additional tools for
empirical mode decomposition (EMD) and Hilbert spectral analysis. It also
implements the ensemble empirical decomposition (EEMD) and the complete
ensemble empirical mode decomposition (CEEMD) methods to avoid mode mixing and
intermittency problems found in EMD analysis. The package comes with several
plotting methods that can be used to view intrinsic mode functions, the HHT
spectrum, and the Fourier spectrum. To see the version history and download
the bleeding-edge version (at your own risk!), see the project website at
code.google.com below. See the other links for PDF files describing numerical
and exact analytical methods for determining instantaneous frequency, some
examples of signals processed with this package, and some examples of the
ensemble empirical mode decomposition method.

http://cran.r-project.org/web/packages/hht/index.html[+http://cran.r-project.org/web/packages/hht/index.html+]

https://code.google.com/p/r-hht/[+https://code.google.com/p/r-hht/+]

http://www.unc.edu/i\~haksaeng/hht/eemd_examples.pdf[+http://www.unc.edu/~haksaeng/hht/eemd_examples.pdf+]

http://www.unc.edu/i\~haksaeng/hht/interesting_signals.pdf[+http://www.unc.edu/~haksaeng/hht/interesting_signals.pdf+]

Rlibeemd
^^^^^^^^

An R interface for C library libeemd for performing the ensemble empirical
mode decomposition (EEMD), its complete variant (CEEMDAN) or the regular
empirical mode decomposition (EMD).

http://cran.r-project.org/web/packages/Rlibeemd/index.html[+http://cran.r-project.org/web/packages/Rlibeemd/index.html+]

Rserve
^^^^^^

Rserve is a TCP/IP server which allows other programs to use facilities of R
(see www.r-project.org) from various languages without the need to initialize
R or link against R library. Every connection has a separate workspace and
working directory. Client-side implementations are available for popular
languages such as C/Cxx, PHP and Java. Rserve supports remote connection,
authentication and file transfer. Typical use is to integrate R backend for
computation of statstical models, plots etc. in other applications. 

http://www.rforge.net/Rserve/[+http://www.rforge.net/Rserve/+]

Shiny
^^^^^

Shiny makes it incredibly easy to build interactive web applications with R.
Automatic "reactive" binding between inputs and outputs and extensive
pre-built widgets make it possible to build beautiful, responsive, and
powerful applications with minimal effort.

http://cran.r-project.org/web/packages/shiny/index.html[+http://cran.r-project.org/web/packages/shiny/index.html+]

https://github.com/rstudio/shiny[+https://github.com/rstudio/shiny+]

http://shiny.rstudio.com/[+http://shiny.rstudio.com/+]

slp
^^^

Discrete Prolate Spheroidal Sequence (Slepian) Regression Smoothers.

https://github.com/wesleyburr/slp[+https://github.com/wesleyburr/slp+]

smr
^^^

Package for discrete Morse-Smale complex approximation based on kNN graph. The
Morse-Smale complex provides a decomposition of the domain. This package
provides methods to compute a hierarchical sequence of Morse-Smale complicies
and tools that exploit this domain decomposition for regression and
visualization of scalar functions.

http://cran.r-project.org/web/packages/msr/index.html[+http://cran.r-project.org/web/packages/msr/index.html+]

SpatPCA
^^^^^^^

This package provides regularized principal component analysis incorporating
smoothness, sparseness and orthogonality of eigenfunctions by using
alternating direction method of multipliers (ADMM) algorithm.

http://cran.r-project.org/web/packages/SpatPCA/index.html[+http://cran.r-project.org/web/packages/SpatPCA/index.html+]

TSclust
^^^^^^^

This package contains a set of measures of dissimilarity between time series
to perform time series clustering. Metrics based on raw data, on generating
models and on the forecast behavior are implemented. Some additional utilities
related to time series clustering are also provided, such as clustering
algorithms and cluster evaluation metrics.

https://github.com/jeffwong/tsclust[+https://github.com/jeffwong/tsclust+]

http://cran.r-project.org/web/packages/TSclust/index.html[+http://cran.r-project.org/web/packages/TSclust/index.html+]

http://www.jstatsoft.org/v62/i01[+http://www.jstatsoft.org/v62/i01+]

W2CWM2C
^^^^^^^

The W2CWM2C package is a set of functions to produce new graphical tools for
wavelet correlation (bivariate and multivariate cases) using some routines
from the waveslim and wavemulcor packages.

http://cran.r-project.org/web/packages/W2CWM2C/index.html[+http://cran.r-project.org/web/packages/W2CWM2C/index.html+]

http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6894486[+http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6894486+]

WaveletComp
^^^^^^^^^^^

Wavelet analysis and reconstruction of time series, cross-wavelets and
phase-difference (with filtering options), significance with simulation
algorithms.

http://cran.r-project.org/web/packages/WaveletComp/index.html[+http://cran.r-project.org/web/packages/WaveletComp/index.html+]

waveslim
^^^^^^^^

Basic wavelet routines for time series (1D), image (2D) and array (3D)
analysis. The code provided here is based on wavelet methodology developed in
Percival and Walden (2000); Gencay, Selcuk and Whitcher (2001); the dual-tree
complex wavelet transform (DTCWT) from Kingsbury (1999, 2001) as implemented
by Selesnick; and Hilbert wavelet pairs (Selesnick 2001, 2002).

http://cran.r-project.org/web/packages/waveslim/index.html[+http://cran.r-project.org/web/packages/waveslim/index.html+]

wux
^^^

Methods to calculate and interpret climate change signals and time series from
climate multi-model ensembles. Climate model output in binary NetCDF format is
read in and aggregated over a specified region to a data.frame for statistical
analysis. Global circulation models (GCMs), as the CMIP5 or CMIP3 simulations,
can be read in the same way as Regional Climate Models (RCMs), as e.g. the
CORDEX or ENSEMBLES simulations.

http://cran.r-project.org/web/packages/wux/index.html[+http://cran.r-project.org/web/packages/wux/index.html+]

yuima
^^^^^

Simulation and Inference for Stochastic Differential Equations.
The YUIMA Project is an open source and collaborative effort aimed at
developing the R package yuima for simulation and inference of stochastic
differential equations. In the yuima package stochastic differential equations
can be of very abstract type, multidimensional, driven by Wiener process or
fractional Brownian motion with general Hurst parameter, with or without jumps
specified as Lévy noise. The yuima package is intended to offer the basic
infrastructure on which complex models and inference procedures can be built
on. This paper explains the design of the yuima package and provides some
examples of applications.

http://www.jstatsoft.org/v57/i04[+http://www.jstatsoft.org/v57/i04+]

http://cran.r-project.org/web/packages/yuima/index.html[+http://cran.r-project.org/web/packages/yuima/index.html+]

[[rambrain]]
rambrain
~~~~~~~~

A user space library that manages memory consumption of your code. Using Rambrain you can overcommit memory over the size of physical memory present in the system. Rambrain takes care of temporarily swapping out data to disk and can handle multiples of the physical memory size present. Rambrain is thread-safe, OpenMP and MPI compatible and supports Asynchronous IO. The library was designed to require minimal changes to existing programs and to be easy to use.

https://github.com/mimgrund/rambrain[+https://github.com/mimgrund/rambrain+]

*Rambrain - a library for virtually extending physical memory* - http://arxiv.org/abs/1510.06358[+http://arxiv.org/abs/1510.06358+]

RaPyDLI
~~~~~~~

The Rapid Python Deep Learning Infrastructure (RaPyDLI) project is based on
the objective to combine high level Python, C/Cxx and Java environments with
carefully designed libraries supporting GPU accelerators and MIC coprocessors.
Interactive analysis and visualization will be supported together with scaling
from the current terabyte size to Petabyte datasets to enable substantial
progress in the complexity and capability of the DL applications. A broad
range of storage models will be supported including network file systems,
databases and HDFS. The partnership of Indiana University, University of
Tennessee Knoxville, and Stanford University combines leaders in parallel
computing algorithms and run times, Big Data, clouds, and deep learning. 

http://salsaproj.indiana.edu/RaPyDLI/[+http://salsaproj.indiana.edu/RaPyDLI/+]

Rasdaman
~~~~~~~~

Array Databases allow storing and querying massive multi-dimensional
arrays, such as sensor, image, simulation, and statistics data appearing
in domains like earth, space, and life science.

The rasdaman ("raster data manager") is the leading array analytics engine
distinguished by its flexibility, performance, and scalability. Rasdaman
embeds itself smoothly into PostgreSQL, but can also run standalone on
file systems. In fact, rasdaman has pioneered Array Databases being the first
fully implemented, operationally used system with an array query language and
optimized processing engine; known rasdaman databases exceed 230 TB. 

http://rasdaman.org/[+http://rasdaman.org/+]

https://wiki.52north.org/bin/view/Projects/GSoC2014SosRasdaman[+https://wiki.52north.org/bin/view/Projects/GSoC2014SosRasdaman+]

Petascope
^^^^^^^^^

The petascope component of rasdaman implements the OGC interface
standards WCS 2.0, WCS-T 1.4, WCPS 1.0, WPS 1.0, and
WMS 1.1. For this purpose, petascope maintains its additional metadata
(such as georeferencing) which is kept in separate relational tables. Note
that not all rasdaman raster objects and collections are available through
petascope by default; rather, they need to be registered through the petascope
administration interface.

Petascope is implemented as a war file of servlets which give access to
coverages (in the OGC sense) stored in rasdaman. Internally, incoming
requests requiring coverage evaluation are translated into rasql queries by
petascope. These queries are passed on to rasdaman, which constitutes the
central workhorse. Results returned from rasdaman are forwarded to the client,
finally. 

http://kahlua.eecs.jacobs-university.de/trac/rasdaman/wiki/PetascopeUserGuide[+http://kahlua.eecs.jacobs-university.de/trac/rasdaman/wiki/PetascopeUserGuide+]

[[Raspberry_Pi]]
Raspberry Pi
~~~~~~~~~~~~

*Measuring Temperature with RethinkDB and Raspberry Pi* - http://rethinkdb.com/blog/temperature-sensors-and-a-side-of-pi/[+http://rethinkdb.com/blog/temperature-sensors-and-a-side-of-pi/+]

[[RaspberryCloud]]
RaspberryCloud
^^^^^^^^^^^^^^

A fully functioning command-line Dropbox client built using the Dropbox Python API for Raspberry Pi. Features include downloading entire directories as zip files, editing files in a local editor, uploading/downloading, and many many more.

https://github.com/JaredMHall/RaspberryCloud[+https://github.com/JaredMHall/RaspberryCloud+]

[[Raspbian]]
Raspbian
^^^^^^^^

A free operating system based on Debian optimized for the Raspberry Pi hardware. An operating system is the set of basic programs and utilities that make your Raspberry Pi run. However, Raspbian provides more than a pure OS: it comes with over 35,000 packages, pre-compiled software bundled in a nice format for easy installation on your Raspberry Pi.

https://www.raspbian.org/[+https://www.raspbian.org/+]

Raster Numpy Basics
~~~~~~~~~~~~~~~~~~~

IPython notebook tutorial on nbviewer.

http://nbviewer.ipython.org/gist/perrygeo/2147bae876b64f39246d[+http://nbviewer.ipython.org/gist/perrygeo/2147bae876b64f39246d+]

RBF-QR
~~~~~~

A MATLAB implementation of the RBF-QR method for radial basis function
interpolation in the small shape parameter range.

http://www.it.uu.se/research/scicomp/software/rbf_qr[+http://www.it.uu.se/research/scicomp/software/rbf_qr+]

Recki-CT
~~~~~~~~

Recki-CT is a set of tools that implement a PHP compiler, in PHP. 
It doesn’t provide a VM, so it can’t run PHP by itself. However, it can parse
PHP code and generate other code from it.
Recki uses the well-known PHP-Parser library to generate a graph-based
representation of the code, and convert it to an intermediate representation.
This intermediate form is pretty low-level, and it is comparatively simple to
generate code from it for a variety of targets. One of the targets Recki can
use is a second component, JitFu, which is a PHP extension allowing us to
generate machine code at run time.

http://mgdm.net/weblog/php-at-the-speed-of-c/[+http://mgdm.net/weblog/php-at-the-speed-of-c/+]

https://github.com/google/recki-ct[+https://github.com/google/recki-ct+]

JIT-Fu
^^^^^^

JIT-Fu is a PHP extension that exposes an OO API for the creation of native
instructions to PHP userland, using libjit.

https://github.com/krakjoe/jitfu[+https://github.com/krakjoe/jitfu+]

LibJIT
^^^^^^

LibJIT is a library that provides generic Just-In-Time compiler functionality
independent of any particular bytecode, language, or runtime.
The goal of the libjit project is to provide an extensive set of routines that
takes care of the bulk of the JIT process, without tying the programmer down
with language specifics. Where we provide support for common object models, we
do so strictly in add-on libraries, not as part of the core code. 

https://www.gnu.org/software/libjit/[+https://www.gnu.org/software/libjit/+]

[[rb]]
rb
^^

A library that implements non-replicated sharding for redis. It implements a custom routing system on top of python redis that allows you to automatically target different servers without having to manually route requests to the individual nodes.

http://rb.readthedocs.org/en/latest/[+http://rb.readthedocs.org/en/latest/+]

RLaB
~~~~

RLaB is an interactive, interpreted scientific programming environment for
fast numerical prototyping and program development. rlabplus provides the
third release of the environment for 32- and 64-bit linux systems on Intel and
ARM/RaspberryPi architectures. The environment integrates large number of
numerical solvers and functions from various sources, most notably from the
Gnu Scientific Library (GSL) and from the netlib. Within the environment it is
possible to visualize data using gnuplot, xmgrace, and pgplot xor plplot; get
and post data using uniform resource locator implementing HDF5 or world wide
web; and control serial, GPIB or TCP/IP connection. RLaB supports embedded
python, java and xref:ngspice[ngspice] interpreters.
RLaB was created by Ian Searle and collaborators. rlabplus is being actively
developed by Marijan Kostrun. 

http://rlabplus.sourceforge.net/[+http://rlabplus.sourceforge.net/+]

Rodinia
~~~~~~~

A vision of heterogeneous computer systems that incorporate diverse
accelerators and automatically select the best
computational unit for a particular task is widely shared among researchers
and many industry analysts; however,
there are no agreed-upon benchmarks to support the research needed in the
development of such a platform. There
are many suites for parallel computing on general-purpose CPU architectures,
but accelerators fall into a gap that is
not covered by previous benchmark development. Rodinia is released to address
this concern. 

http://www.cs.virginia.edu/~skadron/wiki/rodinia/index.php/Main_Page[+http://www.cs.virginia.edu/~skadron/wiki/rodinia/index.php/Main_Page+]

http://lava.cs.virginia.edu/Rodinia/download_links.htm[+http://lava.cs.virginia.edu/Rodinia/download_links.htm+]

ROS
~~~

The Robot Operating System (ROS) is a flexible framework for writing robot
software. It is a collection of tools, libraries, and conventions that aim to
simplify the task of creating complex and robust robot behavior across a wide
variety of robotic platforms. 

http://www.ros.org/[+http://www.ros.org/+]

rp
~~

This professional scientific software computes recurrence plots, cross
recurrence plots, joint recurrence plots and recurrence quantification
analysis on commandline of Unix and DOS/DOS-emulated systems. It is able to
work with really long data series. However, the output of the results (plots)
have to be prepared with external programmes (e.g. gnuplot or Matlab).

The state space trajectory can be reconstructed from single time-series by
time-delay embedding. Alternatively, the columns of input data can be used as
the components of the state space vectors. 

http://tocsy.pik-potsdam.de/commandline-rp.php[+http://tocsy.pik-potsdam.de/commandline-rp.php+]

http://www.recurrence-plot.tk/[+http://www.recurrence-plot.tk/+]

RPerl
~~~~~

RPerl is an upgrade to the popular Perl 5 programming language. RPerl gives
software developers a compiler to make their apps run really fast on parallel
computing platforms like multi-core processors, the cloud, clusters, and
supercomputers. RPerl stands for Restricted Perl, in that we restrict our use
of Perl to those parts which can be made to run fast.

The input to the RPerl compiler is low-magic Perl 5 source code. RPerl
converts the low-magic Perl 5 source code into Cxx source code using Perl
and/or Cxx data structures. Inline::CPP converts the Cxx source code into XS
source code. Perl's XS tools and a standard Cxx compiler convert the XS source
code into machine-readable binary code, which can be directly linked back into
normal high-magic Perl 5 source code.

The output of the RPerl compiler is fast-running binary code that is exactly
equivalent to, and compatible with, the original low-magic Perl 5 source code
input. The net effect is that RPerl compiles slow low-magic Perl 5 code into
fast binary code, which can optionally be mixed back into high-magic Perl
apps.

https://github.com/wbraswell/rperl[+https://github.com/wbraswell/rperl+]

http://rperl.org/[+http://rperl.org/+]

RPython
~~~~~~~

A  translation and support framework for producing implementations of dynamic languages, emphasizing a clean separation between language specification and implementation aspects.
By separating concerns in this way, our implementation of Python - and other dynamic languages - is able to automatically generate a Just-in-Time compiler for any dynamic language. It also allows a mix-and-match approach to implementation decisions, including many that have historically been outside of a user’s control, such as target platform, memory and threading models, garbage collection strategies, and optimizations applied, including whether or not to have a JIT in the first place.

http://rpython.readthedocs.org/en/latest/[+http://rpython.readthedocs.org/en/latest/+]

*The Magic of RPython* - http://kirbyfan64.github.io/posts/the-magic-of-rpython.html[+http://kirbyfan64.github.io/posts/the-magic-of-rpython.html+]

RSVDPACK
~~~~~~~~

This document describes an implementation in C of a set of randomized
algorithms for computing partial Singular Value Decompositions (SVDs). The
techniques largely follow the prescriptions in the article "Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix
decompositions," N. Halko, P.G. Martinsson, J. Tropp, SIAM Review, 53(2),
2011, pp. 217-288, but with some modifications to improve performance. The
codes implement a number of low rank SVD computing routines for three
different sets of hardware: (1) single core CPU, (2) multi core CPU, and (3)
massively multicore GPU. 

http://arxiv.org/abs/1502.05366[+http://arxiv.org/abs/1502.05366+]

https://github.com/sergeyvoronin/LowRankSVDCodes[+https://github.com/sergeyvoronin/LowRankSVDCodes+]

http://arxiv.org/abs/0909.4061[+http://arxiv.org/abs/0909.4061+]

RTL-SDR
~~~~~~~

RTL-SDR is a very cheap software defined radio that uses a DVB-T TV tuner
dongle based on the RTL2832U chipset. With the combined efforts of Antti
Palosaari, Eric Fry and Osmocom it was found that the signal I/Q data could be
accessed directly, which allowed the DVB-T TV tuner to be converted into a
wideband software defined radio via a new software driver.

Essentially, this means that a cheap $20 TV tuner USB dongle with the RTL2832U
chip can be used as a computer based radio scanner. This sort of scanner
capability would have cost hundreds or even thousands just a few years ago.
The RTL-SDR is also often referred to as RTL2832U, DVB-T SDR, or the “$20
Software Defined Radio”.

http://www.rtl-sdr.com/about-rtl-sdr/[+http://www.rtl-sdr.com/about-rtl-sdr/+]

http://www.reddit.com/r/RTLSDR[+http://www.reddit.com/r/RTLSDR+]

CubicSDR
^^^^^^^^

A new cross platform SDR receiver which is based on the liquid-dsp libraries.

https://github.com/cjcliffe/CubicSDR[+https://github.com/cjcliffe/CubicSDR+]

gr-osmosdr
^^^^^^^^^^

OsmoSDR is a small form-factor inexpensive SDR (Software Defined Radio)
project. 
If you are familiar with existing SDR receivers, then OsmoSDR can be thought
of something in between a FunCube Dongle (only 96kHz bandwidth) and a
USRP (much more expensive). For a very cheap SDR (with limited dynamic range),
you can use the DVB-T USB stick using the RTL2832U chip, as documented in
rtl-sdr. 
It consists of a USB-attached Hardware, associated Firmware as well as
GrOsmoSDR gnuradio integration on the PC.

The gr-osmosdr software is a GNU Radio block to work with OsmoSDR and rtl-sdr,
although it also supports at least a dozen other types of hardware.

http://sdr.osmocom.org/trac/[+http://sdr.osmocom.org/trac/+]

http://sdr.osmocom.org/trac/wiki/GrOsmoSDR[+http://sdr.osmocom.org/trac/wiki/GrOsmoSDR+]

liquid-dsp
^^^^^^^^^^

A digital signal processing (DSP) library designed specifically for
software-defined radios on embedded platforms. The aim is to provide a
lightweight DSP library that does not rely on a myriad of external
dependencies or proprietary and otherwise cumbersome frameworks. All signal
processing elements are designed to be flexible, scalable, and dynamic,
including filters, filter design, oscillators, modems, synchronizers, and
complex mathematical operations.

https://github.com/jgaeddert/liquid-dsp[+https://github.com/jgaeddert/liquid-dsp+]

http://liquidsdr.org/[+http://liquidsdr.org/+]

rtl-sdr
^^^^^^^

Software to turn the RTL2832U into a SDR.
Much software is available for the RTL2832. Most of the user-level packages
rely on the librtlsdr library which comes as part of the rtl-sdr codebase.
This codebase contains both the library itself and also a number of command
line tools such as rtl_test, rtl_sdr, rtl_tcp, and rtl_fm. These command line
tools use the library to test for the existence of RTL2832 devices and to
perform basic data transfer functions to and from the device.

At the user level, there are several options for interacting with the
hardware. The rtl-sdr codebase contains a basic FM receiver program that
operates from the command line. The rtl_fm program is a command line tool that
can initialize the RTL2832, tune to a given frequency, and output the received
audio to a file or pipe the output to command line audio players such as the
alsa aplay or the sox play commands. There is also the rtl_sdr program that
will output the raw I-Q data to a file for more basic analysis. 

If you want to do more advanced experiments, the GNU Radio collection of
tools can be used to build custom radio devices. GNU Radio can be used both
from a GUI perspective in which you can drag-and-drop radio components to
build a radio and also programmatically where software programs written in C
or Python are created that directly reference the internal GNU Radio
functions.

The use of GNU Radio is attractive because of the large number of pre-built
functions that can easily be connected together. However, be aware that this
is a large body of software with dependencies on many libraries. Thankfully
there is a simple script that will perform the installation but still, the
time required can be on the order of hours. When starting out, it might be
good to try the command line programs that come with the rtl-sdr package first
and then install the GNU Radio system later. 

http://sdr.osmocom.org/trac/wiki/rtl-sdr[+http://sdr.osmocom.org/trac/wiki/rtl-sdr+]

http://git.osmocom.org/rtl-sdr/[+http://git.osmocom.org/rtl-sdr/+]

Ruby
~~~~

This...

[[Prawn]]
Prawn
^^^^^

Prawn is a nimble PDF writer for Ruby. More important, it’s a hackable
platform that offers both high level APIs for the most common needs and low
level APIs for bending the document model to accomodate special circumstances.

With Prawn, you can write text, draw lines and shapes and place images
anywhere on the page and add as much color as you like. In addition, it brings
a fluent API and aggressive code re-use to the printable document space.

http://prawnpdf.org/api-docs/2.0/[+http://prawnpdf.org/api-docs/2.0/+]

https://github.com/asciidoctor/asciidoctor-pdf[+https://github.com/asciidoctor/asciidoctor-pdf+]

RxPY
~~~~

Reactive extensions to Python.

The Reactive Extensions for Python (RxPY) is a set of libraries for composing
asynchronous and event-based programs using observable sequences and
LINQ-style query operators in Python. Using Rx, developers represent
asynchronous data streams with Observables, query asynchronous data streams
using LINQ operators, and parameterize the concurrency in the asynchronous
data streams using Schedulers. Simply put, Rx = Observables + LINQ +
Schedulers.

https://github.com/ReactiveX/RxPY[+https://github.com/ReactiveX/RxPY+]

[[NS]]
////
NSSS
////

[[SageManifolds]]
SageManifolds
~~~~~~~~~~~~~

SageManifolds is a package under development for the modern computer algebra
system Sage, implementing differential geometry and tensor calculus.

SageManifolds deals with real differentiable manifolds of arbitrary dimension.
The basic objects are tensor fields and not tensor components in a given
vector frame or coordinate chart. In other words, various charts and frames
can be introduced on the manifold and a given tensor field can have
representations in each of them.

An important class of treated manifolds is that of pseudo-Riemannian
manifolds, among which Riemannian manifolds and Lorentzian manifolds, with
applications to General Relativity. In particular, SageManifolds implements
the computation of the Riemann curvature tensor and associated objects (Ricci
tensor, Weyl tensor). SageManifolds can also deal with generic affine
connections, not necessarily Levi-Civita ones. 

The SageManifolds project aims at extending the mathematics software system
Sage towards differential geometry and tensor calculus. Like Sage,
SageManifolds is free, open-source and is based on the Python programming
language. We discuss here some details of the implementation, which relies on
Sage's parent/element framework, and present a concrete example of use. 

http://arxiv.org/abs/1412.4765[+http://arxiv.org/abs/1412.4765+]

http://sagemanifolds.obspm.fr/[+http://sagemanifolds.obspm.fr/+]

Sailfish
~~~~~~~~

Sailfish is an open source (LGPL) fluid dynamics solver based on the lattice
Boltzmann method (LBM). It is uses run-time code generation techniques to
automatically generate optimized, simulation specific code for GPU devices
(both CUDA and OpenCL targets are supported).
Documentation

https://github.com/sailfish-team/sailfish[+https://github.com/sailfish-team/sailfish+]

http://code.google.com/p/sailfish-cfd/[+http://code.google.com/p/sailfish-cfd/+]

http://arxiv.org/abs/1311.2404[+http://arxiv.org/abs/1311.2404+]

http://sailfish.us.edu.pl/[+http://sailfish.us.edu.pl/+]

http://www.sciencedirect.com/science/article/pii/S0010465514001520[+http://www.sciencedirect.com/science/article/pii/S0010465514001520+]

[[SAMRAI]]
SAMRAI
~~~~~~

The Center for Applied Scientific Computing (CASC) at Lawrence Livermore
National Laboratory is developing algorithms and software technology to enable
the application of structured adaptive mesh refinement (SAMR) to large-scale
multi-physics problems relevant to U.S. Department of Energy programs. 

SAMRAI (Structured Adaptive Mesh Refinement Application Infrastructure) is an
object-oriented Cxx software library enables exploration of numerical,
algorithmic, parallel computing, and software issues associated with applying
structured adaptive mesh refinement (SAMR) technology in large-scale parallel
application development. SAMRAI provides software tools for developing SAMR
applications that involve coupled physics models, sophisticated numerical
solution methods, and which require high-performance parallel computing
hardware. SAMRAI enables integration of SAMR technology into existing codes
and simplifies the exploration of SAMR methods in new application domains. Due
to judicious application of object-oriented design, SAMRAI capabilities are
readily enhanced and extended to meet specific problem requirements. The
SAMRAI team collaborates with application researchers at LLNL and other
institutions. These interactions motivate the continued evolution of the
SAMRAI library. 

http://computation.llnl.gov/project/SAMRAI/index.php[+http://computation.llnl.gov/project/SAMRAI/index.php+]

http://computation.llnl.gov/project/SAMRAI/software.php[+http://computation.llnl.gov/project/SAMRAI/software.php+]

[[SatNOGS]]
SatNOGS
~~~~~~~

A complete platform of an Open Source Networked Ground Station. The scope of the project is to create a full stack of open technologies based on open standards , and the construction of a full ground station as a showcase of the stack.

https://satnogs.org/[+https://satnogs.org/+]

Scala
~~~~~

An object-functional programming language for general software
applications. Scala has full support for functional programming and a very
strong static type system. This allows programs written in Scala to be very
concise and thus smaller in size than other general-purpose programming
languages. Many of Scala's design decisions were inspired by criticism over
the shortcomings of Java.

Scala source code is intended to be compiled to Java bytecode, so that the
resulting executable code runs on a Java virtual machine. Java libraries may
be used directly in Scala code and vice versa (Language interoperability).[8]
Like Java, Scala is object-oriented, and uses a curly-brace syntax reminiscent
of the C programming language. Unlike Java, Scala has many features of
functional programming languages like Scheme, Standard ML and Haskell,
including currying, type inference, immutability, lazy evaluation, and pattern
matching. It also has an advanced type system supporting algebraic data types,
covariance and contravariance, higher-order types, and anonymous types. Other
features of Scala not present in Java include operator overloading, optional
parameters, named parameters, raw strings, and no checked exceptions.

http://www.scala-lang.org/index.html[+http://www.scala-lang.org/index.html+]

Saddle
^^^^^^

Saddle is a data manipulation library for Scala that provides array-backed,
indexed, one- and two-dimensional data structures that are judiciously
specialized on JVM primitives to avoid the overhead of boxing and unboxing.

Saddle offers vectorized numerical calculations, automatic alignment of data
along indices, robustness to missing (N/A) values, and facilities for I/O.

Saddle draws inspiration from several sources, among them the R programming
language & statistical environment, the numpy and pandas Python libraries, and
the Scala collections library.

http://saddle.github.io/[+http://saddle.github.io/+]

Scala.js
^^^^^^^^

A Scala to JavaScript compiler.
Scala.js compiles Scala code to JavaScript, allowing you to write your web
application entirely in Scala.

http://www.scala-js.org/[+http://www.scala-js.org/+]

http://scala-lang.org/news/2015/02/05/scala-js-no-longer-experimental.html[+http://scala-lang.org/news/2015/02/05/scala-js-no-longer-experimental.html+]

http://www.scala-js.org/news/2015/02/05/announcing-scalajs-0.6.0/[+http://www.scala-js.org/news/2015/02/05/announcing-scalajs-0.6.0/+]

http://www.scala-js.org/doc/tutorial.html[+http://www.scala-js.org/doc/tutorial.html+]

ScalaLab
^^^^^^^^

The ScalaLab project aims to provide an efficient scientific programming
environment for the Java Virtual Machine. The scripting language is based on
the Scala programming language enhanced with high level scientific operators
and with an integrated environment that provides a MATLAB-like working style.
Also, all the huge libraries of Java scientific code can be easily accessible
(and many times with a more convenient syntax). The main potential of the
ScalaLab is numerical code speed and flexibility. The statically typed Scala
language can provide speeds of scripting code similar to pure Java. A major
design priority of ScalaLab is its user-friendly interface. We like the user
to enjoy writing scientific code, and with this objective we design the whole
framework. 

http://code.google.com/p/scalalab/[+http://code.google.com/p/scalalab/+]

http://sourceforge.net/projects/scalalab/[+http://sourceforge.net/projects/scalalab/+]

ScalaNLP
^^^^^^^^

A suite of machine learning and numerical computing libraries.
ScalaNLP is the umbrella project for several libraries, including Breeze and
Epic. Breeze is a set of libraries for machine learning and numerical
computing. Epic is a high-performance statistical parser and structured
prediction library.

http://www.scalanlp.org/[+http://www.scalanlp.org/+]

ScMathML
^^^^^^^^

ScMathML is a Scala library for executing Content MathML.
Content MathML is a move towards a standard, open format for representing
mathematics with relatively well defined semantics.
ScMathML takes formulas, and evaluates them in a Context, which provides
access to domain objects, constants etc.

https://bitbucket.org/mo_seph/scmathml/wiki/Home[+https://bitbucket.org/mo_seph/scmathml/wiki/Home+]

sensor-web-harvester
^^^^^^^^^^^^^^^^^^^^

A Scala project that harvests sensor data from web sources. The data is then
pushed to an SOS using the sos-injection module project. SosInjector is a
project that wraps an Sensor Observation Service (SOS). The sos-injection
module provides Java classes to enter stations, sensors, and observations into
an SOS.

sensor-web-harvester is used to fill an SOS with observations from many
well-known sensor sources (such as NOAA and NERRS). This project pulls sensor
observation values from the source’s stations. It then formats the data to be
placed into the user’s SOS by using the sos-injector. The source stations used
are filtered by a chosen bounding box area. 

https://github.com/asascience-open/sensor-web-harvester[+https://github.com/asascience-open/sensor-web-harvester+]

Schur
~~~~~

Schur is a stand alone C program for interactively calculating properties of
Lie groups and symmetric functions. Schur has been designed to answer
questions of relevance to a wide range of problems of special interest to
chemists, mathematicians and physicists - particularly for persons who need
specific knowledge relating to some aspect of Lie groups or symmetric
functions and yet do not wish to be encumbered with complex algorithms. The
objective of Schur is to supply results with the complexity of the algorithms
hidden from view so that the user can effectively use Schur as a scratch pad,
obtaining a result and then using that result to derive new results in a fully
interactive manner. Schur can be used as a tool for calculating branching
rules, Kronecker products, Casimir invariants, dimensions, plethysms,
S-function operations, Young diagrams and their hook lengths etc.

As well as being a research tool Schur forms an excellent tool for helping
students to independently explore the properties of Lie groups and symmetric
functions and to test their understanding by creating simple examples and
moving on to more complex examples. The user has at his or her disposal over
160 commands which may be nested to give a vast variety of potential
operations. Every command, with examples, is described in a 200 page manual.
Attention has been given to input/output issues to simplify input and to give
a well organized output. The output may be obtained in TeX form if desired.
Log files may be created for subsequent editing. On line help files may be
brought to screen at any time. 

http://schur.sourceforge.net/[+http://schur.sourceforge.net/+]

SciRuby
~~~~~~~

The SciRuby Project aims to provide Ruby with scientific capabilities similar
to what the wonderful NumPy and SciPy libraries bring to Python. Our goal is
to provide a complete suite of statistical, numerical, and visualization
software tools for scientific computing.

http://sciruby.com/[+http://sciruby.com/+]

http://google-opensource.blogspot.com/2015/01/google-summer-of-code-wrap-up-sciruby.html[+http://google-opensource.blogspot.com/2015/01/google-summer-of-code-wrap-up-sciruby.html+]

SciTools Github
~~~~~~~~~~~~~~~

https://github.com/SciTools[+https://github.com/SciTools+]

security
~~~~~~~~

My First 5 Minutes On A Server; Or, Essential Security for Linux Servers

EncFS
^^^^^

EncFS provides an encrypted filesystem in user-space. It runs with regular
user permissions using the xref:FUSE[FUSE] library. 

https://vgough.github.io/encfs/[+https://vgough.github.io/encfs/+]

http://en.wikipedia.org/wiki/EncFS[+http://en.wikipedia.org/wiki/EncFS+]

https://github.com/vgough/encfs[+https://github.com/vgough/encfs+]

Fail2ban
^^^^^^^^

Fail2ban scans log files (e.g. /var/log/apache/error_log) and bans IPs that
show the malicious signs -- too many password failures, seeking for exploits,
etc. Generally Fail2Ban is then used to update firewall rules to reject the IP
addresses for a specified amount of time, although any arbitrary other action
(e.g. sending an email) could also be configured. Out of the box Fail2Ban
comes with filters for various services (apache, courier, ssh, etc).

Fail2Ban is able to reduce the rate of incorrect authentications attempts
however it cannot eliminate the risk that weak authentication presents.
Configure services to use only two factor or public/private authentication
mechanisms if you really want to protect services. 

http://www.fail2ban.org/wiki/index.php/Main_Page[+http://www.fail2ban.org/wiki/index.php/Main_Page+]

[[shells]]
shells
~~~~~~

[[svsh]]
svsh
^^^^

A command line shell for process supervision suites of the daemontools family. Currently, it supports daemontools, perp, s6 and runit. It provides a unified interface allowing easy inspection and manipulation of services (i.e. processes) managed by these supervisors.

http://ido50.github.io/Svsh/[+http://ido50.github.io/Svsh/+]

Simbody
~~~~~~~

This project is a SimTK toolset providing general multibody dynamics
capability, that is, the ability to solve Newton's 2nd law F=ma in any set of
generalized coordinates subject to arbitrary constraints. (That's Isaac
himself in the oval.) Simbody is provided as an open source, object-oriented
Cxx API and delivers high-performance, accuracy-controlled
science/engineering-quality results.

Simbody uses an advanced Featherstone-style formulation of rigid body
mechanics to provide results in Order(n) time for any set of n generalized
coordinates. This can be used for internal coordinate modeling of molecules,
or for coarse-grained models based on larger chunks. It is also useful for
large-scale mechanical models, such as neuromuscular models of human gait,
robotics, avatars, and animation. Simbody can also be used in real time
interactive applications for biosimulation as well as for virtual worlds and
games.

This toolset was developed originally by Michael Sherman at the Simbios Center
at Stanford, with major contributions from Peter Eastman and others. Simbody
descends directly from the public domain NIH Internal Variable Dynamics Module
(IVM) facility for molecular dynamics developed and kindly provided by Charles
Schwieters. IVM is in turn based on the spatial operator algebra of Rodriguez
and Jain from NASA's Jet Propulsion Laboratory (JPL), and Simbody has adopted
that formulation.

See also xref:PyCraft[PyCraft]

https://github.com/simbody/simbody[+https://github.com/simbody/simbody+]

https://simtk.org/home/simbody[+https://simtk.org/home/simbody+]

https://simtk.org/docman/view.php/47/1589/Sherman-2011-SethDelp-Simbody-ProcediaIUTAM-v2-p241.pdf[+https://simtk.org/docman/view.php/47/1589/Sherman-2011-SethDelp-Simbody-ProcediaIUTAM-v2-p241.pdf+]

https://simtk.org/docman/view.php/47/231/SimbodyTheoryManual.pdf[+https://simtk.org/docman/view.php/47/231/SimbodyTheoryManual.pdf+]

SIMD.js
~~~~~~~

SIMD.js is a new API being developed by Intel, Google, and Mozilla for
JavaScript which introduces several new types and functions for doing SIMD
computations. For example, the Float32x4 type represents 4 float32 values
packed up together. The API contains functions to operate on those values
together, including all the basic arithmetic operations, and operations to
rearrange, load, and store such values. The intent is for browsers to
implement this API directly, and provide optimized implementations that make
use of SIMD instructions in the underlying hardware.

The SIMD.js API itself is in active development. The ecmascript_simd github
repository is currently serving as a provision specification as well as
providing a polyfill implementation to provide the functionality, though of
course not the accelerated performance, of the SIMD API on existing browsers.
It also includes some benchmarks which also serve as examples of basic SIMD.js
usage.

https://hacks.mozilla.org/2014/10/introducing-simd-js/[+https://hacks.mozilla.org/2014/10/introducing-simd-js/+]

https://github.com/johnmccutchan/ecmascript_simd[+https://github.com/johnmccutchan/ecmascript_simd+]

Simple-Pie
~~~~~~~~~~

This project provides tools for
postprocessing data on triangular grids
(simplex cells), such as computing
meridional and barotropic stream
functions and several transports through
user defined slices. The data are
interpolated onto a regular grid of user
defined mesh size, equidistant in each
(horizontal) coordinate direction.
Postprocessing takes place on this
regular grid. 

http://aforge.awi.de/gf/project/simplepie/[+http://aforge.awi.de/gf/project/simplepie/+]

http://aforge.awi.de/gf/project/simplepie/scmsvn/[+http://aforge.awi.de/gf/project/simplepie/scmsvn/+]

Simulocean
~~~~~~~~~~

A web-based scientific application deployment and visualization framework for
coastal modeling and beyond.

http://simulocean.ngchc.org/[+http://simulocean.ngchc.org/+]

http://lsu.ngchc.org/toolkit/download/[+http://lsu.ngchc.org/toolkit/download/+]

SINGE
~~~~~

SINGE is a Python 3 code. It computes, for full spheres and spherical shells,
inertial and inertia-gravito modes in the mantle frame of reference.
Boussinesq, homegeneous and viscous fluids are taken into account, with
various different boundary conditions (no slip / stress-free for the velocity
field, constant heat flux / isothermal for the temperature).

It uses a parallel pseudo-spectral approach in spherical geometry. The velociy
field is projetcted onto poloidal and toroidal scalars, which are expanded on
spherical harmonics in the angular directions and finite differences on an
irregular mesh in the radial direction.

https://bitbucket.org/nschaeff/singe[+https://bitbucket.org/nschaeff/singe+]

Singular
~~~~~~~~

A computer algebra system for polynomial computations, with special emphasis
on commutative and non-commutative algebra, algebraic geometry, and
singularity theory.

Its advanced algorithms, contained in currently more than 90 libraries,
address topics such as absolute factorization, algebraic D-modules,
classification of singularities, deformation theory, Gauss-Manin systems,
Hamburger-Noether (Puiseux) development, invariant theory, (non-) commutative
homological algebra, normalization, primary decomposition, resolution of
singularities, and sheaf cohomology.

Further functionality is obtained by combining Singular with third-party
software linked to SINGULAR. This includes tools for convex geometry, tropical
geometry, and visualization.

http://www.singular.uni-kl.de/[+http://www.singular.uni-kl.de/+]

http://www.singular.uni-kl.de/index.php/third-party-software/13692.html[+http://www.singular.uni-kl.de/index.php/third-party-software/13692.html+]

Sirius
~~~~~~

Sirius is an open end-to-end standalone speech and vision based intelligent
personal assistant (IPA) similar to Apple’s Siri, Google’s Google Now,
Microsoft’s Cortana, and Amazon’s Echo. Sirius implements the core
functionalities of an IPA including speech recognition, image matching,
natural language processing and a question-and-answer system.

http://sirius.clarity-lab.org/index.html[+http://sirius.clarity-lab.org/index.html+]

https://github.com/jhauswald/sirius[+https://github.com/jhauswald/sirius+]

http://sirius.clarity-lab.org/sirius.1.html[+http://sirius.clarity-lab.org/sirius.1.html+]

[[SkePU]]
SkePU
~~~~~

Skeleton programming is an approach where an application is written with the
help of "skeletons". A skeleton is a pre-defined, generic component such as
map, reduce, scan, farm, pipeline etc. that implements a common specific
pattern of computation and data dependence, and that can be customized with
(sequential) user-defined code parameters.
Skeletons provide a high degree of abstraction and portability with a
quasi-sequential programming interface, as their implementations encapsulate
all low-level and platform-specific details such as parallelization,
synchronization, communication, memory management, accelerator usage and other
optimizations.

SkePU poster SkePU is an open-source skeleton programming framework for
multicore CPUs and multi-GPU systems.
It is a Cxx template library with six data-parallel and one task-parallel
skeletons, two generic container types, and support for execution on multi-GPU
systems both with CUDA and OpenCL. 

http://www.ida.liu.se/~chrke/skepu/[+http://www.ida.liu.se/~chrke/skepu/+]

SkyNet
~~~~~~

SkyNet is an efficient and robust neural network training code for machine
learning. It is able to train large and deep feed-forward neural networks,
including autoencoders, for use in a wide range of supervised and unsupervised
learning applications, such as regression, classification, density estimation,
clustering and dimensionality reduction. SkyNet is implemented in C/Cxx and
fully parallelised using MPI.

BAMBI (Blind Accelerated Multimodal Bayesian Inference) is a Bayesian
inference engine that combines the benefits of SkyNet with MultiNest. It
operated by simulateneously performing Bayesian inference using MultiNest and
learning the likelihood function using SkyNet. Once SkyNet has learnt the
likelihood to sufficient accuracy, inference finishes almost instantaneously.

http://ccpforge.cse.rl.ac.uk/gf/project/skynet/[+http://ccpforge.cse.rl.ac.uk/gf/project/skynet/+]

http://xxx.lanl.gov/abs/1309.0790[+http://xxx.lanl.gov/abs/1309.0790+]

http://xxx.lanl.gov/abs/1110.2997[+http://xxx.lanl.gov/abs/1110.2997+]

Slicer
~~~~~~

Slicer, or 3D Slicer, is a free, open source software package for
visualization and image analysis.

http://www.slicer.org/[+http://www.slicer.org/+]

SMS
~~~

The Surface Water Modeling System (SMS) is a comprehensive graphical
environment for one-, two-, and three-dimensional hydrodynamic modeling. A
pre- and post-processor for surface water modeling and design, SMS includes 2D
finite element, 2D finite difference, 3D visualization modeling tools, and
limited 1D support. Supported models include the USACE-ERDC supported TABS-MD
(GFGEN, RMA2, RMA4, SED2D-WES), ADCIRC, ADH, CGWAVE, CMS-Flow, CMS-Wave,
STWAVE, and PTM models. Comprehensive interfaces have also been developed for
facilitating the use of the FHWA commissioned analysis FESWMS package. SMS
also includes a generic model interface, which can be used to support models
which have not been officially incorporated into the system.

The numeric models supported in SMS compute a variety of information
applicable to surface water modeling. Primary applications of the models
include calculation of water surface elevations and flow velocities for
shallow water flow problems, for both steady-state or dynamic conditions.
Additional applications include the modeling of contaminant migration,
salinity intrusion, sediment transport (scour and deposition), wave energy
dispersion, wave properties (directions, magnitudes and amplitudes) and
others.

The SMS interface is composed of various modules which streamline the modeling
process: Scatter Data, Map conceptualization, GIS, particle tracking,
annotation, and the new raster module.

http://chl.erdc.usace.army.mil/sms[+http://chl.erdc.usace.army.mil/sms+]

SocketCluster
~~~~~~~~~~~~~

SocketCluster is an open source, multi-process realtime environment written in
JavaScript (Node.js). You can build entire applications on top of it or you
can use it alongside existing systems written in other languages.

SC supports both direct client-server communication (like Socket.io) and group
communication via pub/sub channels.

SC is designed to scale both vertically across multiple CPU cores and
horizontally across multiple machines/instances (via pub/sub channel
synchronization).


http://socketcluster.io/[+http://socketcluster.io/+]

https://github.com/topcloud/socketcluster[+https://github.com/topcloud/socketcluster+]

http://arxiv.org/abs/1409.3367[+http://arxiv.org/abs/1409.3367+]

SOFA
~~~~

SOFA is an Open Source framework primarily targeted at real-time simulation,
with an emphasis on medical simulation. It is mostly intended for the research
community to help develop newer algorithms, but can also be used as an
efficient prototyping tool.

The SOFA architecture relies on several innovative concepts, in particular the
notion of multi-model representation. In SOFA, most simulation components
(deformable models, collision models, instruments, ...) can have several
representations, connected together through a mechanism called mapping. Each
representation can then be optimized for a particular task (e.g. collision
detection, visualization) while at the same time improving interoperability by
creating a clear separation be tween the functional aspects of the simulation
components. As a consequence, it is possible to have models of very different
nature interact together, for instance rigid bodies, deformable objects, and
fluids. At a finer level of granularity, we also propose a decomposition of
physical models (i.e. any model that behaves according to the laws of physics)
into a set of basic components. This decomposition leads for instance to a
representation of mechanical models as a set of degrees of freedom and force
fields acting on these degrees of freedom. Another key aspect of SOFA is the
use of a scene-graph to organize and process the elements of a simulation
while clearly separating the computation tasks from their possibly parallel
scheduling. 

http://www.sofa-framework.org/[+http://www.sofa-framework.org/+]

https://gforge.inria.fr/scm/?group_id=690[+https://gforge.inria.fr/scm/?group_id=690+]

Software Collections
~~~~~~~~~~~~~~~~~~~~

Software Collections give you power to build, install, and use multiple
versions of software on the same system, without affecting system-wide
installed packages.

https://www.softwarecollections.org/en/[+https://www.softwarecollections.org/en/+]

software-defined radio
~~~~~~~~~~~~~~~~~~~~~~

[[CubicSDR]]
CubicSDR
^^^^^^^^

CubicSDR is a cross-platform Software-Defined Radio application which allows you to navigate the radio spectrum and demodulate any signals you might discover.  It currently includes several common analog demodulation schemes such as AM and FM and will support digital modes in the future.  Many digital decoding applications are available now that can use the analog outputs to process digital signals by “piping” the data from CubicSDR to another program using software like Soundflower, Jack Audio or VBCable.

Currently CubicSDR supports the RTL-SDR which is an inexpensive SDR device that can be purchased online for about $10 and up.   Search for “RTL2832U” and “820T” or “820T2″ on sites such as eBay or Amazon to see what’s available.   Support for more advanced and higher bandwidth devices will be added in the future.

http://cubicsdr.com/[+http://cubicsdr.com/+]

http://mattg.co.uk/words/noaa_sdr/[+http://mattg.co.uk/words/noaa_sdr/+]

Somoclu
~~~~~~~

Somoclu is a massively parallel tool for training self-organizing maps on
large data sets written in Cxx. It builds on OpenMP for multicore execution,
and on MPI for distributing the workload across the nodes in a cluster. It is
also able to boost training by using CUDA if graphics processing units are
available. A sparse kernel is included, which is useful for high-dimensional
but sparse data, such as the vector spaces common in text mining workflows.
Python, R and MATLAB interfaces facilitate interactive use. Apart from fast
execution, memory use is highly optimized, enabling training large emergent
maps even on a single node. 

https://github.com/peterwittek/somoclu[+https://github.com/peterwittek/somoclu+]

http://arxiv.org/abs/1305.1422[+http://arxiv.org/abs/1305.1422+]

souper
~~~~~~

A superoptimizer for LLVM IR.

https://github.com/google/souper[+https://github.com/google/souper+]

http://blog.regehr.org/archives/1219[+http://blog.regehr.org/archives/1219+]

spack
~~~~~

A flexible package manager designed to support multiple versions,
configurations, platforms, and compilers. 

Spack is a package management tool designed to support multiple versions and
configurations of software on a wide variety of platforms and environments. It
was designed for large supercomputing centers, where many users and
application teams share common installations of software on clusters with
exotic architectures, using libraries that do not have a standard ABI. Spack
is non-destructive: installing a new version does not break existing
installations, so many configurations can coexist on the same system.

https://github.com/scalability-llnl/spack[+https://github.com/scalability-llnl/spack+]

http://scalability-llnl.github.io/spack/[+http://scalability-llnl.github.io/spack/+]

[[Spark]]
Spark
~~~~~

Apache Spark is an open-source cluster computing framework originally
developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage
disk-based MapReduce paradigm, Spark's in-memory primitives provide
performance up to 100 times faster for certain applications.[1] By allowing
user programs to load data into a cluster's memory and query it repeatedly,
Spark is well suited to machine learning algorithms.[2]

Spark requires a cluster manager and a distributed storage system. For cluster
manager, Spark supports standalone (native Spark cluster), Hadoop YARN, or
Apache Mesos.[3] For distributed storage, Spark can interface with a wide
variety, including Hadoop Distributed File System (HDFS),[4] Cassandra,[5]
OpenStack Swift, and Amazon S3. Spark also supports a pseudo-distributed mode,
usually used only for development or testing purposes, where distributed
storage is not required and the local file system can be used instead; in the
scenario, Spark is running on a single machine with one worker per CPU core.

http://spark.apache.org/[+http://spark.apache.org/+]

SparkCL
^^^^^^^^

We introduce SparkCL, an open source unified programming framework based on Java, OpenCL and the Apache Spark framework. The motivation behind this work is to bring unconventional compute cores such as FPGAs/GPUs/APUs/DSPs and future core types into mainstream programming use. The framework allows equal treatment of different computing devices under the Spark framework and introduces the ability to offload computations to acceleration devices. The new framework is seamlessly integrated into the standard Spark framework via a Java-OpenCL device programming layer which is based on Aparapi and a Spark programming layer that includes new kernel function types and modified Spark transformations and actions. The framework allows a single code base to target any type of compute core that supports OpenCL and easy integration of new core types into a Spark cluster.

https://gitlab.com/mora/spark-ucores[+https://gitlab.com/mora/spark-ucores+]

http://arxiv.org/abs/1505.01120[+http://arxiv.org/abs/1505.01120+]

SPASM
~~~~~

SPASM is a templatised Cxx library for the storage and manipulation of a
variety of probabilistic representations. Represenationtations currently
considered are: Gaussian, Gaussian Mixtures, Parzen density Estimates,
Particles and Discrete grids. this library will include: 

* Storage classes for PDF's and likelihoods
* Basic operators such as multiplication, division, convolution and
addition.
* Conversion between common types (e.g. Gaussian)
* Information measures (such as entropy etc.)
* Distance measures
* Complexity Reduction (Gaussian Mixtures have a habit of growing after
common operations)

Other libraries for Bayesian filtering exist such as Bayesxx and BFL . These
libraries are focused more on filtering in general and not the methods used
for storage and manipulation of a variety of representations.

This library has been motivated by robotics applications, such as feature
tracking, SLAM and other forms of localisation. Numerous other applications
exist that require probability representations, such a statistical learning. 

http://spasm.sourceforge.net/[+http://spasm.sourceforge.net/+]

spatial indexing
~~~~~~~~~~~~~~~~

libspatialindex
^^^^^^^^^^^^^^^

A Cxx implementation of R*-tree, an MVR-tree and a TPR-tree with C API.
The library was created to provide an extensible framework that will support
robust spatial indexing methods, support for sophisticated spatial queries,
enable easy to use interfaces for inserting, deleting and updating
information,
enable a wide variety of customization capabilities, and provide
index persistence.

http://libspatialindex.github.io/[+http://libspatialindex.github.io/+]

https://github.com/libspatialindex/libspatialindex[+https://github.com/libspatialindex/libspatialindex+]

Rtree
xxxx+

A ctypes Python wrapper of libspatialindex that provides a number of advanced
spatial indexing features for the spatially curious Python user.
The features include nearest neighbor search, intersection
search, multi-dimensional indexes, clustered indexes, bulk
loading, deletion, disk serialization, and custom
storage implemenation.

http://toblerity.org/rtree/[+http://toblerity.org/rtree/+]

https://pypi.python.org/pypi/Rtree/[+https://pypi.python.org/pypi/Rtree/+]

python-geohash
^^^^^^^^^^^^^^

A fast, accurate Python geohashing library.
A string representation of two dimensional geometric coordinates. This
technique is well described at Wikipedia Geohash. It is basically, a form of
Z-order curve. 
Quadtree can be used to construct a string representation. In this library, 0
for SW, 1 for SE, 2 for NW and 3 for NE. 
This project will maintain fast, accurate geohash, quadhash and grid code
python library.

https://github.com/hkwi/python-geohash[+https://github.com/hkwi/python-geohash+]

[[SPEEDO]]
SPEEDO
~~~~~~

The SPEEDO model is an 'intermediate-complexity', three-dimensional coupled atmosphere-ocean-sea ice General Circulation Model.
The atmospheric component is the KNMI version of the Speedy model developed at ICTP, a spectral T30 global eight level primitive equation model. The oceanic component is the CLIO model which is a primitive equation, free-surface ocean general circulation model coupled to a thermodynamic-dynamic sea-ice model developed at UCL. In addition to CLIO, a prescribed SST and a slab ocean model are included. Two land models are available: a prescribed land surface model and a land-bucket model with a simple hyrological cycle. 

http://www.knmi.nl/research/CKO/SPEEDO.html[+http://www.knmi.nl/research/CKO/SPEEDO.html+]

http://www.geosci-model-dev.net/3/105/2010/gmd-3-105-2010.html[+http://www.geosci-model-dev.net/3/105/2010/gmd-3-105-2010.html+]

Spherical Harmonics Manipulator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This software computes synthesis of spherical harmonics models on sparse
coordinates or grids (provided in a geodetic or geocentric reference system).
It exploits basic parallelism using openmp directives.
A binary that requires the MCR library shown below.

http://sourceforge.net/projects/hmanipulator/[+http://sourceforge.net/projects/hmanipulator/+]

MCR
^^^

Matlab compiler runtime.

http://www.mathworks.com/products/compiler/mcr/[+http://www.mathworks.com/products/compiler/mcr/+]

Spiral
~~~~~~

Library generators for high-performance numerical kernels.

http://www.spiral.net/index.html[+http://www.spiral.net/index.html+]

http://www.spiral.net/software/spiral-scala.html[+http://www.spiral.net/software/spiral-scala.html+]

http://spiral.ece.cmu.edu:8080/pub-spiral/abstract.jsp?id=1[+http://spiral.ece.cmu.edu:8080/pub-spiral/abstract.jsp?id=1+]

spirrid
~~~~~~~

Tool for estimation of statistical characteristics of multivariate random
functions.

This paper examines the feasibility of high-level Python based utilities for
numerically intensive applications via an example of a multidimensional
integration for the evaluation of the statistical characteristics of a random
variable. We discuss the approaches to the implementation of mathematically
formulated incremental expressions using high-level scripting code and
low-level compiled code. Due to the dynamic typing of the Python language,
components of the algorithm can be easily coded in a generic way as
algorithmic templates. Using the Enthought Development Suite they can be
effectively assembled into a flexible computational framework that can be
configured to execute the code for arbitrary combinations of integration
schemes and versions of instantiated code. The paper describes the development
cycle using a simple running example involving averaging of a random
two-parametric function that includes discontinuity. This example is also used
to compare the performance of the available algorithmic and executional
features. 

https://github.com/simvisage/spirrid[+https://github.com/simvisage/spirrid+]

http://www.sciencedirect.com/science/article/pii/S0010465512003086[+http://www.sciencedirect.com/science/article/pii/S0010465512003086+]

SQuadGen
~~~~~~~~

SQuadGen is a mesh generation utility that uses a cubed-sphere base mesh to
generate quadrilateral meshes with user-specified enhancements. In order to
determine where enhancement is desired, the user provides a PNG file which
corresponds to a latitude-longitude grid. Raster values with higher brightness
(whiter values) are tagged for refinement. The algorithm uses a basic paving
technique and supports two paving stencil types: Low-connectivity (LOWCONN)
and CUBIT-type transition regions.

http://climate.ucdavis.edu/squadgen.php[+http://climate.ucdavis.edu/squadgen.php+]

SQLAlchemy
~~~~~~~~~~

A Python SQL toolkit and Object Relational Mapper that gives application
developers the full power and flexibility of SQL.
It provides a full suite of well known enterprise-level persistence patterns,
designed for efficient and high-performing database access, adapted into a
simple and Pythonic domain language.

SQLAlchemy considers the database to be a relational algebra engine, not just
a collection of tables. Rows can be selected from not only tables but also
joins and other select statements; any of these units can be composed into a
larger structure. SQLAlchemy's expression language builds on this concept from
its core.

SQLAlchemy is most famous for its object-relational mapper (ORM), an optional
component that provides the data mapper pattern, where classes can be mapped
to the database in open ended, multiple ways - allowing the object model and
database schema to develop in a cleanly decoupled way from the beginning.

http://www.sqlalchemy.org/[+http://www.sqlalchemy.org/+]

GeoAlchemy
^^^^^^^^^^

GeoAlchemy is an extension of SQLAlchemy. It provides support for Geospatial
data types at the ORM layer using SQLAlchemy. It aims to support spatial
operations and relations specified by the Open Geospatial Consortium (OGC).
GeoAlchemy supports PostGIS, xref:SpatiaLite[SpatiaLite] and MySQL.

http://geoalchemy.readthedocs.org/en/latest/[+http://geoalchemy.readthedocs.org/en/latest/+]

https://pypi.python.org/pypi/GeoAlchemy[+https://pypi.python.org/pypi/GeoAlchemy+]

SqueezePlug/Max2Play
~~~~~~~~~~~~~~~~~~~~

SqueezePlug & Max2Play is the new combination of SqueezePlug, the famous
Multiroom Audio Solution and Max2Play, the web based framework for controlling
Linux based Mini-Computer like e.g. Raspberry Pi, Odroid and others by a
simple web interface without any, absolutely any Linux Know-How. SqueezePlug
is now a plug-in in Max2Play to make the configuration as easy as possible.
There is no need for connecting a monitor, a keyboard or a mouse on the device
itself. It all runs headless and no special tools like Putty are needed any
more. The configuration is as simple as configuring a router from a web
interface.

SqueezePlug & Max2Play is a Multiroom Audio Solution with server and player
components. Multiroom Audio Solution means, that you have one server and as
much players as you like. Every player uses the shared music from the server.
Players can be synced to play the same music, or they can play different
music. A Mini-Computer can be a server, a player or a server and a player
together. The files can resist on the Mini-Computer itself e.g. on an directly
attached USB-HD or on another location on your network. A Mini-Computer
doesn’t make any noise, cause it don’t have any cooling components and
consumes very little power.

http://www.squeezeplug.eu/[+http://www.squeezeplug.eu/+]

SSF
~~~

Secure Socket Funneling (SSF) is a network tool and toolkit.
It provides simple and efficient ways to forward data from multiple sockets (TCP or UDP) through a single secure TLS link to a remote computer.
SSF command-line was designed as a drop-in replacement for SSH (on equivalent features, such as port forwarding or SOCKS), so that you can benefit of the performance and security gain with minimal - or even, not a single - update for your scripts and tools.

https://securesocketfunneling.github.io/ssf/[+https://securesocketfunneling.github.io/ssf/+]

ssh
~~~

Fabric
^^^^^^

A Python 2.5-2.7 library and command-line tool for streamlining the use of SSH
for application deployment or systems administration tasks.
It provides a basic suite of operations for executing local or remote shell
commands (normally or via sudo) and uploading/downloading files, as well as
auxiliary functionality such as prompting the running user for input, or
aborting execution.  Typical use involves creating a Python module containing
one or more functions, then executing them via the fab command-line tool.

http://www.fabfile.org/[+http://www.fabfile.org/+]

http://www.linuxjournal.com/content/fabric-system-administrators-best-friend#[+http://www.linuxjournal.com/content/fabric-system-administrators-best-friend#+]

[[hpn-ssh]]
hpn-ssh
^^^^^^^

SCP and the underlying SSH2 protocol implementation in OpenSSH is network performance limited by statically defined internal flow control buffers. These buffers often end up acting as a bottleneck for network throughput of SCP, especially on long and high bandwith network links. Modifying the ssh code to allow the buffers to be defined at run time eliminates this bottleneck. We have created a patch that will remove the bottlenecks in OpenSSH and is fully interoperable with other servers and clients. In addition HPN clients will be able to download faster from non HPN servers, and HPN servers will be able to receive uploads faster from non HPN clients. However, the host receiving the data must have a properly tuned TCP/IP stack. 

http://www.psc.edu/index.php/hpn-ssh[+http://www.psc.edu/index.php/hpn-ssh+]

http://www.psc.edu/index.php/networking/641-tcp-tune[+http://www.psc.edu/index.php/networking/641-tcp-tune+]

Paramiko
^^^^^^^^

A Python (2.6+, 3.3+) implementation of the SSHv2 protocol [1], providing both
client and server functionality. While it leverages a Python C extension for
low level cryptography (PyCrypto), Paramiko itself is a pure Python interface
around SSH networking concepts.

http://www.paramiko.org/[+http://www.paramiko.org/+]

Stan
~~~~

A  probabilistic programming language for Bayesian inference written in
Cxx.[1] The Stan language is used to specify a Bayesian statistical model,
which is an imperative declaration of the log probability density function.
It has interfaces for R and Python as well as a command-line interface.

http://mc-stan.org/[+http://mc-stan.org/+]

https://github.com/stan-dev/stan[+https://github.com/stan-dev/stan+]

STELLA
~~~~~~

STELLA is a strongly typed, object-oriented, Lisp-like language, designed to
facilitate symbolic programming tasks in artificial intelligence applications.
STELLA preserves those features of Common Lisp deemed essential for symbolic
programming such as built-in support for dynamic data structures,
heterogeneous collections, first-class symbols, powerful iteration constructs,
name spaces, an object-oriented type system with a meta-object protocol,
exception handling, and language extensibility through macros, but without
compromising execution speed, interoperability with non-STELLA programs, and
platform independence. STELLA programs are translated into a target language
such as Cxx, Common Lisp, or Java, and then compiled with the native target
language compiler to generate executable code. The language constructs of
STELLA are restricted to those that can be translated directly into native
constructs of the intended target languages, thus enabling the generation of
highly efficient as well as readable code.

http://www.isi.edu/isd/LOOM/Stella/[+http://www.isi.edu/isd/LOOM/Stella/+]

http://arxiv.org/abs/1409.8563[+http://arxiv.org/abs/1409.8563+]

stetl
~~~~~

Stetl (Streaming ETL) is a lightweight, geospatial ETL framework for
the conversion of rich (e.g. GML) geospatial data. Stetl uses
existing transformation tools like GDAL/OGR and XSLT and is glued through
Python. A config file specifies an ETL chain of modules. Stetl is
speed-optimized by using native calls like ogr2ogr, libxml and libxslt (via
lxml).

https://github.com/justb4/stetl[+https://github.com/justb4/stetl+]

https://pypi.python.org/pypi/Stetl[+https://pypi.python.org/pypi/Stetl+]

StocPy
~~~~~~

StocPy is an expressive probabilistic programming language, provided as a
Python library.

We introduce the first, general purpose, slice sampling inference engine for
probabilistic programs. This engine is released as part of StocPy, a new
Turing-Complete probabilistic programming language, available as a Python
library. We present a transdimensional generalisation of slice sampling which
is necessary for the inference engine to work on traces with different numbers
of random variables. We show that StocPy compares favourably to other PPLs in
terms of flexibility and usability, and that slice sampling can outperform
previously introduced inference methods. Our experiments include a logistic
regression, HMM, and Bayesian Neural Net. 

http://arxiv.org/abs/1501.04684[+http://arxiv.org/abs/1501.04684+]

https://github.com/RazvanRanca/StocPy[+https://github.com/RazvanRanca/StocPy+]

Streamline
~~~~~~~~~~

Streamline Version 4 is a versatile Fortran 77 & Cxx program for calculating
charged test particle trajectories or field-lines for user-specified fields
using the test-particle method. The user has the freedom to specify any type
of field (analytical, tabulated in files, time dependent, etc.) and maintains
complete control over initial conditions of trajectories/field-lines and
boundary conditions of specified fields. The structure of Streamline was
redesigned from previous versions in order to know not only particle or
field-lines positions and velocities at each step of the simulations, but also
the instantaneous field values as seen by particles. This was made to compute
the instantaneous value of the particle’s magnetic moment, but other
applications are possible too. Accuracy tests of the code are shown for
different cases, i.e., particles moving in constant magnetic field, magnetic
plus constant electric field and wave field. In addition in the last part of
the paper we concentrate our discussion on the study of velocity space
diffusion of charged particles in turbulent slab fields, paying attention to
the discretization of the fields and the temporal discretization of the
dynamical equations. The diffusion of charged particles is a very common topic
in plasma physics and astrophysics since it plays an important role in many
different phenomena such as stochastic particle acceleration, diffusive shock
acceleration, solar energetic particle propagation, and the scattering
required for the solar modulation of galactic cosmic rays.

http://www.sciencedirect.com/science/article/pii/S0010465512001610[+http://www.sciencedirect.com/science/article/pii/S0010465512001610+]

streamtools
~~~~~~~~~~~

Streamtools is a new, open source project by The New York Times R&D Lab which
provides a general purpose, graphical tool for dealing with streams of data.
It provides a vocabulary of operations that can be connected together to
create live data processing systems without the need for programming or
complicated infrastructure.
These systems are assembled using a visual interface that affords both
immediate understanding and live manipulation of the system.

https://nytlabs.github.io/streamtools/[+https://nytlabs.github.io/streamtools/+]

http://blog.nytlabs.com/2014/03/12/streamtools-a-graphical-tool-for-working-with-streams-of-data/[+http://blog.nytlabs.com/2014/03/12/streamtools-a-graphical-tool-for-working-with-streams-of-data/+]

[[Structure_Synth]]
Structure Synth
~~~~~~~~~~~~~~~

Structure Synth is a cross-platform application for generating 3D structures
by specifying a design grammar. Even simple systems may generate surprising
and complex structures.
Structure Synth is built in Cxx, OpenGL, and Qt 4.

See also xref:Context_Free_Art[Context Free Art].

http://structuresynth.sourceforge.net/[+http://structuresynth.sourceforge.net/+]

[[stci_python]]
stci_python
~~~~~~~~~~~

A  library of Python routines and C extensions that has been developed to
provide a general astronomical data analysis infrastructure.

http://www.stsci.edu/institute/software_hardware/pyraf/stsci_python[+http://www.stsci.edu/institute/software_hardware/pyraf/stsci_python+]

[[DrizzlePac]]
DrizzlePac
^^^^^^^^^^

A package for aligning and combining Hubble Space
Telescope images.  Starting in July 2012, all drizzled data products obtained
from MAST were produced with AstroDrizzle. An abbreviation for Astrometric
Drizzle, AstroDrizzle was designed from the ground-up to substantially improve
the handling of distortion in the image header World Coordinate System.

AstroDrizzle removes geometric distortion, corrects for sky background
variations, flags cosmic-rays, and combines images with optional subsampling.
Drizzled data products from MAST are generated for single visit associations
only. 

To combine data from additional visits, TweakReg may be used to update the
image WCS using matched source lists. Once the full set of images of a given
target are properly aligned, they may be combined using AstroDrizzle.

http://drizzlepac.stsci.edu/[+http://drizzlepac.stsci.edu/+]

[[PyFITS]]
PyFITS
^^^^^^

PyFITS provides an interface to FITS formatted files in the Python scripting
language and PyRAF, the Python-based interface to IRAF. It is useful both for
interactive data analysis and for writing analysis scripts in Python using
FITS files as either input or output. PyFITS is a development project of the
Science Software Branch at the Space Telescope Science Institute. 

All of the functionality of PyFITS is now available in Astropy as the
astropy.io.fits package, which is now publicly available. Although we will
continue to release PyFITS separately in the short term, including any
critical bug fixes, we will eventually stop releasing new versions of PyFITS
as a stand-alone product. 

http://www.stsci.edu/institute/software_hardware/pyfits[+http://www.stsci.edu/institute/software_hardware/pyfits+]

[[PyRAF]]
PyRAF
^^^^^

PyRAF is a command language for running
xref:IRAF[IRAF] tasks that is based on the Python
scripting language. It gives users the ability to run IRAF tasks in an
environment that has all the power and flexibility of Python. PyRAF can be
installed along with an existing IRAF installation; users can then choose to
run either PyRAF or the IRAF CL. 

http://www.stsci.edu/institute/software_hardware/pyraf[+http://www.stsci.edu/institute/software_hardware/pyraf+]

pysynphot
^^^^^^^^^

A synthetic photometry package.

http://stsdas.stsci.edu/pysynphot/[+http://stsdas.stsci.edu/pysynphot/+]

[[STRIPACK]]
STRIPACK
~~~~~~~~

A Fortran 90 which carries out some computational geometry tasks on the unit sphere in 3D.
STRIPACK can compute the Delaunay triangulation or the Voronoi diagram of a set of points on the unit sphere, and make a PostScript plot of the Delaunay triangulation or the Voronoi diagram from a given point of view.

See also xref:MPI-SCVT[MPI-SCVT].

http://people.sc.fsu.edu/\~jburkardt/f_src/stripack/stripack.html[+http://people.sc.fsu.edu/~jburkardt/f_src/stripack/stripack.html+]

[[SuiteSparse]]
SuiteSparse
~~~~~~~~~~~

A suite of sparse matrix algorithms including:

[[UMFPACK]]
* *UMFPACK* for multifrontal LU factorization;
[[CHOLMOD]]
* *CHOLMOD* for supernodal Cholesky decomposition;
[[SPQR]]
* *SPQR* for multifrontal QR;
* *KLU* and *BTF* for sparse LU factorization;
* ordering methods *AMD*, *CAMD*, *COLAMD* and *CCOLAMD*;
* *CSparse* and *CXSparse* for concise sparse Cholesky
factorization;
* *UFGet* a Matlab interface for the collection;
* *spqr_rank*, a Matlab package for reliable sparse
rank detection, null set bases, pseudoinverse solutions, and basic solutions;
* *Factorize*, an object-oriented solver for MATLAB;
* *SSMULT* and *SFMULT*, for sparse matrix multiplication;
and more.

http://faculty.cse.tamu.edu/davis/suitesparse.html[+http://faculty.cse.tamu.edu/davis/suitesparse.html+]

[[SuperCollider]]
SuperCollider
~~~~~~~~~~~~~

SuperCollider is an environment and programming language for real time audio
synthesis and algorithmic composition. It provides an interpreted
object-oriented language which functions as a network client to a state of the
art, realtime sound synthesis server.

SuperCollider is an environment and programming language originally released
in 1996 by James McCartney for real-time audio synthesis and algorithmic
composition.
Since then it has been evolving into a system used and further developed by
both scientists and artists working with sound. It is an efficient and
expressive dynamic programming language providing a framework for acoustic
research, algorithmic music, and interactive programming.

http://supercollider.github.io/[+http://supercollider.github.io/+]

http://supercollider.sourceforge.net/[+http://supercollider.sourceforge.net/+]

http://en.wikipedia.org/wiki/SuperCollider[+http://en.wikipedia.org/wiki/SuperCollider+]

http://www.amazon.com/SuperCollider-Book-Scott-Wilson/dp/0262232693[+http://www.amazon.com/SuperCollider-Book-Scott-Wilson/dp/0262232693+]

http://www.caseyanderson.com/teaching/ipython-to-supercollider-via-osc/[+http://www.caseyanderson.com/teaching/ipython-to-supercollider-via-osc/+]

https://trac.v2.nl/wiki/pyOSC[+https://trac.v2.nl/wiki/pyOSC+]

Surfer
~~~~~~

An interactive mathematics visualization program.
With SURFER you can experience the relation between formulas and forms, i. e.
mathematics and art, in an interactive way. You can enter simple equations
that produce beautiful images, which are surfaces in space.

Mathematically, the program visualizes real algebraic geometry in real-time.
The surfaces shown are given by the zero set of a polynomial equation in the
variables x, y and z. All points in space that solve the equation are
displayed and form the surface.

http://imaginary.org/program/surfer[+http://imaginary.org/program/surfer+]

[[SYMPLER]]
SYMPLER
~~~~~~~

An object-oriented particle dynamics code SYMPLER. With this freely available
software, simulations can be performed ranging from microscopic classical
molecular dynamics up to the Lagrangian particle-based discretisation of
macroscopic continuum mechanics equations. We show how the runtime definition
of arbitrary degrees of freedom and of arbitrary equations of motion allows
for modular and symbolic computation with high flexibility. Arbitrary symbolic
expressions for inter-particle forces can be defined as well as fluxes of
arbitrarily many additional scalar, vectorial or tensorial degrees of freedom.
The integration in a high performance grid computing environment makes huge
geographically distributed computational resources accessible to the software
by an easy-to-use interface.

http://sympler.org/[+http://sympler.org/+]

http://www.sciencedirect.com/science/article/pii/S0010465513004104[+http://www.sciencedirect.com/science/article/pii/S0010465513004104+]

SystemTap
~~~~~~~~~

SystemTap (stap) is a scripting language and tool for dynamically
instrumenting running production Linux kernel-based operating systems. System
administrators can use SystemTap to extract, filter and summarize data in
order to enable diagnosis of complex performance or functional problems.

SystemTap files written in the SystemTap language (based on the language
reference.[7]) run with the stap command-line[8] and are saved as .stp files.
The system carries out a number of passes on the script before allowing it to
run, at which point the script is compiled into a loadable kernel module and
loaded into the kernel. Listing modules shows each SystemTap script as
'stap_<UUID>'. The module is unloaded when the tap has finished running.
Scripts generally focus on events (such as starting or finishing a script),
compiled-in probe points such as linux "tracepoints", or the execution of
functions or statements in the kernel or user-space.

https://sourceware.org/systemtap/[+https://sourceware.org/systemtap/+]

http://en.wikipedia.org/wiki/SystemTap[+http://en.wikipedia.org/wiki/SystemTap+]

[[NT]]
////
NTTT
////

TACO
~~~~

TACO is an object oriented control system originally developed at the European
Synchrotron Radiation Facility (ESRF) to control accelerators and beamlines
and data acquisition systems. 
TACO is very scalable and can be used for simple single device laboratory like
setups with only a few devices or for a big installation comprising thousands
of devices. TACO is a cheap and simple solution for doing distributed home
automation. TACO is available free of charge without warranties. 

TACO is object oriented because it treats ALL (physical and logical) control
points in a control system as objects in a distributed environment. All
actions are implement in classes. New classes can be constructed out of
existing classes in a hierarchical manner thereby ensuring a high-level of
software reuse. Classes can be written in Cxx, in C (using a methodology
called Objects in C), in Python or in LabView (using G).

This has been largedly superseded by xref:TANGO[TANGO].

Tahoe-LAFS
~~~~~~~~~~

Tahoe-LAFS is a Free and Open decentralized cloud storage system. It
distributes your data across multiple servers. Even if some of the servers
fail or are taken over by an attacker, the entire file store continues to
function correctly, preserving your privacy and security.

https://tahoe-lafs.org/trac/tahoe-lafs[+https://tahoe-lafs.org/trac/tahoe-lafs+]

Taiga
~~~~~

Taiga greatly simplifies the use of science data. It is a self-sufficient
bundle of free/open source software that webifies major scientific data
formats, such as NetCDF, HDF4 and HDF5. Through webification (w10n), meta
attributes and data arrays inside a file can be directly retrieved,
transformed, or manipulated using clear and meaningful URLs.

http://scifari.org/taiga/[+http://scifari.org/taiga/+]

TakTuk
~~~~~~

TakTuk is a tool for deploying parallel remote executions of commands to a
potentially large set of remote nodes. It spreads itself using an adaptive
algorithm and sets up an interconnection network to transport commands and
perform I/Os multiplexing/demultiplexing. The TakTuk mechanics dynamically
adapt to environment (machine performance and current load, network
contention) by using a reactive work-stealing algorithm that mixes local
parallelization and work distribution. 

TakTuk is a tool especially suited to the administration of parallel machines
because it eases the handling of groups of hosts. It might be used in batch
mode for simple machine state tests (e.g. test hosts responsiveness simply by
letting the engine setting the network up using its default connector - ssh)
or in interactive mode for deep investigation on several hosts, using the
TakTuk command interpreter to execute multiple commands on multiple hosts
(standard test sequence on a group of hosts, ping pong test between several
machines, ...). 

http://taktuk.gforge.inria.fr/[+http://taktuk.gforge.inria.fr/+]

[[TAUCS]]
TAUCS
~~~~~

A C library of solvers for sparse linear algebra.

https://code.google.com/p/taucs/[+https://code.google.com/p/taucs/+]


[[tensors]]
tensors
~~~~~~~

A tensor is a multidimensional or N-way array. Decompositions of higher-order
tensors (i.e., N-way arrays with N >= 3) have applications in psychometrics,
chemometrics, signal processing, numerical linear algebra, computer vision,
numerical analysis, data mining, neuroscience, graph analysis, and elsewhere.
Two particular tensor decompositions can be considered to be higher-order
extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP)
decomposes a tensor as a sum of rankone tensors, and the Tucker decomposition
is a higher-order form of principal component analysis. There are many other
tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and
PARATUCK2 as well as nonnegative variants of all of the above.

See also xref:ApproxFun[ApproxFun], xref:FactoMineR[FactoMineR], xref:MADNESS[MADNESS], xref:PTAk[PTAk],
xref:SageManifolds[SageManifolds], xref:SIAL[SIAL] and xref:SYMPLER[SYMPLER].

Tensor rank decomposition (Wikipedia) -
http://en.wikipedia.org/wiki/Tensor_rank_decomposition[+http://en.wikipedia.org/wiki/Tensor_rank_decomposition+]

Tensor Decompositions and Applications (paper, PDF, 2009, 46) - T. G. Kolda &
B. W. Bader -
http://www.sandia.gov/\~tgkolda/pubs/pubfiles/TensorReview.pdf[+http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf+]

Future directions in tensor-based computation and modeling (TR, PDF, 2009, 20)
- Charles Van Loan et al. -
http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf[+http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf+]

Exploiting symmetry in tensors for high performance (TR, PDF, 2014, 25) -
Martin D. Schatz et al. -
http://www.cs.utexas.edu/users/flame/pubs/SymTensorSISC.pdf[+http://www.cs.utexas.edu/users/flame/pubs/SymTensorSISC.pdf+]

Algorithms for numerical analysis in higher dimensions (paper, PDF, 2005, 27)
- G. Beylkin & Mohlenkamp -
http://amath.colorado.edu/faculty/beylkin/papers/BEY-MOH-2005.pdf[+http://amath.colorado.edu/faculty/beylkin/papers/BEY-MOH-2005.pdf+]

Era of Big Data Processing: A New Approach via Tensor Networks and Tensor
Decompositions (paper, PDF, 2014, 30) - Andrzej Cichocki -
http://arxiv.org/abs/1403.2048[+http://arxiv.org/abs/1403.2048+]

PARAFAC: Tutorial and applications (online) - Rasmus Bro -
http://www.models.life.ku.dk/\~rasmus/presentations/parafac_tutorial/paraf.htm[+http://www.models.life.ku.dk/~rasmus/presentations/parafac_tutorial/paraf.htm+]

Fundamental Tensor Operations for Large-Scale Data Analysis in Tensor Train
Formats (paper, PDF, 2014, 31) - Namgil Lee & Andrzej Cichocki -
http://arxiv.org/abs/1405.7786[+http://arxiv.org/abs/1405.7786+]

Most tensor problems are NP-hard (paper, PDF, 2009, 38) - Christopher Hillar &
Lek-Heng Lim -
http://arxiv.org/abs/0911.1393[+http://arxiv.org/abs/0911.1393+]

[[chemora]]
chemora
^^^^^^^

Starting from a high-level problem description in terms of partial
differential equations using abstract tensor notation, the Chemora framework
discretizes, optimizes, and generates complete high performance codes for a
wide range of compute architectures. Chemora extends the capabilities of
xref:Cactus[Cactus], facilitating the usage of large-scale CPU/GPU systems in an efficient
manner for complex applications, without low-level code tuning. Chemora
achieves parallelism through MPI and multi-threading, combining OpenMP and
CUDA. Optimizations include high-level code transformations, efficient loop
traversal strategies, dynamically selected data and instruction cache usage
strategies, and JIT compilation of GPU code tailored to the problem
characteristics. The discretization is based on higher-order finite
differences on multi-block domains. Chemora's capabilities are demonstrated by
simulations of black hole collisions. This problem provides an acid test of
the framework, as the Einstein equations contain hundreds of variables and
thousands of terms.

http://chemoracode.org/index.php/Main_Page[+http://chemoracode.org/index.php/Main_Page+]

http://code.google.com/p/chemora/[+http://code.google.com/p/chemora/+]

Chemora: A PDE solving framework for modern HPC architectures (paper, PDF,
2014, 11) - Erik Schnetter et al. - http://arxiv.org/abs/1410.1764[+http://arxiv.org/abs/1410.1764+]

From physics model to results: An optimizing framework for cross-architecture
code generation (paper, PDF, 2013, 18) - Marek Blazewicz et al. - http://arxiv.org/abs/1307.6488[+http://arxiv.org/abs/1307.6488+]

Cyclops Tensor Framework
^^^^^^^^^^^^^^^^^^^^^^^^

Cyclops Tensor Framework (CTF) is a distributed-memory library that provides
support for high-dimensional arrays (tensors). CTF arrays are distributed over
MPI communicators and two-level parallelism (MPI + threads) is supported with
via extensive internal usage of OpenMP and capability to exploit threaded BLAS
effectively. CTF is capable of performing summation and contraction, as well
as data manipulation and mapping.

CTF aims to provide support for distributed memory tensors (scalars, vectors,
matrices, etc.). CTF provides summation and contraction routines in Einstein
notation, so that any for loops are implicitly described by the index
notation. The tensors in CTF are templated (only double and complex<double>
currently tested), associated with an MPI communicator, and custom
element-wise functions can be defined for contract and sum. A number of
example codes using CTF are provided in the examples/ subdirectory. CTF uses
hybrid parallelism with MPI and OpenMP.

http://ctf.eecs.berkeley.edu/[+http://ctf.eecs.berkeley.edu/+]

https://github.com/solomonik/ctf[+https://github.com/solomonik/ctf+]

http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-29.pdf[+http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-29.pdf+]

http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-11.pdf[+http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-11.pdf+]

HaTen2
^^^^^^

How can we find useful patterns and anomalies in large scale real-world data
with multiple attributes? Tensors are suitable for modeling these
multidimensional data, and widely used for the analysis of social networks,
web data, network traffic, and in many other settings. HaTen2 is a scalable
distributed algorithm of tensor decomposition for large scale tensors running
on the MapReduce platform. HaTen2 decomposes 100X larger tensors compared to
existing methods.

http://kdmlab.org/haten2/[+http://kdmlab.org/haten2/+]

http://kdmlab.org/haten2/haten2.pdf[+http://kdmlab.org/haten2/haten2.pdf+]

N-Way Toolbox
^^^^^^^^^^^^^

The N-way Toolbox for MATLAB w is a freely
available collection of functions and algorithms for
modelling multiway data sets by a range of multilinear
models. Several types of models are covered;
canonical decomposition–parallel factor analysis
(CANDECOMP–PARAFAC), multilinear partial
least-squares regression (PLSR), generalised rank
annihilation method (GRAM), direct trilinear decomposition
(DTLD) and the class of Tucker models.

http://www.models.life.ku.dk/nwaytoolbox[+http://www.models.life.ku.dk/nwaytoolbox+]

http://www.models.kvl.dk/\~pih/parafac/chap0contents.htm[+http://www.models.kvl.dk/~pih/parafac/chap0contents.htm+]

TCE
^^^

The Tensor Contraction Engine (TCE) is a compiler for a domain-specific language
that allows chemists to specify complicated scientification computations in
quantum chemistry and physics.
The TCE searches for an optimal implementation and generates FORTRAN code.
First, algebraic transformations are used to reduce the number of operations.
We then minimize the storage requirements to fit the computation within the
disk limits by fusing loops. We have designed an algorithm that finds the
optimal evaluation order if intermediate arrays are allocated dynamically and
are working on combining loop fusion with dynamic memory allocation. If the
computation does not fit within the disk limits, recomputation must be traded
off for a reduction in storage requirements. If the target machine is a
multi-processor machine, we optimize the communication cost together with
finding a fusion configuration for minimizing storage. Finally, we minimize
the data access times by minimizing disk-to-memory and memory-to-cache traffic
and generate FORTRAN code. We have completed a first prototype of the TCE and
are working on implementing the communication minimization and data access
optimization algorithms. In future research, we will extend this approach to
handle common subexpressions, symmetric matrices, and sparse matrices.

The Tensor Contraction Engine (TCE) is the application of compiler
optimization and source-to-source translation technology to craft a domain
specific language for many-body theories in chemistry and physics. The
underlying equations of these theories are all expressed as contractions of
many-dimensional arrays or tensors There may be many thousands of such terms
in any one problem but their regularity means that they can be translated into
efficient massively parallel code that respects the boundedness of each level
of the memory hierarchy and minimizes overall runtime with effective trade-off
of increased computation for reduced memory consumption.

http://www.csc.lsu.edu/\~gb/TCE/[+http://www.csc.lsu.edu/~gb/TCE/+]

http://www.csc.lsu.edu/\~gb/TCE/Publications/SynthApproach-ProcIEEE05.pdf[+http://www.csc.lsu.edu/~gb/TCE/Publications/SynthApproach-ProcIEEE05.pdf+]

TDALAB
^^^^^^

Tensor Toolbox (TDALAB) provides fundamental data structures and functions for
tensor data processing. TDALAB attempts to provide an easy to use,
user-friendly toolbox for experimentation and application of tensor
decomposition and analysis.

http://www.bsp.brain.riken.jp/TDALAB/[+http://www.bsp.brain.riken.jp/TDALAB/+]

TenEig
^^^^^^

TenEig is a MATLAB toolbox to find eigenpairs of a tensor.
TenEig is written in MATLAB based on PSOLVE and provides routines for solving
different kinds of eigenvalue problems of a tensor, like E-eigenvalues,
eigenvalues, Z-eigenvalues (real E-eigenvalues), H-eigenvalues (real
eigenvalues), or more general mode-k eigenvalues. The corresponding eigenpairs
are also provided.

http://www.math.msu.edu/\~chenlipi/TenEig.html[+http://www.math.msu.edu/~chenlipi/TenEig.html+]

http://arxiv.org/abs/1501.04201[+http://arxiv.org/abs/1501.04201+]

Tensorbox
^^^^^^^^^

TENSORBOX is a MATLAB package consisting of state-of-the-art algorithms for
tensor decompositions such as CANDECOMP/PARAFAC (CP), tensor deflation,
structured CP, Tucker decompositions and Kronecker tensor decomposition.

http://www.bsp.brain.riken.jp/\~phan/index.html#tensorbox[+http://www.bsp.brain.riken.jp/~phan/index.html#tensorbox+]

Tensorlab
^^^^^^^^^

A Matlab toolbox for tensor computations.

http://www.esat.kuleuven.be/sista/tensorlab/[+http://www.esat.kuleuven.be/sista/tensorlab/+]

TensorToolbox
^^^^^^^^^^^^^

The Python Tensor Toolbox provides functionalities for the decomposition of tensors
in tensor-train format [1] and spectral tensor-train format [2].

https://pypi.python.org/pypi/TensorToolbox/[+https://pypi.python.org/pypi/TensorToolbox/+]

http://epubs.siam.org/doi/pdf/10.1137/090752286[+http://epubs.siam.org/doi/pdf/10.1137/090752286+]

http://arxiv.org/abs/1405.5713[+http://arxiv.org/abs/1405.5713+]

Tensor Toolbox
^^^^^^^^^^^^^^

The Tensor Toolbox provides the following classes for manipulating dense,
sparse, and structured tensors using MATLAB's object-oriented features: 

* tensor - A (dense) multidimensional array (extends MATLAB's current
capabilities).
* sptensor - A sparse multidimensional array.
* tenmat - Store a tensor as a matrix, with extra information so that it can
be converted back into a tensor.
* sptenmat - Store an sptensor as a sparse matrix in coordinate format, with
extra information so that it can be converted back into an sptensor.
* ttensor - Store a tensor decomposed as a Tucker operator 
* ktensor - Store a tensor decomposed as a Kruskal operator

http://www.sandia.gov/\~tgkolda/TensorToolbox/[+http://www.sandia.gov/~tgkolda/TensorToolbox/+]

http://dl.acm.org/citation.cfm?doid=1186785.1186794[+http://dl.acm.org/citation.cfm?doid=1186785.1186794+]

http://epubs.siam.org/doi/abs/10.1137/060676489[+http://epubs.siam.org/doi/abs/10.1137/060676489+]

TT-Toolbox
^^^^^^^^^^

TT(Tensor Train) format is an efficient way for low-parametric
representation of high-dimensional tensors. The TT-Toolbox
is a MATLAB implementation of basic operations with
tensors in TT-format.

https://github.com/oseledets/TT-Toolbox[+https://github.com/oseledets/TT-Toolbox+]

http://epubs.siam.org/doi/abs/10.1137/100811647[+http://epubs.siam.org/doi/abs/10.1137/100811647+]

http://arxiv.org/abs/1306.2269[+http://arxiv.org/abs/1306.2269+]

ttpy
xxxx

Python implementation of the TT-Toolbox. It contains several important
packages for working with the Tensor Train (TT) format in Python. It is able
to do interpolation, solve linear systems, eigenproblems, solve dynamical
problems. Several computational routines are done in Fortran (which can be
used separatedly), and are wrapped with the f2py tool.

https://github.com/oseledets/ttpy[+https://github.com/oseledets/ttpy+]

terminal emulation
~~~~~~~~~~~~~~~~~~

tmate
^^^^^

Instant terminal sharing.

http://tmate.io/[+http://tmate.io/+]

Terra
~~~~~

Terra is a new low-level system programming language that is designed to
interoperate seamlessly with the Lua programming language.

Like C, Terra is a simple, statically-typed, compiled language with manual
memory management. But unlike C, it is designed from the beginning to
interoperate with Lua. Terra functions are first-class Lua values created
using the terra keyword. When needed they are JIT-compiled to machine code.

http://terralang.org/[+http://terralang.org/+]

[[tesseract]]
tesseract
~~~~~~~~~

An OCR engine.

This requires xref:Leptonica[Leptonica].

https://github.com/tesseract-ocr/tesseract[+https://github.com/tesseract-ocr/tesseract+]

[[lector]]
lector
^^^^^^

An interface to xref:tesseract[tesseract].

https://github.com/zdenop/lector[+https://github.com/zdenop/lector+]

TeX
~~~

Offshoots of Donald Knuth's side project for typesetting books.

ConTeXt
^^^^^^^

ConTeXt can be used to typeset complex and large collections of documents,
like educational materials, user guides and technical manuals. Such documents
often have high demands regarding structure, design and accessibility. Ease of
maintenance, reuse of content and typographic consistency are important
prerequisites.
ConTeXt is developed for those who are responsible for producing such
documents. ConTeXt is written in the typographical programming language TeX.
For using ConTeXt, no TeX programming skills and no technical background are
needed. Some basic knowledge of typography and document design will enable you
to use the full power of ConTeXt.

http://wiki.contextgarden.net/Main_Page[+http://wiki.contextgarden.net/Main_Page+]

KaTeX
^^^^^

A fast, easy-to-use JavaScript library for TeX math rendering on the
web.
KaTeX renders its math synchronously and doesn't need to reflow the page, and
the layout is based on that of TeX.
It has no dependencies and can easily be bundled with website
resources.
KaTeX produces the same output regardless of browser or environment, so you
can pre-render expressions using Node.js and send them as plain HTML.

https://github.com/Khan/KaTeX[+https://github.com/Khan/KaTeX+]

[[Tufte_CSS]]
Tufte CSS
^^^^^^^^^

Tufte CSS provides tools to style web articles using the ideas demonstrated by Edward Tufte's books and handouts. Tufte’s style is known for its simplicity, extensive use of sidenotes, tight integration of graphics with text, and carefully chosen typography.

http://www.daveliepmann.com/tufte-css/[+http://www.daveliepmann.com/tufte-css/+]

text editors
~~~~~~~~~~~~

[[Atom]]
Atom
^^^^

A hackable text editor for the 21st century.

https://atom.io/[+https://atom.io/+]

[[CodeMirror]]
CodeMirror
^^^^^^^^^^

A versatile text editor implemented in JavaScript for the browser. It is specialized for editing code, and comes with a number of language modes and addons that implement more advanced editing functionality.
A rich programming API and a CSS theming system are available for customizing CodeMirror to fit your application, and extending it with new functionality.

http://codemirror.net/[+http://codemirror.net/+]

[[Sublime]]
Sublime
^^^^^^^

A sophisticated text editor for code, markup and prose.

http://www.sublimetext.com/[+http://www.sublimetext.com/+]

[[Package_Control]]
Package Control
xxxxxxxxxxxxxx+

A package manager for Sublime that makes it exceedingly simple to find, install and keep packages up-to-date.

https://packagecontrol.io/[+https://packagecontrol.io/+]

[[vi]]
vim/vi
^^^^^^

An advanced text editor that seeks to provide the power of the de-facto Unix editor +vi+, with a more complete feature set. 

http://www.vim.org/[+http://www.vim.org/+]

[[spf13-vim]]
spf13-vim
xxxxxxxx+

A distribution of vim plugins and resources for Vim, Gvim and MacVim.
It is a good starting point for anyone intending to use VIM for development running equally well on Windows, Linux and Mac.

https://github.com/spf13/spf13-vim[+https://github.com/spf13/spf13-vim+]

[[Vundle]]
Vundle
xxxxxx

A plugin manager for vim.

https://github.com/VundleVim/Vundle.vim[+https://github.com/VundleVim/Vundle.vim+]

TEXTUS
~~~~~~

An open source platform for working with collections of texts. It enables
students, researchers and teachers to share and collaborate around texts using
a simple and intuitive interface.  TEXTUS currently enables users to:

* Collaboratively annotate texts and view the annotations of others
* Reliably cite electronic versions of texts
* Create bibliographies with stable URLs to online versions of those texts

http://textusproject.org/[+http://textusproject.org/+]

https://github.com/okfn/textus[+https://github.com/okfn/textus+]

TGCM
~~~~

The High Altitude Observatory at the National Center for Atmospheric Research
has developed a series of numeric simulation models of the Earth's upper
atmosphere, including the upper Stratosphere, Mesosphere, and Thermosphere.
The Thermospheric General Circulation Models (TGCM's) are three-dimensional,
time-dependent models of the EARTH's neutral upper atmosphere. The models use
a finite differencing technique to obtain a self-consistent solution for the
coupled, nonlinear equations of hydrodynamics, thermodynamics, continuity of
the neutral gas and for the coupling between the dynamics and the composition. 

Recent models in the series include a self-consistent aeronomic scheme for the
coupled Thermosphere/Ionosphere system, the Thermosphere Ionosphere
Electrodynamic General Circulation Model (TIEGCM), and an extension of the
lower boundary from 97 to 30 km, including the physical and chemical processes
appropriate for the Mesosphere and upper Stratosphere, the Thermosphere
Ionosphere Mesosphere Electrodynamic General Circulation Model (TIME-GCM). A
global mean, or column model, has also been developed in parallel with the
TGCM's. The global mean model is used as a time-dependent, one-dimensional
platform from which new chemical, dynamic and numeric schemes are developed
and tested before being introduced into the 3-d GCM's. 

http://www.hao.ucar.edu/modeling/tgcm/[+http://www.hao.ucar.edu/modeling/tgcm/+]

Theano
~~~~~~

A Python library that allows you to define, optimize, and evaluate
mathematical expressions involving multi-dimensional arrays efficiently.

http://deeplearning.net/software/theano/[+http://deeplearning.net/software/theano/+]

http://arxiv.org/abs/1412.2302[+http://arxiv.org/abs/1412.2302+]

Janus
^^^^^

A tool that allows NumPy and Theano to be used simultaneously with no
additional code.

https://github.com/jlowin/janus[+https://github.com/jlowin/janus+]

Keras
^^^^^

Keras is a minimalist, highly modular neural network library in the spirit of
Torch, written in Python / Theano so as not to have to deal with the dearth of
ecosystem in Lua. It was developed with a focus on enabling fast
experimentation. Being able to go from idea to result with the least possible
delay is key to doing good research.

https://github.com/fchollet/keras[+https://github.com/fchollet/keras+]

Pylearn2
^^^^^^^^^

A machine learning research library based on Theano.

https://github.com/lisa-lab/pylearn2[+https://github.com/lisa-lab/pylearn2+]

http://arxiv.org/abs/1308.4214[+http://arxiv.org/abs/1308.4214+]

[[threads]]
threads
~~~~~~~

A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler.The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within the same process, executing concurrently (one starting before others finish) and share resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its instructions (executable code) and its context (the values of its variables at any given moment).

[[pthreads]]
POSIX threads - typically called pthreads - is a POSIX standard for threads.
It defines a set of C programming language types, functions and constants, and
is implemented with a +pthread.h+ header and a thread library.

[[NPTL]]
The Native POSIX Thread Library (NPTL) is part of the Linux kernel for efficiently
running programs written to use POSIX threads.

See also xref:OpenMP[OpenMP].

[[Pth]]
Pth
^^^

A portable POSIX/ANSI-C based library for Unix platforms which provides non-preemptive priority-based scheduling for multiple threads of execution (aka ``multithreading'') inside event-driven applications. All threads run in the same address space of the server application, but each thread has it's own individual program-counter, run-time stack, signal mask and errno variable. 

Additionally Pth provides an optional emulation API for POSIX.1c threads (xref:pthreads[pthreads]) which can be used for backward compatibility to existing multithreaded applications. 

https://www.gnu.org/software/pth/[+https://www.gnu.org/software/pth/+]

[[TBB]]
TBB
^^^

A `Cxx`  template library developed by Intel for writing software programs that take advantage of multi-core processors. The library consists of data structures and algorithms that allow a programmer to avoid some complications arising from the use of native threading packages such as POSIX threads, Windows threads, or the portable Boost Threads in which individual threads of execution are created, synchronized, and terminated manually. Instead the library abstracts access to the multiple processors by allowing the operations to be treated as "tasks", which are allocated to individual cores dynamically by the library's run-time engine, and by automating efficient use of the CPU cache. A TBB program creates, synchronizes and destroys graphs of dependent tasks according to algorithms, i.e. high-level parallel programming paradigms (a.k.a. Algorithmic Skeletons). Tasks are then executed respecting graph dependencies. This approach groups TBB in a family of solutions for parallel programming aiming to decouple the programming from the particulars of the underlying machine.

https://www.threadingbuildingblocks.org/[+https://www.threadingbuildingblocks.org/+]

https://en.wikipedia.org/wiki/Threading_Building_Blocks[+https://en.wikipedia.org/wiki/Threading_Building_Blocks+]

[[Thrust]]
Thrust
~~~~~~

Thrust is a parallel algorithms library which resembles the Cxx Standard
Template Library (STL). Thrust's high-level interface greatly enhances
developer productivity while enabling performance portability between GPUs and
multicore CPUs. Interoperability with established technologies (such as CUDA,
xref:TBB[TBB] and xref:OpenMP[OpenMP]) facilitates integration with existing software.

https://github.com/STEllAR-GROUP/thrust[+https://github.com/STEllAR-GROUP/thrust+]

https://code.google.com/p/thrust/[+https://code.google.com/p/thrust/+]

https://github.com/thrust/thrust[+https://github.com/thrust/thrust+]

tintl
~~~~~

Library for the Fourier interpolation of regular three-dimensional data-sets. 

https://github.com/FrancisRussell/tintl[+https://github.com/FrancisRussell/tintl+]

http://www.sciencedirect.com/science/article/pii/S001046551400335X[+http://www.sciencedirect.com/science/article/pii/S001046551400335+]

TinyOS
~~~~~~

TinyOS is an "operating system" designed for low-power wireless embedded
systems. Fundamentally, it is a work scheduler and a collection of drivers for
microcontrollers and other ICs commonly used in wireless embedded platforms. 

http://tinyos.stanford.edu/tinyos-wiki/index.php/Main_Page[+http://tinyos.stanford.edu/tinyos-wiki/index.php/Main_Page+]

https://github.com/tinyos[+https://github.com/tinyos+]

tmve
~~~~

Topic modeling is a machine learning method that learns underlying themes in a
collection of documents, which can be used to summarize and organize the
documents. We have created a method for visualizing topic models, allowing
users to explore a corpus by navigating between high level topic descriptions
and individual documents, hopefully deepening their understanding of the
corpus.

https://github.com/jseabold/tmve[+https://github.com/jseabold/tmve+]

http://www.cs.columbia.edu/\~blei/topicmodeling.html[+http://www.cs.columbia.edu/~blei/topicmodeling.html+]

https://github.com/ajbc/tmv[+https://github.com/ajbc/tmv+]

http://code.google.com/p/tmve/[+http://code.google.com/p/tmve/+]

http://www.princeton.edu/\~achaney/tmve/wiki100k/browse/topic-presence.html[+http://www.princeton.edu/~achaney/tmve/wiki100k/browse/topic-presence.html+]

TOCSY
~~~~~

With Toolboxes for Complex Systems we provide a compilation of innovative
methods for modern nonlinear data analysis. These methods were developed
during scientific research in the Interdisciplinary Center for Dynamics of
Complex Systems Potsdam, the Cardiovascular Physics Group at the
Humboldt-Universität zu Berlin, and the Potsdam Institute for Climate Impact
Research (PIK).

http://tocsy.pik-potsdam.de/[+http://tocsy.pik-potsdam.de/+]

Tornado
~~~~~~~

Tornado is a Python web framework and asynchronous networking library,
originally developed at FriendFeed. By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for long
polling, WebSockets, and other applications that require a long-lived
connection to each user.

The components are:

* A web framework (including RequestHandler which is subclassed to create web
applications, and various supporting classes).
* Client- and server-side implementions of HTTP (HTTPServer and
AsyncHTTPClient).
* An asynchronous networking library (IOLoop and IOStream), which serve as the
building blocks for the HTTP components and can also be used to implement
other protocols.
* A coroutine library (tornado.gen) which allows asynchronous code to be
written in a more straightforward way than chaining callbacks.

The Tornado web framework and HTTP server together offer a full-stack
alternative to WSGI. While it is possible to use the Tornado web framework in
a WSGI container (WSGIAdapter), or use the Tornado HTTP server as a container
for other WSGI frameworks (WSGIContainer), each of these combinations has
limitations and to take full advantage of Tornado you will need to use the
Tornado’s web framework and HTTP server together.

http://www.tornadoweb.org/en/stable/[+http://www.tornadoweb.org/en/stable/+]

Tox
~~~

With the rise of governmental monitoring programs, Tox, a FOSS initiative,
aims to be an easy to use, all-in-one communication platform that ensures
their users full privacy and secure message delivery.
The goal of this project is to create a configuration-free P2P Skype
replacement. “Configuration-free” means that the user will simply have to open
the program and will be capable of adding people and communicating with them
without having to set up an account.

https://tox.im/[+https://tox.im/+]

https://github.com/irungentoo/toxcore[+https://github.com/irungentoo/toxcore+]

PyTox
^^^^^

Python binding for Project Tox.

https://github.com/aitjcize/PyTox[+https://github.com/aitjcize/PyTox+]

Transmageddon
~~~~~~~~~~~~~

Transmageddon is a video transcoder for Linux and Unix systems built using
GStreamer. It supports almost any format as its input and can generate a very
large host of output files. The goal of the application was to help people to
create the files they need to be able to play on their mobile devices and for
people not hugely experienced with multimedia to generate a multimedia file
without having to resort to command line tools with ungainly syntaxes. 

http://www.linuxrising.org/[+http://www.linuxrising.org/+]

TsunAWI
~~~~~~~

TsunAWI was developped in the framework of the GITEWS project
(German-Indonesian Tsunami Early Warning System). It discretizes the
non-linear shallow equations on an unstructured triangular mesh and allows to
simulate the propagation of tsunamis from the origin to the inundation on
land. It was used to calculate 4500 scenarios for the Indonesian tsunami early
warning system.

http://aforge.awi.de/gf/project/tsunawi/[+http://aforge.awi.de/gf/project/tsunawi/+]

ttpy
^^^^

Python implementation of the TT-Toolbox.

https://github.com/oseledets/ttpy[+https://github.com/oseledets/ttpy+]

Tulip
~~~~~

Tulip is an information visualization framework dedicated to the analysis and
visualization of relational data. Tulip aims to provide the developer with a
complete library, supporting the design of interactive information
visualization applications for relational data that can be tailored to the
problems he or she is addressing.

Written in Cxx the framework enables the development of algorithms, visual
encodings, interaction techniques, data models, and domain-specific
visualizations. One of the goal of Tulip is to facilitates the reuse of
components and allows the developers to focus on programming their
application. This development pipeline makes the framework efficient for
research prototyping as well as the development of end-user applications.

http://tulip.labri.fr/TulipDrupal/[+http://tulip.labri.fr/TulipDrupal/+]

Tulip Python
^^^^^^^^^^^^

Tulip Python is a set of modules that exposes to Python almost all the content
of the Tulip Cxx API. The bindings has been developed with the SIP tool from
Riverbank.

http://tulip.labri.fr/Documentation/current/tulip-python/html/index.html[+http://tulip.labri.fr/Documentation/current/tulip-python/html/index.html+]

tus
~~~

Users want to share more and more photos and videos. But mobile networks are
fragile. Platform APIs are a mess. Every project builds its own file uploader.
A thousand one week projects that barely work, when all we need is one real
project, done right.

We are going to do this right. We will solve reliable file uploads for once
and for all. A new open protocol for resumable uploads built on HTTP. Simple,
cheap, reusable stacks for clients and servers. Any language, any platform,
any network.

http://www.tus.io/[+http://www.tus.io/+]

Twisted
~~~~~~~

Twisted is an event-driven networking engine written in Python and licensed
under the open source.

http://twistedmatrix.com/trac/[+http://twistedmatrix.com/trac/+]

http://twistedmatrix.com/trac/wiki/ProjectsUsingTwisted[+http://twistedmatrix.com/trac/wiki/ProjectsUsingTwisted+]

txThings
^^^^^^^^

A xref:CoAP[CoAP] library for the Twisted framework.

https://github.com/siskin/txThings/[+https://github.com/siskin/txThings/+]

TYPO3
~~~~~

A free and open source web content management system (CMS) based on PHP.
It can run on several web servers, such as Apache or IIS, on top of many
operating systems.
3 is credited to be highly flexible. It can be extended by new functions
without writing any program code. Also, the software is available in more than
50 languages and has a built-in localization system, therefore supports
publishing content in multiple languages. Due to its features, scalability and
maturity, TYPO3 is used to build and manage websites of different types and
size ranges, from small sites for individuals or nonprofit organizations to
multilingual enterprise solutions for large corporations.

Delivered with a base set of interfaces, functions and modules, TYPO3's
functionality spectrum is implemented by extensions. More than 5000 extensions
are currently available for TYPO3 for download under the GNU General Public
License from a repository called the TYPO3 Extension Repository, or TER.

https://github.com/TYPO3/TYPO3.CMS[+https://github.com/TYPO3/TYPO3.CMS+]

http://typo3.org/[+http://typo3.org/+]

[[NU]]
////
NUUU
////

[[UDT]]
UDT
~~~

A reliable UDP based application level data transport protocol for distributed
data intensive applications over wide area high-speed networks. UDT uses UDP
to transfer bulk data with its own reliability control and congestion control
mechanisms. The new protocol can transfer data at a much higher speed than TCP
does.

http://udt.sourceforge.net/[+http://udt.sourceforge.net/+]

http://sector.sourceforge.net/[+http://sector.sourceforge.net/+]

[[udt4py]]
udt4py
^^^^^^

A xref:UDT[UDT] wrapper for Python written with xref:Cython[Cython].

https://github.com/Samsung/udt4py[+https://github.com/Samsung/udt4py+]

Uintah
~~~~~~

The Uintah software suite is a set of libraries and applications for
simulating and analyzing complex chemical and physical reactions. These
reactions are modeled by solving partial differential equations on structured
adaptive grids using hundreds to thousands of processors (though smaller
simulations may also be run on a scientist's desktop computer). Key software
applications have been developed for exploring the fine details of metal
containers (encompassing energetic materials) embedded in large hydrocarbon
fires. Uintah's underlying technologies have led to novel techniques for
understanding large pool eddy fires as well as new methods for simulating
fluid-structure interactions. The software is general purpose in nature and
the breadth of simulation domains continues to grow beyond the original focus
of the C-SAFE initiative.

http://uintah-build.sci.utah.edu/trac/wiki[+http://uintah-build.sci.utah.edu/trac/wiki+]

http://www.uintah.utah.edu/[+http://www.uintah.utah.edu/+]

[[UNICORE]]
UNICORE
~~~~~~~

UNICORE (Uniform Interface to Computing Resources) offers a ready-to-run Grid system including client and server software. UNICORE makes distributed computing and data resources available in a seamless and secure way in intranets and the internet.

http://www.unicore.eu/[+http://www.unicore.eu/+]

UNIX Utilities
~~~~~~~~~~~~~~

The various available flavours.

Busybox
^^^^^^^

BusyBox combines tiny versions of many common UNIX utilities into a single
small executable. It provides replacements for most of the utilities you
usually find in GNU fileutils, shellutils, etc. The utilities in BusyBox
generally have fewer options than their full-featured GNU cousins; however,
the options that are included provide the expected functionality and behave
very much like their GNU counterparts. BusyBox provides a fairly complete
environment for any small or embedded system.

BusyBox has been written with size-optimization and limited resources in mind.
It is also extremely modular so you can easily include or exclude commands (or
features) at compile time. This makes it easy to customize your embedded
systems. To create a working system, just add some device nodes in /dev, a few
configuration files in /etc, and a Linux kernel.

http://www.busybox.net/[+http://www.busybox.net/+]

http://en.wikipedia.org/wiki/BusyBox[+http://en.wikipedia.org/wiki/BusyBox+]

GNU Core Utilities
^^^^^^^^^^^^^^^^^^

The GNU Core Utilities are the basic file, shell and text manipulation
utilities of the GNU operating system.
These are the core utilities which are expected to exist on every operating
system. 

The GNU Core Utilities or coreutils is a package of GNU software containing
many of the basic tools, such as cat, ls, and rm, needed for Unix-like
operating systems. It is a combination of a number of earlier packages,
including textutils, shellutils, and fileutils, along with some other
miscellaneous utilities.

http://en.wikipedia.org/wiki/GNU_Core_Utilities[+http://en.wikipedia.org/wiki/GNU_Core_Utilities+]

https://www.gnu.org/software/coreutils/[+https://www.gnu.org/software/coreutils/+]

Heirloom Toolchest
^^^^^^^^^^^^^^^^^^

The Heirloom Toolchest is a collection of standard Unix utilities.
Derived from original Unix material released as Open Source by Caldera and
Sun.
Multiple versions of many utilities are provided to approach compatibility
with various specifications and Unix flavors.
Support for lines of arbitrary length and in many cases binary input data.
Support for multibyte characters in UTF-8 and many East Asian encodings.
Extensive documentation including a manual page for any utility.

http://heirloom.sourceforge.net/tools.html[+http://heirloom.sourceforge.net/tools.html+]

sbase
^^^^^

A collection of  unix  tools that  are inherently  portable
across UNIX and UNIX-like systems.
The  complement of  sbase  is  ubase[1] which  is  Linux-specific  and
provides all  the non-portable tools.   Together they are  intended to
form a base system similar to busybox but much smaller and suckless.
You  can  also  build  sbase-box,  which  generates  a  single  binary
containing  all  the  required  tools.    You  can  then  symlink  the
individual tools to sbase-box or run: make sbase-box-install.
Ideally you will  want to statically link sbase.  If  you are on Linux
we recommend using musl-libc.

http://git.2f30.org/sbase/about/[+http://git.2f30.org/sbase/about/+]

http://git.suckless.org/ubase/[+http://git.suckless.org/ubase/+]

http://www.musl-libc.org/[+http://www.musl-libc.org/+]

ToyBox
^^^^^^

Toybox combines the most common Linux command line utilities together into a
single BSD-licensed executable. It's simple, small, fast, and reasonably
standards-compliant (POSIX-2008 and LSB 4.1).

Toybox's 1.0 release goal is to turn generic Android into a development
environment capable of compiling
http://www.linuxfromscratch.org/[Linux From Scratch]. A tiny system
http://www.landley.net/aboriginal/[built from]
just toybox, linux, a C library, and a C compiler (such as LLVM or gcc
4.2.1+binutils 2.17) should be able to rebuild itself from source code without
needing any other packages.

Toybox is an implementation of some Linux command line utilities started in
2006,[1] and became a BSD-licensed BusyBox alternative.

http://www.landley.net/toybox/about.html[+http://www.landley.net/toybox/about.html+]

http://en.wikipedia.org/wiki/Toybox[+http://en.wikipedia.org/wiki/Toybox+]

util-linux
^^^^^^^^^^

A set of approximately 100 basic Linux system utilities not included in GNU
Core Utilities.

http://en.wikipedia.org/wiki/Util-linux[+http://en.wikipedia.org/wiki/Util-linux+]

[[UQTk]]
UQTk
~~~~

A collection of libraries and tools for the quantification of uncertainty in numerical model predictions. Version 2.1.1 offers intrusive and non-intrusive methods for propagating input uncertainties through computational models, tools for sensitivity analysis, methods for sparse surrogate construction, and Bayesian inference tools for inferring parameters from experimental data. 

http://www.sandia.gov/UQToolkit/[+http://www.sandia.gov/UQToolkit/+]

Urbit
~~~~~

We got tired of system software from the 1970s. So we wrote our own on top.
From scratch.
Urbit is a new general-purpose computing layer. We look forward to a future
where the current Internet exists only as an underground series of tubes which
transports you to your Urbit.

More specifically, Urbit is a personal cloud computer. Right now, the cloud
computers we use run OSes designed for minicomputers in the '70s. An ordinary
user can no more drive a Linux box in the cloud than fly an A320. So she has
to sit in coach class as a row in someone else's database. It's definitely air
travel. It's not exactly flying.

The user of the future will fly her own computer. She will own and control her
own identity and her own data. She will even host her own apps. She will not
be part of someone else's Big Data. She will be her own Little Data. Unless
she's a really severe geek, she will pay some service to store and execute her
Urbit ship - but she can move it anywhere else, anytime, for the cost of the
bandwidth.

Urbit at present is not good for anything but screwing around. For screwing
around, it answers all your needs and is happy to consume any amount of
unwanted time.

It does basically work as described above, though. Urbit at present propagates
all its updates through its own filesystem, runs its own chat server over its
own network protocol, etc. As of early 2014, it is an interesting prototype,
not a useful device.

http://doc.urbit.org/[+http://doc.urbit.org/+]

User-Mode Linux
~~~~~~~~~~~~~~~

User-Mode Linux is a safe, secure way of running Linux versions and Linux
processes. Run buggy software, experiment with new Linux kernels or
distributions, and poke around in the internals of Linux, all without risking
your main Linux setup.

User-Mode Linux gives you a virtual machine that may have more hardware and
software virtual resources than your actual, physical computer. Disk storage
for the virtual machine is entirely contained inside a single file on your
physical machine. You can assign your virtual machine only the hardware access
you want it to have. With properly limited access, nothing you do on the
virtual machine can change or damage your real computer, or its software. 

http://user-mode-linux.sourceforge.net/[+http://user-mode-linux.sourceforge.net/+]

UV-CDAT
~~~~~~~

UV-CDAT is a powerful and complete front-end to a rich set of visual-data
exploration and analysis capabilities well suited for climate-data analysis
problems.

http://uvcdat.llnl.gov/[+http://uvcdat.llnl.gov/+]

https://github.com/UV-CDAT[+https://github.com/UV-CDAT+]

[[NV]]
////
NVVV
////

VAGRANT
~~~~~~~

Vagrant is a tool for building complete development environments. With an
easy-to-use workflow and focus on automation, Vagrant lowers development
environment setup time, increases development/production parity, and makes the
"works on my machine" excuse a relic of the past.

Vagrant provides easy to configure, reproducible, and portable work
environments built on top of industry-standard technology and controlled by a
single consistent workflow to help maximize the productivity and flexibility
of you and your team.

To achieve its magic, Vagrant stands on the shoulders of giants. Machines are
provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then,
industry-standard provisioning tools such as shell scripts, Chef, or Puppet,
can be used to automatically install and configure software on the machine.

https://www.vagrantup.com/[+https://www.vagrantup.com/+]

Vaucanson
~~~~~~~~~

The Vaucanson platform VCSN is a software dedicated to the computation of, and
with, finite state machines. Here finite state machines is to be understood in
the broadest possible sense: finite automata with output — often called
transducers then — or even more generally finite automata with multiplicity,
that is, automata that not only accept, or recognize, sequences of symbols but
compute for every such sequence a value that is associated with it and which
can be taken in any semiring. Hence the variety of situations that can thus be
modellized.

VCSN has been designed with (at least) three goals in mind: to allow generic
programming of a wide class of finite automata, to provide a language close to
the mathematical description of algorithms on automata, to be a free and open
software.

http://www.vaucanson-project.org/Vaucanson/[+http://www.vaucanson-project.org/Vaucanson/+]

http://www.vaucanson-project.org/[+http://www.vaucanson-project.org/+]

http://ageinghacker.net/talks/wata-presentation--2014-05-07.pdf[+http://ageinghacker.net/talks/wata-presentation--2014-05-07.pdf+]

[[Vega]]
Vega
~~~~

A declarative format for creating, saving, and sharing visualization designs. With Vega, visualizations are described in JSON, and generate interactive views using either HTML5 Canvas or SVG. 

http://vega.github.io/[+http://vega.github.io/+]

https://github.com/vega/vega[+https://github.com/vega/vega+]

[[Vega-lite]]
Vega-lite
^^^^^^^^^

Provides a higher-level grammar for visual analysis, comparable to ggplot or Tableau, that generates complete Vega specifications.

https://github.com/vega/vega-lite[+https://github.com/vega/vega-lite+]

[[Vincent]]
Vincent
^^^^^^^

Vincent allows you to build Vega specifications in a Pythonic way, and performs type-checking to help ensure that your specifications are correct. It also has a number of convenience chart-building methods that quickly turn Python data structures into Vega visualization grammar, enabling graphical exploration. It allows for quick iteration of visualization designs via getters and setters on grammar elements, and outputs the final visualization to JSON.
Perhaps most importantly, Vincent has Pandas-Fu, and is built specifically to allow for quick plotting of DataFrames and Series.

http://vincent.readthedocs.org/en/latest/index.html[+http://vincent.readthedocs.org/en/latest/index.html+]

Verilog
~~~~~~~

[[Chisel]]
Chisel
^^^^^^

An open-source hardware construction language developed at UC Berkeley that supports advanced hardware design using highly parameterized generators and layered domain-specific hardware languages.

https://chisel.eecs.berkeley.edu/index.html[+https://chisel.eecs.berkeley.edu/index.html+]

[[Yosys]]
Yosys
^^^^^

Yosys is a framework for Verilog RTL synthesis. It currently has extensive Verilog-2005 support and provides a basic set of synthesis algorithms for various application domains.
Yosys can be adapted to perform any synthesis job by combining the existing passes (algorithms) using synthesis scripts and adding additional passes as needed by extending the Yosys Cxx code base.

http://www.clifford.at/yosys/about.html[+http://www.clifford.at/yosys/about.html+]

VEST
~~~~

We present a new package, VEST (Vector Einstein Summation Tools), that
performs abstract vector calculus computations in Mathematica. Through the use
of index notation, VEST is able to reduce three-dimensional scalar and vector
expressions of a very general type to a well defined standard form. In
addition, utilizing properties of the Levi-Civita symbol, the program can
derive types of multi-term vector identities that are not recognized by
reduction, subsequently applying these to simplify large expressions.

http://www.sciencedirect.com/science/article/pii/S0010465513002944[+http://www.sciencedirect.com/science/article/pii/S0010465513002944+]

http://arxiv.org/abs/1309.2561[+http://arxiv.org/abs/1309.2561+]

http://arxiv.org/abs/1312.3973[+http://arxiv.org/abs/1312.3973+]

VIGRA
~~~~~

VIGRA stands for "Vision with Generic Algorithms". It's an image processing
and analysis library that puts its main emphasis on customizable algorithms
and data structures. VIGRA is especially strong for multi-dimensional images,
because many algorithms (e.g. filters, feature computation, superpixels) are
implemented for arbitrary high dimensions. By using template techniques
similar to those in the Cxx Standard Template Library, you can easily adapt
any VIGRA component to the needs of your application, without thereby giving
up execution speed. As of version 1.7.1, VIGRA also provides extensive Python
bindings on the basis of the popular numpy framework. 

http://ukoethe.github.io/vigra/[+http://ukoethe.github.io/vigra/+]

Virtualization
~~~~~~~~~~~~~~

FAUmachine
^^^^^^^^^^

FAUmachine is a virtual machine, similar in many respects to VMWare[tm], QEMU
or Virtual PC[tm]. What distinguishes FAUmachine from these other virtual
machines, are the following features: 

* The FAUmachine virtual machine runs as a normal user process (no root
privileges or kernel modules needed) on top of (currently) Linux on i386 and
AMD64 hardware. The port of FAUmachine to OpenBSD and Mac OS X (intel) is in
progress. 
* Fault injection capability for experimentation in FAUmachine. 
* VHDL interpreter for automating experiments and tests based upon our project
fauhdlc. We also ship example scripts for our VHDL interpreter that allow
the automatic installation of several Linux distributions and other
operating systems using the distribution's cdrom. 
* The CPU of FAUmachine is based on the virtual CPU from Fabrice Bellard's
excellent xref:QEMU[QEMU] simulator, which can execute anything a real x86/AMD64 CPU
can execute, too. 

http://www3.cs.fau.de/Research/FAUmachine/[+http://www3.cs.fau.de/Research/FAUmachine/+]

micro-CernVM
^^^^^^^^^^^^

micro-CernVM is the heart of the CernVM 3 virtual appliance.  It is based on
Scientific Linux 6 combined with a custom, virtualization-friendly Linux
kernel.  This image is also fully RPM based; you can use yum and rpm to
install additional packages.

micro-CernVM's outstanding feature is that it does not require a hard disk
image to
be distributed (hence "micro").  Instead it is distributed as a CD-ROM image
of ~10MB containing a Linux kernel and the CernVM-FS client.  The rest of the
operating system is downloaded and cached on demand by CernVM-FS.  The virtual
machine still requires a hard disk as a persistent cache, but this hard disk
is initially empty and can be created instantaneously, instead of being
pre-created and distributed.

http://cernvm.cern.ch/portal/ucernvm[+http://cernvm.cern.ch/portal/ucernvm+]

https://github.com/cernvm/cernvm-micro[+https://github.com/cernvm/cernvm-micro+]

http://arxiv.org/abs/1311.2426[+http://arxiv.org/abs/1311.2426+]

MirageOS
^^^^^^^^

Mirage OS is a library operating system that constructs unikernels for secure,
high-performance network applications across a variety of cloud computing and
mobile platforms. Code can be developed on a normal OS such as Linux or MacOS
X, and then compiled into a fully-standalone, specialised unikernel that runs
under the Xen hypervisor.

Since Xen powers most public cloud computing infrastructure such as Amazon EC2
or Rackspace, this lets your servers run more cheaply, securely and with finer
control than with a full software stack.

Mirage uses the OCaml language, with libraries that provide networking,
storage and concurrency support that work under Unix during development, but
become operating system drivers when being compiled for production deployment.
The framework is fully event-driven, with no support for preemptive threading.

http://openmirage.org/[+http://openmirage.org/+]

OpenShift
^^^^^^^^^

OpenShift Origin is Red Hat’s open source Platform as a Service (PaaS)
offering. OpenShift Origin is an application platform where application
developers and teams can build, test, deploy, and run their applications. 

http://www.openshift.org/[+http://www.openshift.org/+]

http://commons.openshift.org/[+http://commons.openshift.org/+]

https://blog.openshift.com/[+https://blog.openshift.com/+]

http://origin.ly/[+http://origin.ly/+]

OpenVZ
^^^^^^

OpenVZ is container-based virtualization for Linux. OpenVZ creates multiple
secure, isolated Linux containers (otherwise known as VEs or VPSs) on a single
physical server enabling better server utilization and ensuring that
applications do not conflict. Each container performs and executes exactly
like a stand-alone server; a container can be rebooted independently and have
root access, users, IP addresses, memory, processes, files, applications,
system libraries and configuration files.

OpenVZ software consists of an optional custom Linux kernel and command-line
tools (mainly vzctl). Our kernel developers work hard to merge containers
functionality into the upstream Linux kernel, making OpenVZ team the biggest
contributor to Linux Containers (LXC) kernel, with features such as PID and
network namespaces, memory controller, checkpoint-restore (see CRIU.org) and
much more.

http://openvz.org/Main_Page[+http://openvz.org/Main_Page+]

QEMU
^^^^

QEMU is a generic and open source machine emulator and virtualizer.

When used as a machine emulator, QEMU can run OSes and programs made for one
machine (e.g. an ARM board) on a different machine (e.g. your own PC). By
using dynamic translation, it achieves very good performance.

When used as a virtualizer, QEMU achieves near native performances by
executing the guest code directly on the host CPU. QEMU supports
virtualization when executing under the Xen hypervisor or using the KVM kernel
module in Linux. When using KVM, QEMU can virtualize x86, server and embedded
PowerPC, and S390 guests. 

http://wiki.qemu.org/Main_Page[+http://wiki.qemu.org/Main_Page+]

rkt
^^^

A CLI for running app containers, and an implementation of the App Container
Spec. The goal of rkt is to be composable, secure, and fast.
The CLI for rkt is called rkt, and is currently supported on amd64 Linux. A
modern kernel is required but there should be no other system dependencies. We
recommend booting up a fresh virtual machine to test out rkt.

https://github.com/coreos/rkt[+https://github.com/coreos/rkt+]

https://coreos.com/blog/rocket/[+https://coreos.com/blog/rocket/+]

[[VisAD]]
VisAD
~~~~~

A Java component library for interactive and collaborative visualization and analysis of numerical data.
VisAD uses a general mathematical data model that can be adapted to virtually any numerical data, that supports data sharing among different users, different data sources and different scientific disciplines, and that provides transparent access to data independent of storage format and location (i.e., memory, disk or remote). The data model has been adapted to netCDF, HDF-5, FITS, HDF-EOS, McIDAS, Vis5D, GIF, JPEG, TIFF, QuickTime, ASCII and many other file formats. 

It also features a general display model that supports interactive 3-D, data fusion, multiple data views, direct manipulation, collaboration, and virtual reality. The display model has been adapted to Java3D and Java2D and used in an ImmersaDesk virtual reality display. 

http://www.ssec.wisc.edu/\~billh/visad.html[+http://www.ssec.wisc.edu/~billh/visad.html+]

Visionworkbench
~~~~~~~~~~~~~~~

The NASA Vision Workbench (VW) is a general purpose image processing and
computer vision library developed by the Autonomous Systems and Robotics (ASR)
Area in the Intelligent Systems Division at the NASA Ames Research Center.

http://ti.arc.nasa.gov/tech/asr/intelligent-robotics/nasa-vision-workbench/[+http://ti.arc.nasa.gov/tech/asr/intelligent-robotics/nasa-vision-workbench/+]

https://github.com/visionworkbench/visionworkbench[+https://github.com/visionworkbench/visionworkbench+]

Vispy
~~~~~

Vispy is a high-performance interactive 2D/3D data visualization library.
Vispy leverages the computational power of modern Graphics Processing Units
(GPUs) through the OpenGL library to display very large datasets.

http://vispy.org/[+http://vispy.org/+]

http://ipython-books.github.io/featured-06/[+http://ipython-books.github.io/featured-06/+]

Vistle
~~~~~~

Vistle, the VISualization Testing Laboratory for Exascale computing, is an
extensible software environment that integrates simulations on supercomputers,
post-processing and parallel interactive visualization.

It is under active development at HLRS since 2012 within the European project
CRESTA and bwVisu. The objective is to provide a highly scalable successor to
COVISE, exploiting data, task and pipeline parallelism in hybrid shared and
distributed memory environments with acceleration hardware. Domain
decompositions used during simulation can be reused for visualization.

A Vistle work flow consists of several processing modules, each of which is a
parallel MPI program that uses OpenMP within nodes. These can be configured
graphically or from Python. Shared memory is used for transfering data between
modules on a single node. Work flows can be distributed across several
clusters.

For rendering in immersive projection systems, Vistle uses OpenCOVER.
Visualization parameters can be manipulated from within the virtual
environment. Large data sets can be displayed with OpenGL sort-last parallel
rendering and depth compositing. For scaling with the simulation on remote HPC
resources, a CPU based hybrid sort-last/sort first parallel ray casting
renderer is available. "Remote hybrid rendering" allows to combine its output
with local rendering, while ensuring smooth interactivity by decoupling it
from remote rendering.

The Vistle system is modular and can be extended easily with additional
visualization algorithms. Source code is available on GitHub and licensed
under the LPGL. 

https://github.com/vistle/vistle[+https://github.com/vistle/vistle+]

http://www.hlrs.de/organization/av/vis/vistle/[+http://www.hlrs.de/organization/av/vis/vistle/+]

COVISE
^^^^^^

COVISE, the collaborative visualization and simulation environment, is a
modular distributed visualization system. As its focus is on visualization of
scientific data in virtual environments, it comprises the VR renderer
OpenCOVER. 

https://github.com/hlrs-vis/covise[+https://github.com/hlrs-vis/covise+]

VMD
~~~

VMD is designed for modeling, visualization, and analysis of biological
systems such as proteins, nucleic acids, lipid bilayer assemblies, etc. It may
be used to view more general molecules, as VMD can read standard Protein Data
Bank (PDB) files and display the contained structure. VMD provides a wide
variety of methods for rendering and coloring a molecule: simple points and
lines, CPK spheres and cylinders, licorice bonds, backbone tubes and ribbons,
cartoon drawings, and others. VMD can be used to animate and analyze the
trajectory of a molecular dynamics (MD) simulation. In particular, VMD can act
as a graphical front end for an external MD program by displaying and
animating a molecule undergoing simulation on a remote computer. 

http://www.ks.uiuc.edu/Research/vmd/[+http://www.ks.uiuc.edu/Research/vmd/+]

http://www.ks.uiuc.edu/Research/vmd/plugins/[+http://www.ks.uiuc.edu/Research/vmd/plugins/+]

VPaint
~~~~~~

VPaint is an innovative yet simple vector graphics program. This means that
unlike pixel-based graphics program (e.g., Photoshop), VPaint allows you to
edit any of your pen strokes at any time, and resize your illustrations at any
resolution without loss of detail or sharpness.
Using VPaint is extremely easy. First, you sketch curves with your mouse,
touch screen, or pen tablet. Then, you use the paint bucket to fill closed
areas delimited by curves. Finally, you can edit your illustration by dragging
painted areas or sculpting curves.

http://www.vpaint.org/[+http://www.vpaint.org/+]

http://www.dalboris.com/research/vgc/[+http://www.dalboris.com/research/vgc/+]

[[NW]]
////
NWWW
////

WACCM
~~~~~

The Whole Atmosphere Community Climate Model (WACCM) is a comprehensive
numerical model, spanning the range of altitude from the Earth's surface to
the thermosphere. The development of WACCM is an inter-divisional
collaboration that unifies certain aspects of the upper atmospheric modeling
of HAO, the middle atmosphere modeling of ACD, and the tropospheric modeling
of CGD, using the NCAR Community Earth System Model (CESM) as a common
numerical framework.

http://www2.cesm.ucar.edu/working-groups/wawg[+http://www2.cesm.ucar.edu/working-groups/wawg+]

Wakari
~~~~~~

Web-based Python Data Analysis.

A Distributed Approach to Ocean Etc. Model Data Interoperability -
http://testbed.sura.org/sites/default/files/2014-01-30_COMT.pdf[+http://testbed.sura.org/sites/default/files/2014-01-30_COMT.pdf+]

Accessing ncSOS with OWSLib and Pyoos -
https://www.wakari.io/sharing/bundle/rsignell/ncSOS_and_OWSlib_and_pyoos[+https://www.wakari.io/sharing/bundle/rsignell/ncSOS_and_OWSlib_and_pyoos+]

Testing pyoos ioos.get_observation parser with a Milestone 1 XML template file
- https://www.wakari.io/sharing/bundle/emayorga/pyoos1_test_3[+https://www.wakari.io/sharing/bundle/emayorga/pyoos1_test_3+]

Creating your own IPython Notebook Blog environment on Wakari -
https://github.com/rsignell-usgs/blog/blob/master/wakari.md[+https://github.com/rsignell-usgs/blog/blob/master/wakari.md+]

Ocean Model Assessment for Everyone -
https://conference.scipy.org/scipy2014/schedule/presentation/1695/[+https://conference.scipy.org/scipy2014/schedule/presentation/1695/+]

Testing IOOS Infrastructure with Wakari -
https://github.com/ioos/ipython-notebooks/blob/master/wakari.md[+https://github.com/ioos/ipython-notebooks/blob/master/wakari.md+]

https://wakari.io/[+https://wakari.io/+]

Installing Iris on Wakari -
https://www.youtube.com/watch?v=2qF-EbjNL50[+https://www.youtube.com/watch?v=2qF-EbjNL50+]

Versioning Wakari Notesbooks on Github -
https://www.youtube.com/watch?v=0SQa2a18tMM[+https://www.youtube.com/watch?v=0SQa2a18tMM+]

Running a Shared Wakari Notebook/Environment -
https://www.youtube.com/watch?v=4NyMWK4as-U[+https://www.youtube.com/watch?v=4NyMWK4as-U+]

Walrus
~~~~~~

 Walrus is a tool for interactively visualizing large directed graphs in
three-dimensional space. It is technically possible to display graphs
containing a million nodes or more, but visual clutter, occlusion, and other
factors can diminish the effectiveness of Walrus as the number of nodes, or
the degree of their connectivity, increases. Thus, in practice, Walrus is best
suited to visualizing moderately sized graphs that are nearly trees. A graph
with a few hundred thousand nodes and only a slightly greater number of links
is likely to be comfortable to work with.

Walrus computes its layout based on a user-supplied spanning tree. Because the
specifics of the supplied spanning tree greatly affect the resulting display,
it is crucial that the user supply a spanning tree that is both meaningful for
the underlying data and appropriate for the desired insight. The prominence
and orderliness that Walrus gives to the links in the spanning tree, in
contrast to all other links, means that an arbitrarily chosen spanning tree
may create a misleading or ineffective visualization. Ideally, the input
graphs should be inherently hierarchical.

Walrus uses 3D hyperbolic geometry to display graphs under a fisheye-like
distortion. At any moment, the amount of magnification, and thus the level of
visible detail, varies across the display. This allows the user to examine the
fine details of a small area while always having a view of the whole graph
available as a frame of reference. Graphs are rendered inside a sphere that
contains the Euclidean projection of 3D hyperbolic space. Points within the
sphere are magnified according to their radial distance from the center.
Objects near the center are magnified, while those near the boundary are
shrunk. The amount of magnification decreases continuously and at an
accelerated rate from the center to the boundary, until objects are reduced to
zero size at the latter, which represents infinity. By bringing different
parts of a graph to the magnified central region, the user can examine every
part of the graph in detail. 

http://www.caida.org/tools/visualization/walrus/[+http://www.caida.org/tools/visualization/walrus/+]

wavelets
~~~~~~~~

Also ridgelets, curvelets, starlets, wedgelets, bandlets, etc.
See

http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html[+http://www.laurent-duval.eu/siva-wits-where-is-the-starlet.html+]

http://www.sciencedirect.com/science/article/pii/S0165168411001356[+http://www.sciencedirect.com/science/article/pii/S0165168411001356+]

for more about the whole zoo.

http://www.multiresolutions.com/sparsesignalrecipes/software.html[+http://www.multiresolutions.com/sparsesignalrecipes/software.html+]

CurveLab
^^^^^^^^

The Curvelet transform is a higher dimensional generalization of the Wavelet
transform designed to represent images at different scales and different
angles.
CurveLab is a toolbox implementing the Fast Discrete Curvelet Transform, both
in Matlab and Cxx.

http://www.curvelet.org/[+http://www.curvelet.org/+]

FFST
^^^^

In recent years it has turned out that shearlets have the potential to
retrieve directional information so that they became interesting for many
applications. Moreover the continuous shearlet transform has the outstanding
property to stem from a square integrable group representation. However, to
use shearlets and the shearlet transform for reasonable applications one needs
fast algorithms to compute a discrete shearlet transform. In this tutorial we
present the steps towards an implementation of a fast and finite shearlet
transform that is only based on the FFT.

The FFST package provides a fast implementation of the Finite Shearlet
Transform. Following the the path via the continuous shearlet transform, its
counterpart on cones and finally its discretization on the full grid we obtain
the translation invariant discrete shearlet transform. Our discrete shearlet
transform can be efficiently computed by the fast Fourier transform (FFT). The
discrete shearlets constitute a Parseval frame of the finite Euclidean space
such that the inversion of the shearlet transform can be simply done by
applying the adjoint transform.

http://www.mathematik.uni-kl.de/imagepro/software/ffst/[+http://www.mathematik.uni-kl.de/imagepro/software/ffst/+]

http://arxiv.org/abs/1202.1773[+http://arxiv.org/abs/1202.1773+]

[[iSAP]]
iSAP
^^^^

The Interactive Sparse Astronomical Data Analysis packages are a
collection of packages in IDL, Cxx and Matlab related to sparsity and its
application in astronomical data analysis.
The components include:

* Sparse2D - sparse decomposition, denoising and deconvolution
for 1- and 2-D datasets
* MSVST - multi-scale variance stabilizing transform for
1- and 2-D datasets
* MRS - sparse, multiresolution representation on the sphere
* SparsePOL - polarized wavelets and curvelets on the sphere
* MRS-MSVSTS - multi-scale variance stabilizing transform
on the sphere

http://www.cosmostat.org/software/isap/[+http://www.cosmostat.org/software/isap/+]

http://jstarck.free.fr/mrs.html[+http://jstarck.free.fr/mrs.html+]

http://jstarck.free.fr/sphMCA_revised_20070823.pdf[+http://jstarck.free.fr/sphMCA_revised_20070823.pdf+]

http://jstarck.free.fr/aa_sphere05.pdf[+http://jstarck.free.fr/aa_sphere05.pdf+]

http://jstarck.free.fr/chapter3d.pdf[+http://jstarck.free.fr/chapter3d.pdf+]

PyrTools
^^^^^^^^

This package contains some MatLab tools for multi-scale image
processing.  Briefly, the tools include:

* Recursive multi-scale image decompositions (pyramids), including
Laplacian pyramids, QMFs, Wavelets, and steerable pyramids.  These
operate on 1D or 2D signals of arbitrary dimension.  Data
structures are compatible with the MatLab wavelet toolbox.
* Fast 2D convolution routines, with subsampling and boundary-handling.
* Fast point-operations, histograms, histogram-matching.
* Fast synthetic image generation: sine gratings, zone plates, fractals,
etc.
* Display routines for images and pyramids.  These include several
auto-scaling options, rounding to integer zoom factors to avoid 
resampling artifacts, and useful labeling (dimensions and gray-range).

http://www.cns.nyu.edu/\~eero/software.php[+http://www.cns.nyu.edu/~eero/software.php+]

PyWavelets
^^^^^^^^^^

PyWavelets is a free Open Source wavelet transform software for Python
programming language. It is written in Python, Cython and C for a mix of easy
and powerful high-level interface and the best performance.

http://www.pybytes.com/pywavelets/[+http://www.pybytes.com/pywavelets/+]

https://github.com/nigma/pywt[+https://github.com/nigma/pywt+]

S2DW
^^^^

The S2DW code provides functionality to perform the scale discretised wavelet transform on the sphere developed in our paper: Exact reconstruction with directional wavelets on the sphere (ArXiv|DOI). Routines are provided to compute wavelet and scaling coefficients from the spherical harmonic coefficients of a signal on the sphere and to synthesise the spherical harmonic coefficients of the original signal from its wavelet and scaling coefficients. The reconstruction of the spherical harmonic coefficients of the original signal is exact to numerical precision. Typically, maximum reconstruction errors are of the order 10^(-12) or smaller. Please see our paper for further details of the wavelet transform and a discussion of typical reconstruction errors and execution times of this implementation.

http://www.jasonmcewen.org/codes/s2dw/doc/index_s2dw.html[+http://www.jasonmcewen.org/codes/s2dw/doc/index_s2dw.html+]

ShearLab
^^^^^^^^

Wavelets and their associated transforms are highly efficient when
approximating and analyzing one-
dimensional signals. However, multivariate signals such as images or videos
typically exhibit curvilinear
singularities, which wavelets are provably deficient of sparsely approximating
and also of analyzing in the
sense of, for instance, detecting their direction. Shearlets are a directional
representation system extending
the wavelet framework, which overcomes those deficiencies. Similar to
wavelets, shearlets allow a faithful
implementation and fast associated transforms.

This package provides MATLAB code for a novel faithful algorithmic realization
of the 2D and 3D shearlet
transform (and their inverses) associated with compactly supported universal
shearlet systems incorporat-
ing the option of using CUDA.

http://www.shearlab.org/[+http://www.shearlab.org/+]

http://www.shearlab.org/papers/final_ShearLab3D_revised.pdf[+http://www.shearlab.org/papers/final_ShearLab3D_revised.pdf+]

Wayland
~~~~~~~

Wayland is intended as a simpler replacement for X, easier to develop and
maintain. GNOME and KDE are expected to be ported to it.

Wayland is a protocol that specifies the communication between a display
server (called Wayland compositor) and its clients, as well as a reference
implementation of the protocol in the C programming language.[4]

Wayland is developed by a group of volunteers led by Kristian Høgsberg as a
free and open-source software community-driven project with the aim of
replacing the X Window System with a modern, simpler windowing system in Linux
and Unix-like operating systems.

Wayland consists of a protocol and a reference implementation named Weston.
The project is also developing versions of GTK+ and Qt that render to Wayland
instead of to X. Most applications are expected to gain support for Wayland
through one of these libraries without modification to the application.

http://wayland.freedesktop.org/[+http://wayland.freedesktop.org/+]

http://wayland.freedesktop.org/faq.html[+http://wayland.freedesktop.org/faq.html+]

http://en.wikipedia.org/wiki/Wayland_%28display_server_protocol%29[+http://en.wikipedia.org/wiki/Wayland_%28display_server_protocol%29+]

WCSAxes
~~~~~~~

WCSAxes is a framework for making plots of Astronomical data in Matplotlib.

http://wcsaxes.readthedocs.org/en/latest/[+http://wcsaxes.readthedocs.org/en/latest/+]

web servers
~~~~~~~~~~~

[[rwasa]]
rwasa
^^^^^

A full-featured, high performance, scalable web server designed to compete with the likes of nginx. It has been built from the ground-up with no externel library dependencies entirely in x86_64 assembly language, and is the result of many years' experience with high volume web environments. In addition to all of the common things you'd expect a modern web server to do, we also include assembly language function hooks ready-made to facilitate Rapid Web Application Server (in Assembler) development.

https://2ton.com.au/rwasa/[+https://2ton.com.au/rwasa/+]

websocketd
~~~~~~~~~~

Full duplex messaing between web browsers and servers.
It takes care of handling the WebSocket connections,
launching your programs to handle the WebSockets,
and passing messages between programs and web-browser.
It's like CGI, twenty years later, for WebSockets.

http://websocketd.com/[+http://websocketd.com/+]

word2vec
~~~~~~~~

This tool provides an efficient implementation of the continuous bag-of-words
and skip-gram architectures for computing vector representations of words.
These representations can be subsequently used in many natural language
processing applications and for further research. 

The word2vec tool takes a text corpus as input and produces the word vectors
as output. It first constructs a vocabulary from the training text data and
then learns vector representation of words. The resulting word vector file can
be used as features in many natural language processing and machine learning
applications. 

https://code.google.com/p/word2vec/[+https://code.google.com/p/word2vec/+]

http://google-opensource.blogspot.com/2013/08/learning-meaning-behind-words.html[+http://google-opensource.blogspot.com/2013/08/learning-meaning-behind-words.html+]

https://gigaom.com/2013/08/16/were-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later/[+https://gigaom.com/2013/08/16/were-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later/+]

worldview
~~~~~~~~~

This tool from NASA's EOSDIS provides the capability to interactively browse
global, full-resolution satellite imagery and then download the underlying
data. Most of the 100+ available products are updated within three hours of
observation, essentially showing the entire Earth as it looks "right now".
This supports time-critical application areas such as wildfire management, air
quality measurements, and flood monitoring. Arctic and Antarctic views of
several products are also available for a "full globe" perspective. Browsing
on tablet and smartphone devices is generally supported for mobile access to
the imagery.

Worldview uses the Global Imagery Browse Services (GIBS) to rapidly retrieve
its imagery for an interactive browsing experience. While Worldview uses
OpenLayers as its mapping library, GIBS imagery can also be accessed from
Google Earth, NASA World Wind, and several other clients. We encourage
interested developers to build their own clients or integrate NASA imagery
into their existing ones using these services.

https://github.com/nasa-gibs/worldview[+https://github.com/nasa-gibs/worldview+]

WRF
~~~

The Weather Research and Forecasting (WRF) Model is a next-generation
mesoscale numerical weather prediction system designed to serve both
atmospheric research and operational forecasting needs. It features two
dynamical cores, a data assimilation system, and a software architecture
facilitating parallel computation and system extensibility. The model serves a
wide range of meteorological applications across scales from tens of meters to
thousands of kilometers.

WRF allows researchers to generate atmospheric simulations based on real data
(observations, analyses) or idealized conditions. WRF offers operational
forecasting a flexible and computationally-efficient platform, while providing
advances in physics, numerics, and data assimilation contributed by developers
in the broader research community. WRF is currently in operational use at
NCEP, AFWA, and other centers.

http://wrf-model.org/index.php[+http://wrf-model.org/index.php+]

GSI
^^^

The Community Gridpoint Statistical Interpolation (GSI) system is a
variational data assimilation system, designed to be flexible, state-of-art,
and run efficiently on various parallel computing platforms. The GSI system is
in the public domain and is freely available for community use. 
The testing and support of this GSI system at the DTC currently focus on
regional numerical weather prediction (NWP) applications coupled with the
Weather Research and Forecasting (WRF) Model , but the GSI can be applied to
Global Forecast System(GFS) as well as other modelling systems. 

The GSI version 3.3 GSI is an operational data assimilation system available
for community use. Some of these GSI advanced features is listed as follows: 

* Combined with an ensemble system, this version of GSI can be used as an
ensemble-variational hybrid data assimilation system.
* Coupled with forecast models and their adjoint models, GSI can be turned
into a four-dimensional variational (4D-Var) system.
* GSI features capabilities for observation sensitivity calculation.
* The observation operators in GSI can be used in an EnKF system or other data
analysis systems, transforming model variables to observed variables at the
observational space.

http://www.dtcenter.org/com-GSI/users/[+http://www.dtcenter.org/com-GSI/users/+]

HWRF
^^^^

The Weather Research and Forecasting (WRF) Model is designed to serve both
operational forecasting and atmospheric research needs. It features two
dynamic cores, multiple physical parameterizations, a variational data
assimilation system, ability to couple with an ocean model, and a software
architecture allowing for computational parallelism and system extensibility.
WRF is suitable for a broad spectrum of applications, including tropical
storms.

Two robust configurations of WRF for tropical storms are the NOAA operational
model Hurricane WRF (HWRF) and the National Center for Atmospheric Research
(NCAR) Advanced Research Hurricane WRF (AHW). In this website users can obtain
codes, datasets, and information for running both HWRF and AHW. 

http://www.dtcenter.org/HurrWRF/users/index.php[+http://www.dtcenter.org/HurrWRF/users/index.php+]

MET
^^^

The Model Evaluation Tools (MET) verification package is a
highly-configurable, state-of-the-art suite of verification tools. It was
developed using output from the Weather Research and Forecasting (WRF)
modeling system but may be applied to the output of other modeling systems as
well.

http://www.dtcenter.org/met/users/[+http://www.dtcenter.org/met/users/+]

NEMS-NMMB
^^^^^^^^^

The NOAA Environmental Modeling System (NEMS) Nonhydrostatic Multiscale Model
on the B-grid (NMMB) was developed and continues to be enhanced to establish a
common modeling framework that facilitates streamlined interactions of
analysis, forecast, and post-processing systems within NCEP. The NEMS
architecture is a high performance software superstructure and infrastructure
based on the Earth System Modeling Framework (ESMF) for use in operational
prediction models at NCEP.

The NAM modeling suite was the first operational implementation of NEMS at
NCEP. The NAM prediction model within the NEMS framework is the Nonhydrostatic
Multiscale Model on B-grid (NMMB), which can be run globally or regionally
with embedded nests.

http://www.dtcenter.org/nems-nmmb/users/[+http://www.dtcenter.org/nems-nmmb/users/+]

http://www.emc.ncep.noaa.gov/index.php?branch=NEMS[+http://www.emc.ncep.noaa.gov/index.php?branch=NEMS+]

WRFDA
^^^^^

The Weather Research and Forecasting (WRF) model data assimilation system (WRFDA)
 is designed to be a flexible, state-of-the-art atmospheric data assimilation system that is portable and efficient on available parallel computing platforms. WRFDA is suitable for use in a broad range of applications, across scales ranging from kilometers for regional and mesoscale modeling to thousands of kilometers for global scale modeling. 

http://www2.mmm.ucar.edu/wrf/users/wrfda/index.html[+http://www2.mmm.ucar.edu/wrf/users/wrfda/index.html+]

WRF-NMM
^^^^^^^

The Nonhydrostatic Mesoscale Model (NMM) core of the Weather Research and
Forecasting (WRF) system is designed to be a flexible, state-of-the-art
atmospheric simulation system that is portable and efficient on available
parallel computing platforms. WRF-NMM is suitable for use in a broad range of
applications across scales ranging from meters to thousands of kilometers.
The package includes:

* Nonhydrostatic Mesoscale Model (NMM) dynamic solver, including one-way and
two-way static nesting;
* WRF Preprocessing System (WPS);
* Numerous physics packages contributed by WRF partners and the research
community; and
* Unified Post Processor (UPP) software package and sample scripts for several
graphical packages.

http://www.dtcenter.org/wrf-nmm/users/[+http://www.dtcenter.org/wrf-nmm/users/+]

WRFPLUS
^^^^^^^

WRFPLUS is a package containing the adjoint and tangent-linear version of WRF model, in addition to a specialized version of the non-linear WRF model (previously known as WRFNL). It is tailored for use with WRFDA 4D-Var only.

http://www2.mmm.ucar.edu/wrf/users/wrfda/download/wrfplus.html[+http://www2.mmm.ucar.edu/wrf/users/wrfda/download/wrfplus.html+]

http://www.geosci-model-dev.net/8/1857/2015/gmd-8-1857-2015.html[+http://www.geosci-model-dev.net/8/1857/2015/gmd-8-1857-2015.html+]

WTURB
~~~~~

This code is a reference implementation of our paper Wavelet Turbulence for
Fluid Simulation. The code is intended as a pedagogical example, so clarity
has been given preference over performance. Optimizations that inhibit
readability have been removed, so the running times experienced will be longer
than those reported in the paper.

http://www.cs.cornell.edu/\~tedkim/WTURB/source.html[+http://www.cs.cornell.edu/~tedkim/WTURB/source.html+]

http://www.cs.cornell.edu/\~tedkim/WTURB/[+http://www.cs.cornell.edu/~tedkim/WTURB/+]

[[WXtoImg]]
WXtoImg
~~~~~~~

WXtoImg is a fully automated APT and WEFAX weather satellite (wxsat) decoder. The software supports recording, decoding, editing, and viewing on all versions of Windows, Linux, and Mac OS X. WXtoImg supports real-time decoding, map overlays, advanced colour enhancements, 3-D images, animations, multi-pass images, projection transformation (e.g. Mercator), text overlays, automated web page creation, temperature display, GPS interfacing, wide-area composite image creation and computer control for many weather satellite receivers, communications receivers, and scanners.

WXtoImg makes use of the 16-bit sampling capabilities of soundcards to provide better decoding than is possible with expensive purpose-designed hardware decoders.
WXtoImg comes in a basic freeware version that provides a large range of features. Improved automation, new enhancements, a wider variety of options, projection transformations and improved quality images from communications receivers and scanners are available by upgrading the software.

http://www.wxtoimg.com/[+http://www.wxtoimg.com/+]

[[NXXX]]
////
NXXX
////

XBeach
~~~~~~

XBeach is a two-dimensional model for wave propagation, long waves and mean
flow, sediment transport and morphological changes of the nearshore area,
beaches, dunes and backbarrier during storms.

http://oss.deltares.nl/web/xbeach/[+http://oss.deltares.nl/web/xbeach/+]

XBee
~~~~

XBee is the brand name from Digi International for a family of form factor
compatible radio modules. The first XBee radios were introduced under the
MaxStream brand in 2005[2] and were based on the 802.15.4-2003 standard
designed for point-to-point and star communications at over-the-air baud rates
of 250 kbit/s.

XBee API Mode Tutorial Using Python and Arduino -
http://serdmanczyk.github.io/XBeeAPI-PythonArduino-Tutorial/[+http://serdmanczyk.github.io/XBeeAPI-PythonArduino-Tutorial/+]

xbee-arduino
^^^^^^^^^^^^

An Arduino library for communicating with XBees in API mode, with support for
both Series 1 (802.15.4) and Series 2 (ZB Pro/ZNet). This library Includes
support for the majority of packet types, including: TX/RX, AT Command, Remote
AT, I/O Samples and Modem Status.

http://code.google.com/p/xbee-arduino/[+http://code.google.com/p/xbee-arduino/+]

XBraid
~~~~~~

Scientists at LLNL have developed an open source, non-intrusive, and general
purpose parallel-in-time code, XBraid.  The algorithm enables a scalable
parallel-in-time approach by applying multigrid to the time dimension.
It is designed to be nonintrusive. That is, users apply their existing
sequential time-stepping code according to our interface, and then XBraid does
the rest. Users have spent years, sometimes decades, developing the right
time-stepping scheme for their problem. XBraid allows users to keep their
schemes, but enjoy parallelism in the time dimension.

Traditional sequential time-marching algorithms are a critical part of any
computer simulation of a time-dependent problem, but these algorithms are
currently facing a sequential bottleneck. This bottleneck is driven by the
broad trend that future performance gains will come from greater concurrency,
not faster clock speeds. Previously, ever-increasing clock speeds decreased
the compute time for each time step, thus allowing more time steps to be
calculated without increasing the overall compute time. Now that clock speeds
are stagnant, further refinements in time (i.e., increases in the number of
time steps) will simply increase the simulation’s overall compute time. Many
of these refinements in time will be required to maintain balance between
spatial and temporal accuracies. Additionally, some simulations are already
fully resolved in space, and it is unclear how such simulations will take
advantage of the coming increases in concurrency.

LLNL researchers have advanced an alternative solution—solving all of the time
steps simultaneously, with the help of a new multilevel algorithm and the
massively parallel processing capabilities of current and future
high-performance computers. This approach has already shown an ability to
dramatically decrease the solution time for some simulations by ten-fold or
more.

https://computation.llnl.gov/project/parallel-time-integration/[+https://computation.llnl.gov/project/parallel-time-integration/+]

XIOS
~~~~

XIOS stands for XML-IO-SERVER, a library dedicated to I/O management
of climate codes and model output.

https://forge.ipsl.jussieu.fr/ioserver[+https://forge.ipsl.jussieu.fr/ioserver+]

http://hpcforge.org/plugins/mediawiki/wiki/climate-io/index.php/Downloads[+http://hpcforge.org/plugins/mediawiki/wiki/climate-io/index.php/Downloads+]

https://hpcforge.org/projects/climate-io/[+https://hpcforge.org/projects/climate-io/+]

XMDS2
~~~~~

A software package that allows the fast and easy solution of sets of ordinary,
partial and stochastic differential equations, using a variety of efficient
numerical algorithms.
XMDS2 is a cross-platform, GPL-licensed, open source package for numerically
integrating initial value problems that range from a single ordinary
differential equation up to systems of coupled stochastic partial differential
equations. The equations are described in a high-level XML-based script, and
the package generates low-level optionally parallelised Cxx code for the
efficient solution of those equations. It combines the advantages of
high-level simulations, namely fast and low-error development, with the speed,
portability and scalability of hand-written code.

http://www.xmds.org/[+http://www.xmds.org/+]

http://www.sciencedirect.com/science/article/pii/S0010465512002822[+http://www.sciencedirect.com/science/article/pii/S0010465512002822+]

XMPP
~~~~

The Extensible Messaging and Presence Protocol (XMPP) is an open technology
for real-time communication, which powers a wide range of applications
including instant messaging, presence, multi-party chat, voice and video
calls, collaboration, lightweight middleware, content syndication, and
generalized routing of XML data.

http://xmpp.org/[+http://xmpp.org/+]

XSHELLS
~~~~~~~

XSHELLS is yet another code simulating incompressible ﬂuids in a spherical
cavity. In addition to the Navier-Stokes equation with an optional Coriolis
force, it can also time-step the coupled induction equation for MHD (with
imposed magnetic ﬁeld or in a dynamo regime), as well as the temperature (or
codensity) equation in the Boussineq framework. 

XSHELLS uses ﬁnite diﬀerences (second order) in the radial direction and
spherical harmonic decomposition (pseudo-spectral). The time-stepping uses
semi-implicit Crank-Nicolson scheme for the diﬀusive terms, while the
non-linear terms can be handled either by an Adams-Bashforth or a
Predictor-Corrector scheme (both second order in time). 

XSHELLS is written in Cxx and designed for speed. It uses the blazingly fast
spherical harmonic transform library SHTns, as well as hybrid parallelization
using OpenMP and/or MPI. This allows it to run eﬃciently on your laptop or on
parallel supercomputers. A post-processing program is provided to extract
useful data and export ﬁelds to matlab/octave, python/matplotlib or paraview. 

https://bitbucket.org/nschaeff/xshells[+https://bitbucket.org/nschaeff/xshells+]

X-Stack
~~~~~~~

The X-Stack Program was created to support research that targets significant
advances in programming models, languages, compilers, runtime systems and
tools. The expected results of this program are complete solutions to the
system software stack for Exascale computing platforms (X-Stack)which address
fundamental challenges identified in the ASCR Exascale Programming Challenges
Workshop, captured in the workshop report, as well as the ones identified in
the ASCR Exascale Tools Workshop, captured in the workshop report. Solutions
being researched involve radically new approaches to programming Exascale
applications and algorithms and will demonstrate the viability of such
solutions in a broad high performance programming context and will enable
automatic semantics and performance preserving transformations of applications
(possibly with users in the loop). 

https://xstackwiki.modelado.org/Extreme_Scale_Software_Stack[+https://xstackwiki.modelado.org/Extreme_Scale_Software_Stack+]

ROSE
^^^^

ROSE is an open source compiler infrastructure to build source-to-source
program transformation and analysis tools for large-scale C(C89 and C98),
Cxx(Cxx98 and Cxx11), UPC, Fortran (77/95/2003), OpenMP, Java, Python and PHP
applications. 

http://rosecompiler.org/[+http://rosecompiler.org/+]

https://github.com/rose-compiler/edg4x-rose[+https://github.com/rose-compiler/edg4x-rose+]

http://portal.nersc.gov/project/rosecompiler/dtec/wordpress/[+http://portal.nersc.gov/project/rosecompiler/dtec/wordpress/+]


SLEEC
^^^^^

We are building a generic, extensible compiler infrastructure that can
incorporate semantic information from domain-specific libraries to enable
transformations that leverage domain-specific properties of library methods.
Rather than building domain-specific compilers for each domain, our extensible
compiler becomes a domain specific compiler for a domain when paired with
domain-specific libraries.

https://xstackwiki.modelado.org/SLEEC[+https://xstackwiki.modelado.org/SLEEC+]

https://engineering.purdue.edu/SLEEC/[+https://engineering.purdue.edu/SLEEC/+]

[[NY]]
////
NYYY
////

Yael
~~~~

Yael is a library implementing computationally intensive functions used in
large scale image retrieval, such as neighbor search, clustering and inverted
files. The library offers interfaces for C, Python and Matlab.

http://yael.gforge.inria.fr/[+http://yael.gforge.inria.fr/+]

https://gforge.inria.fr/projects/yael/[+https://gforge.inria.fr/projects/yael/+]

Yesod
~~~~~

Yesod is a Haskell web framework for productive development of type-safe,
RESTful, high performance web applications.

http://www.yesodweb.com/[+http://www.yesodweb.com/+]

LambdaCms
^^^^^^^^^

LambdaCms is a set of packaged libraries, containing subsites for the Yesod
application framework, which allow rapid development of robust and highly
performant websites with content management functionality.

http://lambdacms.org/[+http://lambdacms.org/+]

Yorick
~~~~~~

Yorick is an interpreted programming language for scientific simulations or
calculations, postprocessing or steering large simulation codes, interactive
scientific graphics, and reading, writing, or translating large files of
numbers. Yorick includes an interactive graphics package, and a binary file
package capable of translating to and from the raw numeric formats of all
modern computers. Yorick is written in ANSI C and runs on most operating
systems.

http://yorick.sourceforge.net/[+http://yorick.sourceforge.net/+]

yt
~~

The yt project aims to produce an integrated science environment for
collaboratively asking and answering astrophysical questions. To do so, it
will encompass the creation of initial conditions, the execution of
simulations, and the detailed exploration and visualization of the resultant
data. It will also provide a standard framework based on physical quantities
interoperability between codes.

http://yt-project.org/[+http://yt-project.org/+]

[[NZ]]
////
NZZZ
////

Zato
~~~~

Open Source ESB, SOA, REST, APIs and Cloud Integrations in Python.
Build and orchestrate integration services, expose new or existing APIs,
either cloud or on-premise, and use a wide range of connectors, data formats
and protocols.

Zato facilitates intercommunication across applications and data sources
spanning your organization's business or technical boundaries and beyond,
enabling you to access, design, develop or discover new opportunities and
processes. 

The protocols, standards and formats supported are HTTP, REST, JSON, SOAP,
AMQP, JMS WebSphere MQ, ZeroMQ, Redis, SQL, Cassandra, Amazon S3, OpenStack
Swift, Odoo/OpenERP, SMTP, IMAP, FTP, Solr, ElasticSearch, publish/subscribe,
integration patterns, RBAC, and more.

https://zato.io/[+https://zato.io/+]

ZKCM
~~~~

ZKCM is a Cxx library developed for the purpose of multiprecision matrix
computation, on the basis of the GNU MP and MPFR libraries. It provides an
easy-to-use syntax and convenient functions for matrix manipulations including
those often used in numerical simulations in quantum physics. Its extension
library, ZKCM_QC, is developed for simulating quantum computing using the
time-dependent matrix-product-state simulation method.

http://www.sciencedirect.com/science/article/pii/S0010465513001306[+http://www.sciencedirect.com/science/article/pii/S0010465513001306+]

== ORIGINAL SECTION

Notes about how to install and use cool software.

Meta
~~~~

http://code.ohloh.net/[+http://code.ohloh.net/+]

https://hpcforge.org/softwaremap/full_list.php[+https://hpcforge.org/softwaremap/full_list.php+]

http://forge.ipsl.jussieu.fr/[+http://forge.ipsl.jussieu.fr/+]

http://www.scipy.org/topical-software.html[+http://www.scipy.org/topical-software.html+]

http://www.hdfgroup.org/products/hdf5_tools/SWSummarybyName.htm[+http://www.hdfgroup.org/products/hdf5_tools/SWSummarybyName.htm+]

http://www.unidata.ucar.edu/software/netcdf/software.html[+http://www.unidata.ucar.edu/software/netcdf/software.html+]

http://cadadr.org/fm/[+http://cadadr.org/fm/+]

http://code.google.com/[+http://code.google.com/+]

http://gna.org/[+http://gna.org/+]

https://launchpad.net/[+https://launchpad.net/+]

http://savannah.nongnu.org/[+http://savannah.nongnu.org/+]

http://savannah.gnu.org/[+http://savannah.gnu.org/+]

http://code.nasa.gov/project/[+http://code.nasa.gov/project/+]

http://aforge.awi.de/gf/project[+http://aforge.awi.de/gf/project+]

http://www.vrplumber.com/py3d.py[+http://www.vrplumber.com/py3d.py+]

http://en.wikipedia.org/wiki/Category:Free_science_software[+http://en.wikipedia.org/wiki/Category:Free_science_software+]

http://en.wikipedia.org/wiki/List_of_free_and_open-source_software_packages[+http://en.wikipedia.org/wiki/List_of_free_and_open-source_software_packages+]

http://www.dmoz.org/Science/Software/[+http://www.dmoz.org/Science/Software/+]

http://directory.fsf.org/wiki/Main_Page[+http://directory.fsf.org/wiki/Main_Page+]

Machine Learning and Data Mining Info
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Discovering and Visualizing Patterns with Python*

PDF cheatsheet (7 pp.)

http://refcardz.dzone.com/refcardz/data-mining-discovering-and[+http://refcardz.dzone.com/refcardz/data-mining-discovering-and+]

Weblog of cheatsheet author:

http://glowingpython.blogspot.it/[+http://glowingpython.blogspot.it/+]

https://github.com/JustGlowing/minisom[+https://github.com/JustGlowing/minisom+]

*Machine Learning: An Algorithmic Perspective*

A book with lots of Python examples, the code for which is available at the
link shown.

http://seat.massey.ac.nz/personal/s.r.marsland/MLbook.html[+http://seat.massey.ac.nz/personal/s.r.marsland/MLbook.html+]

*Neural Network Emulations for Complex Multidimensional Geophysical Mappings*

PDF review paper (34 pp.)

http://polar.ncep.noaa.gov/mmab/people/vladimir/publications/Reprint_2006RG000200.pdf[+http://polar.ncep.noaa.gov/mmab/people/vladimir/publications/Reprint_2006RG000200.pdf+]

*Predicting Solar Energy from Weather Forecasts Using Python*

Using Python to read data from NetCDF files and then perform data mining.

http://fastml.com/predicting-solar-energy-from-weather-forecasts-plus-a-netcdf4-tutorial/[+http://fastml.com/predicting-solar-energy-from-weather-forecasts-plus-a-netcdf4-tutorial/+]

*Application of Machine Learning Methods to Spatial Interpolation of
Environmental Variables*

PDF paper (13 pp.)

http://www.sciencedirect.com/science/article/pii/S1364815211001654[+http://www.sciencedirect.com/science/article/pii/S1364815211001654+]

*Review of Spatial Interpolation Methods for Environmental Scientists*

PDF technical report (154 pp.)

http://www.ga.gov.au/image_cache/GA12526.pdf[+http://www.ga.gov.au/image_cache/GA12526.pdf+]

*Climate Informatics*

PDF review paper (46 pp.)

http://www-users.cs.umn.edu/\~ksteinha/papers/CIDASD13.pdf[+http://www-users.cs.umn.edu/~ksteinha/papers/CIDASD13.pdf+]

*Comparing Predictive Power in Climate Data: Clustering Matters*

PDF paper (17 pp.)

http://www-users.cs.umn.edu/\~ksteinha/papers/SSTD11.pdf[+http://www-users.cs.umn.edu/~ksteinha/papers/SSTD11.pdf+]

*Applying Machine Learning Methods to Climate Variability*

http://vimeo.com/45157625[+http://vimeo.com/45157625+]

*Nonlinear Multivariate and Time Series Analysis by Neural Network Methods*

http://www.ocgy.ubc.ca/\~william/Pubs/Rev.Geop.pdf[+http://www.ocgy.ubc.ca/~william/Pubs/Rev.Geop.pdf+]

*Pattern Recognition in Time Series*

PDF paper (28 pp.)

http://www.cs.gmu.edu/\~jessica/publications/astronomy11.pdf[+http://www.cs.gmu.edu/~jessica/publications/astronomy11.pdf+]

*Application of Statistical Learning to Plankton Image Analysis*

http://dspace.mit.edu/handle/1721.1/39206[+http://dspace.mit.edu/handle/1721.1/39206+]

*Machine Learning Algorithms for Real Data Sources with Applications to
Climate Science*

PDF slides (46 pp.)

http://www.cs.rutgers.edu/\~mcgrew/AdditionalEventFiles/Monteleoni/MLAlgorithms.pdf[+http://www.cs.rutgers.edu/~mcgrew/AdditionalEventFiles/Monteleoni/MLAlgorithms.pdf+]

*Machine Learning for Climate Science*

Online slides (196 pp.)

https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnwxc3RjbGltYXRlaW5mb3JtYXRpY3N8Z3g6MmVhMTJkZmNiMTNlYzExNw[+https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnwxc3RjbGltYXRlaW5mb3JtYXRpY3N8Z3g6MmVhMTJkZmNiMTNlYzExNw+]

*Applicability of Data Mining Techniques for Climate Prediction*

PDF paper (4 pp.)

http://www.scribd.com/doc/33728981/Applicability-of-Data-Mining-Techniques-for-Climate-Prediction-%E2%80%93-A-Survey-Approach[+http://www.scribd.com/doc/33728981/Applicability-of-Data-Mining-Techniques-for-Climate-Prediction-%E2%80%93-A-Survey-Approach+]

*Outstanding Problems at the Interface of Climate Prediction and Data Mining*

Online slides (35 pp.)

https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnwxc3RjbGltYXRlaW5mb3JtYXRpY3N8Z3g6Mzk0MTY2NDQ0ZmFkOTUwMw[+https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnwxc3RjbGltYXRlaW5mb3JtYXRpY3N8Z3g6Mzk0MTY2NDQ0ZmFkOTUwMw+]

*Unsupervised Machine Learning Techniques for Studying Climate Variability*

PDF slides (21 pp.)

http://imsc.pacificclimate.org/proceedings/10IMSCPDFs/r820.am01_Alexander_llin.pdf[+http://imsc.pacificclimate.org/proceedings/10IMSCPDFs/r820.am01_Alexander_llin.pdf+]

*Tracking Climate Models*

PDF paper (15 pp.)

http://www1.ccls.columbia.edu/\~cmontel/mss10.pdf[+http://www1.ccls.columbia.edu/~cmontel/mss10.pdf+]

*Streaming Data Mining*

PDF slides (229 pp.)

http://www.cs.yale.edu/homes/el327/papers/streaming_data_mining.pdf[+http://www.cs.yale.edu/homes/el327/papers/streaming_data_mining.pdf+]

*Machine Learning for Hackers*

Book (324 pp.) with examples using R.

http://proquest.safaribooksonline.com/9781449330514[+http://proquest.safaribooksonline.com/9781449330514+]

Python and Matlab
-----------------

http://stackoverflow.com/questions/9845292/converting-matlab-to-python[+http://stackoverflow.com/questions/9845292/converting-matlab-to-python+]

[[2DECOMP]]
2DECOMP
-------

A software framework in Fortran to build large-scale parallel applications. It
is designed for applications using three-dimensional structured mesh and
spatially implicit numerical algorithms. At the foundation it implements a
general-purpose 2D pencil decomposition for data distribution on
distributed-memory platforms. On top it provides a highly scalable and
efficient interface to perform three-dimensional distributed FFTs. The library
is optimised for supercomputers and scales well to hundreds of thousands of
cores. It relies on MPI but provides a user-friendly programming interface
that hides communication details from application developers.

See xref:Incompact3d[Incompact3d].

http://www.2decomp.org/[+http://www.2decomp.org/+]

http://www.hector.ac.uk/cse/distributedcse/technical2/dCSEtech2011_Incompact3D.pdf[+http://www.hector.ac.uk/cse/distributedcse/technical2/dCSEtech2011_Incompact3D.pdf+]

accULL
------

A programming environment for heterogeneous architectures.

http://cap.pcg.ull.es/en/accULL[+http://cap.pcg.ull.es/en/accULL+]

http://yacf.googlecode.com/hg/yacf/doc/_build/html/llcLanguage.html[+http://yacf.googlecode.com/hg/yacf/doc/_build/html/llcLanguage.html+]

http://accull.wordpress.com/[+http://accull.wordpress.com/+]

ACTS
----

A set of DOE-developed software tools, sometimes in collaboration with other
funding agencies (DARPA, NSF), that make it easier for programmers to write
high performance scientific applications for high-end computers.

http://acts.nersc.gov/[+http://acts.nersc.gov/+]

ADAGUC
------

A geographical information system to visualize netCDF files via the web. The
software consists of a server side Cxx application and a client side
JavaScript application. The software provides several features to access and
visualize data over the web, it uses OGC standards for data dissemination. 

http://adaguc.knmi.nl/[+http://adaguc.knmi.nl/+]

ADAMS
-----

The Advanced Data mining And Machine learning System (ADAMS) is a novel,
flexible workflow engine aimed at quickly building and maintaining real-world,
complex knowledge workflows.

https://adams.cms.waikato.ac.nz/[+https://adams.cms.waikato.ac.nz/+]


ADCIRC
------

A system of computer programs for solving time dependent, free surface
circulation and transport problems in two and three dimensions. These programs
utilize the finite element method in space allowing the use of highly
flexible, unstructured grids. 

http://adcirc.org/[+http://adcirc.org/+]

http://isec.nacse.org/models/[+http://isec.nacse.org/models/+]

ADLB
----

A software library designed to help rapidly build scalable parallel programs. 

https://cs.mtsu.edu/\~rbutler/adlb/[+https://cs.mtsu.edu/~rbutler/adlb/+]

Akantu
------

An opensource object-oriented Finite Element library which has the ambition to
be generic and efficient. Akantu is developed within the LSMS (Computational
Solid Mechanics Laboratory, lsms. epfl.ch), where research is conducted at the
interface of mechanics, material science, and scientific computing. The
open-source philosophy is important for any scientific software project
evolution. The collaboration permitted by shared codes enforces sanity when
users (and not only developers) can criticize the implementation details.
Akantu was born with the vision to associate genericity, robustness and
efficiency while benefiting the open-source visibility. 

http://lsms.epfl.ch/akantu[+http://lsms.epfl.ch/akantu+]

Algorithms
----------

Implementations of a few algorithms and datastructures for fun and profit.

https://github.com/prakhar1989/Algorithms[+https://github.com/prakhar1989/Algorithms+]

Alchemy
-------

A  software package providing a series of algorithms for statistical
relational learning and probabilistic logic inference, based on the Markov
logic representation.

http://alchemy.cs.washington.edu/[+http://alchemy.cs.washington.edu/+]

amatos
------

The Adaptive Mesh generator for Atmospheric and Ocean Simulation is
a mesh generator for adaptive algorithms. It is capable of handling complex
geometries as well as highly non-uniform refinement regions. It has a
relatively simple programming interface and incorporates some optimization.
There is even a 3D version of amatos.

http://www.amatos.info/[+http://www.amatos.info/+]

AMPI
----

The Adaptive Message Passing Interface is an implementation of MPI that
supports dynamic load balancing and multithreading for MPI applications.

http://charm.cs.uiuc.edu/research/ampi/[+http://charm.cs.uiuc.edu/research/ampi/+]

http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_6_Panetta.pdf[+http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_6_Panetta.pdf+]

Charmxx
~~~~~~~

A machine independent parallel programming system. Programs written using this
system will run unchanged on MIMD machines with or without a shared memory. It
provides high-level mechanisms and strategies to facilitate the task of
developing even highly complex parallel applications.

http://charm.cs.uiuc.edu/research/charm[+http://charm.cs.uiuc.edu/research/charm+]

AMUSE
-----

The Astrophysical Multipurpose Software Environment
provides a software framework astrophysical simulations, in which existing
codes from different domains, such as stellar dynamics, stellar evolution,
hydrodynamics and radiative transfer can be easily coupled. 
AMUSE uses  Python to interface with existing numerical codes. The AMUSE
interface handles unit conversions, provides consistent object oriented
interfaces, manages the state of the underlying simulation codes and provides
transparent distributed computing.

http://amusecode.org/[+http://amusecode.org/+]

ArmaNpy
-------

Python bindings for the Armadillo matrix library.

http://sourceforge.net/projects/armanpy/[+http://sourceforge.net/projects/armanpy/+]

Armadillo
~~~~~~~~~

A Cxx linear algebra library.

http://arma.sourceforge.net/[+http://arma.sourceforge.net/+]

arpack-ng
---------

A collection of Fortran77 subroutines designed to solve large scale eigenvalue
problems.

https://github.com/opencollab/arpack-ng[+https://github.com/opencollab/arpack-ng+]

http://forge.scilab.org/index.php/p/arpack-ng/[+http://forge.scilab.org/index.php/p/arpack-ng/+]

ASAGI
-----

A parallel server for adaptive geoinformation.

https://github.com/TUM-I5/ASAGI[+https://github.com/TUM-I5/ASAGI+]

http://www5.in.tum.de/pub/rettenberger_asagi_siam_2013.pdf[+http://www5.in.tum.de/pub/rettenberger_asagi_siam_2013.pdf+]

ASCEND
------

A free open-source software program for solving small to very large
mathematical models. ASCEND can solve systems of non-linear equations, linear
and nonlinear optimisation problems, and dynamic systems expressed in the form
of differential/algebraic equations. 

http://ascend4.org/[+http://ascend4.org/+]

Asp
---

A SEJITS implementation for Python.
Asp is a research prototype and implementation of SEJITS (Selective, Embedded
Just-in-Time Specialization) for Python. With the aid of application-specific
specializers, it compiles fragments of Python down to low-level parallelized
CPU and GPU implementations.

https://github.com/shoaibkamil/asp/wiki[+https://github.com/shoaibkamil/asp/wiki+]

http://sejits.eecs.berkeley.edu/[+http://sejits.eecs.berkeley.edu/+]

Aspen
-----

A Python web framework that makes the most of the filesystem. Simplates are
Cacti
-----

A complete network graphing solution designed to harness the power of
RRDTool's data storage and graphing functionality. Cacti provides a fast
poller, advanced graph templating, multiple data acquisition methods, and user
management features out of the box. All of this is wrapped in an intuitive,
easy to use interface that makes sense for LAN-sized installations up to
complex networks with hundreds of devices.

http://cacti.net/[+http://cacti.net/+]

RRDtool
~~~~~~~

The OpenSource industry standard, high performance data logging and graphing
system for time series data. RRDtool can be easily integrated in shell
scripts, perl, python, ruby, lua or tcl applications.

http://oss.oetiker.ch/rrdtool/[+http://oss.oetiker.ch/rrdtool/+]

Cactus
------

Cactus is an open source problem solving environment designed for scientists
and engineers. Its modular structure easily enables parallel computation
across different architectures and collaborative code development between
different groups. Cactus originated in the academic research community, where
it was developed and used over many years by a large international
collaboration of physicists and computational scientists.

The name Cactus comes from the design of a central core ("flesh") which
connects to application modules ("thorns") through an extensible interface.
Thorns can implement custom developed scientific or engineering applications,
such as computational fluid dynamics. Other thorns from a standard
computational toolkit provide a range of computational capabilities, such as
parallel I/O, data distribution, or checkpointing.

Cactus runs on many architectures. Applications, developed on standard
workstations or laptops, can be seamlessly run on clusters or supercomputers.
Cactus provides easy access to many cutting edge software technologies being
developed in the academic research community, including the Globus
Metacomputing Toolkit, HDF5 parallel file I/O, the PETSc scientific library,
adaptive mesh refinement, web interfaces, and advanced visualization tools. 

http://cactuscode.org/[+http://cactuscode.org/+]

http://cactuscode.org/documentation/usersguide/UsersGuide.html[+http://cactuscode.org/documentation/usersguide/UsersGuide.html+]

[[Caffe]]
Caffe
-----

A framework for convolutional neural network algorithms, developed with speed
in mind.
Caffe aims to provide computer vision scientists and practitioners with a
clean and modifiable implementation of state-of-the-art deep learning
algorithms. For example, network structure is easily specified in separate
config files, with no mess of hard-coded parameters in the code.
At the same time, Caffe fits industry needs, with blazing fast Cxx/CUDA code
for GPU computation. 

http://caffe.berkeleyvision.org/[+http://caffe.berkeleyvision.org/+]

Caffe provides multimedia scientists and practitioners with a clean and
modifiable framework for state-of-the-art deep learning algorithms and a
collection of reference models. The framework is a BSD-licensed Cxx library
with Python and MATLAB bindings for training and deploying general-purpose
convolutional neural networks and other deep models efficiently on commodity
architectures. Caffe fits industry and internet-scale media needs by CUDA GPU
computation, processing over 40 million images a day on a single K40 or Titan
GPU (≈ 2.5 ms per image). By separating model representation from actual
implementation, Caffe allows experimentation and seamless switching among
platforms for ease of development and deployment from prototyping machines to
cloud environments. Caffe is maintained and developed by the Berkeley Vision
and Learning Center (BVLC) with the help of an active community of
contributors on GitHub. It powers ongoing research projects, large-scale
industrial applications, and startup prototypes in vision, speech, and
multimedia. 

http://arxiv.org/abs/1408.5093[+http://arxiv.org/abs/1408.5093+]

Cameleon
--------

The Cameleon language is a graphical data flow language following a two-scale
paradigm. It allows an easy up-scale that is the integration of any library
writing in Cxx in the data flow language. Cameleon language aims to
democratize macro-programming by an intuitive interaction between the human
and the computer where building an application based on a data-process and a
GUI is a simple task to learn and to do. Cameleon language allows conditional
execution and repetition to solve complex macro-problems. In this paper we
introduce a new model based on the extension of the petri net model for the
description of how the Cameleon language executes a composition. 

http://www.shinoe.org/cameleon/[+http://www.shinoe.org/cameleon/+]

http://arxiv.org/abs/1110.4802[+http://arxiv.org/abs/1110.4802+]

casacore
--------

A set of libraries for performing various tasks in radoi astronomy.

http://code.google.com/p/casacore/[+http://code.google.com/p/casacore/+]

http://www.atnf.csiro.au/computing/software/casacore/casacore-1.2.0/doc/html/[+http://www.atnf.csiro.au/computing/software/casacore/casacore-1.2.0/doc/html/+]

pyrap
~~~~~

Python bindings for the casacore radio astronomy libraries.

https://code.google.com/p/pyrap/[+https://code.google.com/p/pyrap/+]

CCI
---

A simple, portable, high-performance, scalable, and robust communication
interface for HPC and Data Centers.
Targeted towards high performance computing (HPC) environments as well as
large data centers, CCI can provide a common network abstraction layer (NAL)
for persistent services as well as general interprocess communication. In HPC,
MPI is the de facto standard for communication within a job. Persistent
services such as distributed file systems, code coupling (e.g. a simulation
sending output to an analysis application sending its output to a
visualization process), health monitoring, debugging, and performance
monitoring, however, exist outside of scheduler jobs or span multiple jobs. In
these cases, these persistent services tend to use either BSD sockets for
portability to avoid having to rewrite the applications for each new
interconnect or they implement their own NAL which takes developer time and
effort. CCI can simplify support for these persistent services by providing a
common NAL which minimizes the maintenance and support for these services
while providing improved performance (i.e. reduced latency and increased
bandwidth) compared to Sockets.

http://cci-forum.com/[+http://cci-forum.com/+]

https://www.olcf.ornl.gov/center-projects/common-communication-interface/[+https://www.olcf.ornl.gov/center-projects/common-communication-interface/+]

Celery
------

An asynchronous task queue/job queue based on distributed message passing. It
is focused on real-time operation, but supports scheduling as well.
The execution units, called tasks, are executed concurrently on a single or
more worker servers using multiprocessing, Eventlet, or gevent. Tasks can
execute asynchronously (in the background) or synchronously (wait until
ready).

http://www.celeryproject.org/[+http://www.celeryproject.org/+]

RabbitMQ
~~~~~~~~

Robust messaging for applications.

http://www.rabbitmq.com/[+http://www.rabbitmq.com/+]

Cello
-----

Cello is a GNU99 C library which brings higher level programming to C.

http://libcello.org/[+http://libcello.org/+]

CentPack
--------

A package of high-resolution central schemes for nonlinear
conservation laws and related problems.

http://www.cscamm.umd.edu/centpack/software/index.htm[+http://www.cscamm.umd.edu/centpack/software/index.htm+]

CETUS
-----

A compiler infrastructure for the source-to-source transformation of software
programs.

http://cetus.ecn.purdue.edu/[+http://cetus.ecn.purdue.edu/+]

Cesium
------

A JavaScript library for creating 3D and 2D maps in a web browser without a
plugin.  The features include:

* Create data-driven time-dynamic scenes using CZML. See the CZML Guide.
* Visualize high-resolution worldwide Terrain. See STK World Terrain.
* Layer imagery from multiple sources, including WMS, TMS, WMTS, OpenStreetMap,
Bing Maps, ArcGIS MapServer, Google Earth Enterprise, and standard image
files. Each layer can be alpha-blended with the layers below it, and its
brightness, contrast, gamma, hue, and saturation can be dynamically changed.
* Draw GeoJSON and TopoJSON.
* Draw 3D models using COLLADA and glTF with animations and skins.
* Draw and style a wide range of geometries
* Draw the atmosphere, sun, sun lighting, moon, stars, and water.
* Individual object picking.
* Camera navigation with mouse and touch handlers for rotate, zoom, pan with
inertia, flights, free look, and terrain collision detection.
* Batching, culling, and JavaScript and GPU optimizations for performance.
* Precision handling for large view distances (avoiding z-fighting) and large
world coordinates (avoiding jitter)

http://cesiumjs.org/[+http://cesiumjs.org/+]

*Representing Time-Dynamic Geospatial Objects on Virtual Globes Using CZML—Part I: Overview and Key Issues* - https://www.mdpi.com/2220-9964/7/3/97[+https://www.mdpi.com/2220-9964/7/3/97+]

*Representing Time-Dynamic Geospatial Objects on Virtual Globes Using CZML—Part II: Impact, Comparison, and Future Developments* - https://www.mdpi.com/2220-9964/7/3/102[+https://www.mdpi.com/2220-9964/7/3/102+]

czml
~~~~

A library to read and write CZML files for Cesium

https://pypi.python.org/pypi/czml/0.1dev[+https://pypi.python.org/pypi/czml/0.1dev+]

cf-python
---------

Implements the CF data model for the reading, writing and processing of data
and its metadata.

http://cfpython.bitbucket.org/[+http://cfpython.bitbucket.org/+]

cfunits-python
~~~~~~~~~~~~~~

A Python interface to UNIDATA's Udunits-2 package with CF extensions.

http://code.google.com/p/cfunits-python/[+http://code.google.com/p/cfunits-python/+]

C-Graph
-------

Demonstrates the theory of convolution underlying engineering systems and
signal analysis.
Designed to enhance the learning experience, C-Graph features an attractive
array of scalable pulses, periodic, and aperiodic signal types of variable
frequency fundamental to the study of systems theory. The package displays the
spectra of any two waveforms chosen by the user, computes their linear
convolution, then compares their circular convolution according to the
convolution theorem. Each signal is modelled by a register of N discrete
values (samples), and the discrete Fourier Transform (DFT) computed by the
Fast Fourier Transform (FFT).  Students of signal and systems theory will find
GNU C-Graph to be of value in visualizing convolution.

http://www.gnu.org/software/c-graph/[+http://www.gnu.org/software/c-graph/+]

Charlemagne
-----------

A versatile genetic programming application which includes a command-line
client and an interactive console mode. It features built in input-output
mapping support, and is user-extensible for complex fitness evaluation in
Python and Lisp.

http://charlemagne.sourceforge.net/[+http://charlemagne.sourceforge.net/+]

Cilk Plus
---------

Adds simple language extensions to the C and Cxx languages to express task and
data parallelism. These language extensions are powerful, yet easy to apply
and use in a wide range of applications.

This was an MIT research program that got folded into the commercially
available Intel Cxx Compiler Suite.  There is a branch of the GCC compiler
development stack that's also in the process of including Cilk.

http://www.cilkplus.org/[+http://www.cilkplus.org/+]

http://supertech.csail.mit.edu/cilk/[+http://supertech.csail.mit.edu/cilk/+]

http://gcc.gnu.org/svn/gcc/branches/[+http://gcc.gnu.org/svn/gcc/branches/+]

http://gcc.gnu.org/svn/gcc/branches/cilkplus-4_8-branch/[+http://gcc.gnu.org/svn/gcc/branches/cilkplus-4_8-branch/+]

CIM
---

The main objectives of the METAFOR project were to develop and promulgate an
ipso-facto standard for describing climate models and associated data. This
standard has been formalized and named the Common Information Model (CIM).
Adoption of the CIM standard will allow the climate science community to
nurture an eco-system of CIM compliant tools and services to be integrated
into the day to day activities of climate research institutes worldwide.
The CIM is an ontology, i.e. an informational model describing a particular
domain (i.e. climate science). Such a model is formed using a construct known
as a class (e.g. simulation). Classes form relationships with other classes
(e.g. a simulation has data). Related classes are grouped into packages. The
CIM is formally defined using the Unified Modelling Language.

https://earthsystemcog.org/projects/es-doc-models/cim[+https://earthsystemcog.org/projects/es-doc-models/cim+]

http://www.ceda.ac.uk/projects/pimms/[+http://www.ceda.ac.uk/projects/pimms/+]

CLARAty
-------

The Coupled-Layer Architecture for Robotic Autonomy is a reusable
robotic software framework.
CLARAty is a framework that promotes reusable robotic software. It was
designed to support heterogeneous robotic platforms and integrate advanced
robotic capabilities from multiple institutions. Consequently, its design had
to be portable, modular, flexible and extendable.

https://www-robotics.jpl.nasa.gov/news/newsStory.cfm?NewsID=69[+https://www-robotics.jpl.nasa.gov/news/newsStory.cfm?NewsID=69+]

https://claraty.jpl.nasa.gov/man/overview/index.php[+https://claraty.jpl.nasa.gov/man/overview/index.php+]

cliffordlib
-----------

A lightweight Clifford algebra template library.

http://sourceforge.net/projects/cliffordlib/[+http://sourceforge.net/projects/cliffordlib/+]

CLIM-X-DETECT
-------------

Robustly detects extremes against a time-dependent background in climate and
weather time series. 

http://www.manfredmudelsee.com/soft/index.htm[+http://www.manfredmudelsee.com/soft/index.htm+]

CLUCalc
-------

A freely* available software tool for 3D visualizations and scientific
calculations that was conceived and written by Dr. Christian Perwass. CLUCalc
interprets a script language called ‘CLUScript’, which has been designed to
make mathematical calculations and visualisations very intuitive. 

http://www.clucalc.info/[+http://www.clucalc.info/+]

http://vixra.org/abs/1306.0155[+http://vixra.org/abs/1306.0155+]

http://www.perwass.de/progs/Manual_en.pdf[+http://www.perwass.de/progs/Manual_en.pdf+]

CNEM
----

An implementation of the constrained natural element method in 2D and 3D.
It is written in Cxx and has Python and Matlab wrappers.

http://plateformesn-m2p.ensam.eu/SphinxDoc/cnem/index.html[+http://plateformesn-m2p.ensam.eu/SphinxDoc/cnem/index.html+]

Coarray Fortran
---------------

A SPMD parallel programming model based on a small set of language extensions
to Fortran 90. CAF supports access to non-local data using a natural extension
to Fortran 90 syntax, lightweight and flexible synchronization primitives,
pointers, and dynamic allocation of shared data. An executing CAF program
consists of a static collection of asynchronous process images.Rice's
implementation of Coarray Fortran 2.0 is a work in progress. We are working to
create an open-source, portable, retargetable, high-quality CAF 2.0 compiler
suitable for use with production codes. To achieve portability, our compiler
performs a source-to-source translation from CAF to Fortran 90 with calls to
our CAF 2.0 runtime library primitives. Our CAF compiler's generated code can
be compiled by any Fortran 90 compiler that supports Cray pointers. To achieve
high performance, we generate Fortran 90 that is readily optimizable by vendor
compilers. Our CAF 2.0 runtime library uses UC Berkeley's GASNet library as a
substrate for communication. GASNet's get and put operations are used to read
and write remote coarray elements. GASNet's active message support is used to
invoke operations on remote nodes. This capability is used to form teams and
to look up information about remote coarrays so that process images can read
and write them directly. 

http://caf.rice.edu/[+http://caf.rice.edu/+]

CODA
----

The Common Data Access toolbox (CODA) provides a set of interfaces for reading
remote sensing data from earth observation data files. These interfaces
consist of command line applications, libraries, interfaces to scientific
applications (such as IDL and MATLAB), and interfaces to programming languages
(such as C, Fortran, Python, and Java).

CODA provides a single interface to access data in a wide variety of data
formats, including ASCII, binary, XML, netCDF, HDF4, HDF5, GRIB, RINEX, and
SP3. This is done by using a generic high level type hierarchy mapping for
each data format. For self describing formats such as netCDF, HDF, and GRIB,
CODA will automatically construct this mapping based on the file itself. For
raw ASCII and binary (and partially also XML) formats CODA makes use of an
external format definition stored in .codadef files to determine this mapping.
On the download section of this website you will find .codadef files for
various earth observation missions that can be used with CODA.

http://stcorp.nl/coda/[+http://stcorp.nl/coda/+]

http://stcorp.nl/coda/doc/[+http://stcorp.nl/coda/doc/+]

CODESH
------

The COllaborative DEvelopment SHell project provides an automatic persistent
logbook for sessions of personal command-line work by recording what and how
is being done: for private use/reuse and for sharing selected parts with
collaborators.

http://sourceforge.net/projects/codesh/[+http://sourceforge.net/projects/codesh/+]

http://bourilko.web.cern.ch/bourilko/caves.html[+http://bourilko.web.cern.ch/bourilko/caves.html+]

Conda
-----

The  primary interface for managing Anaconda installations. It can query and
search the Anaconda package index and current Anaconda installation, create
new Anaconda environments, and install and update packages into existing
Anaconda environments.

http://docs.continuum.io/conda/index.html[+http://docs.continuum.io/conda/index.html+]

http://www.continuum.io/blog/conda[+http://www.continuum.io/blog/conda+]

Conedy
------

A scientific tool for the numerical integration of dynamical systems whose
mutual couplings are described by a network. Its name is an abbreviation of
“Complex Networks Dynamics”.

Conedy supports different dynamical systems with various integration schemes,
including ordinary differential equations, iterated maps, stochastic
differential equations, and pulse coupled oscillators which are handled via
events. In addition, it provides a simple way to handle arbitrary node
dynamics. Each dynamical system is associated with a node in a network and
edges between such nodes represent couplings. Conedy provides functions to
build a network from various node and edge types.

http://www.conedy.org/[+http://www.conedy.org/+]

http://arxiv.org/abs/1202.3074[+http://arxiv.org/abs/1202.3074+]


Connectivity Modeling System
----------------------------

A community multiscale modeling system, based on a stochastic Lagrangian
framework. It was developed to study complex larval migrations and give
probability estimates of population connectivity. In addition, the CMS can
also provide a Lagrangian descriptions of oceanic phenomena (advection,
dispersion, retention) and can be used in a broad range of applications, from
the dispersion and fate of pollutants to marine spatial conservation. 

http://code.google.com/p/connectivity-modeling-system/[+http://code.google.com/p/connectivity-modeling-system/+]

http://www.sciencedirect.com/science/article/pii/S136481521200312X[+http://www.sciencedirect.com/science/article/pii/S136481521200312X+]

Coopr
-----

A collection of open-source optimization-related Python packages that supports
a diverse set of optimization capabilities for formulating and analyzing
optimization models.

https://software.sandia.gov/trac/coopr[+https://software.sandia.gov/trac/coopr+]

https://software.sandia.gov/trac/coopr/wiki/Documentation[+https://software.sandia.gov/trac/coopr/wiki/Documentation+]

http://link.springer.com/book/10.1007/978-1-4614-3226-5/page/1[+http://link.springer.com/book/10.1007/978-1-4614-3226-5/page/1+]

Copperhead
----------

A data parallel subset of Python which can be dynamically compiled and
executed on parallel platforms.  Currently, we target NVIDIA GPUs, as well as
multicore CPUs through OpenMP and Threading Building Blocks (xref:TBB[TBB]).

http://copperhead.github.io/[+http://copperhead.github.io/+]

COWS WPS
--------

A generic web service and offline processing tool developed within the Centre
for Environmental Data Archival (CEDA). The CEDA OGC web services (COWS) is a
set of Python libraries that allow rapid development and deployment of
geospatial web applications and services built around the standards managed by
the Open Geospatial Consortium [OGC].
A Python software framework for implementing Open Geospatial Consortium web
service standards. COWS emphasises rapid service development by providing a
lightweight layer of OGC web service logic on top of Pylons [Pylons], a mature
web application framework for the Python language. This approach provides
developers with a flexible web service development environment without
compromising access to the full range of web application tools and patterns:
Model-View-Controller paradigm, XML templating, Object-Relational-Mapper
integration and authentication/authorisation. COWS contains pre-configured
implementations of WMS, WCS and WFS services, a web client and WPS.

http://cows.ceda.ac.uk/[+http://cows.ceda.ac.uk/+]

http://ceda-wps2.badc.rl.ac.uk/docs/[+http://ceda-wps2.badc.rl.ac.uk/docs/+]

http://proj.badc.rl.ac.uk/cows[+http://proj.badc.rl.ac.uk/cows+]

http://proj.badc.rl.ac.uk/cows/browser/cows_wps[+http://proj.badc.rl.ac.uk/cows/browser/cows_wps+]

CPL
---

A set of libraries providing a comprehensive, efficient and robust softwae
toolkit for creating automated astronomical data-reduction tasks.

http://www.eso.org/sci/software/cpl/[+http://www.eso.org/sci/software/cpl/+]

python-cpl
~~~~~~~~

Python interface to CPL.

https://pypi.python.org/pypi/python-cpl/0.5[+https://pypi.python.org/pypi/python-cpl/0.5+]

CPython Compiler Tools
----------------------

Various compiler tools for Python.

http://compilers.pydata.org/[+http://compilers.pydata.org/+]

CSDMS
-----

The Community Surface Dynamics Modeling System (CSDMS) deals with the Earth's
surface - the ever-changing, dynamic interface between lithosphere,
hydrosphere, cryosphere, and atmosphere. We are a diverse community of experts
promoting the modeling of earth surface processes by developing, supporting,
and disseminating integrated software modules that predict the movement of
fluids, and the flux (production, erosion, transport, and deposition) of
sediment and solutes in landscapes and their sedimentary basins.

http://csdms.colorado.edu/wiki/Main_Page[+http://csdms.colorado.edu/wiki/Main_Page+]

Cuba
----

A library for multidimensional numerical integration.
The Cuba library offers a choice of four independent routines for
multidimensional numerical integration: Vegas, Suave, Divonne, and Cuhre. 
All four have a C/Cxx, Fortran, and Mathematica interface and can integrate
vector integrands. Their invocation is very similar, so it is easy to
substitute one method by another for cross-checking. For further safeguarding,
the output is supplemented by a chi-square probability which quantifies the
reliability of the error estimate.

http://www.feynarts.de/cuba/[+http://www.feynarts.de/cuba/+]

http://www.sciencedirect.com/science/article/pii/S0010465505000792[+http://www.sciencedirect.com/science/article/pii/S0010465505000792+]

http://www.feynarts.de/cuba/acat05.pdf[+http://www.feynarts.de/cuba/acat05.pdf+]

cubes
-----

Light-weight Python framework and OLAP HTTP server for easy development of
reporting applications and aggregate browsing of multi-dimensionally modeled
data.

http://cubes.databrewery.org/[+http://cubes.databrewery.org/+]

CUDA
----

CUDA™ is a parallel computing platform and programming model invented by
NVIDIA. It enables dramatic increases in computing performance by harnessing
the power of the graphics processing unit (GPU).

http://www.nvidia.com/object/cuda_home_new.html[+http://www.nvidia.com/object/cuda_home_new.html+]

http://http.developer.nvidia.com/GPUGems2/gpugems2_frontmatter.html[+http://http.developer.nvidia.com/GPUGems2/gpugems2_frontmatter.html+]

http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html[+http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html+]

cuda-convnet
~~~~~~~~~~~~

A fast Cxx/CUDA implementation of convolutional (or more generally,
feed-forward) neural networks. It can model arbitrary layer connectivity and
network depth. Any directed acyclic graph of layers will do. Training is done
using the back-propagation algorithm. 

https://code.google.com/p/cuda-convnet/[+https://code.google.com/p/cuda-convnet/+]

Cylc
----

A ") is a suite engine and meta-scheduler that specializes in suites of
cycling tasks for weather and climate forecasting and related processing (it
can also be used for one-off workflows of non-cycling tasks, which is a
simpler problem).

http://cylc.github.io/cylc/[+http://cylc.github.io/cylc/+]

http://cylc.github.io/cylc/html/single/cug-html.html[+http://cylc.github.io/cylc/html/single/cug-html.html+]

D3
--

A JavaScript library for manipulating documents based on data. D3 helps you
bring data to life using HTML, SVG and CSS. D3’s emphasis on web standards
gives you the full capabilities of modern browsers without tying yourself to a
proprietary framework, combining powerful visualization components and a
data-driven approach to DOM manipulation.

http://d3js.org/[+http://d3js.org/+]

http://blog.visual.ly/why-d3-js-is-so-great-for-data-visualization/[+http://blog.visual.ly/why-d3-js-is-so-great-for-data-visualization/+]

DASSFLOW
--------

A software package for numerical simulation of river hydraulics (2D / 1D). It
is designed especially for parameter identification, calibration and
variational data assimilation. It is interfaced with few pre and
post-processors.

http://dassflow.gforge.inria.fr/[+http://dassflow.gforge.inria.fr/+]


DataFinder
----------

A lightweight data management application developed in Python that primarily
targets the management of huge data accumulations, often encountered in the
scientific field. The system is able to handle large amounts of data and can
be easily integrated in existing working environments. It can be optimised to
fit any situation by embedding scripts. 

https://wiki.sistec.dlr.de/DataFinderOpenSource[+https://wiki.sistec.dlr.de/DataFinderOpenSource+]

https://launchpad.net/datafinder[+https://launchpad.net/datafinder+]

http://sourceforge.net/projects/datafinder/[+http://sourceforge.net/projects/datafinder/+]

DataTurbine
-----------

A  robust real-time streaming data engine that lets you quickly stream live
data from experiments, labs, web cams and even Java enabled cell phones. It
acts as a "black box" to which applications and devices send and receive data.
Think of it as express delivery for your data, be it numbers, video, sound or
text. 
DataTurbine is a buffered middleware, not simply a publish/subscribe system.
It can receive data from various sources (experiments, web cams, etc) and send
data to various sinks (visualization interfaces, analysis tools, databases,
etc). It has "TiVO" like functionality that lets applications pause and rewind
live streaming data.

http://www.dataturbine.org/[+http://www.dataturbine.org/+]

http://www.dataturbine.org/sites/default/files/biblio/Sensor%20Networks%20Workshop.pdf[+http://www.dataturbine.org/sites/default/files/biblio/Sensor%20Networks%20Workshop.pdf+]

DBpedia
-------

A crowd-sourced community effort to extract structured information from
Wikipedia and make this information available on the Web. DBpedia allows you
to ask sophisticated queries against Wikipedia, and to link the different data
sets on the Web to Wikipedia data.

http://dbpedia.org/About[+http://dbpedia.org/About+]

DEAP
----

A novel evolutionary computation framework for rapid prototyping and testing
of ideas. It seeks to make algorithms explicit and data structures
transparent. It works in perfect harmony with parallelisation mechanism such
as multiprocessing and SCOOP. 

https://code.google.com/p/deap/[+https://code.google.com/p/deap/+]

Dedalus
-------

A pseudospectral solver for fluid equations. Its primary applications are in
Astrophysics and Cosmology. Written primarily in python, and making use of the
FFTW libraries, Dedalus aims to be a simple, fast, and elegant hydrodynamic
and magnetohydrodynamic code.

https://bitbucket.org/jsoishi/dedalus/wiki/Home[+https://bitbucket.org/jsoishi/dedalus/wiki/Home+]

deegree
-------

An open source software for spatial data infrastructures and the geospatial
web. deegree includes components for geospatial data management, including
data access, visualization, discovery and security. Open standards are at the
heart of deegree. The software is built on the standards of the Open
Geospatial Consortium (OGC) and the ISO Technical Committee 211.
It includes the OGC Web Map Service (WMS) reference implementation, a fully
compliant Web Feature Service (WFS) as well as packages for Catalogue Service
(CSW), Web Coverage Service (WCS), Web Processing Service (WPS) and Web Map
Tile Service (WMTS).

http://www.deegree.org/[+http://www.deegree.org/+]

Delft3D
-------

A modeling suite to investigate hydrodynamics, sediment transport and
morphology and water quality for fluvial, estuarine and coastal environments. 
The FLOW module is the heart of Delft3D and is a multi-dimensional (2D or 3D)
hydrodynamic (and transport) simulation programme which calculates non-steady
flow and transport phenomena resulting from tidal and meteorological forcing
on a curvilinear, boundary fitted grid or sperical coordinates. In 3D
simulations, the vertical grid is defined following the so-called sigma
coordinate approach or Z-layer approach. The MOR module computes sediment
transport (both suspended and bed total load) and morphological changes for an
arbitrary number of cohesive and non-cohesive fractions. Both currents and
waves act as driving forces and a wide variety of transport formulae have been
incorporated. For the suspended load this module connects to the 2D or 3D
advection-diffusion solver of the FLOW module; density effects may be taken
into account. An essential feature of the MOR module is the dynamic feedback
with the FLOW and WAVE modules, which allow the flows and waves to adjust
themselves to the local bathymetry and allows for simulations on any time
scale from days (storm impact) to centuries (system dynamics). It can keep
track of the bed composition to build up a stratigraphic record. The MOR
module may be extended to include extensive features to simulate dredging and
dumping scenarios.

http://oss.deltares.nl/web/delft3d[+http://oss.deltares.nl/web/delft3d+]

Dexy
----

Dexy was created out of a desire to unify software documentation and
scientific document automation, resulting in a tool that is better at both of
these than anything that has gone before.

http://www.dexy.it/[+http://www.dexy.it/+]

http://www.dexy.it/guide/the-dexy-guide.html[+http://www.dexy.it/guide/the-dexy-guide.html+]

DIANE
-----

A ightweight job execution control framework for parallel scientific
applications. DIANE improves the reliability and efficiency of job execution
by providing automatic load balancing, fine-grained scheduling and failure
recovery.
DIANE provides an environment in which the existing applications may be more
easily ported to heterogenous computing environments such as the Grid, batch
farms or interactive clusters.
The default scheduling plugin algorithms are suited for bag of tasks
applications and data-parallel problems with no inter-task communication.
However the framework is designed to make it easy to plug in other scheduling
algorithms for more complex task synchronization patterns and workflows, for
example DAG4DIANE plugin provides support for directed acyclic graph (DAG)
applications, MOTEUR plugin provides support for workflow applications. 

http://it-proj-diane.web.cern.ch/it-proj-diane/[+http://it-proj-diane.web.cern.ch/it-proj-diane/+]

DINEOF
------

An EOF-based method to fill in missing data from geophysical fields, such as
clouds in sea surface temperature.

http://www.data-assimilation.net/mediawiki/index.php/DINEOF[+http://www.data-assimilation.net/mediawiki/index.php/DINEOF+]

Disco
-----

A lightweight, open-source framework for distributed computing based on the
MapReduce paradigm. 

http://discoproject.org/[+http://discoproject.org/+]

DistNumPy
---------

A version of NumPy that parallelizes array operations in a manner completely
transparent to the user - from the perspective of the user, the difference
the next.

http://www.dspace.org/[+http://www.dspace.org/+]

DUNE
----

The Distributed and Unified Numerics Environment is a modular toolbox for
solving partial differential equations (PDEs) with grid-based methods. It
supports the easy implementation of methods like Finite Elements (FE), Finite
Volumes (FV), and also Finite Differences (FD).

http://www.dune-project.org/dune.html[+http://www.dune-project.org/dune.html+]

DYNAMICO
--------

A project to develop a new dynamical core for LMD-Z, the atmospheric general
circulation model (GCM) part of IPSL-CM Earth System Model. 

http://www.lmd.polytechnique.fr/\~dubos/interests.html[+http://www.lmd.polytechnique.fr/~dubos/interests.html+]

http://forge.ipsl.jussieu.fr/dynamico/browser/codes[+http://forge.ipsl.jussieu.fr/dynamico/browser/codes+]

Earthworm
---------

An an open architecture, open source public software for data acquisition,
processing, archival and distribution. Originally developed by the United
States Geological Survey, Earthworm binaries and source files are freely
available to everyone.

http://love.isti.com/trac/ew/[+http://love.isti.com/trac/ew/+]

http://www.earthwormcentral.org/[+http://www.earthwormcentral.org/+]

PythonEw
~~~~~~~~

Python wrapper for accessing an Earthworm shared memory ring.

http://osop.github.io/osop-python-ew/[+http://osop.github.io/osop-python-ew/+]

EDEN
----

A visual analytics tool for exploring multivariate data sets. EDEN helps you
see the associations among variables for guided analysis. 
EDEN harnesses the parallel coordinates visualization technique and is
augmented with graphical indicators of key descriptive statistics.

http://cda.ornl.gov/projects/eden/[+http://cda.ornl.gov/projects/eden/+]

http://code.google.com/p/eden-vis/[+http://code.google.com/p/eden-vis/+]

Eigen
-----

A Cxx template library for linear algebra: matrices, vectors, numerical
solvers, and related algorithms.

http://eigen.tuxfamily.org/index.php?title=Main_Page[+http://eigen.tuxfamily.org/index.php?title=Main_Page+]

Elixir
------

Elixir is a functional, meta-programming aware language built on top of the
Erlang VM. It is a dynamic language with flexible syntax and macro support
that leverages Erlang's abilities to build concurrent, distributed and
fault-tolerant applications with hot code upgrades.

http://elixir-lang.org/[+http://elixir-lang.org/+]

http://www.creativedeletion.com/2015/04/19/elixir_next_language.html[+http://www.creativedeletion.com/2015/04/19/elixir_next_language.html+]

Ellipsoidal Potential Theory
----------------------------

Open-source (BSD) implementations of ellipsoidal harmonic expansions for
solving problems of potential theory using separation of variables. 

https://bitbucket.org/knepley/ellipsoidal-potential-theory[+https://bitbucket.org/knepley/ellipsoidal-potential-theory+]

http://arxiv.org/abs/1204.0267[+http://arxiv.org/abs/1204.0267+]

emcee
-----

An extensible, pure-Python implementation of Goodman & Weare's Affine
Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler. It's designed for
Bayesian parameter estimation.

http://dan.iel.fm/emcee/[+http://dan.iel.fm/emcee/+]

http://arxiv.org/abs/1202.3665[+http://arxiv.org/abs/1202.3665+]

EMD
---

Empirical Mode Decomposition is an algorithm that finds common rotational
modes among all the channels of n-channel data, and is a generic
multidimensional extension of the standard EMD. 

http://www.commsp.ee.ic.ac.uk/\~mandic/research/emd.htm[+http://www.commsp.ee.ic.ac.uk/~mandic/research/emd.htm+]

Engauge
-------

This open source, digitizing software converts an image file showing a graph
or map, into numbers. The image file can come from a scanner, digital camera
or screenshot. The numbers can be read on the screen, and written or copied to
a spreadsheet.

The process starts with an image file containing a graph or map. The final
result is digitized data that can be used by other tools such as Microsoft
Excel and Gnumeric. 

http://digitizer.sourceforge.net/[+http://digitizer.sourceforge.net/+]

EnKF
----

The EnKF is a sophisticated sequental data assimilation method. It applies an
ensemble of model states to represent the error statistics of the model
estimate, it applies ensemble integrations to predict the error statistics
forward in time, and it uses an analysis scheme which operates directly on the
ensemble of model states when observations are assimilated. The EnKF has
proven to efficiently handle strongly nonlinear dynamics and large state
spaces and is now used in realistic applications with primitive equation
models for the ocean and atmosphere.

http://enkf.nersc.no/[+http://enkf.nersc.no/+]

http://www.siam.org/pdf/news/362.pdf[+http://www.siam.org/pdf/news/362.pdf+]

Enthought Tool Suite
--------------------

A suite of Python tools for constructing custom scientific applications.

http://code.enthought.com/projects/index.php[+http://code.enthought.com/projects/index.php+]

Chaco
~~~~~

A Python plotting application toolkit that facilitates writing plotting
applications at all levels of complexity, from simple scripts with hard-coded
data to large plotting programs with complex data interrelationships and a
multitude of interactive tools. While Chaco generates attractive static plots
for publication and presentation, it also works well for interactive data
visualization and exploration.

http://code.enthought.com/projects/chaco/[+http://code.enthought.com/projects/chaco/+]

Enaml
~~~~~

Enaml is Not A Markup Language. Enaml is a library for creating professional
quality user interfaces with minimal effort. Enaml combines a domain specific
declarative language with a constraints based layout system to allow users to
easily define rich UIs with complex and flexible layouts. Enaml applications
can transparently run on multiple backends (Qt and Wx) and on multiple
operating systems.

https://github.com/enthought/enaml[+https://github.com/enthought/enaml+]

https://github.com/nucleic/enaml[+https://github.com/nucleic/enaml+]

Mayavi
~~~~~~

A Python package for 3D scientific visualization.  The project includes
Mayavi, a tool for easy, interactive visualization of data that's integrated
with Python scientific libraries, and TVTK, a Traits-based wrapper for
xref:VTK[VTK].

http://code.enthought.com/projects/mayavi/[+http://code.enthought.com/projects/mayavi/+]

Traits
~~~~~~

A  trait is a type definition that can be used for normal Python object
attributes, giving the attributes some additional characteristics such
as initializatino, validation, delegation, notification and visualization.
The Traits package was developed to address some of the problems caused by not
having declared variable types, in those cases where problems might arise.

http://code.enthought.com/projects/traits/[+http://code.enthought.com/projects/traits/+]

Envisat CFI
-----------

A collection of precompiled C libraries for timing, coordinate conversions,
orbit propagation, satellite pointing calculations, and target visibility
calculations. This software is made available by the Envisat project to any
user involved in the Envisat mission preparation/exploitation.

http://eop-cfi.esa.int/index.php/mission-cfi-software/envcfi-software[+http://eop-cfi.esa.int/index.php/mission-cfi-software/envcfi-software+]

EOCFI
-----

The Earth Observation CFI software is a collection of precompiled C libraries
for timing, coordinate conversions, orbit propagation, satellite pointing
calculations, and target visibility calculations. This software is made
available by the EOP system support division to any user involved in the Earth
Observation missions preparation/exploitation.
As of version 4.0, the Earth Observation CFI Software is available both as C
and Cxx precompiled libraries and Java libraries.

http://eop-cfi.esa.int/index.php/mission-cfi-software/eocfi-software[+http://eop-cfi.esa.int/index.php/mission-cfi-software/eocfi-software+]

EOxServer
---------

A server for earth observation data.
EOxServer implements the OGC Implementation Specifications EO-WCS and EO-WMS
on top of MapServer's WCS and WMS implementations. EOxServer is released under
the EOxServer Open License also a MIT-style license and written in Python and
entirely based on Open Source software including MapServer, Django, GDAL,
xref:SpatiaLite[SpatiaLite], or PostGIS, and PROJ.4.
The functionality includes:

* Support of GML AP – Coverages for RectifiedGridCoverages
* Support of adopted WCS 2.0 specification (Core including GetCapabilities,
DescribeCoverage, and GetCoverage requests, KVP-, and XML/POST protocol
binding)
* Anticipated support of envisaged extensions: Coverage format, GeoTIFF
encoding, predefined (or EPSG) CRSs, scaling & interpolation, and
non-referenced access. By "anticipating" we mean to reflect the latest WCS.SWG
discussions as well as to follow the relevant parts of the previous 1.1 and
1.0 versions of WCS.
* Support of 2-D EO Coverages derived from gmlcov:RectifiedGridCoverage
* Support of 2-D EO Coverages derived from gmlcov:ReferenceableGridCoverage
* Support of Dataset Series as a collection of EO Coverages e.g. in a time
series
* Support of new DescribeEOCoverageSet operation on Dataset Series and EO
Coverages
* Support of Stitched Mosaic of Rectified EO Coverages including concept of
contributingFootprint
* Support of EO Metadata (retrieval and evaluation in DescribeEOCoverageSet
operation
* Support of KVP and XML/POST protocol bindings
* Support of GeoTIFF and GDAL library coverage formats
* Support of EO-WMS for EO coverages

https://github.com/EOxServer[+https://github.com/EOxServer+]

http://eoxserver.org/[+http://eoxserver.org/+]

http://wiki.ieee-earth.org/Documents/GEOSS_Tutorials/GEOSS_Provider_Tutorials/Web_Coverage_Service_Tutorial_for_GEOSS_Providers/Section_3_%3A_Use_Cases/3.4%3A_WCS_2.0_and_EO-WCS_with_MapServer_and_EOxServer_%28EOX%29[+http://wiki.ieee-earth.org/Documents/GEOSS_Tutorials/GEOSS_Provider_Tutorials/Web_Coverage_Service_Tutorial_for_GEOSS_Providers/Section_3_%3A_Use_Cases/3.4%3A_WCS_2.0_and_EO-WCS_with_MapServer_and_EOxServer_%28EOX%29+]

Escript
-------

A  programming tool for implementing mathematical models in python using the
finite element method (FEM). As users do not access the data structures it is
very easy to use and scripts can run on desktop computers as well as highly
parallel supercomputer without changes. Application areas for escript include
earth mantle convection, geophysical inversion, earthquakes, porous media
flow, reactive transport, plate subduction, erosion, and tsunamis.

Escript is designed as an easy-to-use environment for implementing
mathematical models based on non-linear, coupled, time-dependent partial
differential equations. It uses the finite element method (FEM) for spatial
discretization and data representation. Escript is used through python and is
suitable for rapid prototyping (e.g for a student project or thesis) as well
as for large software projects. Scripts are executed in parallel using MPI,
OpenMP and hybrid mode processing over 50 million unknowns on several thousand
cores on a parallel computer.

https://launchpad.net/escript-finley/[+https://launchpad.net/escript-finley/+]


ESGF
----

The Earth System Grid Federation (ESGF) Peer-to-Peer (P2P) enterprise system
is a collaboration that develops, deploys and maintains software
infrastructure for the management, dissemination, and analysis of model output
and observational data. ESGF's primary goal is to facilitate advancements in
Earth System Science. ESGF P2P is a component architecture expressly designed
to handle large-scale data management for worldwide distribution. The team of
computer scientists and climate scientists has developed an operational system
for serving climate data from multiple locations and sources. Model
simulations, satellite observations, and reanalysis products are all being
served from the ESGF P2P distributed data archive. 

http://esgf.llnl.gov/[+http://esgf.llnl.gov/+]

https://github.com/ESGF[+https://github.com/ESGF+]

scigest
~~~~~~~

A tool for publishing scientific dataset (climate data in particular) to
http://esgf.org/wiki. 

http://code.google.com/p/scigest/[+http://code.google.com/p/scigest/+]

emgr
----

Empirical gramians can be computed for linear and nonlinear control systems
for purposes of model order reduction (MOR), uncertainty quantification (UQ)
or system identification (SYSID). Model reduction using empirical gramians can
be applied to the state space, to the parameter space or to both through
combined reduction. For state reduction the empirical controllability gramian
and the empirical observability gramian, for balanced truncation, are
available, or alternatively the empirical cross gramian for direct truncation.
For parameter reduction, parameter identification and sensitivity analysis the
empirical sensitivity gramian (controllability of parameters) or the empirical
identifiability gramian (observability of parameters) are provided. Combined
state and parameter reduction is enabled by the empirical joint gramian, which
computes controllability and observability of states and parameter
concurrently. The emgr framework is a compact open source toolbox for
(empirical) GRAMIAN-based model reduction and compatible with OCTAVE and
MATLAB.

http://gramian.de/[+http://gramian.de/+]

EMOSLIB
-------

A interpolation and encoding library for ECMWF data.

https://software.ecmwf.int/wiki/display/EMOS/Emoslib[+https://software.ecmwf.int/wiki/display/EMOS/Emoslib+]

http://www.ecmwf.int/products/data/software/interpolation.html[+http://www.ecmwf.int/products/data/software/interpolation.html+]

Eukleides
---------

A computer language devoted to elementary plane geometry. It aims to be a
fairly comprehensive system to create geometric figures, either static or
dynamic. Eukleides allows to handle basic types of data: numbers and strings,
as well as geometric types of data: points, vectors, sets (of points), lines,
circles and conics.

A Eukleides script usually consists in a declarative part where objects are
defined, and a descriptive part where objects are drawn. Nevertheless,
Eukleides is also a full featured programming language, providing conditional
and iterative structures, user defined functions, modules, etc. Hence, it can
easily be extended.

The Eukleides distribution mainly provides two interpreters: eukleides and
euktopst. The former produces Encapsulated PostScript (EPS) files. It can
also, using a converter, yield animated GIFs. The later produces PSTricks
macros. It enables to include Eukleides figures into LaTeX documents. 

http://www.eukleides.org/[+http://www.eukleides.org/+]

EULAG
-----

EULAG is a numerical solver for all-scale geophysical flows. The underlying
anelastic equations are either solved in an EULerian (flux form), or a
LAGrangian (advective form) framework.

EULAG model is an ideal tool to perform numerical experiments in a virtual
laboratory with time-dependent adaptive meshes and within complex, and even
time-dependent model geometries. These abilities are due to the unique model
design that combines the nonoscillatory forward-in-time (NFT) numerical
algorithms and a robust elliptic solver with generalized coordinates. The code
is written as a research tool with numerous options controlling the numerical
accuracy and to allow for a wide range of numerical sensitivity tests. These
capabilities give the researcher confidence in the numerical solutions of
his/her problem. The formulation of the model equations allow for various
derivatives of the code including codes for stellar atmospheres, ocean
currents, sand dune propagation or biomechanical flows. EULAG is a fully
parallelized code and is easily portable between different platforms. 

http://www.mmm.ucar.edu/eulag/[+http://www.mmm.ucar.edu/eulag/+]

Euler
-----

A program for quickly and interactively computing with real and complex
numbers and matrices, or with intervals, in the style of MatLab, Octave,... It
can draw and animate your functions in two and three dimensions.

http://euler.sourceforge.net/[+http://euler.sourceforge.net/+]

Eureqa
------

A software tool for detecting equations and hidden mathematical relationships
in your data. Its goal is to identify the simplest mathematical formulas which
could describe the underlying mechanisms that produced the data. 

http://creativemachines.cornell.edu/eureqa[+http://creativemachines.cornell.edu/eureqa+]

http://code.google.com/p/eureqa-api/[+http://code.google.com/p/eureqa-api/+]

http://arxiv.org/abs/1203.1023[+http://arxiv.org/abs/1203.1023+]

eureqa_python
~~~~~~~~~~~~~

https://github.com/marcin-franc/eureqa_python/wiki[+https://github.com/marcin-franc/eureqa_python/wiki+]

EWT
---

Empirical wavelet transform toolbox for Matlab.

http://www.mathworks.com/matlabcentral/fileexchange/42141-empirical-wavelet-transforms[+http://www.mathworks.com/matlabcentral/fileexchange/42141-empirical-wavelet-transforms+]

ftp://ftp.math.ucla.edu/pub/camreport/cam13-33.pdf[+ftp://ftp.math.ucla.edu/pub/camreport/cam13-33.pdf+]

ftp://ftp.math.ucla.edu/pub/camreport/cam13-35.pdf[+ftp://ftp.math.ucla.edu/pub/camreport/cam13-35.pdf+]

https://www.ipam.ucla.edu/publications/ada2013/ada2013_10868.pdf[+https://www.ipam.ucla.edu/publications/ada2013/ada2013_10868.pdf+]

F2C-ACC
-------

A Fortran to CUDA (or C) compiler.

http://www.esrl.noaa.gov/gsd/ab/ac/Accelerators.html[+http://www.esrl.noaa.gov/gsd/ab/ac/Accelerators.html+]

http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_3_Govett.pdf[+http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_3_Govett.pdf+]

Falcon
------

An extension module for Python which implements a optimized, register machine
based interpreter, inside of your interpreter. You specify which functions you
want Falcon to wrap (or your entire module), and Falcon takes over execution
from there. 

https://github.com/rjpower/falcon/[+https://github.com/rjpower/falcon/+]

http://arxiv.org/abs/1306.6047[+http://arxiv.org/abs/1306.6047+]

Falkon
------

Falkon aims to enable the rapid and efficient execution of many tasks on large
compute clusters, and to improve application performance and scalability using
novel data management techniques.

http://dev.globus.org/wiki/Incubator/Falkon[+http://dev.globus.org/wiki/Incubator/Falkon+]

FANN
----

Fast Artificial Neural Network Library is a free open source neural network
library, which implements multilayer artificial neural networks in C with
support for both fully connected and sparsely connected networks.
Cross-platform execution in both fixed and floating point are supported. It
includes a framework for easy handling of training data sets. It is easy to
use, versatile, well documented, and fast. Bindings to more than 15
programming languages are available.

http://leenissen.dk/fann/wp/[+http://leenissen.dk/fann/wp/+]

FastBit
-------

A data processing library that offers a set of searching functions
supported by compressed bitmap indexes.
The key technology underlying the FastBit software is a set of compressed
bitmap indexes. In database systems, an index is a data structure to
accelerate data accesses and reduce the query response time. Most of the
commonly used indexes are variants of the B-tree, such as B+-tree and B*-tree.
FastBit implements a set of alternative indexes called compressed bitmap
indexes. Compared with B-tree variants, these indexes provide very efficient
searching and retrieval operations, but are somewhat slower to update after a
modification of an individual record.

https://sdm.lbl.gov/twiki/bin/view/Projects/FastBit/WebHome[+https://sdm.lbl.gov/twiki/bin/view/Projects/FastBit/WebHome+]

pyfastbit
~~~~~~~~~

Python bindings for FastBit.

http://code.google.com/p/pyfastbit/[+http://code.google.com/p/pyfastbit/+]

fastcluster
-----------

A Cxx library for hierarchical, agglomerative clustering. It provides a fast
implementation of the most efficient, current algorithms when the input is a
dissimilarity index. Moreover, it features memory-saving routines for
hierarchical clustering of vector data. It improves both asymptotic time
complexity (in most cases) and practical performance (in all cases) compared
to the existing implementations in standard software: several R packages,
MATLAB, Mathematica, Python with SciPy.

http://www.jstatsoft.org/v53/i09/[+http://www.jstatsoft.org/v53/i09/+]

FastICA
-------

A Matlab program that implements the fast fixed-point algorithm for
independent component analysis and projection pursuit. It features an
easy-to-use graphical user interface, and a computationally powerful
algorithm. 

The FastICA algorithm is also available in the
xref:MDP[MDP] and xref:scikit-learn[scikit-learn] packages.

http://research.ics.aalto.fi/ica/fastica/[+http://research.ics.aalto.fi/ica/fastica/+]



Fatiando a Terra
----------------

A Python toolkit for geophysical modeling and inversion.

http://fatiando.org/[+http://fatiando.org/+]

FCM
---

FCM uses Subversion for code management but defines a common process and
naming convention to simplify usage. It adds a layer on top of Subversion to
provide a more natural and user-friendly interface. 
FCM features a powerful build system, mainly aimed at building modern Fortran
software applications.

http://www.metoffice.gov.uk/research/collaboration/fcm[+http://www.metoffice.gov.uk/research/collaboration/fcm+]


FDO
---

An API for manipulating, defining and analyzing geospatial information
regardless of where it is stored. FDO uses a provider-based model for
supporting a variety of geospatial data sources, where each provider typically
supports a particular data format or data store.

http://fdo.osgeo.org/[+http://fdo.osgeo.org/+]

FEAST
-----

A free high-performance numerical library for solving the standard or
generalized eigenvalue problem.

http://www.ecs.umass.edu/\~polizzi/feast/[+http://www.ecs.umass.edu/~polizzi/feast/+]

http://arxiv.org/abs/1203.4031[+http://arxiv.org/abs/1203.4031+]

Figaro
------

Developing a new probabilistic model requires developing a representation for
the model and a reasoning algorithm that can draw useful conclusions from
evidence, which can be challenging tasks. Furthermore, it can be difficult to
integrate a probabilistic model into a larger program.

Figaro is a probabilistic programming language that helps address both these
issues. Figaro makes it possible to express probabilistic models using the
power of programming languages, giving the modeler the expressive tools to
create all sorts of models. Figaro comes with a number of built-in reasoning
algorithms that can be applied automatically to new models. In addition,
Figaro models are data structures in the Scala programming language, which is
interoperable with Java, and can be constructed, manipulated, and used
directly within any Scala or Java program.

https://www.cra.com/commercial-solutions/probabilistic-modeling-services.asp[+https://www.cra.com/commercial-solutions/probabilistic-modeling-services.asp+]

http://www.youtube.com/watch?v=8ly4tEm0x0Q[+http://www.youtube.com/watch?v=8ly4tEm0x0Q+]

[[Fiona]]
Fiona
-----

Python interfaces to functions in OGR, a library for reading and
writing geographic vector data.

See also xref:keytree[keytree], xref:Shapely[Shapely] and xref:Rtree[Rtree].

http://github.com/Toblerity/[+http://github.com/Toblerity/+]

http://toblerity.org/fiona/[+http://toblerity.org/fiona/+]

FLAME
-----

The objective of the FLAME project is to transform the development of dense
linear algebra libraries from an art reserved for experts to a science that
can be understood by novice and expert alike. Rather than being only a
library, the project encompasses a new notation for expressing algorithms, a
methodology for systematic derivation of algorithms, Application Program
Interfaces (APIs) for representing the algorithms in code, and tools for
mechanical derivation, implementation and analysis of algorithms and
implementations.

http://www.cs.utexas.edu/\~flame/web/[+http://www.cs.utexas.edu/~flame/web/+]

libFLAME
~~~~~~~~

A high performance dense linaer algebra library that is the result of the
FLAME methodology for systematically developing dense linear algebra
libraries.

http://www.cs.utexas.edu/\~flame/web/libFLAME.html[+http://www.cs.utexas.edu/~flame/web/libFLAME.html+]

Fluids
------

A large-scale, open source fluid simulator for the CPU and GPU using the
smooth particle hydrodynamics method. Fluids is capable of efficiently
simulating up to 8 million particles on the GPU (on 1500 MB of ram).

http://fluids3.com/[+http://fluids3.com/+]

https://github.com/erwincoumans/fluids_v3[+https://github.com/erwincoumans/fluids_v3+]

FMS
---

A software framework for supporting the efficient development, construction,
execution, and scientific interpretation of atmospheric, oceanic, and climate
system models.

http://www.gfdl.noaa.gov/fms/[+http://www.gfdl.noaa.gov/fms/+]

ForestGOMP
----------

An OpenMP runtime compatible with GCC 4.2, offering a structured way to
efficiently execute OpenMP applications onto hierarchical (NUMA)
architectures. 

http://runtime.bordeaux.inria.fr/forestgomp/[+http://runtime.bordeaux.inria.fr/forestgomp/+]

Marcel
~~~~~~

Marcel is a thread library that was originaly developped to meet the needs of
the PM2 multithreaded environment. Marcel provides a POSIX-compliant interface
and a set of original extensions. It can also be compiled to provide
ABI-compabiblity with NTPL threads under Linux, so that multithreaded
applications can use Marcel without being recompiled.
Marcel features a two-level thread scheduler (also called N:M scheduler) that
achieves the performance of a user-level thread package while being able to
exploit multiprocessor machines. The architecture of Marcel was carefully
designed to support a high number of threads and to efficiently exploit
hierarchical architectures (e.g. multi-core chips, NUMA machines). 

http://runtime.bordeaux.inria.fr/marcel/[+http://runtime.bordeaux.inria.fr/marcel/+]

FoSSI
-----

The  Family of Simplified Solver Interfaces is designed for an easy
integration and selection of parallel solvers in Fortran codes which make use
of compressed sparse row matrix format (CSR). FoSSI contains rather similar
interfaces to the most popular and wide spread parallel solver libraries
obtainable on the web: PETSC, HYPRE, AZTEC and xref:MUMPS[MUMPS]. Furthermore, an
interface to the PILUT-library is included together with the PILUT-solver
itself. 

http://www.awi.de/en/research/scientific_computing/high_performance_computing/fossi/[+http://www.awi.de/en/research/scientific_computing/high_performance_computing/fossi/+]

http://aforge.awi.de/gf/project/fossi/[+http://aforge.awi.de/gf/project/fossi/+]

http://www.awi.de/fileadmin/user_upload/Infrastructure/Scientific_Computing/Projects/Research_Projects/TR_0103.pdf[+http://www.awi.de/fileadmin/user_upload/Infrastructure/Scientific_Computing/Projects/Research_Projects/TR_0103.pdf+]

http://www.sciencedirect.com/science/article/pii/S1463500304000605[+http://www.sciencedirect.com/science/article/pii/S1463500304000605+]

FullSWOF
--------

The Shallow Water equations for Overland Flow solves the shallow water
equations
using finite volumes.

http://www.univ-orleans.fr/mapmo/soft/FullSWOF/[+http://www.univ-orleans.fr/mapmo/soft/FullSWOF/+]

https://sourcesup.renater.fr/projects/fullswof-2d/[+https://sourcesup.renater.fr/projects/fullswof-2d/+]

http://arxiv.org/abs/1401.4125[+http://arxiv.org/abs/1401.4125+]

http://arxiv.org/abs/1307.4839[+http://arxiv.org/abs/1307.4839+]

FUNWAVE
-------

A phase-resolving, time-stepping Boussinesq model for ocean surface wave
propagation in the nearshore. The present version of FUNWAVE is based on the
MUSCLE-TVD finite volume scheme together with adaptive Runge Kutta time
stepping. The code is parallelized using MPI and has been tested in linux and
unix (Mac OS X) environments. 

http://chinacat.coastal.udel.edu/programs/funwave/funwave.html[+http://chinacat.coastal.udel.edu/programs/funwave/funwave.html+]

http://falk.ucsd.edu/funwaveC.html[+http://falk.ucsd.edu/funwaveC.html+]

http://isec.nacse.org/models/[+http://isec.nacse.org/models/+]

CaFunwave
---------

Enables simulation of Boussinesq or shallow water equations. CaFunwave is
based on the Funwave.

http://www.coastalhazards.org/cafaunwave.html[+http://www.coastalhazards.org/cafaunwave.html+]

http://cactuscode.org/projects/cafunwave/index.php[+http://cactuscode.org/projects/cafunwave/index.php+]

FVCOM
-----

A prognostic, unstructured-grid, finite-volume, free-surface, 3-D primitive
equation coastal ocean circulation model developed by UMASSD-WHOI joint
efforts. The model consists of momentum, continuity, temperature, salinity and
density equations and is closed physically and mathematically using turbulence
closure submodels. The horizontal grid is comprised of unstructured triangular
cells and the irregular bottom is preseented using generalized
terrain-following coordinates. The General Ocean Turbulent Model (GOTM)
developed by Burchard’s research group in Germany (Burchard, 2002) has been
added to FVCOM to provide optional vertical turbulent closure schemes.

http://fvcom.smast.umassd.edu/fvcom/[+http://fvcom.smast.umassd.edu/fvcom/+]

Gaalet
------

The Geometric Algebra Algorithms Expression Templates library is a Cxx library
for evaluating geometric algebra expressions. It offers comfortable
implementation and reasonable speed by using expression templates and
metaprogramming techniques.

The basic idea of fast Geometric Algebra implementations is to do the grading
operations beforehand, so only basic operations on the coordinates are
performed at runtime. Gaalet does so by applying the grading operations with
Cpp metaprogramming techniques at compile time. These grading operations are
incorporated into expression templates, also a metaprogramming technique,
which offers Cxx compilers a good starting point for code optimization as well
as programmers the concept of lazy evaluation. 

http://sourceforge.net/apps/trac/gaalet/[+http://sourceforge.net/apps/trac/gaalet/+]

Gaalop
------

Gaalop (Geometic Algebra Algorithms Optimizer) is a software to optimize
geometric algebra files.
Algorithms can be developed by using the freely available CLUCalc software by
Christian Perwass. Gaalop optimizes the algorithm and produces Cxx, OpenCL,
CUDA, CLUCalc or LaTeX output (other output-formats will follow).
The optimized code has no more geometric algebra operations and can be run
very efficiently on various platforms.

http://www.gaalop.de/[+http://www.gaalop.de/+]

GAIO
----

A software package for the global numerical analysis of dynamical systems and
optimization problems based on set oriented techniques. It may e.g. be used to
compute invariant sets, invariant manifolds, invariant measures and almost
invariant sets in dynamical systems and to compute the globally optimal
solutions of both scalar and multiobjective problems.

http://www2.math.uni-paderborn.de/ags/ag-dellnitz/software.html[+http://www2.math.uni-paderborn.de/ags/ag-dellnitz/software.html+]

http://books.google.com/books?id=f6dcLIMzuCMC&pg=PA145&lpg=PA145&dq=gaio+global+analysis+invariant&source=bl&ots=tP2V5sFY_p&sig=6RdTZP9FQSBxQHAozSp4cWdsgYk&hl=en&sa=X&ei=_ZkbUsrQKMGt2QXfjoDIAQ&ved=0CGQQ6AEwBg#v=onepage&q=gaio%20global%20analysis%20invariant&f=false[+http://books.google.com/books?id=f6dcLIMzuCMC&pg=PA145&lpg=PA145&dq=gaio+global+analysis+invariant&source=bl&ots=tP2V5sFY_p&sig=6RdTZP9FQSBxQHAozSp4cWdsgYk&hl=en&sa=X&ei=_ZkbUsrQKMGt2QXfjoDIAQ&ved=0CGQQ6AEwBg#v=onepage&q=gaio%20global%20analysis%20invariant&f=false+]

GALEON
------

The Geo-interface to Atmosphere, Land, Earth, Ocean, NetCDF is an
interoperability experiment for implementing  and testing clients and servers
for WCS gateways to netCDF datasets.

http://www.ogcnetwork.net/galeon[+http://www.ogcnetwork.net/galeon+]

*Implementations*: http://www.ogcnetwork.net/node/113[+http://www.ogcnetwork.net/node/113+]

Galileo
-------

High throughput storage and retrieval of multidimensional data.
Time-series data occurs in settings such as observations initiated by radars
and satellites, checkpointing data representing state of the system at regular
intervals, and analytics representing the evolution of extracted knowledge
over time. Galileo is a demonstrably scalable storage framework for managing
such time-series data.

http://galileo.cs.colostate.edu/[+http://galileo.cs.colostate.edu/+]


Galois
------

A system that automatically executes "Galoized" serial Cxx or Java code in
parallel on shared-memory machines. It works by exploiting amorphous
data-parallelism, which is present even in irregular codes that are organized
around pointer-based data structures such as graphs and trees. The Galois
system includes the Lonestar benchmark suite and the ParaMeter profiler. 

Multicore processors are becoming increasingly the norm. As a result, we need
to find ways to make it easier to write parallel programs. Galois allows the
programmer to write serial Cxx or Java code while still getting the
performance of parallel execution. All the programmer has to do is use
Galois-provided data structures, which are necessary for correct concurrent
execution, and annotate which loops should be run in parallel. The Galois
system then speculatively extracts as much parallelism as it can. The current
release includes a dozen sample benchmarks applications from a broad range of
domains that are written using the Galois extensions and classes. 

http://iss.ices.utexas.edu/?p=projects/galois[+http://iss.ices.utexas.edu/?p=projects/galois+]



GASNet
------

A language-independent, low-level networking layer that provides
network-independent, high-performance communication primitives tailored for
implementing parallel global address space SPMD languages such as UPC,
Titanium, and Co-Array Fortran. The interface is primarily intended as a
compilation target and for use by runtime library writers (as opposed to end
users), and the primary goals are high performance, interface portability, and
expressiveness. GASNet stands for "Global-Address Space Networking". 

http://gasnet.cs.berkeley.edu/[+http://gasnet.cs.berkeley.edu/+]

PyGAS
~~~~~~

Python bindings for GASNet.

https://github.com/mbdriscoll/pygas[+https://github.com/mbdriscoll/pygas+]

GASpAR
------

An object-oriented geophysical and astrophysical spectral-element adaptive
refinement code. Like most spectral-element codes, GASpAR combines
finite-element efficiency with spectral-method accuracy. It is also designed
to be flexible enough for a range of geophysics and astrophysics applications
where turbulence or other complex multi-scale problems arise. The formalism
accommodates both conforming and non-conforming elements.

http://www.image.ucar.edu/TNT/Software/GASpAR/[+http://www.image.ucar.edu/TNT/Software/GASpAR/+]

GAViewer
--------

A multi-purpose program for performing geometric algebra computations
and visualizing geometric algebra.

http://www.geometricalgebra.net/gaviewer_download.html[+http://www.geometricalgebra.net/gaviewer_download.html+]

http://www.geometricalgebra.net/figures.html[+http://www.geometricalgebra.net/figures.html+]

GCRM
----

The Global Cloud Resolving Model.

https://svn.pnl.gov/gcrm[+https://svn.pnl.gov/gcrm+]

https://svn.pnl.gov/gcrm/wiki/overview[+https://svn.pnl.gov/gcrm/wiki/overview+]

+svn co svn://kiwi.atmos.colostate.edu/GCRM+

GeoLearn
--------

GeoLearn is designed to enable rapid processing of large size satellite remote
sensing data available in HDF EOS format. It has been tested primarily with
MODIS land-surface data products. Use and analysis of these datasets are at
the heart of a variety of scientific investigations pertaining to the study of
the interaction between land-surface and climate, and prediction of
terrestrial hydrologic processes. 

http://isda.ncsa.uiuc.edu/download/index.php?project=GeoLearn&sort=category[+http://isda.ncsa.uiuc.edu/download/index.php?project=GeoLearn&sort=category+]

http://isda.ncsa.uiuc.edu/geolearn/[+http://isda.ncsa.uiuc.edu/geolearn/+]

GeoScript
---------

Adds spatial capabilities to scripting languages, e.g. Python.

http://geoscript.org/[+http://geoscript.org/+]


https://conference.scipy.org/scipy2013/presentation_detail.php?id=146[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=146+]

GmtPy
-----

GmtPy provides seamless integration of GMT plotting into Python programs. On
top of that it provides (in an opt-in fashion): autoscaling, automatic tick
increment determination, layout management, and more.

http://emolch.github.io/gmtpy/[+http://emolch.github.io/gmtpy/+]


GNOME
-----

General NOAA Operational Modeling Environment, TNG.

https://github.com/NOAA-ORR-ERD/GNOME[+https://github.com/NOAA-ORR-ERD/GNOME+]

https://conference.scipy.org/scipy2013/presentation_detail.php?id=158[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=158+]

google-chartwrapper
-------------------

A Python wrapper for the Google Chart API. The wrapper can render the URL of
the Google chart, based on your parameters, or it can render an HTML img tag
to insert into webpages on the fly. Made for dynamic python websites
(Django,Zope,CGI,etc.) that need on the fly chart generation without any extra
modules.

http://code.google.com/p/google-chartwrapper/[+http://code.google.com/p/google-chartwrapper/+]

Google Maps JavaScript API
--------------------------

The Google Maps Javascript API lets you embed Google Maps in your own web
pages. Version 3 of this API is especially designed to be faster and more
applicable to mobile devices, as well as traditional desktop browser
applications.
The API provides a number of utilities for manipulating maps (just like on the
http://maps.google.com web page) and adding content to the map through a
variety of services, allowing you to create robust maps applications on your
website.

https://developers.google.com/maps/documentation/javascript/[xxhttps://developers.google.com/maps/documentation/javascript/]

GPI-2
-----

An API for the development of scalable, asynchronous and fault tolerant
parallel applications. 

http://www.gpi-site.com/gpi2/[+http://www.gpi-site.com/gpi2/+]

GPU Ocelot
----------

An open-source dynamic JIT compilation framework for GPU compute applications
targetinga range of GPU and non-GPU execution targets. Ocelot supports CUDA
applications and provides animplementation of the CUDA Runtime API enabling
seamless integration. NVIDIA’s PTX virtualinstruction set architecture is used
as adevice-agnostic program representation that captures the data-parallel
SIMT execution model ofCUDA applications. Ocelot supports several backend
execution targets – a PTX emulator, NVIDIA GPUs,AMD GPUs, and a translator to
LLVM for efficient execution of GPU kernels on multicore CPUs.

http://gpuocelot.gatech.edu/[+http://gpuocelot.gatech.edu/+]

http://code.google.com/p/gpuocelot/[+http://code.google.com/p/gpuocelot/+]

http://data1.gfdl.noaa.gov/multi-core/2011/presentations/Vetter_Enabling%20Heterogenous%20Computing%20for%20the%20Open%20Science%20Community.pdf[+http://data1.gfdl.noaa.gov/multi-core/2011/presentations/Vetter_Enabling%20Heterogenous%20Computing%20for%20the%20Open%20Science%20Community.pdf+]

Graal/Truffle
-------------

Graal is a new experimental just-in-time compiler for Java that is integrated
with the HotSpot virtual machine. Its focus is to provide excellent peak
performance via new techniques in the area of method inlining, removing object
allocations, and speculative execution. The term GraalVM is used to denote a
HotSpot virtual machine configured with Graal.

Truffle is a multi-language framework for executing dynamic languages that
achieves high performance when combined with Graal. This OTN release includes
a Truffle-based JavaScript execution engine that can be used to run JavaScript
applications with GraalVM. There are several open source projects building
Truffle-based runtimes for other languages, e.g., Ruby (see ''TruffleRuby''),
R (see ''FastR''), or Python (see ''ZipPy'').

http://www.oracle.com/technetwork/oracle-labs/program-languages/overview/index-2301583.html[+http://www.oracle.com/technetwork/oracle-labs/program-languages/overview/index-2301583.html+]

GRACE Software
--------------

A collection of hundreds of Matlab scripts, many of which are
useful for the geosciences.

http://geoweb.princeton.edu/people/simons/software.html[+http://geoweb.princeton.edu/people/simons/software.html+]

GRASS
-----

A free and open source Geographic Information System (GIS) software suite used
for geospatial data management and analysis, image processing, graphics and
maps production, spatial modeling, and visualization.

http://grass.osgeo.org/[+http://grass.osgeo.org/+]

Grbl
----

Software for controlling the motion of machines that make things. If the maker
movement was an industry, Grbl would be the industry standard.

http://bengler.no/grbl[+http://bengler.no/grbl+]

gridgen-c
---------

A C code that provides a command line utility for non-interactive generation
of multi-corner quasi-orthogonal grids inside simply connected polygonal
regions.It is based on the CRDT algorithm that makes it possible to handle
regions with elongated channels in a numerically robust way. 

http://code.google.com/p/gridgen-c/[+http://code.google.com/p/gridgen-c/+]

gridutils
~~~~~~~~~

Provides C library functions and command line utilities for working with
curvilinear grids. gridutils has been developed and used mainly for grids
generated by gridgen, but can be used to handle arbitrary 2D quadrilateral
simply connected multi-corner grids. 

http://code.google.com/p/gridutils-c/[+http://code.google.com/p/gridutils-c/+]

gsw
---

Python implementation of the thermodynamic equation of seawater (TEOS-10).

https://pypi.python.org/pypi/gsw/3.0.1[+https://pypi.python.org/pypi/gsw/3.0.1+]

http://www.teos-10.org/[+http://www.teos-10.org/+]

guidata
-------

A Python library for generating GUIs for easy dataset editing and display.

http://pythonhosted.org/guidata/[+http://pythonhosted.org/guidata/+]

GUT
---

The GOCE User Toolbox GUT is a compilation of tools for the utilisation and
analysis of GOCE Level 2 products. GUT supports applications in Geodesy,
Oceanography and Solid Earth Physics.  GUT is a tool to facilitate the use,
viewing and post-processing of GOCE Level 2 mission data products for optimal
use in the fields of geodesy, oceanography and solid Earth physics. GUT is a
command-line processor that has been designed for users at all levels of
expertise.

https://earth.esa.int/web/guest/software-tools/gut/about-gut/overview[+https://earth.esa.int/web/guest/software-tools/gut/about-gut/overview+]

GWL
---

The Geophysical Wavelet Library (GWL) is a software package based on the
continuous wavelet transform that allows to perform the direct and inverse
continuous wavelet transform, 2C and 3C polarization analysis and filtering,
modeling the dispersed and attenuated wave propagation in the time-frequency
domain and optimization in signal and wavelet domains with the aim to extract
velocities and attenuation parameters from a seismogram. The novelty of this
package is that we incorporate the continuous wavelet transform into the
library, where the kernel is the time-frequency polarization and dispersion
analysis. This library has a wide range of potential applications in the field
of signal analysis and may be particularly suitable in geophysical problems
that we illustrate by analyzing synthetic, geomagnetic and real seismic data. 

http://users.math.uni-potsdam.de/\~gwl/[+http://users.math.uni-potsdam.de/~gwl/+]

http://www.sciencedirect.com/science/article/pii/S0098300408001568[+http://www.sciencedirect.com/science/article/pii/S0098300408001568+]

HaLoop
------

A modified version of the Hadoop MapReduce framework, designed to serve these
applications. HaLoop not only extends MapReduce with programming support for
iterative applications, but also dramatically improves their efficiency by
making the task scheduler loop-aware and by adding various caching mechanisms.
We evaluate HaLoop on real queries and real datasets and find that, on
average, HaLoop reduces query runtimes by 1.85 compared with Hadoop, and
shuffles only 4% of the data between mappers and reducers compared with
Hadoop. 

http://code.google.com/p/haloop/[+http://code.google.com/p/haloop/+]

HDF5
----

http://www.hdfgroup.org/HDF5/[+http://www.hdfgroup.org/HDF5/+]

h5edit
~~~~~~

An HDF5 file editor.

ftp://ftp.hdfgroup.org/HDF5/projects/jpss/h5edit/[+ftp://ftp.hdfgroup.org/HDF5/projects/jpss/h5edit/+]

h5py
~~~~

A Python interface to HDF5.

https://code.google.com/p/h5py/[+https://code.google.com/p/h5py/+]

h5utils
-------

A set of utilities for visualization and conversion of scientific data in the
free, portable HDF5 format.  Besides providing a simple tool for batch
visualization as PNG images, h5utils also includes programs to convert HDF5
datasets into the formats required by other free visualization software (e.g.
plain text, Vis5d, and VTK). 

http://ab-initio.mit.edu/wiki/index.php/H5utils[+http://ab-initio.mit.edu/wiki/index.php/H5utils+]

HDFView
~~~~~~~

A visual tool for browsing and editing HDF4 and HDF5 files.

http://www.hdfgroup.org/hdf-java-html/hdfview/index.html[+http://www.hdfgroup.org/hdf-java-html/hdfview/index.html+]

HL-HDF
~~~~~~

A high level interface to the Heirarchical Data Format, version 5, developed
and maintained by the HDF group at the National Center for Supercomputing
Applications (NCSA), at the University of Illinois at Urbana-Champaign. HDF5
is a file format designed for maximum flexibility and efficiency and it makes
use of modern software technology. HDF5 sports such fundamental
characteristics as platform independence and efficient built-in compression,
and it can be used to store virtually any kind of scientific data. HL-HDF is
designed to focus on selected HDF5 functionality and make it available to
users at a high level of abstraction to facilitate data management. This
distribution contains HL-HDF source code and associated documentation. The
first version also comes prebuilt for a multitude of platforms.

ftp://ftp.hdfgroup.org/HDF5/contrib/hl-hdf5/README.html[+ftp://ftp.hdfgroup.org/HDF5/contrib/hl-hdf5/README.html+]

ftp://ftp.hdfgroup.org/HDF5/contrib/hl-hdf5/doc/hlhdf.html[+ftp://ftp.hdfgroup.org/HDF5/contrib/hl-hdf5/doc/hlhdf.html+]

H5hut
~~~~~

The H5hut library is an implementation of several data models for
particle-based simulations that encapsulates the complexity of parallel HDF5
and is simple to use, yet does not compromise performance. 
 H5hut is tuned for writing collectively from all processors to a single,
shared file. Although collective I/O performance is typically (but not always)
lower than that of file-per- processor, having a shared file simplifies
scientific workflows in which simulation data needs to be analyzed or
visualized. In this scenario, the file-per-processor approach leads to data
management headaches because large collections of files are unwieldy to manage
from a file system standpoint. On a parallel file system like Lustre, even the
ls utility will break when presented with tens of thousands of files, and
performance begins to degrade with this number of files because of contention
at the metadata server. Often a post-processing step is necessary to refactor
file-per-processor data into a format that is readable by the analysis tool.
In contrast, H5hut files can be directly loaded in parallel by visualization
tools like VisIt and ParaView.
H5hut is a veneer API for HDF5: H5hut files are also valid HDF5 files and are
compatible with other HDF5-based interfaces and tools. For example, the h5dump
tool that comes standard with HDF5 can export H5hut files to ASCII or XML for
additional portability. H5hut also includes tools to convert H5hut data to the
Visualization ToolKit (VTK) format and to generate scripts for the GNUplot
data plotting tool.

http://vis.lbl.gov/Research/H5hut/[+http://vis.lbl.gov/Research/H5hut/+]

H5root
~~~~~~

http://amas.web.psi.ch/tools/H5root/index.html[+http://amas.web.psi.ch/tools/H5root/index.html+]

Hedge
-----

An unstructured, high-order, parallel Discontinuous Galerkin (DG) code that I
am developing as part my PhD project. hedge's design is focused on two things:
being fast and easy to use. While the need for speed dictates implementation
in a low level language, these same low-level languages become quite
cumbersome at a higher level of abstraction. This is where the "h" in hedge
comes from; it takes a hybrid approach. While a small core is written in Cxx
for speed, all user-visible functionality is driven from Python.

http://mathema.tician.de/software/hedge[+http://mathema.tician.de/software/hedge+]

http://wiki.tiker.net/Hedge/HowTo/InstallingFromGit[+http://wiki.tiker.net/Hedge/HowTo/InstallingFromGit+]

HiFlow
------

A multi-purpose finite element software providing powerful tools for efficient
and accurate solution of a wide range of problems modeled by partial
differential equations (PDEs). Based on object-oriented concepts and the full
capabilities of Cxx the HiFlow³ project follows a modular and generic approach
for building efficient parallel numerical solvers. It provides highly capable
modules dealing with the mesh setup, finite element spaces, degrees of
freedom, linear algebra routines, numerical solvers, and output data for
visualization. Parallelism – as the basis for high performance simulations on
modern computing systems – is introduced on two levels: coarse-grained
parallelism by means of distributed grids and distributed data structures, and
fine-grained parallelism by means of platform-optimized linear algebra
back-ends.

http://hiflow3.org/[+http://hiflow3.org/+]

hpGEM
-----

A  Cxx software package for discontinuous Galerkin method. This framework is
intended to those who want to easily develop and apply discontinuous Galerkin
methods for various physical problems, especially partial differential
equations, arising from fluid mechancis and electro-magnetism. Using HPGEM,
one can numerically solve the simplest class room examples such as linear
advection and Burgers equations to the most complicated practical examples
such as shallow water, Euler, Navier-Stokes and Maxwell equations.

http://einder.ewi.utwente.nl/hpGEM/[+http://einder.ewi.utwente.nl/hpGEM/+]

[[html5lib-python]]
html5lib-python
---------------

Standards-compliant library for parsing and serializing HTML documents and
fragments in Python.

https://github.com/html5lib/html5lib-python[+https://github.com/html5lib/html5lib-python+]

HTSQL
-----

A comprehensive navigational query language for relational databases.
HTSQL is designed for data analysts and other accidental programmers who have
complex business inquiries to solve and need a productive tool to write and
share database queries.

http://htsql.org/[+http://htsql.org/+]

HUBzero
-------

A software platform for creating dynamic web sites that support scientific
research and educational activities.

http://hubzero.org/[+http://hubzero.org/+]

I2P
---

I2P is an anonymous network, exposing a simple layer that applications can use
to anonymously and securely send messages to each other. The network itself is
strictly message based (a la IP), but there is a library available to allow
reliable streaming communication on top of it (a la TCP). All communication is
end to end encrypted (in total there are four layers of encryption used when
sending a message), and even the end points ("destinations") are cryptographic
identifiers (essentially a pair of public keys).

https://geti2p.net/en/about/intro[+https://geti2p.net/en/about/intro+]

Ibis
----

The Ibis Portability layer (IPL) is a communication library specifically
designed for usage in a grid environment. It has a number of properties which
help to achieve its goal of providing programmers with an easy to use,
reliable grid communication infrastructure.

http://www.cs.vu.nl/ibis/ipl.html[+http://www.cs.vu.nl/ibis/ipl.html+]

IDV
---

A Java-based software framework for analyzing and visualizing geoscience data.
 The IDV "reference application" is a geoscience display and analysis software
system with many of the standard data displays that other Unidata software
(e.g. GEMPAK and McIDAS) provide. It brings together the ability to display
and work with satellite imagery, gridded data (for example, numerical weather
prediction model output), surface observations, balloon soundings, NWS WSR-88D
Level II and Level III RADAR data, and NOAA National Profiler Network data,
all within a unified interface. It also provides 3-D views of the earth system
and allows users to interactively slice, dice, and probe the data, creating
cross-sections, profiles, animations and value read-outs of multi-dimensional
data sets. The IDV can display any Earth-located data if it is provided in a
known format.

http://www.unidata.ucar.edu/software/IDV/index.html[+http://www.unidata.ucar.edu/software/IDV/index.html+]

McIDAS-V
~~~~~~~~

A  free, open source, visualization and data analysis software package that is
the fifth generation in SSEC's 40 year history of sophisticated McIDAS (Man
computer Interactive Data Access System) software packages. McIDAS-V displays
weather satellite (including hyperspectral) and other geophysical data in 2-
and 3-dimensions, and can be used to analyze and manipulate the data with its
powerful mathematical functions.

http://www.ssec.wisc.edu/mcidas/software/v/[+http://www.ssec.wisc.edu/mcidas/software/v/+]

UNAVCO IDV
~~~~~~~~~~

A software package for exploration and visualization of Earth-located
geoscience data. 

http://www.geongrid.org/index.php/projects/idv[+http://www.geongrid.org/index.php/projects/idv+]

http://facility.unavco.org/software/idv/idv.html[+http://facility.unavco.org/software/idv/idv.html+]

Ignition
--------

A Python library for defining domain specific languages and generating high
performance code.

http://ignitionproject.github.io/ignition/[+http://ignitionproject.github.io/ignition/+]

ILNumerics
----------

A high performance math library for programmers and scientists. Extending the
.NET framework with tools needed for scientific computing, it simplifies the
implementation of all kinds of numerical algorithms in convenient, familiar
C#-syntax – optimized to the speed of C and FORTRAN. 

http://ilnumerics.net/[+http://ilnumerics.net/+]

ImageJ
------

A public domain Java image processing program.
It can display, edit, analyze, process, save and print 8-bit, 16-bit and
32-bit images. It can read many image formats including TIFF, GIF, JPEG, BMP,
DICOM, FITS and "raw". It supports "stacks", a series of images that share a
single window. It is multithreaded, so time-consuming operations such as image
file reading can be performed in parallel with other operations. 

ImageJ was designed with an open architecture that provides extensibility via
Java plugins. Custom acquisition, analysis and processing plugins can be
developed using ImageJ's built in editor and Java compiler. User-written
plugins make it possible to solve almost any image processing or analysis
problem. 

http://rsb.info.nih.gov/ij/[+http://rsb.info.nih.gov/ij/+]

Fiji
~~~~

A distribution of ImageJ (and soon ImageJ2) together with Java, Java 3D and a
lot of plugins organized into a coherent menu structure. Fiji compares to
ImageJ as Ubuntu compares to Linux. 

http://fiji.sc/Fiji[+http://fiji.sc/Fiji+]

ij-VTK
~~~~~~

A project to combine VTK with ImageJ.

http://ij-plugins.sourceforge.net/ij-vtk/[+http://ij-plugins.sourceforge.net/ij-vtk/+]

ImageJ2
~~~~~~~

A project to develop the next-generation version of ImageJ.

http://developer.imagej.net/about[+http://developer.imagej.net/about+]

ImageTools
----------

The motivation for developing ImageTools (formerly known as Im2Learn) comes
from academic, government and industrial collaborations that involve
development of new computer methods and solutions for understanding complex
data sets. Images and other types of data generated by various instruments and
sensors form complex and highly heterogeneous data sets, and pose challenges
on knowledge extraction.

The main goal of the ImageTools research and development is to automate
information processing of repetitive, laborious and tedious analysis tasks and
build user-friendly decision-making systems that operate in automated or
semi-automated mode in a variety of applications. The development is based on
theoretical foundations of image and video processing, computer vision, data
fusion, statistical and spectral modeling.

http://isda.ncsa.illinois.edu/drupal/software/ImageTools[+http://isda.ncsa.illinois.edu/drupal/software/ImageTools+]

[[Incompact3d]]
Incompact3d
-----------

A powerful numerical tool for academic research. It can combine the
versatility of industrial codes with the accuracy of spectral codes. Thank to
a very successful project with NAG and HECToR (UK Supercomputing facility),
Incompact3d can be used on up to hundreds of thousands computational cores to
solve the incompressible Navier-Stokes equations. This high level of
parallelisation is achieved thank to a highly scalable 2D decomposition
library and a distributed Fast Fourier Transform (FFT) interface.

See xref:2DECOMP[2DECOMP].

http://code.google.com/p/incompact3d/[+http://code.google.com/p/incompact3d/+]

inspyred
--------

The inspyred library grew out of insights from Ken de Jong’s book
“Evolutionary Computation: A Unified Approach.” The goal of the library is to
separate problem-specific computation from algorithm-specific computation. Any
bio-inspired algorithm has at least two aspects that are entirely
problem-specific: what solutions to the problem look like and how such
solutions are evaluated. These components will certainly change from problem
to problem. For instance, a problem dealing with optimizing the volume of a
box might represent solutions as a three-element list of real values for the
length, width, and height, respectively. In contrast, a problem dealing with
optimizing a set of rules for escaping a maze might represent solutions as a
list of pair of elements, where each pair contains the two-dimensional
neighborhood and the action to take in such a case.

On the other hand, there are algorithm-specific components that may make no
(or only modest) assumptions about the type of solutions upon which they
operate. These components include the mechanism by which parents are selected,
the way offspring are generated, and the way individuals are replaced in
succeeding generations. For example, the ever-popular tournament selection
scheme makes no assumptions whatsoever about the type of solutions it is
selecting. The n-point crossover operator, on the other hand, does make an
assumption that the solutions will be linear lists that can be “sliced up,”
but it makes no assumptions about the contents of such lists. They could be
lists of numbers, strings, other lists, or something even more exotic.

The central design principle for inspyred is to separate problem-specific
components from algorithm-specific components in a clean way so as to make
algorithms as general as possible across a range of different problems.

http://inspyred.github.io/[+http://inspyred.github.io/+]

Invenio
-------

Invenio is a free software suite enabling you to run your own
digital library or document repository on the web. The technology
offered by the software covers all aspects of digital library management from
document ingestion through classification, indexing, and curation to
dissemination. Invenio complies with standards such as the Open Archives
Initiative metadata harvesting protocol (OAI-PMH) and uses MARC 21 as
its underlying bibliographic format. The flexibility and performance of
Invenio make it a comprehensive solution for management of document
repositories of moderate to large sizes (several millions of records).

http://invenio-software.org/[+http://invenio-software.org/+]

IOFSL
-----

A scalable, unified high-end computing I/O forwarding software layer.

http://www.mcs.anl.gov/research/projects/iofsl/[+http://www.mcs.anl.gov/research/projects/iofsl/+]

IPython
-------

*Gallery of IPython Notebook Themes* -
https://github.com/nsonnad/base16-ipython-notebook[+https://github.com/nsonnad/base16-ipython-notebook+]

*A Gallery of Interesting Python Notebooks*

https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks[+https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks+]

*Notebook Gallery*

https://www.wakari.io/gallery[+https://www.wakari.io/gallery+]

Bookstore
~~~~~~~~~

Stores IPython notebooks automagically onto OpenStack clouds through Swift.

https://github.com/rgbkrk/bookstore[+https://github.com/rgbkrk/bookstore+]

http://developer.rackspace.com/blog/bookstore-for-ipython-notebooks.html[+http://developer.rackspace.com/blog/bookstore-for-ipython-notebooks.html+]

IPython-notebook-extensions
~~~~~~~~~~~~~~~~~~~~~~~~~~~

JavaScript extensions for IPython notebook.

https://github.com/ipython-contrib/IPython-notebook-extensions[+https://github.com/ipython-contrib/IPython-notebook-extensions+]

Rmagic
~~~~~~

A package for runing R code within IPython.

http://www.randalolson.com/2013/01/14/filling-in-pythons-gaps-in-statistics-packages-with-rmagic/[+http://www.randalolson.com/2013/01/14/filling-in-pythons-gaps-in-statistics-packages-with-rmagic/+]

Iris
----

A python library for meteorology and climatology.

http://scitools.org.uk/iris/[+http://scitools.org.uk/iris/+]

https://github.com/SciTools/iris[+https://github.com/SciTools/iris+]

https://conference.scipy.org/scipy2013/presentation_detail.php?id=132[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=132+]

*User Guide*:
http://scitools.org.uk/iris/docs/latest/userguide/index.html[+http://scitools.org.uk/iris/docs/latest/userguide/index.html+]

*Reference Guide*:
http://scitools.org.uk/iris/docs/latest/iris/iris.html[+http://scitools.org.uk/iris/docs/latest/iris/iris.html+]

*Examples*:
http://scitools.org.uk/iris/docs/latest/examples/index.html[+http://scitools.org.uk/iris/docs/latest/examples/index.html+]


Requires:  xref:Cartopy[Cartopy], xref:Pyke[Pyke]

iRODS
-----

The integrated Rule-Oriented Data-management System, a community-driven, open
source, data grid software solution. It helps researchers, archivists and
others manage (organize, share, protect, and preserve) large sets of computer
files. Collections can range in size from moderate to a hundred million files
or more totaling petabytes of data. 
This is the open-source successor to SRB.

https://www.irods.org/[+https://www.irods.org/+]

https://www.irods.org/pubs/iRODS_Overview_0903.pdf[+https://www.irods.org/pubs/iRODS_Overview_0903.pdf+]

ISOMAP
------

A global geometric framework for nonlinear dimensionality reduction.
A Matlab package is available.

http://isomap.stanford.edu/[+http://isomap.stanford.edu/+]

http://www.sciencemag.org/content/290/5500/2268.full[+http://www.sciencemag.org/content/290/5500/2268.full+]

ISPH
----

A cross-platform computational fluid dynamics (CFD) library for mesh-free
particle based simulation and visualization of incompressible flows using
Smoothed Particle Hydrodynamics (SPH) methods. The library is open source and
cross-platform, written in pure Cxx and the new standard for parallel
programming of modern processors - OpenCL. The library will make full use of
GPUs, CPUs and other OpenCL enabled devices in running system to accelerate
the computing.

http://isph.sourceforge.net/[+http://isph.sourceforge.net/+]

ISU TAS
-------

A Tile Assembly Model simulator that allows users to design tilesets and seeds
and to simulate assemblies.  The simulator allows for graphical creation of
seed assemblies, fast forwarding and rewinding of assembly growth, and easy
zooming, scrolling, and inspection of assemblies among other features. The
graphical tile type editor allows tile types to be easily designed and
manipulated. Assemblies and tile sets can be created, saved, and reloaded.

Our research is motivated by the prospect, raised by pioneering work of
Seeman, Winfree, and Rothemund, of engineering structures that autonomously
assemble themselves from molecular components. We are primarily interested in
understanding the power and limitations of this "programming of matter". Our
work includes the development and analysis of mathematical models of
self-assembly, the creation and use of software environments for developing
and simulating self-assembly systems, and studies of the self-assembly of
fractals and other complex structures. We also work to adapt methods that
software engineers have developed for creating, controlling, and reasoning
about systems of immense complexity (requirements engineering, programming
languages, formal verification, software safety, ...) to the even greater
challenges that nanotechnology will confront.

http://www.cs.iastate.edu/\~lnsa/software.html[+http://www.cs.iastate.edu/~lnsa/software.html+]

http://arxiv.org/abs/1101.5151[+http://arxiv.org/abs/1101.5151+]

ITxx
----

A C\xx library of mathematical, signal processing and communication classes and
functions. Its main use is in simulation of communication systems and for
performing research in the area of communications. The kernel of the library
consists of generic vector and matrix classes, and a set of accompanying
routines. Such a kernel makes ITxx similar to MATLAB, GNU Octave or SciPy.

http://itpp.sourceforge.net/[+http://itpp.sourceforge.net/+]

ITAPS
-----

Technologies that enable application scientists to easily use multiple mesh
and discretization strategies within a single simulation on petascale
computers.

https://trac.mcs.anl.gov/projects/ITAPS/[+https://trac.mcs.anl.gov/projects/ITAPS/+]

PyTAPS
~~~~~~

Python bindings for ITAPS interfaces.

https://pypi.python.org/pypi/PyTAPS/[+https://pypi.python.org/pypi/PyTAPS/+]

CGM
~~~

A code library which provides geometry functionality used for mesh generation
and other applications. This functionality includes that commonly found in
solid modeling engines, like geometry creation, query and modification; CGMA
also includes capabilities not commonly found in solid modeling engines, like
geometry decomposition tools and support for shared material interfaces.

https://trac.mcs.anl.gov/projects/ITAPS/wiki/CGM[+https://trac.mcs.anl.gov/projects/ITAPS/wiki/CGM+]

MeshKit
~~~~~~~

MeshKit is an open-source library of mesh generation functionality. Its design
philosophy is two-fold: it provides a collection of meshing algorithms for use
in real meshing problems, along with other tools commonly needed to support
mesh generation (coordination of BREP-based meshing process, mesh smoothing,
etc.); and it serves as a platform in which to perform mesh generation
algorithm research.

MeshKit has general mesh manipulation and generation functions such as Copy,
Move, Rotate and Extrude mesh. In addition, new quad mesh and embedded
boundary Cartesian mesh algorithm (EBMesh) are developed to be used.
Interfaces to several public-domain tetrahedral meshing algorithms (Gmsh,
netgen) are also offered.

This library interacts with mesh data mostly through iMesh including accessing
the mesh in parallel. It also can interact with iGeom interface to provide
geometry functionality such as importing solid model based geometries. iGeom
and iMesh are implemented in the CGM and MOAB packages, respectively. For some
non-existing functions in iMesh such as tree-construction and ray-tracing,
MeshKit also interacts with MOAB functions directly.

http://press3.mcs.anl.gov/sigma/meshkit-library/[+http://press3.mcs.anl.gov/sigma/meshkit-library/+]

https://trac.mcs.anl.gov/projects/fathom/wiki/MeshKit[+https://trac.mcs.anl.gov/projects/fathom/wiki/MeshKit+]

MOAB
~~~~

A component for representing and evaluating mesh data.  MOAB implements the
ITAPS iMesh interface; iMesh is a common interface to mesh data implemented by
several different packages, including MOAB. Various tools like smoothing,
adaptive mesh refinement, and parallel mesh communication are implemented on
top of iMesh.

https://trac.mcs.anl.gov/projects/ITAPS/wiki/MOAB[+https://trac.mcs.anl.gov/projects/ITAPS/wiki/MOAB+]

Jarvis
------

An attempt to write a Jarvis-like assistant in Python.

https://github.com/debugger22/Jarvis[+https://github.com/debugger22/Jarvis+]

JJ
--

J is a modern, high-level, general-purpose, high-performance programming
language.
J is particularly strong in the mathematical, statistical, and logical
analysis of data. It is a powerful tool in building new and better solutions
to old problems and even better at finding solutions where the problem is not
already well understood.

http://www.jsoftware.com/[+http://www.jsoftware.com/+]

JHOVE2
------

JHOVE2 is a framework and application for next-generation format-aware
characterization of digital objects. The function of JHOVE2 is encapsulated in
a series of modules that can be configured for use within the framework’s
plug-in architecture. The NetCDF Formatmodule, denominated JANEME: J-NetCDF
Metadata Extractor, provides characterization services for the netCDF family
of formats consisting of the profiles netCDF-3 and netCDF-4 and for the GRIB
family (GRIB 1.0 and 2.0) as well.

JANEME is able to parse and characterize files in NetCDF and GRIB format via
the Unidata netcdf-java library 4.1 (Unidata NetCDF-java) and to fill out
templates conforming to Dublin Core and a c3grid iso19115 compatible profile
with the extracted metadata while supporting JHOVE`s standard output as well.
Additionally, it supplies an axis2 web service deployable on any arbitrary
Java Application Server, i.e., Tomcat.

https://bitbucket.org/jhove2/main/wiki/Home[+https://bitbucket.org/jhove2/main/wiki/Home+]

http://aforge.awi.de/gf/project/jhove2/[+http://aforge.awi.de/gf/project/jhove2/+]

JLAB
----

A set of Matlab functions
for the purpose of analyzing data. It consists of four hundred m-files
spanning thirty-five thousand lines of code. JLAB includes functions ranging
in complexity from one-line aliases to high-level algorithms for certain
specialized tasks. About four hundred automated tests and dozens of scripts
for sample figures help keep things organized. 

-----
     Jarray     - Vector, matrix, and N-D array tools.
     Jmath      - Mathematical aliases and basic functions.
     Jpoly      - Special polynomials, matrices, and functions.
     Jgraph     - Fine-turning and customizing figures.
     Jstrings   - Strings, files, and variables.
     Jstats     - Statistical tools and probability distributions.
     Jsignal    - Signal processing, wavelet and spectral analysis.
     Jellipse   - Elliptical (bivariate) time series analysis.
     Jcell      - Tools for operating on cell arrays of numerical arrays.
     Vtools     - Operations on multiple data arrays simultaneously.
-----

http://www.jmlilly.net/jmlsoft.html[+http://www.jmlilly.net/jmlsoft.html+]

http://arxiv.org/abs/1110.0140[+http://arxiv.org/abs/1110.0140+]

http://www.nonlin-processes-geophys.net/13/467/2006/npg-13-467-2006.html[+http://www.nonlin-processes-geophys.net/13/467/2006/npg-13-467-2006.html+]

Kepler
------

The Kepler Project is dedicated to furthering and supporting the capabilities,
use, and awareness of the free and open source, scientific workflow
application, Kepler.  Kepler is designed to help scientists, analysts, and
computer programmers create, execute, and share models and analyses across a
broad range of scientific and engineering disciplines.  Kepler can operate on
data stored in a variety of formats, locally and over the internet, and is an
effective environment for integrating disparate software components, such as
merging "R" scripts with compiled "C" code, or facilitating remote,
distributed execution of models. Using Kepler's graphical user interface,
users simply select and then connect pertinent analytical components and data
sources to create a "scientific workflow"—an executable representation of the
steps required to generate results. The Kepler software helps users share and
reuse data, workflows, and components developed by the scientific community
to address common needs.

https://kepler-project.org/[+https://kepler-project.org/+]

KernelGen
---------

An auto-parallelizing Fortran/C compiler for NVIDA GPUs.

http://hpcforge.org/projects/kernelgen/[+http://hpcforge.org/projects/kernelgen/+]

http://tesla.parallel.ru/trac/coaccel[+http://tesla.parallel.ru/trac/coaccel+]

http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_3_Mikushin.pdf[+http://data1.gfdl.noaa.gov/multi-core/2012/presentations/Session_3_Mikushin.pdf+]

[[keytree]]
keytree
-------

Python functions for reading and writing KML.

See also xref:Fiona[Fiona], xref:Shapely[Shapely] and xref:Rtree[Rtree].

https://pypi.python.org/pypi/keytree/0.2.1[+https://pypi.python.org/pypi/keytree/0.2.1+]

[[KGPU]]
KGPU
----

KGPU is a GPU computing framework for the Linux kernel. It allows Linux kernel
to call CUDA programs running on GPUs directly. The motivation is to augment
operating systems with GPUs so that not only userspace applications but also
the operating system itself can benefit from GPU acceleration. It can also
free the CPU from some computation intensive work by enabling the GPU as an
extra computing device.

https://code.google.com/p/kgpu/[+https://code.google.com/p/kgpu/+]

[[Kivy]]
Kivy
----

Open source Python library for rapid development of applications
that make use of innovative user interfaces, such as multi-touch apps.

http://kivy.org[+http://kivy.org+]

http://pythonthusiast.pythonblogs.com/230_pythonthusiast/archive/1346_starting_to_use_kivy__developing_letter_of_heroes_an_android_alphabet_teaching_aid_application_for_kids-part_1_of_2.html[+http://pythonthusiast.pythonblogs.com/230_pythonthusiast/archive/1346_starting_to_use_kivy__developing_letter_of_heroes_an_android_alphabet_teaching_aid_application_for_kids-part_1_of_2.html+]

[[KNIIME]]
KNIME
-----

A user-friendly graphical workbench for the entire analysis process: data
access, data transformation, initial investigation, powerful predictive
analytics, visualisation and reporting. The open integration platform provides
over 1000 modules (nodes), including those of the KNIME community and its
extensive partner network.

KNIME can be downloaded onto the desktop and used free of charge. KNIME
products include additional functionalities such as shared repositories,
authentication, remote execution, scheduling, SOA integration and a web user
interface as well as world-class support. Robust big data extensions are
available for distributed frameworks such as Hadoop.

http://www.knime.org/[+http://www.knime.org/+]

Krextor
-------

An extensible XSLT-based framework for extracting RDF from XML, supporting
multiple input languages as well as multiple output RDF notations. 

http://trac.kwarc.info/krextor/[+http://trac.kwarc.info/krextor/+]

L2P
---

Creates PNG images of mathematical expressions formatted in LaTeX. While it
can convert a whole LaTeX document, it is designed to easily generate images
from just a fragment of LaTeX code. It depends on other software: latex,
dvips, and convert. (The last one is from the ImageMagick graphics toolset.)
If you already work with LaTeX on a modern Unix or Linux system, you probably
already have all of that installed.

http://redsymbol.net/software/l2p/[+http://redsymbol.net/software/l2p/+]

lagrangian
----------

To calculate backward-in-time, finite-size Lyapunov exponents (FSLEs) of the
global oceans. 

http://code.google.com/p/lagrangian/[+http://code.google.com/p/lagrangian/+]

LastWave
--------

A signal processing oriented command language with matlab-like syntax which
includes a high level object-oriented graphic language. It allows to deal with
high-level structures such as signals, images, wavelet transforms, extrema
representation, short time fourier transform, etc.

http://www.cmap.polytechnique.fr/\~bacry/LastWave/[+http://www.cmap.polytechnique.fr/~bacry/LastWave/+]

LaTeXML
-------

In the process of developing the Digital Library of Mathematical Functions, we
needed a means of transforming the LaTeX sources of our material into XML
which would be used for further manipulations, rearrangements and construction
of the web site. In particular, a true ‘Digital Library’ should focus on the
semantics of the material, and so we should convert the mathematical material
into both content and presentation MathML. At the time, we found no software
suitable to our needs, so we began development of LaTeXML in-house.

In brief, latexml is a program, written in Perl, that attempts to faithfully
mimic TeX’s behavior, but produces XML instead of dvi. The document model of
the target XML makes explicit the model implied by LaTeX. The processing and
model are both extensible; you can define the mapping between TeX constructs
and the XML fragments to be created. A postprocessor, latexmlpost converts
this XML into other formats such as HTML or XHTML, with options to convert the
math into MathML (currently only presentation) or images.

https://github.com/brucemiller/LaTeXML[+https://github.com/brucemiller/LaTeXML+]

http://dlmf.nist.gov/LaTeXML/[+http://dlmf.nist.gov/LaTeXML/+]

LETKF
-----

The Local Ensemble Transform Kalman Filter is  an advanced data assimilation
method for many possible applications. 

http://code.google.com/p/miyoshi/[+http://code.google.com/p/miyoshi/+]

libadjoint
----------

Much computational science deals with the approximate solution of models
described by systems of partial differential equations; these are used across
the entire breadth of the quantitative sciences. Such a model takes as input
the physical state at some initial time and runs forward in time to compute
the state at some later time of interest; that is, it maps cause to effect,
and so it is referred to as the forward model. For a given forward model, one
can associate an adjoint model, which does the opposite: it maps from effect
back to cause, and so runs backwards in time. Once an adjoint model is
available, it makes possible a number of very powerful techniques: optimise
engineering designs, assimilate data from physical measurements, estimate
unknown parameters in the forward model, and estimate the approximation error
in quantities of interest. Such applications are of huge interest and
importance across all of engineering and the quantitative sciences. As
computational science moves from mere simulation to optimisation, adjoint
modelling will only grow in importance.

The fundamental abstraction of algorithmic differentiation is that it treats
the model as a sequence of primitive instructions, each of which may be
differentiated in turn and composed using the chain rule. libadjoint explores
a similar, but higher-level abstraction: that the model is a sequence of
linear solves. In this approach, the model is instrumented with library calls
that record what operators it is assembling and what they are being applied
to, in an analogous manner to building a tape for reverse-mode AD. The model
developer then provides callback routines that compute the action of or
assemble these operators to the library. With this information, the library
may then assemble the adjoint of each equation solved in the forward model
automatically. This promises to make adjointing models significantly easier
than it currently is.

https://launchpad.net/libadjoint/[+https://launchpad.net/libadjoint/+]

http://dolfin-adjoint.org/[+http://dolfin-adjoint.org/+]

http://arxiv.org/abs/1204.5577[+http://arxiv.org/abs/1204.5577+]

http://amcg.ese.ic.ac.uk/\~pef/[+http://amcg.ese.ic.ac.uk/~pef/+]

libLAS
------

A C/Cxx library for reading and writing the very common LAS LiDAR format. The
ASPRS LAS format is a sequential binary format used to store data from LiDAR
sensors and by LiDAR processing software for data interchange and archival.

http://www.liblas.org/[+http://www.liblas.org/+]

http://live.osgeo.org/en/overview/liblas_overview.html[+http://live.osgeo.org/en/overview/liblas_overview.html+]

paraview_las_plugin
~~~~~~~~~~~~~~~~~~~

A LAS reader plugin for ParaView.

https://github.com/chambbj/paraview_las_plugin[+https://github.com/chambbj/paraview_las_plugin+]

LibDyND
-------

A Cxx library for dynamic, multidimensional arrays.

https://github.com/ContinuumIO/libdynd[+https://github.com/ContinuumIO/libdynd+]

DyND-Python
~~~~~~~~~~~

Python bindings for LibDyND.

https://github.com/ContinuumIO/dynd-python[+https://github.com/ContinuumIO/dynd-python+]

LibMultiScale
-------------

A C\xx parallel framework for the multiscale coupling methods dedicated to
material simulations. This framework is designed with the form of a library
providing an API which makes it possible to program coupled simulations. At
the present time, stable implemented coupling method is based on Bridging
Method.  The coupled parts can be provided by existing projects. In such a
manner, the API gives Cxx templated interfaces to reduce to the maximum the
cost of integration taking the form of plugins or alike. Such codes have been
integrated to provide a functional prototype of the framework. For example,
molecular dynamics software that have been integrated is Stamp (a code of the
CEA) and Lammps (Sandia laboratories). The unique software of continuum
mechanics, discretized by finite elements, is based on the libMesh framework.

http://libmultiscale.gforge.inria.fr/[+http://libmultiscale.gforge.inria.fr/+]

https://gforge.inria.fr/projects/libmultiscale/[+https://gforge.inria.fr/projects/libmultiscale/+]

libpca
------

A Cxx library computing a principal component analysis plus corresponding
transformations.  This requires the Armadillo library. 

http://sourceforge.net/projects/libpca/[+http://sourceforge.net/projects/libpca/+]

LIME
----

A software tool for creating multiphysics simulation codes. 

http://sourceforge.net/projects/lime1/[+http://sourceforge.net/projects/lime1/+]

LinBox
------

A a Cxx template library for exact, high-performance linear algebra
computation with dense, sparse, and structured matrices over the integers and
over finite fields. 

http://www.linalg.org/[+http://www.linalg.org/+]

http://arxiv.org/abs/1009.1317[+http://arxiv.org/abs/1009.1317+]

LTFAT
-----

The The Large Time/Frequency Analysis Toolbox (LTFAT) is a Matlab/Octave
toolbox for working with time-frequency analysis and synthesis. It is intended
both as an educational and a computational tool. The toolbox provides a large
number of linear transforms including Gabor and wavelet transforms along with
routines for constructing windows (filter prototypes) and routines for
manipulating coefficients.

http://ltfat.sourceforge.net/[+http://ltfat.sourceforge.net/+]

LWPR
----

Locally Weighted Projection Regression (LWPR) is a recent algorithm that
achieves nonlinear function approximation in high dimensional spaces with
redundant and irrelevant input dimensions. At its core, it uses locally linear
models, spanned by a small number of univariate regressions in selected
directions in input space. A locally weighted variant of Partial Least Squares
(PLS) is employed for doing the dimensionality reduction.

A Python version is available.

http://wcms.inf.ed.ac.uk/ipab/slmc/research/software-lwpr[+http://wcms.inf.ed.ac.uk/ipab/slmc/research/software-lwpr+]

Madagascar
----------

An open-source software package for multidimensional data analysis and
reproducible computational experiments.

http://www.ahay.org/wiki/Main_Page[+http://www.ahay.org/wiki/Main_Page+]

Magicsxx
--------

The latest generation of the ECMWF's Meteorological plotting software MAGICS.
Although completely redesigned in Cpp, it is intended to be as
backwards-compatible as possible with the Fortran interface. The contour
package was rewritten and no longer depends on the CONICON licence. Besides
its programming interfaces (Fortran and C), Magics offers MagML, a plot
description language based on XML.
Magics supports the plotting of contours, wind fields, observations,
satellite images, symbols, text, axis and graphs (including boxplots). Data
fields to be plotted may be presented in various formats, for instance GRIB 1
and 2 code data, gaussian grid, regularly spaced grid and fitted data. GRIB
data is handled via ECMWF's GRIB API software. Input data can also be in BUFR
and NetCDF format or retrieved from an ODB database. The produced
meteorological plots can be saved in various formats, such as PostScript, EPS,
PDF, GIF, PNG and SVG.

https://software.ecmwf.int/wiki/display/MAGP/Magics[+https://software.ecmwf.int/wiki/display/MAGP/Magics+]

magpie
------

Git-backed Evernote replacement in Python.

https://github.com/charlesthomas/magpie[+https://github.com/charlesthomas/magpie+]

Mahotas
-------

A a set of functions for image processing and computer vision in Python.

http://luispedro.org/software/mahotas[+http://luispedro.org/software/mahotas+]

https://github.com/luispedro/mahotas[+https://github.com/luispedro/mahotas+]

http://arxiv.org/abs/1211.4907[+http://arxiv.org/abs/1211.4907+]

[[Makeflow]]
Makeflow
--------

A  workflow engine for executing large complex workflows on clusters, clouds,
and grids. Makeflow is very similar to traditional Make, so if you can write a
Makefile, then you can write a Makeflow. You can be up and running workflows
in a matter of minutes.

http://www3.nd.edu/~ccl/software/makeflow/[+http://www3.nd.edu/~ccl/software/makeflow/+]

MASA
----

MASA (Manufactured Analytical Solution Abstraction) is a library written in
Cxx (with C and Fortran90 interfaces) which provides a suite of manufactured
solutions for the software verification of partial differential equation
solvers in multiple dimensions.
MASA provides two methods to import manufactured solutions into the library.
Users can either generate their own source terms, or they can use the
automatic differentiation capabilities provided in MASA. The method by which
solutions can be added to is provided by the "MASA-import" script.

https://red.ices.utexas.edu/projects/software/wiki/MASA[+https://red.ices.utexas.edu/projects/software/wiki/MASA+]

http://link.springer.com/article/10.1007%2Fs00366-012-0267-9[+http://link.springer.com/article/10.1007%2Fs00366-012-0267-9+]

Mastrave
--------

A free software library written to perform vectorized scientific computing and
to be as compatible as possible with both GNU Octave and Matlab computing
frameworks, offering general purpose, portable and freely available features
for the scientific community.  Mastrave is mostly oriented to ease complex
modelling tasks such as those typically needed within environmental models,
even when involving irregular and heterogeneous data series.

http://mastrave.org/[+http://mastrave.org/+]

mcerp
-----

An on-the-fly calculator for Monte Carlo methods that uses latin-hypercube
sampling (see soerp for the Python implementation of the analytical
second-order error propagation original Fortran code 'SOERP' by N. D. Cox) to
perform non-order specific error propagation (or uncertainty analysis). The
mcerp package allows you to easily and transparently track the effects of
uncertainty through mathematical calculations. Advanced mathematical
functions, similar to those in the standard math module can also be evaluated
directly.

https://pypi.python.org/pypi/mcerp[+https://pypi.python.org/pypi/mcerp+]

MCR
---

The MATLAB Compiler Runtime (MCR) is a standalone set of shared libraries that
enables the execution of compiled MATLAB applications or components on
computers that do not have MATLAB installed. When used together, MATLAB,
MATLAB Compiler, and the MCR enable you to create and distribute numerical
applications or software components quickly and securely. 

http://www.mathworks.com/products/compiler/mcr/index.html[+http://www.mathworks.com/products/compiler/mcr/index.html+]

MediaGoblin
-----------

A free software media publishing platform that anyone can run. You can think
of it as a decentralized alternative to Flickr, YouTube, SoundCloud, etc.

http://mediagoblin.org/[+http://mediagoblin.org/+]

MediaWiki
---------

http://www.mediawiki.org/wiki/MediaWiki[+http://www.mediawiki.org/wiki/MediaWiki+]

WorkingWiki
~~~~~~~~~~~

A software extension for MediaWiki that makes it into a powerful environment
for collaborating on publication-quality manuscripts and software projects.

http://www.mediawiki.org/wiki/Extension:WorkingWiki[+http://www.mediawiki.org/wiki/Extension:WorkingWiki+]

http://arxiv.org/abs/1212.1986[+http://arxiv.org/abs/1212.1986+]

MetaGETA
--------

The Metadata Gathering, Extraction and Transformation Application is a Python
application for discovering and extracting metadata from spatial raster
datasets (metadata crawler) and transforming it into xml (metadata
transformation). A number of generic and specialised imagery formats are
supported. The format support has a plugin architecture and more formats can
easily be added. 

http://code.google.com/p/metageta/[+http://code.google.com/p/metageta/+]

Metview
-------

A meteorological workstation application designed to be a complete working
environment for both the operational and research meteorologist. Its
capabilities include powerful data access, processing and visualisation.
It features a powerful icon-based user interface for interactive work, and a
scripting language for batch processing. The two are linked through the
ability to automatically convert icons into their equivalent script code.

https://software.ecmwf.int/wiki/display/METV/Metview[+https://software.ecmwf.int/wiki/display/METV/Metview+]

MFT
---

Multidimensional Fourier transform software.

http://nmr700.chem.uw.edu.pl/software.html[+http://nmr700.chem.uw.edu.pl/software.html+]

http://nmr700.chem.uw.edu.pl/formularz.html[+http://nmr700.chem.uw.edu.pl/formularz.html+]

MGS
---

A domain specific language (DSL) devoted to the simulation of biological
processes, especially those whose state space must be computed jointly with
the current state of the system.
MGS embeds the idea of topological collections and their transformations into
the framework of a simple dynamically typed functional language. Collections
are just new kinds of values and transformations are functions acting on
collections and defined by a specific syntax using rules. MGS is an
applicative programming language: operators acting on values combine values to
give new values, they do not act by side-effect. 

http://mgs.spatial-computing.org/[+http://mgs.spatial-computing.org/+]

minepy
------

An ANSI C library (with Cxx, Python and MATLAB/OCTAVE wrappers) for Maximal
Information-based Nonparametric Exploration (MIC and MINE family).

http://minepy.sourceforge.net/[+http://minepy.sourceforge.net/+]

http://arxiv.org/abs/1208.4271[+http://arxiv.org/abs/1208.4271+]


minfx
-----

A Python package for numerical optimisation, being a large collection of
standard minimisation algorithms. The name minfx is simply a shortening of the
mathematical expression min f(x). 

https://gna.org/projects/minfx[+https://gna.org/projects/minfx+]

http://home.gna.org/minfx/[+http://home.gna.org/minfx/+]

minpower
--------

An open source toolkit for students and researchers in power systems. It is
designed to make working with ED, OPF, and UC problems simple and intuitive.
The goal is to foster collaboration with other researchers and to make
learning easier for students.

http://minpowertoolkit.com/[+http://minpowertoolkit.com/+]

MLbase
------

Implementing and consuming Machine Learning techniques at scale are difficult
tasks for ML Developers and End Users. MLbase is a platform addressing the
issues of both groups, and consists of three components: MLlib, MLI, ML
Optimizer.

http://mlbase.org/[+http://mlbase.org/+]

mlpack
------

A scalable Cxx machine learning library with Python bindings.

http://www.mlpack.org/[+http://www.mlpack.org/+]

mlpy
----

A Python module for machine learning.
It provides a wide range of state-of-the-art machine learning methods for
supervised and unsupervised problems and it is aimed at finding a reasonable
compromise among modularity, maintainability, reproducibility, usability and
efficiency.

http://mlpy.sourceforge.net/[+http://mlpy.sourceforge.net/+]

http://mlpy.sourceforge.net/docs/3.5/[+http://mlpy.sourceforge.net/docs/3.5/+]

http://arxiv.org/abs/1202.6548[+http://arxiv.org/abs/1202.6548+]

Modelica
--------

Modelica® is a non-proprietary, object-oriented, equation based language to
conveniently model complex physical systems containing, e.g., mechanical,
electrical, electronic, hydraulic, thermal, control, electric power or
process-oriented subcomponents.

https://www.modelica.org/[+https://www.modelica.org/+]

JModelica
~~~~~~~~~

JModelica.org is an extensible Modelica-based open source platform for
optimization, simulation and analysis of complex dynamic systems. The main
objective of the project is to create an industrially viable open source
platform for optimization of Modelica models, while offering a flexible
platform serving as a virtual lab for algorithm development and research. As
such, JModelica.org provides a platform for technology transfer where
industrially relevant problems can inspire new research and where state of the
art algorithms can be propagated from academia into industrial use.

http://www.jmodelica.org/[+http://www.jmodelica.org/+]

OpenModelica
~~~~~~~~~~~~

OPENMODELICA is an open-source Modelica-based modeling and simulation
environment intended for industrial and academic usage. 

https://openmodelica.org/[+https://openmodelica.org/+]

MORSE
-----

The goal of Matrices Over Runtime Systems at Exascale (MORSE) project is to
design dense and sparse linear algebra methods that achieve the fastest
possible time to an accurate solution on large-scale multicore systems with
GPU accelerators, using all the processing power that future high end systems
can make available. To develop software that will perform well on petascale
and exascale systems with thousands of nodes and millions of cores, several
daunting challenges have to be overcome, both by the numerical linear algebra
and the runtime system communities. By designing a research framework for
describing linear algebra algorithms at a high level of abstraction,the MORSE
team will enable the strong collaboration between research groups in linear
algebra and runtime systems needed to develop methods and libraries that fully
benefit from the potential of future large-scale machines. Our project will
take a pioneering step in the effort to bridge the immense software gap that
has opened up in front of the High-Performance Computing (HPC) community.

http://icl.eecs.utk.edu/morse/[+http://icl.eecs.utk.edu/morse/+]

MOSAICO
-------

MOdular library for raSter bAsed hydrologIcal appliCatiOn.

https://github.com/gravazza/MOSAICO[+https://github.com/gravazza/MOSAICO+]

MSEAS
-----

The MIT Multidisciplinary Simulation, Estimation, and Assimilation Systems
(MSEAS) group creates, develops and utilizes new mathematical models and
computational methods for ocean predictions and dynamical diagnostics, for
optimization and control of autonomous ocean observation systems, and for data
assimilation and data-model comparisons. Our systems are used for basic and
fundamental research and for realistic simulations and predictions in varied
regions of the world’s ocean.

http://mseas.mit.edu/[+http://mseas.mit.edu/+]

http://www.sciencedirect.com/science/article/pii/S1463500311001399[+http://www.sciencedirect.com/science/article/pii/S1463500311001399+]

MTK
---

The Manifold Toolkit provides easy mechanisms to enable arbitrary algorithms
to operate on manifolds. The main application is the use of 3D rotations
SO(3), as well as the construction of compound manifolds from arbitrary
combinations of sub-manifolds.
We also provide a refactored version of the previously released SLoM framework
which implements Gauss-Newton and Levenberg-Marquardt-based sparse
least-squares optimization on manifolds and a port of MTK to Matlab (MTKM).

http://openslam.org/MTK[+http://openslam.org/MTK+]

MTK2
----

The Mimetic Methods Toolkit is a general purpose API for computer simulation
of physical phenomena based on Mimetic Discretization Methods.
It allows the user to develop numerical models that satisfy physical
conservation laws, while preserving even order of accuracy, up to the boundary
of the considered domain. 
A Python wrapper is available.

http://dl.dropboxusercontent.com/u/5432016/mtk/mtk_website/index.html[+http://dl.dropboxusercontent.com/u/5432016/mtk/mtk_website/index.html+]



MTSPEC
------

A Fortran 90 Library containing different subroutines to estimate the Power
Spectral Density of real time series.

http://wwwprof.uniandes.edu.co/\~gprieto/software/mwlib.html[+http://wwwprof.uniandes.edu.co/~gprieto/software/mwlib.html+]

pymtspec
~~~~~~~~

Python bindings for MTSPEC.

https://svn.geophysik.uni-muenchen.de/trac/mtspecpy/wiki[+https://svn.geophysik.uni-muenchen.de/trac/mtspecpy/wiki+]

https://pypi.python.org/pypi/mtspec[+https://pypi.python.org/pypi/mtspec+]

mxDateTime
----------

A Python extension package that provides three new objects, DateTime,
DateTimeDelta and RelativeDateTime, which let you store and handle date/time
values in a much more natural way than by using ticks (seconds since 1.1.1970
0:00 UTC), the representation used by Python's time module.
You can add, subtract and even multiply instances, pickle and copy them and
convert the results to strings, COM dates, ticks and some other more esoteric
values. In addition, there are several convenient constructors and formatters
at hand to greatly simplify dealing with dates and times in real-world
applications.
In addition to providing an easy-to-use Python interface the package also
exports a comfortable C API interface for other Python extensions to build
upon. 

http://www.egenix.com/products/python/mxBase/mxDateTime/[+http://www.egenix.com/products/python/mxBase/mxDateTime/+]

mxODBC
------

Provides an easy to use, high-performance, reliable and robust Python
interface to ODBC compatible databases such as MS SQL Server and MS Access,
Oracle Database, IBM DB2 and Informix , Sybase ASE and Sybase Anywhere, MySQL,
PostgreSQL, SAP MaxDB and many more.
ODBC refers to Open Database Connectivity and is the industry standard API for
connecting applications to databases. In order to facilitate setting up ODBC
connectivity, operating systems typically provide ODBC Managers which help set
up the ODBC drivers and manage the binding of the applications against these
drivers. On Windows and Mac OS X the ODBC Manager is built into the system. On
Unix platforms, you can choose one of the ODBC managers unixODBC, iODBC or
DataDirect, which provide the same ODBC functionality on most Unix systems.

http://www.egenix.com/products/python/mxODBC/[+http://www.egenix.com/products/python/mxODBC/+]

mystic
------

The mystic framework provides a collection of optimization algorithms and
tools that allows the user to more robustly (and readily) solve optimization
problems. All optimization algorithms included in mystic provide workflow at
the fitting layer, not just access to the algorithms as function calls. Mystic
gives the user fine-grained power to both monitor and steer optimizations as
the fit processes are running.

Where possible, mystic optimizers share a common interface, and thus can be
easily swapped without the user having to write any new code. Mystic solvers
all conform to a solver API, thus also have common method calls to configure
and launch an optimization job. For more details, see mystic.abstract_solver.
The API also makes it easy to bind a favorite 3rd party solver into the mystic
framework.

By providing a robust interface designed to allow the user to easily configure
and control solvers, mystic reduces the barrier to implementing a target
fitting problem as stable code. Thus the user can focus on building their
physical models, and not spend time hacking together an interface to
optimization code.

https://pypi.python.org/pypi/mystic[+https://pypi.python.org/pypi/mystic+]

http://arxiv.org/abs/1202.1056[+http://arxiv.org/abs/1202.1056+]

NCSAVis
-------

ocean isosurfaces
~~~~~~~~~~~~~~~~~

A tool for extracting isosurfaces from oceanographic simulation output, such
as from ROMS or HOPS. It also has the ability to compute depth-adjusted means
and standard deviations, so that statistical isosurfaces (such as temperature
relative to the depth-adjusted mean) may be generated. 

http://virdir.ncsa.illinois.edu/NCSAvis/tools/isosurfaces/[+http://virdir.ncsa.illinois.edu/NCSAvis/tools/isosurfaces/+]

ocean trajectories
~~~~~~~~~~~~~~~~~~

http://virdir.ncsa.illinois.edu/NCSAvis/tools/trajectories/[+http://virdir.ncsa.illinois.edu/NCSAvis/tools/trajectories/+]

Ncvtk
-----

A program for exploring longitude/latitude based data stored in NetCDF file
format. Ncvtk is built on top of the VTK toolbox. 
Ncvtk has been designed with the aim of offering a high degree of
interactivity to scientists who have a need to explore three-dimensional,
time-dependent planetary data. The input data should be stored in a NetCDF
file and the metadata should loosely follow the CDC convention. In particular,
we support codes that are part of the Flexible Modeling System infrastructure
provided the data lie on a longitude/latitude, structured grid. 

http://ncvtk.sourceforge.net/[+http://ncvtk.sourceforge.net/+]

ncWMS
-----

A WMS for geospatial stored in  CF-compliant  NetCDF files.
ncWMS relies heavily on the  Java NetCDF interface from Unidata. This library
does a lot of the work of metadata and data extraction. In particular the
GridDatatype class is frequently used to provide a high-level interface to
gridded geospatial NetCDF files. The library will also read from NetCDF files
on HTTP servers and from  OPeNDAP servers.
ncWMS has now been integrated with the xref:THREDDS[THREDDS] Data Server.

http://www.resc.rdg.ac.uk/trac/ncWMS/[+http://www.resc.rdg.ac.uk/trac/ncWMS/+]

NDSPMHD
-------

Full implementations of 1D, 2D and 3D hydrodynamics and magnetohydrodynamics.

http://users.monash.edu.au/\~dprice/ndspmhd/[+http://users.monash.edu.au/~dprice/ndspmhd/+]

http://arxiv.org/abs/1012.1885[+http://arxiv.org/abs/1012.1885+]

http://users.monash.edu.au/\~dprice/phantom/[+http://users.monash.edu.au/~dprice/phantom/+]

NearCoM
-------

A comprehensive community model that predicts waves, currents, sediment
transport and bathymetric change in the nearshore ocean, between the shoreline
and about 10 m water depth. The model consists of a "backbone", i.e., the
master program, handling data input and output as well as internal storage,
together with a suite of "modules", each of which handles a focused subset of
the physical processes being studied. A wave module will model wave
transformation over arbitrary coastal bathymetry and predict radiation
stresses and wave induced mass fluxes. A circulation module will model the
slowly varying current field driven by waves, wind and buoyancy forcing, and
will provide information about the bottom boundary layer structure. A seabed
module will model sediment transport, determine the bedform geometry,
parameterize the bedform effect on bottom friction, and compute morphological
evolution resulting from spatial variations in local sediment transport rates. 

http://chinacat.coastal.udel.edu/programs/nearcom/index.html[+http://chinacat.coastal.udel.edu/programs/nearcom/index.html+]

Nek5000
-------

A computational fluid dynamics solver based on the spectral element method.

http://nek5000.mcs.anl.gov/index.php/Main_Page[+http://nek5000.mcs.anl.gov/index.php/Main_Page+]

NEMO
----

A state-of-the-art modeling framework for oceanographic research, operational
oceanography seasonal forecast and climate studies.

http://www.nemo-ocean.eu/[+http://www.nemo-ocean.eu/+]

PyDom
~~~~~

A Python package which implements various diagnostics for  NEMO model output.

http://servforge.legi.grenoble-inp.fr/projects/PyDom[+http://servforge.legi.grenoble-inp.fr/projects/PyDom+]

Neo4j
-----

A robust (fully ACID) transactional property graph database. Due to its graph
data model, Neo4j is highly agile and blazing fast. For connected data
operations, Neo4j runs a thousand times faster than relational databases.

http://www.neo4j.org/[+http://www.neo4j.org/+]

neo4j-python
~~~~~~~~~~~~

http://docs.neo4j.org/drivers/python-embedded/snapshot/[+http://docs.neo4j.org/drivers/python-embedded/snapshot/+]

NetCDF
------

EXODUS
~~~~~~

NetCDF extension for finite element grids.

http://sourceforge.net/projects/exodusii/?source=navbar[+http://sourceforge.net/projects/exodusii/?source=navbar+]

Puppy
~~~~~

A DSL for creating NetCDF files.

https://pypi.python.org/pypi/Puppy/0.1.6[+https://pypi.python.org/pypi/Puppy/0.1.6+]

https://bitbucket.org/robertodealmeida/puppy/[+https://bitbucket.org/robertodealmeida/puppy/+]

NeuMATSA
--------

Nonlinear multivariate and time series analysis by neural network methods.

http://www.ocgy.ubc.ca/projects/clim.pred/download.html[+http://www.ocgy.ubc.ca/projects/clim.pred/download.html+]

http://www.ocgy.ubc.ca/projects/clim.pred/index.html[+http://www.ocgy.ubc.ca/projects/clim.pred/index.html+]

Nimbus
------

Cloud computing for science.

http://www.nimbusproject.org/[+http://www.nimbusproject.org/+]

Nitime
------

Time-series analysis for neuroscience in Python.

http://nipy.org/nitime/documentation.html[+http://nipy.org/nitime/documentation.html+]

http://fromstefanimportblog.blogspot.com/2011/04/symbolic-aggregate-approximation-in.html[+http://fromstefanimportblog.blogspot.com/2011/04/symbolic-aggregate-approximation-in.html+]

NLPCA
-----

Nonlinear principal component analysis (NLPCA) is commonly seen as a nonlinear
generalization of standard principal component analysis (PCA). It generalizes
the principal components from straight lines to curves (nonlinear). Thus, the
subspace in the original data space which is described by all nonlinear
components is also curved.
Nonlinear PCA can be achieved by using a neural network with an
autoassociative architecture also known as autoencoder, replicator network,
bottleneck or sandglass type network. Such autoassociative neural network is a
multi-layer perceptron that performs an identity mapping, meaning that the
output of the network is required to be identical to the input. However, in
the middle of the network is a layer that works as a bottleneck in which a
reduction of the dimension of the data is enforced. This bottleneck-layer
provides the desired component values (scores). 

http://www.nlpca.org/[+http://www.nlpca.org/+]

NLSA
----

Nonlinear Laplacian spectrum analysis.

http://www.pnas.org/content/suppl/2012/01/12/1118984109.DCSupplemental/Appendix.pdf[+http://www.pnas.org/content/suppl/2012/01/12/1118984109.DCSupplemental/Appendix.pdf+]

http://onlinelibrary.wiley.com/doi/10.1002/sam.11171/abstract[+http://onlinelibrary.wiley.com/doi/10.1002/sam.11171/abstract+]

http://cims.nyu.edu/\~dimitris/[+http://cims.nyu.edu/~dimitris/+]

NOVAS
-----

NOVAS is an integrated package of subroutines and functions for computing
various commonly needed quantities in positional astronomy. The package can
provide, in one or two subroutine or function calls, the instantaneous
coordinates of any star or planet in a variety of coordinate systems. At a
lower level, NOVAS also supplies astrometric utility transformations, such as
those for precession, nutation, aberration, parallax, and the gravitational
deflection of light. The computations are accurate to better than one
milliarcsecond. The NOVAS package is an easy-to-use facility that can be
incorporated into data reduction programs, telescope control systems, and
simulations. The U.S. parts of The Astronomical Almanac are prepared using
NOVAS. Three editions of NOVAS are available: Fortran, C, and Python.

The algorithms used by NOVAS 3.1 are based on a vector and matrix formulation
that is rigorous and does not use spherical trigonometry at any point. Objects
inside and outside the solar system are treated similarly. The position
vectors formed and operated on by NOVAS place each object at its relevant
distance (in AU) from the solar system barycenter.

Released in late 2009, NOVAS 3.0 provided greater accuracy of star and planet
position calculations (apparent places) by including several small effects not
implemented in the NOVAS 2.0 code of 1998. NOVAS 3.0 also fully implemented
recent resolutions by the International Astronomical Union (IAU) on positional
astronomy, including new reference system definitions and updated models for
precession and nutation. The paper by Kaplan et al. (1989, Astron. J. 97,
1197) describes the overall computational strategy used by NOVAS, although
many of the individual algorithms described there have been improved. USNO
Circular 179 describes the IAU recommendations that underpin much of NOVAS 3.0
and is the basic reference for NOVAS algorithms relating to time, Earth
orientation, and the transformations between various astronomical reference
systems. The current version, NOVAS 3.1, provides some new capabilities and
fixes some bugs.

http://aa.usno.navy.mil/software/novas/novas_info.php[+http://aa.usno.navy.mil/software/novas/novas_info.php+]

openFrameworks
--------------

An open source Cxx toolkit designed to assist the creative process by
providing a simple and intuitive framework for experimentation. The toolkit is
designed to work as a general purpose glue, and wraps together several
commonly used libraries.

http://www.openframeworks.cc/about/[+http://www.openframeworks.cc/about/+]

OpenGM
------

A C\xx template library for discrete factor graph models and distributive
operations on these models. It includes state-of-the-art optimization and
inference algorithms beyond message passing. OpenGM handles large models
efficiently, since (i) functions that occur repeatedly need to be stored only
once and (ii) when functions require different parametric or non-parametric
encodings, multiple encodings can be used alongside each other, in the same
model, using included and custom Cxx code. No restrictions are imposed on the
factor graph or the operations of the model. OpenGM is modular and extendible.
Elementary data types can be chosen to maximize efficiency. The graphical
model data structure, inference algorithms and different encodings of
functions inter-operate through well-defined interfaces. The binary OpenGM
file format is based on the HDF5 standard and incorporates user extensions
automatically. 

http://hci.iwr.uni-heidelberg.de/opengm2/[+http://hci.iwr.uni-heidelberg.de/opengm2/+]

https://github.com/opengm/opengm[+https://github.com/opengm/opengm+]

http://arxiv.org/abs/1206.0111[+http://arxiv.org/abs/1206.0111+]

OpenLayers
----------

OpenLayers makes it easy to put a dynamic map in any web page. It can display
map tiles and markers loaded from any source. OpenLayers has been developed to
further the use of geographic information of all kinds.
OpenLayers is a pure JavaScript library for displaying map data in most modern
web browsers, with no server-side dependencies. OpenLayers implements a
JavaScript API for building rich web-based geographic applications.

http://openlayers.org/[+http://openlayers.org/+]

Open-MX
-------

A high-performance implementation of the Myrinet Express message-passing
stack over generic Ethernet networks.
It provides application-level with wire-protocol compatibility with the
native MXoE
(Myrinet Express over Ethernet) stack.
The following middleware are known to work flawlessly on Open-MX using their
native MX backend thanks to the ABI and API compatibility: Open MPI, Argonne's
MPICH2/Nemesis, Myricom's MPICH-MX and MPICH2-MX, PVFS2, Intel MPI (using the
new TMI interface),
xref:Platform_MPI[Platform MPI] (formerly known as HP-MPI), 
xref:NewMadeleine[NewMadeleine], and
xref:NetPIPE[NetPIPE].

http://open-mx.gforge.inria.fr/[+http://open-mx.gforge.inria.fr/+]  

Open Navigation Surface
-----------------------

A design for a databased alternative to traditional methods of representing
bathymetric data. It aims to preserve the highest level of detail in every
bathymetric dataset and provide methods for their combination and manipulation
to generate multiple products for both hydrographic and non-hydrographic
purposes. The advantages of the method over traditional schemes are such that
a number of commercial vendors have adopted the technology. However, this
means that there is a strong requirement for a method to communicate results
in a vendor neutral technology. The Open Navigation Surface (ONS) project was
designed to fill this gap by implementing a freely available source-code
library to read and write all of the information required for a Navigation
Surface. 

http://www.opennavsurf.org/[+http://www.opennavsurf.org/+]

https://marinemetadata.org/references/bag[+https://marinemetadata.org/references/bag+]


[[OpenOpt]]
OpenOpt
-------

Python package for universal numerical optimization.

http://openopt.org/Welcome[+http://openopt.org/Welcome+]

OpenPalm
--------

Software allowing the concurrent execution and the intercommunication of
programs based on in-house as well as commercial codes. 

http://www.cerfacs.fr/globc/PALM_WEB/[+http://www.cerfacs.fr/globc/PALM_WEB/+]

osgEarth
~~~~~~~~

A Cxx terrain rendering SDK. Just create a simple XML file, point it at your
imagery, elevation, and vector data, load it into your favorite OpenSceneGraph
application, and go! osgEarth supports all kinds of data and comes with lots
of examples to help you get up and running quickly and easily

http://osgearth.org/[+http://osgearth.org/+]

python-novaclient
~~~~~~~~~~~~~~~~~

A Python client for the OpenStack Nova API.

http://docs.openstack.org/developer/python-novaclient/[+http://docs.openstack.org/developer/python-novaclient/+]

Swift
~~~~~

A highly available, distributed, eventually consistent object/blob store.
Organizations can use Swift to store lots of data efficiently, safely, and
cheaply.

http://docs.openstack.org/developer/swift/#[+http://docs.openstack.org/developer/swift/#+]

OpenStudio
----------

A cross-platform (Windows, Mac, and Linux) collection of software tools to
support whole building energy modeling using EnergyPlus and advanced daylight
analysis using Radiance.  OpenStudio is an open source project to facilitate
community development, extension, and private sector adoption. OpenStudio
includes graphical interfaces along with a Software Development Kit (SDK).  

http://openstudio.nrel.gov/[+http://openstudio.nrel.gov/+]

OpenTURNS
---------

A scientific library
usable as a Python module dedicated
to the treatment of uncertainties.

http://openturns.org/[+http://openturns.org/+]

OpenUH
------

An open source, optimizing compiler suite for C, Cxx and Fortran 95. It
supports a variety of architectures including IA-32, X86_64, IA-64. To achieve
portability, OpenUH is able to emit optimized C or Fortran 77 code that may be
compiled by a native compiler on other platforms. The supporting runtime
libraries are also portable - the OpenMP runtime library is based on the
portable Pthreads interface while the Coarray Fortran runtime library is
based, optionally, on the portable GASNet or ARMCI communications interfaces.

http://web.cs.uh.edu/\~openuh/[+http://web.cs.uh.edu/~openuh/+]

OpenWalnut
----------

Multi-modal medical and brain data visualization.

http://www.openwalnut.org/[+http://www.openwalnut.org/+]

Opticks
-------

An expandable remote sensing and imagery analysis software platform.

http://opticks.org/confluence/display/opticks/Welcome+To+Opticks[+http://opticks.org/confluence/display/opticks/Welcome+To+Opticks+]

pv-meshless
~~~~~~~~~~~

ParaView plugin containing a number of useful classes that can be used in the
processing of meshless data.

https://hpcforge.org/projects/pv-meshless[+https://hpcforge.org/projects/pv-meshless+]

https://hpcforge.org/plugins/mediawiki/wiki/pv-meshless/index.php/Main_Page_for_pv-meshless_WIKI[+https://hpcforge.org/plugins/mediawiki/wiki/pv-meshless/index.php/Main_Page_for_pv-meshless_WIKI+]

VisTrails-ParaView
~~~~~~~~~~~~~~~~~~

Incorporates the provenance management capabilities of VisTrails into
ParaView. All of the actions a user performs while building and modifying a
pipeline in ParaView are captured by the plugin. This allows navigation of all
of the pipeline versions that have previously been explored.

http://www.vistrails.org/index.php/ParaView_Plugin[+http://www.vistrails.org/index.php/ParaView_Plugin+]

Partiview
---------

An industrial strength, interactive, mono- or stereoscopic viewer for
4-dimensional datasets. It is written in Cxx/OpenGL.

http://virdir.ncsa.illinois.edu/partiview/[+http://virdir.ncsa.illinois.edu/partiview/+]

Parvis
------

Parallel analysis tools and new visualization techniques for ultra-large
climate data sets.

http://trac.mcs.anl.gov/projects/parvis[+http://trac.mcs.anl.gov/projects/parvis+]

ParGAL
~~~~~~

The parallel gridded analysis library.

http://trac.mcs.anl.gov/projects/parvis/wiki/ParCALDeveloperInfo[+http://trac.mcs.anl.gov/projects/parvis/wiki/ParCALDeveloperInfo+]

ParNCL
~~~~~~

A parallel version of NCL that runs NCL scripts in parallel and performs data
analysis using ParGAL.

http://trac.mcs.anl.gov/projects/parvis/wiki/ParNCLDeveloperInfo[+http://trac.mcs.anl.gov/projects/parvis/wiki/ParNCLDeveloperInfo+]

AMWG
~~~~

Produces over 600 plots and tables from CCSM (CAM) monthly netcdf files. 

http://www.cgd.ucar.edu/amp/amwg/diagnostics/index.html[+http://www.cgd.ucar.edu/amp/amwg/diagnostics/index.html+]

Pattern
-------

Pattern is a web mining module for the Python programming language.

It has tools for data mining (Google, Twitter and Wikipedia API, a web
crawler, a HTML DOM parser), natural language processing (part-of-speech
taggers, n-gram search, sentiment analysis, WordNet), machine learning (vector
space model, clustering, SVM), network analysis and <canvas> visualization.

http://www.clips.ua.ac.be/pages/pattern[+http://www.clips.ua.ac.be/pages/pattern+]

PDT
---

Program Database Toolkit (PDT) is a framework for analyzing source code
written in several programming languages and for making rich program knowledge
accessible to developers of static and dynamic analysis tools. PDT implements
a standard program representation, the program database (PDB), that can be
accessed in a uniform way through a class library supporting common PDB
operations.

http://www.cs.uoregon.edu/research/pdt/home.php[+http://www.cs.uoregon.edu/research/pdt/home.php+]


PEANO
-----

An open source Cxx solver framework. It is based upon the fact that
spacetrees, a generalisation of the classical octree concept, yield a cascade
of adaptive Cartesian grids. Consequently, any spacetree traversal is
equivalent to an element-wise traversal of the hierarchy of the adaptive
Cartesian grids. The software Peano realises such a grid traversal and storage
algorithm, and it provides hook-in points for applications performing
per-element, per-vertex, and so forth operations on the grid. It also provides
interfaces for dynamic load balancing, sophisticated geometry representations,
and other features.

http://www.peano-framework.org/home.shtml[+http://www.peano-framework.org/home.shtml+]

PetIGA
------

This software framework implements a NURBS-based Galerkin finite element
method (FEM), popularly known as isogeometric analysis (IGA). It is heavily
based on PETSc, the Portable, Extensible Toolkit for Scientific Computation.
PETSc is a collection of algorithms and data structures for the solution of
scientific problems, particularly those modeled by partial differential
equations (PDEs). PETSc is written to be applicable to a range of problem
sizes, including large-scale simulations where high performance parallel is a
must. PetIGA can be thought of as an extension of PETSc, which adds the NURBS
discretization capability and the integration of forms. The PetIGA framework
is intended for researchers in the numeric solution of PDEs who have
applications which require extensive computational resources.

https://bitbucket.org/dalcinl/petiga/[+https://bitbucket.org/dalcinl/petiga/+]

http://arxiv.org/abs/1305.4452[+http://arxiv.org/abs/1305.4452+]

PETSc
-----

http://www.mcs.anl.gov/petsc/[+http://www.mcs.anl.gov/petsc/+]

http://acts.nersc.gov/events/Workshop2012/slides/PETSc.pdf[+http://acts.nersc.gov/events/Workshop2012/slides/PETSc.pdf+]

petsc-3.3-omp
~~~~~~~~~~~~~

Branch of PETSc with OpenMP support.

https://bitbucket.org/ggorman/petsc-3.3-omp[+https://bitbucket.org/ggorman/petsc-3.3-omp+]

petsc4py
~~~~~~~~

http://code.google.com/p/petsc4py/[+http://code.google.com/p/petsc4py/+]

PHAT
----

The Persistent Homology Algorithm Toolbox contains methods for computing the
persistence pairs of a filtered cell complex represented by an ordered
boundary matrix with Z2 coefficients.

https://code.google.com/p/phat/[+https://code.google.com/p/phat/+]

PIL
---

The Python Imaging Library (PIL) adds image processing capabilities to your
Python interpreter. This library supports many file formats, and provides
powerful image processing and graphics capabilities.

http://www.pythonware.com/products/pil/[+http://www.pythonware.com/products/pil/+]

PSpike
------

A high-performance, robust, memory efficient, and scalable software for
solving large sparse symmetric and unsymmetric linear systems of equations on
shared-memory and distributed-memory architectures using thousands of compute
cores. PSPIKE combines the robustness of a direct linear solver and the
performance scalability of an iterative linear solver.

Features of the library version: Unsymmetric, or symmetric systems, real,
parallel on distributed-memory clusters, combinatorial graph algorithms 

http://www.pspike-project.org/[+http://www.pspike-project.org/+]

PSSA
----

Matlab code for Posterior Singular Spectrum Analysis.

http://cc.oulu.fi/\~llh/group/software.html[+http://cc.oulu.fi/~llh/group/software.html+]

http://cc.oulu.fi/\~llh/preprints/PSSA.pdf[+http://cc.oulu.fi/~llh/preprints/PSSA.pdf+]

psurface
--------

A Cxx library that handles piecewise linear bijections between triangulated
surfaces. These surfaces can be of arbitrary shape and need not even be
manifolds. 

http://numerik.mi.fu-berlin.de/dune/psurface/index.php[+http://numerik.mi.fu-berlin.de/dune/psurface/index.php+]

Psycopg
-------

A  PostgreSQL adapter for the Python programming language. At its core it
fully implements the Python DB API 2.0 specifications. Several extensions
allow access to many of the features offered by PostgreSQL.

http://initd.org/psycopg/[+http://initd.org/psycopg/+]

PVFS
----

Brings state-of-the-art parallel I/O concepts to production parallel systems.

http://www.pvfs.org/[+http://www.pvfs.org/+]

PWC
---

Partial wavelet coherence is a technique similar to partial correlation that
helps identify the resulting wavelet coherence between two time series after
eliminating the influence of their common dependence. Multiple wavelet
coherence, akin to multiple correlation, is useful in seeking the resulting
wavelet coherence of multiple independent variables on a dependent one.

http://www.cityu.edu.hk/gcacic/wavelet/index.htm[+http://www.cityu.edu.hk/gcacic/wavelet/index.htm+]

wtc
~~~

A Matlab package for performing crosswavelet and wavelet coherence analysis.

http://noc.ac.uk/using-science/crosswavelet-wavelet-coherence[+http://noc.ac.uk/using-science/crosswavelet-wavelet-coherence+]

PyACTS
------

A set of Python based modules that provide a high level user interface to
functionality available in the ACTS Collection: PyBLACS, PyPBLAS and
PyScALAPACK.

http://wiki.python.org/moin/PyACTS[+http://wiki.python.org/moin/PyACTS+]

http://pyacts.sourceforge.net/[+http://pyacts.sourceforge.net/+]

http://acts.nersc.gov/[+http://acts.nersc.gov/+]

PyAMG
-----

Algebraic multigrid solvers in Python.

http://pyamg.org/[+http://pyamg.org/+]

PyClaw
------

A hyperbolic PDE solver in 1D, 2D, and 3D, including mapped grids and
surfaces, built on Clawpack.

http://numerics.kaust.edu.sa/pyclaw/[+http://numerics.kaust.edu.sa/pyclaw/+]

GeoClaw
~~~~~~~

A specialized version of some Clawpack and AMRClaw routines that have been
modified to work well for certain geophysical flow problems.

Currently the focus is on 2d depth-averaged shallow water equations for flow
over varying topography. The term bathymetry is often used for underwater
topography (sea floor or lake bottom), but in this documentation and in the
code the term topography is often used to refer to either.

http://depts.washington.edu/clawpack/users/geoclaw.html[+http://depts.washington.edu/clawpack/users/geoclaw.html+]

http://depts.washington.edu/clawpack/g2s3/doc/index.html[+http://depts.washington.edu/clawpack/g2s3/doc/index.html+]

PyClimate
---------

A Python package designed to accomplish some usual tasks needed during the
analysis of climate variability. It provides functions to handle simple I/O
operations, handling of COARDS-compliante netCDF files, EOF analysis, SVD and
CCA analysis of coupled data sets, some linear digital filters, kernel based
probability density function estimation and access to DCDFLIB.C library from
Python.

http://www.pyclimate.org/[+http://www.pyclimate.org/+]

pyCMBS
------

A  suite of tools to process, analyze, visualize and benchmark scientific
model output against each other or against observational data. It is in
particular useful for analyzing in an efficient way output from climate model
simulations.

https://github.com/pygeo/pycmbs[+https://github.com/pygeo/pycmbs+]

https://pythonhosted.org/pycmbs/[+https://pythonhosted.org/pycmbs/+]

https://code.zmaw.de/projects/pycmbs[+https://code.zmaw.de/projects/pycmbs+]

https://code.zmaw.de/projects/pycmbs/wiki[+https://code.zmaw.de/projects/pycmbs/wiki+]

[[PyCraft]]
PyCraft
-------

A unifying multibody dynamics algorithm development workbench.

http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/37649/1/05-2919.pdf[+http://trs-new.jpl.nasa.gov/dspace/bitstream/2014/37649/1/05-2919.pdf+]

http://dshell.jpl.nasa.gov/SOA/index.php[+http://dshell.jpl.nasa.gov/SOA/index.php+]

http://dshell.jpl.nasa.gov/References/index.php[+http://dshell.jpl.nasa.gov/References/index.php+]

https://simtk.org/home/simbody[+https://simtk.org/home/simbody+]

PyCSP
-----

A project to bring CSP (Communicating Sequential Processes) to Python. 

http://code.google.com/p/pycsp/[+http://code.google.com/p/pycsp/+]

PyCUDA
------

A Python package for accessing Nvidia‘s CUDA parallel computation API.

http://mathema.tician.de/software/pycuda[+http://mathema.tician.de/software/pycuda+]

PyCULA
~~~~~~

Provides  PyCUDA bindings for the CULA port of xref:LAPACK[LAPACK] to NVIDIA's CUDA GPGPU
programming environment. Mixing PyCUDA-style kernel code and CULA device
function calls is supported. 

https://bitbucket.org/louistheran/pycula/overview[+https://bitbucket.org/louistheran/pycula/overview+]

[[pydio]]
pydio
-----

Pydio, formerly known as AjaXplorer, is an open source application that can be
used to setup fileserver on any system. It is an alternative to SaaS Boxes and
Drives, with more control, safety and privacy, and favorable TCOs. It provides
the easy access to files/folders from any system, from any browser over LAN or
WAN.  It has an embedded WebDAV server and native mobile applications for iOS
and Android. It is easy to install, configure and it is designed to provide
enterprise grade security and control. Moreover, Pydio users and groups can be
mapped directly from your external LDAP/AD server, or from the most common
PHP-based CMS.

https://pyd.io/download/[+https://pyd.io/download/+]

https://pydio.com/[+https://pydio.com/+]

http://www.unixmen.com/setup-fileserver-using-pydio-centos-6-56-4/[+http://www.unixmen.com/setup-fileserver-using-pydio-centos-6-56-4/+]

[[PyDSTool]]
PyDSTool
--------

A sophisticated & integrated simulation and analysis environment for dynamical
systems models of physical systems (ODEs, DAEs, maps, and hybrid systems).
PyDSTool is platform independent, written primarily in Python with some
underlying C and Fortran legacy code for fast solving. It makes extensive use
of the numpy and scipy libraries. PyDSTool supports symbolic math,
optimization, phase plane analysis, continuation and bifurcation analysis,
data analysis, and other tools for modeling -- particularly for biological
applications.

http://wiki.python.org/moin/PyDSTool[+http://wiki.python.org/moin/PyDSTool+]

http://www.ni.gsu.edu/\~rclewley/PyDSTool/FrontPage.html[+http://www.ni.gsu.edu/~rclewley/PyDSTool/FrontPage.html+]

[[pyGDP]]
pyGDP
-----

A Python toolset providing access to GDP functionality.

https://my.usgs.gov/confluence/pages/viewpage.action?pageId=250937417[+https://my.usgs.gov/confluence/pages/viewpage.action?pageId=250937417+]

https://github.com/USGS-CIDA/pyGDP[+https://github.com/USGS-CIDA/pyGDP+]

pygeode
-------

A software library intended to simplify the management, analysis, and
visualization of gridded geophysical datasets such as those generated by
climate models. The library provides three main advantages. Firstly, it can
define a geophysical coordinate system for any given dataset, and allows
operations to be carried conceptually in this physical coordinate system, in a
way that is independent of the native coordinate system of a particular
dataset. This greatly simplifies working with datasets from different sources.
Secondly, the library allows mathematical operations to be performed on
datasets which fit on disk but not in memory; this is useful for dealing with
the extremely large datasets generated by climate models, and permits
operations to be performed over networks. Finally, the library provides tools
for visualizing these datasets in a scientifically useful way. The library is
written in Python, and makes use of a number of existing packages to perform
the underlying computations and to create plots. 

http://code.google.com/p/pygeode/[+http://code.google.com/p/pygeode/+]

pyGlobus
--------

The goal of this project is to allow the use of the entire Globus toolkit from
Python, a high-level scripting language. SWIG is used to generate the
necessary interface code. 

http://acs.lbl.gov/projects/gtg/projects/pyGlobus/[+http://acs.lbl.gov/projects/gtg/projects/pyGlobus/+]

pygmyplot
---------

A plotting library for Tkinter Python programmers.
Pygmyplot is based on the popular and powerful matplotlib, but does not
require the python programmer to know nitty-gritty details of matplotlib
programming. However, pygmyplot provides access to all of matplotlib's
functionality just below the surface. Pygmyplot is designed to work more
seamlessly with the Tkinter event loop than matplotlib's own simplified
wrapper, pyplot.

http://www.jamesstroud.com/software/pygmyplot/[+http://www.jamesstroud.com/software/pygmyplot/+]

PyGRADS
-------

A Python interface to GrADS that provides an alternative method of scripting GrADS that
can take advantage of the unique capabilities of Python, and gives you access
to a wealth of numerical and scientific software available for this platform.

http://opengrads.org/wiki/index.php?title=Python_Interface_to_GrADS[+http://opengrads.org/wiki/index.php?title=Python_Interface_to_GrADS+]


[[PyGTS]]
PyGTS
-----

A python package used to construct, manipulate, and perform computations on 3D
triangulated surfaces. It is a hand-crafted and pythonic binding for the GNU
Triangulated Surface (GTS) Library. 

http://pygts.sourceforge.net/[+http://pygts.sourceforge.net/+]

pyjs
----

A Rich Internet Application (RIA) Development Platform for both Web and
Desktop. With pyjs you can write your JavaScript-powered web applications
entirely in Python.
pyjs contains a Python-to-JavaScript compiler, an AJAX framework and a Widget
Set API.

http://pyjs.org/[+http://pyjs.org/+]

pykalman
--------

A Kalman filter, smoother and EM library for Python.

http://pykalman.github.io/[+http://pykalman.github.io/+]

http://greg.czerniak.info/guides/kalman1/[+http://greg.czerniak.info/guides/kalman1/+]

[[Pyke]]
Pyke
----

A knowledge-based inference engine.

http://pyke.sourceforge.net/[+http://pyke.sourceforge.net/+]

PyMathProg
----------

A Python reincarnation of AMPL and GNU MathProg modeling language, implemented
in pure Python, connecting to GLPK via PyGLPK. Create, optimize, report,
change and re-optimize your model with Python, which offers numerous handy
goodies. Being embedded in Python, you can take advantage of the other good
things available in python: such as easy database access, graphical
presentation of your solution, statistical analysis, or use pymprog for
artificial intelligence in games, etc.

http://pymprog.sourceforge.net/[+http://pymprog.sourceforge.net/+]

PyMCT
-----

A suite of software packages necessary to build and run a Python Coupler like
PyCCSM. MCT is a high performance regridding and parallel communication
package designed to address issues of coupling multiple scientific models on
different scales and grids to one another. 

http://code.google.com/p/pyccsm/[+http://code.google.com/p/pyccsm/+]

Pymutt
------

An implementation of Thomson's (1982) multi-taper fourier spectral estimator
plus a python interface. The core code is due to Lees and Park (1995) and uses
the conventions of Percival and Walden (1993). 

http://code.google.com/p/pymutt/[+http://code.google.com/p/pymutt/+]

PyOpenGL
--------

The most common cross platform Python binding to OpenGL and related APIs.

http://pyopengl.sourceforge.net/[+http://pyopengl.sourceforge.net/+]

OpenGLContext
~~~~~~~~~~~~~

A  testing and learning environment for PyOpenGL. 

http://pyopengl.sourceforge.net/context/[+http://pyopengl.sourceforge.net/context/+]

[[PyPDF]]
PyPDF
-----

A Digital Picture Frame Application to be used as MPD client and/or standard
DPF written in Python.

https://github.com/dzubi/pydpf[+https://github.com/dzubi/pydpf+]

py-pod
------

GThe pod package is an implementation of a Proper Orthogonal Decomposition
method.

http://code.google.com/p/py-pod/[+http://code.google.com/p/py-pod/+]

PyQtGraph
---------

A pure-python graphics and GUI library built on PyQt4 / PySide and numpy. It
is intended for use in mathematics / scientific / engineering applications.
Despite being written entirely in python, the library is very fast due to its
heavy leverage of numpy for number crunching and Qt's GraphicsView framework
for fast display. 

http://www.pyqtgraph.org/[+http://www.pyqtgraph.org/+]

Pyrex
-----

Lets you write code that mixes Python and C data types any way you want, and
compiles it into a C extension for Python. 

http://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/[+http://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/+]

pyshp
-----

A library that reads and writes ESRI shapefiles in Python.
You can read and write shp, shx, and dbf files with all types of geometry.
Everything in the public ESRI shapefile specification is implemented.

http://code.google.com/p/pyshp/[+http://code.google.com/p/pyshp/+]

[[Pyston]]
Pyston
------

A new, open-source Python implementation using JIT techniques.

https://github.com/dropbox/pyston[+https://github.com/dropbox/pyston+]

https://tech.dropbox.com/2014/04/introducing-pyston-an-upcoming-jit-based-python-implementation/[+https://tech.dropbox.com/2014/04/introducing-pyston-an-upcoming-jit-based-python-implementation/+]

pytensor
--------

A Python implementation of the tensor toolkit.

http://code.google.com/p/pytensor/[+http://code.google.com/p/pytensor/+]

http://www.cs.cmu.edu/\~cjl/papers/CMU-CS-10-102.pdf[+http://www.cs.cmu.edu/~cjl/papers/CMU-CS-10-102.pdf+]

PyUblas
-------

Provides a seamless glue layer between Numpy and Boost.Ublas for use with
Boost.Python.

https://pypi.python.org/pypi/PyUblas[+https://pypi.python.org/pypi/PyUblas+]

http://documen.tician.de/pyublas/[+http://documen.tician.de/pyublas/+]

PyUblasExt
~~~~~~~~~~

A companion to PyUblas that exposes a variety of useful additions including
a cross-language "operator" class for building matrix-free algorithms,
CG and BiCGSTAB linear solvers that use the operator class,
an ARPACK interface that uses it, a xref:UMFPACK[UMFPACK] interface for sparse
matrices, and an interface to the DASKR ODE solver.

http://mathema.tician.de/software/pyublas/pyublasext[+http://mathema.tician.de/software/pyublas/pyublasext+]

PyXB
----

Python XML schema bindings.

http://sourceforge.net/projects/pyxb/[+http://sourceforge.net/projects/pyxb/+]

http://www.ecmwf.int/newsevents/meetings/workshops/2013/GIS-OGC_standards/Presentations/pdfs/Fucile.pdf[+http://www.ecmwf.int/newsevents/meetings/workshops/2013/GIS-OGC_standards/Presentations/pdfs/Fucile.pdf+]

pyximport
---------

Pyrex is a compiler. Therefore it is natural that people tend to go
through an edit/compile/test cycle with Pyrex modules. But my personal
opinion is that one of the deep insights in Python's implementation is
that a language can be compiled (Python modules are compiled to .pyc)
files and hide that compilation process from the end-user so that they
do not have to worry about it. Pyximport does this for Pyrex modules.

https://github.com/cython/cython/tree/master/pyximport[+https://github.com/cython/cython/tree/master/pyximport+]

https://github.com/scipy/speed[+https://github.com/scipy/speed+]

pyrings
-------

Python package to handle rings/eddies in the ocean.

https://github.com/castelao/pyrings[+https://github.com/castelao/pyrings+]

pyrwt
-----

A Python wrapper for the Rice Wavelet Toolbox.

https://pypi.python.org/pypi/pyrwt/0.2.2[+https://pypi.python.org/pypi/pyrwt/0.2.2+]

PySAL
-----

A cross-platform library of spatial analysis functions written in Python. 
The modules are:

* +pysal.cg+ — Computational geometry
* +pysal.contrib+ - Contributed modules
* +pysal.core+ — Core data structures and IO
* +pysal.esda+ — Exploratory spatial data analysis
* +pysal.examples+ — Data sets
* +pysal.inequality+ — Spatial inequality analysis
* +pysal.network+ - Spatial analysis on networks
* +pysal.region+ — Spatially constrained clustering
* +pysal.spatial_dynamics+ — Spatial dynamics
* +pysal.spreg+ — Regression and diagnostics
* +pysal.weights+ — Spatial weights

http://pythonhosted.org/PySAL/[+http://pythonhosted.org/PySAL/+]

https://github.com/pysal/pysal[+https://github.com/pysal/pysal+]

PySide
------

A Python binding to the Qt library.
The various parts that comprise PySide are:

* +apiextractor+ - Used by the binding generator to parse headers
of a given library and merge this data with information provided by
typesystem (XML) files, resulting in a representation of how the API should be
exported to the chosen target language. The generation of source code for the
bindings is performed by specific generators using the API Extractor library.

* +generatorrunner+ - A utility that parses a collection of header and
typesystem files, generating other files (code, documentation, etc.) as
result.

* +shiboken+ - A Python bindings generator that outputs CPython code.

* +pyside+ - Generates the Qt bindings.

* +pyside-tools+ - Four tools for PySide.

Why?  This is used by xref:Matplotlib[Matplotlib].

http://qt-project.org/wiki/Category:LanguageBindings::PySide[+http://qt-project.org/wiki/Category:LanguageBindings::PySide+]

Build Notes
~~~~~~~~~~~

General instructions on how to build this on Linux can be
found at:

http://qt-project.org/wiki/Building_PySide_on_Linux[+http://qt-project.org/wiki/Building_PySide_on_Linux+]

but there are some other problems I encountered.

First, Python must be built either as a shared library:

-----
./configure --enable-shared --prefix=/opt/python2.7
-----

or with the +-fPIC+ flag set:

-----
export CFLAGS=-fPIC
export CPPFLAGS=-fPIC
./configure --prefix=/opt/python2.7
-----

Second, if you want to use a version of Qt in a non-standard location
such as +/opt/qt-4.8.4+, then you need to specify the following for all
five packages:

-----
export LD_LIBRARY_PATH=/opt/qt-4.8.4/lib:$LD_LIBRARY_PATH
export PYTHONPATH=/opt/python2.7/lib/python2.7/site-packages:$PYTHONPATH
export PATH=/opt/qt-4.8.4/bin:$PATH
export PKG_CONFIG_PATH=/opt/qt-4.8.4/lib/pkgconfig:$PKGCONFIG
-----

to get all the +cmake+ Qt dependencies right.
There is not an obvious way within +cmake+ to specify a general alternate
root directory for Qt, so we must do it in this roundabout way.

Also, for +shiboken+, if you have several
versions of Python, you need to set the following variables either
in +CMakeCache.txt+ directory or indirectly via the +ccmake+ command:

-----
PYTHON_EXECUTABLE                /usr/bin/python2.7                                                    
PYTHON_INCLUDE_DIR               /opt/python2.7/include/python2.7                                      
PYTHON_LIBRARY                   /opt/python2.7/lib/libpython2.7.a
-----

Even after all that, the Python module is installed in the wrong place, i.e.

-----
/usr/local/lib/python2.7/site-packages/shiboken.so
-----

and I can't figure out how to tweak the cmake stuff to get it in the right
place.  Thus, we must do so manually, i.e.

-----
cp /usr/local/lib/python2.7/site-packages/shiboken.so /opt/python2.7/lib/python2.7/site-packages
-----

PySOS
-----

A python-based implementation of the OGC SOS standard. PySOS is a lightweight
set of scripts that work in conjunction with a web server to serve data from a
relational database.

http://sourceforge.net/projects/pysos/[+http://sourceforge.net/projects/pysos/+]

Python Computer Graphics Kit
----------------------------

A collection of Python modules, plugins and utilities that are meant to be
useful for any domain where you have to deal with 3D data of any kind, be it
for visualization, creating photorealistic images, Virtual Reality or even
games. 

http://cgkit.sourceforge.net/introduction.html[+http://cgkit.sourceforge.net/introduction.html+]

python-oceans
-------------

Miscellaneous Python tools for oceanographers.

https://github.com/ocefpaf/python-oceans[+https://github.com/ocefpaf/python-oceans+]

PyWavelets
----------

http://www.pybytes.com/pywavelets/[+http://www.pybytes.com/pywavelets/+]

QGroundControl
--------------

Open-source MAVLink Micro Air Vehicle Communication Protocol with lightweight
serialization functions for microcontrollers.
QGroundControl's main interface protocol is MAVLink, a binary, serial stream
protocol which QGroundControl can receive over UDP or serial links (radio
modems).

http://qgroundcontrol.org/start[+http://qgroundcontrol.org/start+]

QuTIP
-----

Open-source software for simulating the dynamics of open quantum systems. The
QuTiP library depends on the excellent Numpy and Scipy numerical packages. In
addition, graphical output is provided by Matplotlib. QuTiP aims to provide
user-friendly and efficient numerical simulations of a wide variety of
Hamiltonians, including those with arbitrary time-dependence, commonly found
in a wide range of physics applications such as quantum optics, trapped ions,
superconducting circuits, and quantum nanomechanical resonators. 

http://qutip.org/[+http://qutip.org/+]

http://arxiv.org/abs/1211.6518[+http://arxiv.org/abs/1211.6518+]

PyMAVLink
~~~~~~~~~

Python bindings for MAVLink.

http://qgroundcontrol.org/mavlink/pymavlink[+http://qgroundcontrol.org/mavlink/pymavlink+]

RWT
~~~

A collection of Matlab files for 1D and 2D wavelet and filter bank design,
analysis, and processing.

http://dsp.rice.edu/software/rice-wavelet-toolbox[+http://dsp.rice.edu/software/rice-wavelet-toolbox+]

QCL
---

http://tph.tuwien.ac.at/\~oemer/qcl.html[+http://tph.tuwien.ac.at/~oemer/qcl.html+]

Quine-Relay
-----------

https://github.com/mame/quine-relay[+https://github.com/mame/quine-relay+]

RR
--

A bloody useful program.

ncdf
~~~~

A NetCDF package for R.

http://cirrus.ucsd.edu/\~pierce/ncdf/[+http://cirrus.ucsd.edu/~pierce/ncdf/+]

http://www.image.ucar.edu/GSP/Software/Netcdf/[+http://www.image.ucar.edu/GSP/Software/Netcdf/+]

RKML
~~~~

A  package that provides R users with high-level facilities to generate KML,
the Keyhole Markup Language for display in, e.g., Google Earth. By high-level,
we mean that the R user does not have to (but can) create the XML directly
herself. Instead, there are high-level functions which take care of these
lower-level details. 

http://www.omegahat.org/RKML/[+http://www.omegahat.org/RKML/+]

Racket
------

A Scheme-based programming language designed for producing web applications,
system programming and much more.

http://racket-lang.org/[+http://racket-lang.org/+]

rasdaman
--------

Enables Web-based geo data offerings and Big Data Analytics on
multi-dimensional raster ("array") data of unlimited size.

http://www.rasdaman.com/[+http://www.rasdaman.com/+]

Raspberry Pi
------------

FMBerry
~~~~~~~

Build an FM transmitter with a Raspberry Pi.

https://github.com/manawyrm/FMBerry[+https://github.com/manawyrm/FMBerry+]

raspi_sensors_R
---------------

A library for reading sensors connected to a Raspberry Pi using the
R language.

https://plus.google.com/118229326887001973739/posts/gSQB3M6cph7[+https://plus.google.com/118229326887001973739/posts/gSQB3M6cph7+]

Wiring Pi
~~~~~~~~~

GPIO access library written in C for the BCM2835 used in the Raspberry Pi.
It’s released under the GNU LGPLv3 license and is usable from C and Cxx and
many other languages with suitable wrappers (See below) It’s designed to be
familiar to people who have used the Arduino “wiring” system.

http://wiringpi.com/[+http://wiringpi.com/+]

Redland
-------

A set of free software C libraries that provide support for the Resource
Description Framework (RDF).

http://librdf.org/[+http://librdf.org/+]

redland-bindings
~~~~~~~~~~~~~~~~

Redland 
is a library that provides a high-level interface for the Resource Description
Framework (RDF) allowing the RDF graph to be parsed from XML, stored, queried
and manipulated. Redland implements each of the RDF concepts in its own class
via an object based API, reflected into the language APIs, currently Perl,
PHP, Python and Ruby. Several classes providing functionality such as for
parsers, storage are built as modules that can be loaded at compile or
run-time as required.

http://librdf.org/bindings/[+http://librdf.org/bindings/+]

redsvd
------

The RandomizED Singular Value Decomposition
library solves several matrix decompositions including singular value
decomposition (SVD), principal component analysis (PCA), and eigen value
decomposition. redsvd can handle very large matrix efficiently, and optimized
for a truncated SVD of sparse matrices. For example, redsvd can compute a
truncated SVD with top 20 singular values for a 100K x 100K matrix with 1M
nonzero entries in less than one second.
The algorithm is based on the randomized algorithm for computing large-scale
SVD. Although it uses randomized matrices, the results is very accurate with
very high probability. See the experiment part for the detail. 

http://code.google.com/p/redsvd/[+http://code.google.com/p/redsvd/+]

http://code.google.com/p/redsvd/wiki/English[+http://code.google.com/p/redsvd/wiki/English+]

RegEM
-----

Matlab modules for 
the estimation of mean values and covariance matrices from incomplete
datasets, and
the imputation of missing values in incomplete datasets.

http://www.clidyn.ethz.ch/imputation/[+http://www.clidyn.ethz.ch/imputation/+]

Regina
------

Python software for 3-manifold topology and normal surface theory.

http://regina.sourceforge.net/[+http://regina.sourceforge.net/+]

http://arxiv.org/abs/1208.2504[+http://arxiv.org/abs/1208.2504+]

[[Reportlab]]
Reportlab
---------

A Python PDF library.

https://pypi.python.org/pypi/reportlab[+https://pypi.python.org/pypi/reportlab+]

ROMS
----

LiveROMS
~~~~~~~~

A virtual environment for ocean numerical simulations.

http://ronin.dgeo.udec.cl/LiveROMS/[+http://ronin.dgeo.udec.cl/LiveROMS/+]

http://www.sciencedirect.com/science/article/pii/S1364815211001411[+http://www.sciencedirect.com/science/article/pii/S1364815211001411+]

ROMS_AGRIF
~~~~~~~~~~

A three-dimensional numerical oceanic model intended for simulating currents,
ecosystems, biogeochemical cycles, and sediment movement in various coastal
regions. It is called the Regional Oceanic Modeling System (ROMS). This IRD
version of the code, ROMS_AGRIF, makes use of the AGRIF grid refinement
procedure developed at the LJK-IMAG and is accompanied by a powerful toolbox
for ROMS pre- and post-processing: ROMSTOOLS.

http://www.romsagrif.org/[+http://www.romsagrif.org/+]

ROOT
----

The ROOT system provides a set of OO frameworks with all the functionality
needed to handle and analyze large amounts of data in a very efficient way.
Having the data defined as a set of objects, specialized storage methods are
used to get direct access to the separate attributes of the selected objects,
without having to touch the bulk of the data. Included are histograming
methods in an arbitrary number of dimensions, curve fitting, function
evaluation, minimization, graphics and visualization classes to allow the easy
setup of an analysis system that can query and process the data interactively
or in batch mode, as well as a general parallel processing framework, PROOF,
that can considerably speed up an analysis.

The scripting, or macro, language and the programming language are all Cpp. The
interpreter allows for fast prototyping of the macros since it removes the,
time consuming, compile/link cycle.
If more performance is needed the interactively developed macros
can be compiled using a Cxx compiler via a machine independent transparent
compiler interface called ACliC.

http://root.cern.ch/drupal/[+http://root.cern.ch/drupal/+]

PyROOT
~~~~~~

The python programming language is a popular, open-source, dynamic language
with an interactive interpreter. Its interoperability with other programming
languages, both for extending python as well as embedding it, is excellent and
many existing third-party applications and libraries have therefore so-called
"python bindings." PyROOT provides python bindings for ROOT: it enables
cross-calls from ROOT/CINT into python and vice versa, the intermingling of
the two interpreters, and the transport of user-level objects from one
interpreter to the other. PyROOT enables access from ROOT to any application
or library that itself has python bindings, and it makes all ROOT
functionality directly available from the python interpreter. 

http://wlav.web.cern.ch/wlav/pyroot/[+http://wlav.web.cern.ch/wlav/pyroot/+]

Rose
----

Rose is a group of utilities and specifications which aim to provide a common
way to manage the development and running of scientific application suites in
both research and production environments.

http://www.metoffice.gov.uk/research/collaboration/rose[+http://www.metoffice.gov.uk/research/collaboration/rose+]

http://metomi.github.io/rose/doc/rose.html[+http://metomi.github.io/rose/doc/rose.html+]

ROSE
----

An open source compiler infrastructure to build source-to-source program
transformation and analysis tools for large-scale C(C89 and C98), Cxx(Cxx98
flexibly, and without the vast initial overhead that has to be spent when
implementing sparse grids and the corresponding algorithms. To be able to deal
with different kinds of problems in a spatially adaptive way - ranging from
interpolation and quadrature via the solution of differential equations to
regression, classification, and more - a main motivation behind the
development and all considerations was to create a toolbox which can be used
in a very flexible and modular way by different users in different
applications.

The main features of the sparse grid toolbox are efficiency and flexibility,
both of which can sometimes be nasty rivals, for example if the reusability of
an algorithm for different purposes requires extra data structures or control
mechanisms, thus slowing down special algorithmic variants. To ensure
performance at run-time, we use C$$xx$$ for all performance critical parts.
Considering flexibility, we have spent a great deal in ensuring modularity,
reusability and the separation of data structures and algorithms. Furthermore,
we provide the means to use the SG$$xx$$ toolbox from within Python, Matlab, Java,
and Cxx, of course.

http://www5.in.tum.de/SGpp/releases/index.html[+http://www5.in.tum.de/SGpp/releases/index.html+]


[[shape2ge]]
shape2ge
--------

A set of two applications to convert ESRI ShapeFiles into Google Earth KML.

http://code.google.com/p/shape2ge/

[[Shapely]]
Shapely
-------

A Python package for the manipulation and analysis of planar geometric
objects. 

See also xref:keytree[keytree], xref:Fiona[Fiona] and xref:Rtree[Rtree].

http://toblerity.org/shapely/[+http://toblerity.org/shapely/+]

SheafSystem
-----------

A suite of data representation and management tools are based on a patented
sheaf data model.   The SheafSystem™ uses advanced mathematics - posets,
lattices, sheaves, and fiber bundles - to revolutionize the handling of the
complex, structure rich data sets of scientific computing. SheafSystem™ tools
make it easy to construct, manipulate, store, retrieve, and inter-operate
diverse representations of physical data.

https://github.com/LimitPointSystems[+https://github.com/LimitPointSystems+]

http://www.limitpt.com/index.php/products[+http://www.limitpt.com/index.php/products+]

ShearLab
--------

http://www.shearlab.org/[+http://www.shearlab.org/+]

FrameLab
~~~~~~~~

http://www.framelab.org./[+http://www.framelab.org./+]

SHOGUN
------

The machine learning toolbox's focus is on large scale kernel methods and
especially on Support Vector Machines (SVM).   It provides a generic SVM
object interfacing to several different SVM implementations.  The toolbox not
only provides efficient implementations of the most common kernels, like the
Linear, Polynomial, Gaussian and Sigmoid Kernel but also comes with a number
of recent string kernels.  SHOGUN is implemented in Cxx and interfaces to
Matlab(tm), R, Octave and Python.

http://www.shogun-toolbox.org/[+http://www.shogun-toolbox.org/+]

SHT - X
-------

A set of codes for spherical harmonic transforms.

* +SSHT+ - spin spherical harmonic transforms

* +S2+ - functions on the sphere

* +S2LET+ - fast wavelets on the sphere

* +S2DW+ - steerable scale discretised wavelets on the sphere

* +FastCSWT+ - fast directional continuous spherical wavelt transform

* +FLAG+ - exact Fourier-Laguerre transform on the ball

* +FLAGLET+ - exact wavelets on the ball 

http://www.jasonmcewen.org/[+http://www.jasonmcewen.org/+]

http://www.mrao.cam.ac.uk/\~jdm57/software.html[+http://www.mrao.cam.ac.uk/~jdm57/software.html+]

http://arxiv.org/abs/1308.5706[+http://arxiv.org/abs/1308.5706+]

http://arxiv.org/abs/1205.0792[+http://arxiv.org/abs/1205.0792+]

http://arxiv.org/abs/1110.6298[+http://arxiv.org/abs/1110.6298+]

http://arxiv.org/abs/astro-ph/0506308[+http://arxiv.org/abs/astro-ph/0506308+]

http://arxiv.org/abs/arXiv:0712.3519[+http://arxiv.org/abs/arXiv:0712.3519+]

http://arxiv.org/abs/arXiv:1211.1680[+http://arxiv.org/abs/arXiv:1211.1680+]

http://arxiv.org/abs/arXiv:1108.3900[+http://arxiv.org/abs/arXiv:1108.3900+]

SHTns
-----

A high performance spherical harmonic transform library for numerical
simulations.

https://bitbucket.org/nschaeff/shtns[+https://bitbucket.org/nschaeff/shtns+]

http://users.isterre.fr/nschaeff/SHTns/[+http://users.isterre.fr/nschaeff/SHTns/+]

http://arxiv.org/abs/1202.6522[+http://arxiv.org/abs/1202.6522+]

SICOPOLIS
---------

SICOPOLIS (SImulation COde for POLythermal Ice Sheets) is a 3-d
dynamic/thermodynamic model which simulates the evolution of large ice sheets.
It was originally created as a part of the doctoral thesis by Greve (1995) in
a version for the Greenland Ice Sheet. Since then, SICOPOLIS has been
developed continuously and applied to problems of past, present and future
glaciation of Greenland, Antarctica, the entire northern hemisphere and also
the polar ice caps of the planet Mars.

The model is based on the shallow ice approximation for grounded ice and the
shallow shelf approximation for floating ice (e.g., Greve and Blatter 2009).
It is coded in Fortran 90 and uses finite difference discretisation on a
staggered (Arakawa C) grid, the velocity components being taken between grid
points. Its particularity is the detailed treatment of basal temperate layers
(that is, regions with a temperature at the pressure melting point), which are
positioned by fulfilling a Stefan-type jump condition at the interface to the
cold ice regions. Within the temperate layers, the water content is computed,
and its influence on the ice viscosity is taken into account.

The coding is based on a consequent low-tech philosophy. All structures are
kept as simple as possible, and advanced coding techniques are only employed
where it is deemed appropriate. The use of external libraries is kept at an
absolute minimum. In fact, SICOPOLIS can be run without external libraries at
all, which makes the installation very easy and fast.

http://www.sicopolis.net/[+http://www.sicopolis.net/+]

http://aforge.awi.de/gf/project/sicopolis/[+http://aforge.awi.de/gf/project/sicopolis/+]

Sierpinski
----------

An open source software to solve hyperbolic equations on dynamically changing
fully-adaptive conforming 2D triangular grids. A kernel based way to solve
hyperbolic problems and to apply steering during simulation is offered.

http://www5.in.tum.de/sierpinski/[+http://www5.in.tum.de/sierpinski/+]

SkewReduce
----------

Scientists today have the ability to generate data at an unprecedented scale
and rate and, as a result, they must increasingly turn to parallel data
processing engines to perform their analyses. However, the simple execution
model of these engines can make it difficult to implement efficient algorithms
for scientific analytics. In particular, many scientific analytics require the
extraction of features from data represented as either a multidimensional
array or points in a multidimensional space. These applications exhibit
significant computational skew, where the runtime of different partitions
depends on more than just input size and can therefore vary dramatically and
unpredictably. In SkewReduce project, we explore how to alleviate such skew
problem in a large MapReduce cluster by requesting users minimal information
of their analysis tasks. 

http://nuage.cs.washington.edu/projects.php[+http://nuage.cs.washington.edu/projects.php+]

https://code.google.com/p/skewreduce/[+https://code.google.com/p/skewreduce/+]

http://nuage.cs.washington.edu/pubs/socc10.pdf[+http://nuage.cs.washington.edu/pubs/socc10.pdf+]

Silo
----

A  library for reading and writing a wide variety of scientific data to
binary, disk files. The files Silo produces and the data within them can be
easily shared and exchanged between wholly independently developed
applications running on disparate computing platforms. Consequently, Silo
facilitates the development of general purpose tools for processing scientific
data. One of the more popular tools that process Silo data files is the VisIt
visualization tool.

Silo supports gridless (point) meshes, structured meshes, unstructured-zoo and
unstructured-arbitrary-polyhedral meshes, block structured AMR meshes,
constructive solid geometry (CSG) meshes, piecewise-constant (e.g.
zone-centered) and piecewise-linear (e.g. node-centered) variables defined on
the node, edge, face or volume elements of meshes as well as the decomposition
of meshes into arbitrary subset hierarchies including materials and mixing
materials. In addition, Silo supports a wide variety of other useful objects
to address various scientific computing application needs. Although the Silo
library is a serial library, it has some key features which enable it to be
applied quite effectively and scalable in parallel.

Architecturally, the library is divided into two main pieces; an upper-level
application programming interface (API) and a lower-level I/O implementation
called a driver. Silo supports multiple I/O drivers, the two most common of
which are the HDF5 (Hierarchical Data Format 5) and PDB (Portable Data Base)
drivers.

https://wci.llnl.gov/codes/silo/index.html[+https://wci.llnl.gov/codes/silo/index.html+]

[[simpleai]]
simpleai
--------

This lib implements many of the artificial intelligence algorithms described
on the book "Artificial Intelligence, a Modern Approach", from Stuart Russel
and Peter Norvig.

https://github.com/simpleai-team/simpleai[+https://github.com/simpleai-team/simpleai+]

http://simpleai.readthedocs.org/en/latest/[+http://simpleai.readthedocs.org/en/latest/+]

[[SimpleCV]]
SimpleCV
--------

An open source framework for building computer vision applications. With it,
you get access to several high-powered computer vision libraries such as
OpenCV – without having to first learn about bit depths, file formats, color
spaces, buffer management, eigenvalues, or matrix versus bitmap storage.

http://www.simplecv.org/[+http://www.simplecv.org/+]

SLangTNG
--------

A scripting language for stochastic structural mechanics based on Lua.
Actually, SLangTNG provides additional functionality by wrapping C\xx functions
(involving additional C and FORTRAN libraries) in such a way that the Cxx
objects and methods are accessible from the Lua interpreter. This is done by
an automatic wrapping process using SWIG. In addition to the mathematical
algorithms, there is a binding to a GUI providing an interface to the
interpreter, symbols and visualization. 

https://tng.tuxfamily.org/index.php[+https://tng.tuxfamily.org/index.php+]

Snakemake
---------

A scalable bioinformatics workflow engine, i.e. a friendlier version of make.

https://bitbucket.org/johanneskoester/snakemake/wiki/Home[+https://bitbucket.org/johanneskoester/snakemake/wiki/Home+]

somoclu
-------

A cluster-oriented implementation of self-organizing maps. It relies on MPI
for distributing the workload, and it can be accelerated by CUDA on a GPU
cluster. A sparse kernel is also included, which is useful for training maps
on vector spaces generated in text mining processes.

https://github.com/peterwittek/somoclu[+https://github.com/peterwittek/somoclu+]

http://arxiv.org/abs/1305.1422[+http://arxiv.org/abs/1305.1422+]

SpaceFuncs
----------

A tool for N-dimensional geometric modeling with possibilities of parametrized
calculations, numerical optimization, and solving systems of geometrical
equations with automatic differentiation.

http://openopt.org/SpaceFuncs[+http://openopt.org/SpaceFuncs+]

spaCy
-----

spaCy is a new library for text processing in Python and Cython. 

http://honnibal.github.io/spaCy/[+http://honnibal.github.io/spaCy/+]


Spark
-----

An an open source cluster computing system that aims to make data analytics
fast — both fast to run and fast to write.
To run programs faster, Spark provides primitives for in-memory cluster
computing: your job can load data into memory and query it repeatedly much
more quickly than with disk-based systems like Hadoop MapReduce.
To make programming faster, Spark provides clean, concise APIs in Scala, Java
and Python. You can also use Spark interactively from the Scala and Python
shells to rapidly query big datasets.

http://spark-project.org/[+http://spark-project.org/+]

SPHEREPACK
----------

A collection of FORTRAN77 programs and subroutines facilitating computer
modeling of geophysical processes. The package contains subroutines for
computing common differential operators including divergence, vorticity,
latitudinal derivatives, gradients, the Laplacian of both scalar and vector
functions, and the inverses of these operators. For example, given divergence
and vorticity, the package can be used to compute velocity components, then
the Laplacian inverse can be used to solve the scalar and vector Poisson
equations. The package also contains routines for computing the associated
Legendre functions, Gauss points and weights, multiple fast Fourier
transforms, and for converting scalar and vector fields between geophysical
and mathematical spherical coordinates.

http://www2.cisl.ucar.edu/resources/legacy/spherepack[+http://www2.cisl.ucar.edu/resources/legacy/spherepack+]

pyspharm
~~~~~~~~

An object-oriented python interface to the NCAR SPHEREPACK library. Can
perform spherical harmonic transforms to and from regularly spaced and
gaussian lat/lon grids. 

http://code.google.com/p/pyspharm/[+http://code.google.com/p/pyspharm/+]

Spheroidal Wave Functions
-------------------------

This site provides source code for Fortran computer programs
that calculate accurate values for Mathieu functions and both
Swift lets you write parallel scripts that run many copies of ordinary
programs concurrently.

http://swift-lang.org/main/[+http://swift-lang.org/main/+]

http://www.ci.uchicago.edu/swift/main/[+http://www.ci.uchicago.edu/swift/main/+]

SymPy
-----

A Python library for symbolic mathematics.

http://sympy.org/en/index.html[+http://sympy.org/en/index.html+]

https://conference.scipy.org/scipy2013/presentation_detail.php?id=217[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=217+]

http://gaupdate.wordpress.com/2010/10/27/g-utama-geometric-algebra-sympy-module-ga-for-phython/[+http://gaupdate.wordpress.com/2010/10/27/g-utama-geometric-algebra-sympy-module-ga-for-phython/+]

http://faculty.luther.edu/\~macdonal/[+http://faculty.luther.edu/~macdonal/+]

TABOO
-----

Computes Love numbers of a spherically, self-gravitating Earth by viscoelastic
normal mode method. TABOO also simulates the response of the Earth to surface
loading. Post-glacial deformations can be modeled in terms of surface
displacements, geoid height variations and changes of the inertia tensor of
the Earth.

http://www.fis.uniurb.it/spada/TABOO_minipage.html[+http://www.fis.uniurb.it/spada/TABOO_minipage.html+]

SELEN
~~~~~

Solves the sea level equation, i.e. the integral equation  that describes the
spatiotemporal variations of sea level associated with the melting of late
Quaternary ice sheets.

http://www.fis.uniurb.it/spada/SELEN_minipage.html[+http://www.fis.uniurb.it/spada/SELEN_minipage.html+]

ALMA
~~~~

ALMA is a program that computes the "Love numbers" of a spherically symmetric
Earth.

http://www.fis.uniurb.it/spada/ALMA_minipage.html[+http://www.fis.uniurb.it/spada/ALMA_minipage.html+]

Tangelo
-------

A web framework built on top of CherryPy for producing rich web applications
that pair your data with cutting-edge visual interfaces.

http://tangelo.kitware.com/[+http://tangelo.kitware.com/+]

Taverna
-------

A suite of tools used to design and execute scientific workflows and aid in
silico experimentation.

http://www.taverna.org.uk/[+http://www.taverna.org.uk/+]

Teem
----

A coordinated group of libraries for representing, processing, and visualizing
scientific raster data. Teem includes command-line tools that permit the
library functions to be quickly applied to files and streams, without having
to write any code.

http://teem.sourceforge.net/[+http://teem.sourceforge.net/+]

Temboo
------

A swiss army knife library for web programming with a Python
interface.

https://temboo.com/[+https://temboo.com/+]

TEMOA
-----

Tools for Energy Model Optimization and Assessment (Temoa) is an open source
modeling framework for conducting energy system analysis. The core component
of Temoa is a technology explicit energy economy optimization model.
The design of Temoa is intended to fulfill a unique niche within the energy
modeling community by addressing two critical shortcomings: an inability to
conduct third party verification of published model-based results and the
difficulty of conducting rigorous uncertainty analysis with large, complex
models. Temoa leverages a modern revision control system to publicly archive
model source and data, which enables third party verification of all published
modeling work. In addition, Temoa represents the first EEO model to be
designed - from its initial conceptualization - for operation within a high
performance computing environment.

http://www.temoaproject.org/[+http://www.temoaproject.org/+]

TerraLib
--------

A library of GIS classes and functions for the development of GIS tools.
TerraLib provides functions to decode geographical data, spatial analysis
algorithms and a conceptual model for a geographical database.

http://www.dpi.inpe.br/terralib/[+http://www.dpi.inpe.br/terralib/+]

http://www.dpi.inpe.br/terralib/docs/v313/TerraLib313ProgrammingTutorial.htm[+http://www.dpi.inpe.br/terralib/docs/v313/TerraLib313ProgrammingTutorial.htm+]

https://software.ecmwf.int/wiki/display/MAGP/Magics[+https://software.ecmwf.int/wiki/display/MAGP/Magics+]

THOTH
-----

A Python package for the efficient estimation of information-theoretic
quantities from empirical data.

See xref:NPEET[NPEET].

http://tuvalu.santafe.edu/\~simon/page7/page7.html[+http://tuvalu.santafe.edu/~simon/page7/page7.html+]

thrust
------

A parallel algorithms library which resembles the Cxx Standard Template
Library (STL).

https://github.com/thrust/thrust[+https://github.com/thrust/thrust+]

Tigramite
---------

A time series analysis python script with a graphical user interface. It
allows to detect and quantify causal dependencies and create high-quality
plots of the results.

http://tocsy.pik-potsdam.de/tigramite.php[+http://tocsy.pik-potsdam.de/tigramite.php+]

TileMill
--------

TileMill is the design studio to create stunning interactive maps.

https://www.mapbox.com/tilemill/[+https://www.mapbox.com/tilemill/+]

http://hackaday.com/2014/04/21/using-public-data-to-make-laser-cut-maps/[+http://hackaday.com/2014/04/21/using-public-data-to-make-laser-cut-maps/+]

Tizen
-----

An open source, standards-based software platform supported by leading mobile
operators, device manufacturers, and silicon suppliers for multiple device
categories such as smartphones, tablets, netbooks, in-vehicle infotainment
devices, and smart TVs. Tizen offers an innovative operating system,
applications, and a user experience that consumers can take from device to
device.

https://www.tizen.org/[+https://www.tizen.org/+]

http://www.oscon.com/oscon2013/public/schedule/detail/31475[+http://www.oscon.com/oscon2013/public/schedule/detail/31475+]


TopoGrabber
-----------

A package designed to simplify the task of obtaining high resolution USGS
datasets in formats readable by modelling and analysis software packages.
Files are downloaded in GeoTIFF format, then converted to raw tiles for the
WRF preprocessing system (WPS), and NetCDF tiles for the Local Analysis and
Prediction System (LAPS) and the Space and Time Multiscale Analysis System
(STMAS). 

http://laps.noaa.gov/topograbber/[+http://laps.noaa.gov/topograbber/+]

[[Torch]]
Torch
-----

A scientific computing framework with wide support for machine learning
algorithms. It is easy to use and provides a very efficient implementation,
thanks to an easy and fast scripting language, LuaJIT, and an underlying C
implementation.

http://torch.ch/[+http://torch.ch/+]

TT-Toolbox
----------

A MATLAB tool­box that imple­ments basic oper­a­tions with TT-tensors.

http://spring.inm.ras.ru/osel/?page_id=24[+http://spring.inm.ras.ru/osel/?page_id=24+]

tufte-latex
-----------

LaTeX classes for producing handouts and books according to the style of
Edward R. Tufte and Richard Feynman. 

http://code.google.com/p/tufte-latex/[+http://code.google.com/p/tufte-latex/+]

http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000hB[+http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000hB+]

Tulip
-----

An information visualization framework dedicated to the analysis and
visualization of relational data. Tulip aims to provide the developer with a
complete library, supporting the design of interactive information
visualization applications for relational data that can be tailored to the
problems he or she is addressing.
 
Written in Cxx the framework enables the development of algorithms, visual
encodings, interaction techniques, data models, and domain-specific
visualizations. One of the goal of Tulip is to facilitates the reuse of
components and allows the developers to focus on programming their
application. This development pipeline makes the framework efficient for
research prototyping as well as the development of end-user applications.
 
http://tulip.labri.fr/TulipDrupal/[+http://tulip.labri.fr/TulipDrupal/+]

http://tulip.labri.fr/Documentation/4_3/tulip-python/html/index.html[+http://tulip.labri.fr/Documentation/4_3/tulip-python/html/index.html+]

uncertainties
-------------

A free, cross-platform program that transparently handles calculations with
numbers with uncertainties (like 3.14±0.01). It can also yield the derivatives
of any expression.
Calculations of results with uncertainties, or of derivatives, can be
performed either in an interactive session (as with a calculator), or in
programs written in the Python programming language. Existing calculation code
can run with little or no change.
Whatever the complexity of a calculation, this package returns its result with
an uncertainty as predicted by linear error propagation theory. It
automatically calculates derivatives and uses them for calculating
uncertainties. Almost all uncertainty calculations are performed analytically.
Correlations between variables are automatically handled, which sets this
module apart from many existing error propagation codes.

http://pythonhosted.org/uncertainties/[+http://pythonhosted.org/uncertainties/+]

VGeST
-----

A suite of Virtual Geoscience Simulation Tools for modelling discontinuous
systems, i.e. particulate, granular, blocky, layered, fracturing and
fragmenting systems.

http://vgest.net/[+http://vgest.net/+]

VPython
-------

VPython is the Python programming language plus a 3D graphics module called
"visual" originated by David Scherer in 2000. VPython makes it easy to create
navigable 3D displays and animations, even for those with limited programming
experience. Because it is based on Python, it also has much to offer for
experienced programmers and researchers.

http://vpython.org/[+http://vpython.org/+]

[[VTK]]
VTK
---

An open-source, freely available software system for 3D computer graphics,
image processing and visualization. VTK consists of a Cxx class library and
several interpreted interface layers including Tcl/Tk, Java, and Python.
VTK supports a wide variety of visualization algorithms including: scalar,
vector, tensor, texture, and volumetric methods; and advanced modeling
techniques such as: implicit modeling, polygon reduction, mesh smoothing,
cutting, contouring, and Delaunay triangulation. VTK has an extensive
information visualization framework, has a suite of 3D interaction widgets,
supports parallel processing, and integrates with various databases on GUI
toolkits such as Qt and Tk. 

http://www.vtk.org/[+http://www.vtk.org/+]

ITK
~~~

An open-source, cross-platform system that provides developers with an
extensive suite of software tools for image analysis.  Developed through
extreme programming methodologies, ITK employs leading-edge algorithms for
registering and segmenting multidimensional data.

http://www.itk.org/[+http://www.itk.org/+]

wrapitk
~~~~~~~

An effort to automate the language binding process of one of the largest
highly template-oriented cxx libraries, the Insight Toolkit image processing
library.
Currently Python, Java and Tcl language bindings are implemented, but only
Python is fully supported. 

http://code.google.com/p/wrapitk/[+http://code.google.com/p/wrapitk/+]


txt2tags
--------

A document generator that translate a text file with minimal markup into
over a dozen other formats.

http://txt2tags.org/[+http://txt2tags.org/+]

http://txt2tags.org/userguide/[+http://txt2tags.org/userguide/+]

http://mostlylinux.wordpress.com/textanddocument/txt2tagscheatsheet/[+http://mostlylinux.wordpress.com/textanddocument/txt2tagscheatsheet/+]

UFL
---

The Unified Form Language is an embedded domain specific language for
definition of variational forms intended for finite element discretization.
More precisely, it defines a fixed interface for choosing finite element
spaces and defining expressions for weak forms in a notation close to
mathematical notation.

http://fenicsproject.org/documentation/ufl/1.0.0/ufl.html[+http://fenicsproject.org/documentation/ufl/1.0.0/ufl.html+]

http://arxiv.org/abs/1211.4047[+http://arxiv.org/abs/1211.4047+]

underling
---------

The underling library provides simple, scalable means to manipulate
MPI-parallel, three dimensional pencil decompositions using FFTW. Pencil
decompositions are a natural way to distribute O(n^3) data across O(n^2)
processors and are well-suited for memory-intensive, structured spectral
turbulence simulations and postprocessing codes. It may be useful in other
domains as well. The library is written in C99 and may be used by C89 or Cxx
applications.

https://red.ices.utexas.edu/projects/underling[+https://red.ices.utexas.edu/projects/underling+]

https://red.ices.utexas.edu/projects/underling/wiki[+https://red.ices.utexas.edu/projects/underling/wiki+]

Unicorn
-------

An adaptive finite element solver for fluid and structure mechanics.
Unicorn aims at developing one unified continuum mechanics solver for a wide
range of applications, based on the suite DOLFIN/FFC/FIAT.

https://launchpad.net/unicorn[+https://launchpad.net/unicorn+]

http://dryad.csc.kth.se/projects/unicorn-hpc[+http://dryad.csc.kth.se/projects/unicorn-hpc+]

http://dryad.csc.kth.se/projects[+http://dryad.csc.kth.se/projects+]

UV-CDAT
-------

http://uv-cdat.llnl.gov/[+http://uv-cdat.llnl.gov/+]
with multiple features to address the most common tasks performed by
scientific researchers in the publication and spreading of their results.

https://github.com/damianavila/vIPer[+https://github.com/damianavila/vIPer+]

https://conference.scipy.org/scipy2013/presentation_detail.php?id=168[+https://conference.scipy.org/scipy2013/presentation_detail.php?id=168+]

VAV
---

A program for the analysis of ocean tide data.

http://www.upf.pf/ICET/icetvavOK/index.htm[+http://www.upf.pf/ICET/icetvavOK/index.htm+]

http://www.sciencedirect.com/science/article/pii/S0264370705001201[+http://www.sciencedirect.com/science/article/pii/S0264370705001201+]

http://www.upf.pf/ICET/cd_rom_bgi/Cours/Arnoso/VAV_Lanzarote2005.pdf[+http://www.upf.pf/ICET/cd_rom_bgi/Cours/Arnoso/VAV_Lanzarote2005.pdf+]

http://www.deepdyve.com/lp/elsevier/new-version-of-program-vav-for-tidal-data-processing-W0IvvbdYWE/1[+http://www.deepdyve.com/lp/elsevier/new-version-of-program-vav-for-tidal-data-processing-W0IvvbdYWE/1+]

Verdandi
--------

A generic Cxx library for data assimilation.
It aims at providing methods and tools for data assimilation. It is designed
to be relevant to a large class of problems involving high-dimensional
numerical models.
Verdandi provides a Python interface generated by Swig.

http://verdandi.sourceforge.net/[+http://verdandi.sourceforge.net/+]

https://www.rocq.inria.fr/clime/Workshop-July-2012/KevinCharpentier_Verdandi.pdf[+https://www.rocq.inria.fr/clime/Workshop-July-2012/KevinCharpentier_Verdandi.pdf+]

VFML
----

Very Fast Machine Learning is a toolkit for mining high-speed data streams and
very large data sets. VFML is made up of three main components. The first is a
collection of tools and APIs that help a user develop new learning algorithms.
The second component is a collection of implementations of important learning
algorithms. The third component is a collection of scalable learning
algorithms.

VFML provides code to help read and process training data, to gather
sufficient statistics from it, ADTs for several important machine learning
structures, and various helper code. You can get an overview of what is
provided by visiting the Core APIs and Utility APIs sections of the
documentation.

VFML contains a series of tools for working with data sets: cleaning them,
sampling them, splitting them into train/test sets. It also has tools to help
you experiment with learning algorithms. See the Other Tools documentation
heading for more information.

VFML contains tools for learning decision trees, for learning the structure
belief nets (aka Bayesian networks), and for clustering. Much of this code is
easy to modify or extend (several other researchers have benefited from the
bnlearn program, for example), and much of it can scale to learning from very
large data sets or from data streams. You can get an overview of all the
learners by checking out the Learning Programs section.

http://www.cs.washington.edu/dm/vfml/[+http://www.cs.washington.edu/dm/vfml/+]

VISAN
-----

A  cross-platform visualization and analysis application for atmospheric data.
The application uses the Python language as the means through which you
provide commands to the application. The Python interfaces for CODA and
BEAT-II are included so you can directly ingest product data from within
VISAN. Using the Python language and some additional included mathematical
packages you will be able to perform analysis on your data. Finally, VISAN
provides some very powerful visualization functionality for 2D plots and
worldplots.

http://www.stcorp.nl/beat/[+http://www.stcorp.nl/beat/+]

http://www.stcorp.nl/beat/download/download.html[+http://www.stcorp.nl/beat/download/download.html+]

http://www.stcorp.nl/beat/documentation/visan.html[+http://www.stcorp.nl/beat/documentation/visan.html+]

Visit
-----

simulation control, postprocessing and debugging. 

https://yade-dem.org/doc/[+https://yade-dem.org/doc/+]

Yorick
------

An Yorick is an interpreted programming language for scientific simulations or
calculations, postprocessing or steering large simulation codes, interactive
scientific graphics, and reading, writing, or translating large files of
numbers. Yorick includes an interactive graphics package, and a binary file
package capable of translating to and from the raw numeric formats of all
modern computers. Yorick is written in ANSI C and runs on most operating
systems.
Yorick has a compact syntax, similar to C, but with array operators. It is
easily expandable through dynamic linking of C libraries, allows efficient
manipulation of arbitrary size/dimension arrays, and offers extensive graphic
capabilities.

http://dhmunro.github.io/yorick-doc/[+http://dhmunro.github.io/yorick-doc/+]

http://www.linuxjournal.com/article/2184[+http://www.linuxjournal.com/article/2184+]

yt
--

A community-developed analysis and visualization toolkit for astrophysical
simulation data. yt runs both interactively and non-interactively, and has
been designed to support as many operations as possible in parallel.

http://yt-project.org/[+http://yt-project.org/+]

Zelus
-----

Zélus is a synchronous language extended with Ordinary Differential Equations
(ODEs) to model systems with complex interaction between discrete-time and
continuous-time dynamics. It shares the basic principles of Lustre with
features from Lucid Synchrone (type inference, hierarchical automata, and
statically scheduled sequential code. Continuous components are simulated
using off-the-shelf numerical solvers (here Sundials CVODE) and, for the
moment, two built-in solvers (ode23 and ode45). 

http://zelus.di.ens.fr/[+http://zelus.di.ens.fr/+]

ZeptoOS
-------

ZeptoOS is a research project studying operating systems for petascale
architectures with 10,000 to 1 million CPUs. Operating systems and run-time
Blue Gene and Cray’s XT are capable of achieving petaflops performance and
beyond, and make perfect testbeds for computer science explorations.

Our current activities focus on IBM Blue Gene/P, where we provide a complete,
flexible, high-performance software stack, including a Linux-based compute
node kernel, HPC communication libraries, I/O forwarding, and performance
analysis.

http://www.mcs.anl.gov/research/projects/zeptoos/[+http://www.mcs.anl.gov/research/projects/zeptoos/+]

http://wiki.mcs.anl.gov/zeptoos/index.php/ZeptoOS_Documentation[+http://wiki.mcs.anl.gov/zeptoos/index.php/ZeptoOS_Documentation+]

zkcm
----

A Cxx library for multi-precision complex-number matrix calculations.

http://sourceforge.net/p/zkcm/home/Home/[+http://sourceforge.net/p/zkcm/home/Home/+]

http://arxiv.org/abs/1303.6034[+http://arxiv.org/abs/1303.6034+]

Zoo
---

A WPS platform.

http://www.zoo-project.org/[+http://www.zoo-project.org/+]

http://www.opengeospatial.org/standards/wps[+http://www.opengeospatial.org/standards/wps+]

http://en.wikipedia.org/wiki/Web_Processing_Service[+http://en.wikipedia.org/wiki/Web_Processing_Service+]

ZPL
---

An array programming language designed from first principles for fast
execution on both sequential and parallel computers. It provides a convenient
high-level programming medium for supercomputers and large-scale clusters with
efficiency comparable to hand-coded message passing. It is the perfect
alternative to using a sequential language like C or Fortran and a message
passing library like MPI.

http://www.cs.washington.edu/research/zpl/home/index.html[+http://www.cs.washington.edu/research/zpl/home/index.html+]

https://www.ieeetcsc.org/activities/blog/Myths_About_Scalable_Parallel_Programming_Languages_Part_6%3A_Performance_of_Higher-Level_Languages[+https://www.ieeetcsc.org/activities/blog/Myths_About_Scalable_Parallel_Programming_Languages_Part_6%3A_Performance_of_Higher-Level_Languages+]





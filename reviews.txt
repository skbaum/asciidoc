Review Papers
=============
Steven K. Baum
v0.1, 2014-07-14
:doctype: book
:toc:
:icons:

:numbered!:

1943
----

*Stochastic Problems in Physics and Astronomy* - S. Chandrasekhar
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.15.1[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.15.1+]

1973
----

*Planetary fluid dynamics* - J. G. Charney
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/chapter/10.1007/978-94-010-2599-7_2[+https://link.springer.com/chapter/10.1007/978-94-010-2599-7_2+]

The term “planetary” will be applied to fluid motions on the earth whose space and time scales are so large that the earth’s rotation may not be ignored. Such motions have properties which are not to be found in nonrotating systems. For example, the action of external forces such as gravity invariably bring into being Coriolis forces which in turn produce circulatory motions. If a stone is thrown into an infinite resting ocean, the gravitational oscillations engendered will radiate their energy to infinity and leave the ocean finally undisturbed ; if the stone is thrown into an infinite rotating ocean, some of the energy of the gravitational oscillations will be converted by the action of the Coriolis forces into rotational motions, and these will persist until they are dissipated by viscosity.

*Space and Time Meteorological Data Analysis and Initialization* - P. Morel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/chapter/10.1007/978-94-010-2599-7_5[+https://link.springer.com/chapter/10.1007/978-94-010-2599-7_5+]

The guideline of this Summer School on Dynamic Meteorology was to explore the problem of predicting the general circulation of the atmosphere from one specified state of motion at some initial time to. The previous speakers, particularly Professor PHILLIPS, showed that this initial value problem is approximately solved by replacing the continuous meteorological fields by a finite set of discrete values and integrating numerically the corresponding finite difference equations. Alternatively, one may choose to expand the continuous fields in series of orthogonal functions truncated at some finite order and solve numerically a set of algebraic interaction equations. In any case, the forecasting procedure starts from a set of initial values inferred from observations of the real atmosphere. The purpose of these talks is to review the methods used to infer these field values from the available experimental data. Several such methods, each claiming to be “optimal”, have been proposed and some actually implemented in operational practice. It should be understood as the discussion progresses from straightforward to sophisticated approaches, that the best method from a mathematical or physical standpoint, may not be economical of computer processing time. But one must also keep in mind that the data assimilation and analysis process is an essential link between the world observing system and global extended range forecasting.

*Boundary Layers in Planetary Atmospheres* - A. S. Monin
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/chapter/10.1007/978-94-010-2599-7_4[+https://link.springer.com/chapter/10.1007/978-94-010-2599-7_4+]

In large-scale air currents, near the surface of a planet, the combined action of the pressure gradient, turbulent friction and Coriolis force results in the formation of the atmospheric boundary layer. Unlike most boundary layers dealt with in aerodynamical engineering, the atmospheric boundary layer is characterized by the influence of the Coriolis force (i.e., the planet’s rotation) and the density stratification of air (affecting turbulence through buoyancy forces). Thus the atmospheric boundary layer is a turbulent boundary layer in a rotating stratified fluid.

*Principles of Large Scale Numerical Weather Prediction* - N. A. Phillips
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/chapter/10.1007/978-94-010-2599-7_1[+https://link.springer.com/chapter/10.1007/978-94-010-2599-7_1+]

The motion of the atmosphere, when treated from the viewpoint of continuum mechanics, is governed by Newton’s law of motion relating the acceleration to the forces, the thermodynamic law relating the rate of change of internal energy to the rate of heating, the principle of conservation of mass, the thermodynamic state relations, detailed mathematical formulations of the forces and rates of heating, and, finally, appropriate conditions at the boundaries of the particular mathematical model of the atmosphere being considered. Implied in this statement already is a recognition that we limit our attention to an approximate model of the real “atmosphere”. For example, at very high altitudes the mean free path and time interval between molecular collisions becomes large enough that the quasi-equilibrium assumptions of continuum mechanics and thermodynamics break down. As other examples of the limitation of our meteorological viewpoint we may cite on the one hand our neglect of the interaction between the “atmosphere” and its extension to interplanetary space, and on the other hand our ignoring of the exchange of dry air across the air-ground and air-ocean interface, which we shall treat as impervious to the flow of dry air. These somewhat trite examples are cited only to show immediately that we admit to a simplified atmospheric model ; in fact, however, the practical analysis of large-scale atmospheric motions at the present state of hydrodynamical theory forces approximations upon dynamical meteorologists which have a much greater effect on the accuracy of the results than do the somewhat esoteric examples mentioned above.

1977
----

*West Antarctic ice streams*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/RG015i001p00001[+https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/RG015i001p00001+]

Solar heat is the acknowledged driving force for climatic change. However, ice sheets are also capable of causing climatic change. This property of ice sheets derives from the facts that ice and rock are crystalline whereas the oceans and atmosphere are fluids and that ice sheets are massive enough to depress the earth's crust well below sea level. These features allow time constants for glacial flow and isostatic compensation to be much larger than those for ocean and atmospheric circulation and therefore somewhat independent of the solar variations that control this circulation. This review examines the nature of dynamic processes in ice streams that give ice sheets their degree of independent behavior and emphasizes the consequences of viscoplastic instability inherent in anisotropic polycrystalline solids such as glacial ice. Viscoplastic instability and subglacial topography are responsible for the formation of ice streams near ice sheet margins grounded below sea level. As a result the West Antarctic marine ice sheet is inherently unstable and can be rapidly carved away by calving bays which migrate up surging ice streams. Analyses of tidal flexure along floating ice stream margins, stress and velocity fields in ice streams, and ice stream boundary conditions are presented and used to interpret ERTS 1 photomosaics for West Antarctica in terms of characteristic ice sheet crevasse patterns that can be used to monitor ice stream surges and to study calving bay dynamics.

1979
----

*Geostrophic Turbulence* - P. Rhines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/10.1146/annurev.fl.11.010179.002153[+https://www.annualreviews.org/doi/10.1146/annurev.fl.11.010179.002153+]

1980
----

*Two-dimensional turbulence* - R. H. Kraichnan et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/43/5/001/meta[+https://iopscience.iop.org/article/10.1088/0034-4885/43/5/001/meta+]

The theory of turbulence in two dimensions is reviewed and unified and a number of hydrodynamic and plasma applications are surveyed. The topics treated include the basic dynamical equations, equilibrium statistical mechanics of continuous and discrete vorticity distributions, turbulent cascades, predictability theory, turbulence on a rotating sphere, turbulent diffusion, two-dimensional magnetohydrodynamics, and superfluidity in thin films.

*The Milankovitch astronomical theory of paleoclimates: A modern review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/0083665680900264[+https://www.sciencedirect.com/science/article/pii/0083665680900264+]

1982
----

*On the reconstruction of pleistocene ice sheets: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/0277379182900178[+https://www.sciencedirect.com/science/article/pii/0277379182900178+]

Pleistocene ice sheets can be reconstructed through three separate approaches: (1) Evidence based on glacial geological studies, such as erratic trains, till composition, crossing striations and exposures of multiple tills/nonglacial sediments. (2) Reconstructions based on glaciological theory and observations. These can be either two- or three-dimensional models; they can be constrained by ‘known’ ice margins at specific times; or they can be ‘open-ended’ with the history of growth and retreat controlled by parameters resting entirely within the model. (3) Glacial isostatic rebound after deglaciation provides a measure of the distribution of mass (ice) across a region. A ‘best fit’ ice sheet model can be developed that closely approximates a series of relative sea level curves within an area of a former ice sheet; in addition, the model should also provide a reasonable sea level fit to relative sea level curves at sites well removed from glaciation.

This paper reviews some of the results of a variety of ice sheet reconstructions and concentrates on the various attempts to reconstruct the ice sheets of the last (Wisconsin, Weischelian, Würm, Devensian) glaciation. Evidence from glacial geology suggests flow patterns at variance with simple, single-domed ice sheets over North America and Europe. In addition, reconstruction of ice sheets from glacial isostatic sea level data suggests that the ice sheets were significantly thinner than estimates based on 18 ka equilibrium ice sheets (cf. Denton and Hughes, 1981). The review indicates it is important to differentiate between ice divides, which control the directions of glacial flow, and areas of maximum ice thickness, which control the glacial isostatic rebound of the crust upon deglaciation. Recent studies from the Laurentide Ice Sheet region indicate that the center of mass was not over Hudson Bay; that a major ice divide lay east of Hudson Bay so that flow across the Hudson Bay and James Bay lowlands was from the northeast; that Hudson Bay was probably open to marine invasions two or three times during the Wisconsin Glaciation; and that the Laurentide Ice Sheet was thinner than an equilibrium reconstruction would suggest.

1986
----

*Eddies, Waves, Circulation, and Mixing: Statistical Geofluid Mechanics* - G. Holloway
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/10.1146/annurev.fl.18.010186.000515[+https://www.annualreviews.org/doi/10.1146/annurev.fl.18.010186.000515+]

1988
----

*Milankovitch Theory and climate*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/RG026i004p00624[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/RG026i004p00624+]

Among the longest astrophysical and astronomical cycles that might influence climate (and even among all forcing mechanisms external to the climatic system itself), only those involving variations in the elements of the Earth's orbit have been found to be significantly related to the long‐term climatic data deduced from the geological record. The aim of the astronomical theory of paleoclimates, a particular version of which being due to Milankovitch, is to study this relationship between insolation and climate at the global scale. It comprises four different parts: the orbital elements, the insolation, the climate model, and the geological data.

In the nineteenth century, Croll and Pilgrim stressed the importance of severe winters as a cause of ice ages. Later, mainly during the first half of the twentieth century, Köppen, Spitaler, and Milankovitch regarded mild winters and cool summers as favoring glaciation. After Köppen and Wegener related the Milankovitch new radiation curve to Penck and Brückner's subdivision of the Quaternary, there was a long‐lasting debate on whether or not such changes in the insolation can explain the Quaternary glacial‐interglacial cycles. In the 1970s, with the improvements in dating, in acquiring, and in interpreting the geological data, with the advent of computers, and with the development of astronomical and climate models, the Milankovitch theory revived.

Over the last 5 years it overcame most of the geological, astronomical, and climatological difficulties. The accuracy of the long‐term variations of the astronomical elements and of the insolation values and the stability of their spectra have been analyzed by comparing seven different astronomical solutions and four different time spans (0–0.8 million years before present (Myr B.P.), 0.8–1.6 Myr B.P., 1.6–2.4 Myr B.P., and 2.4–3.2 Myr B.P.). For accuracy in the time domain, improvements are necessary for periods earlier than 2 Myr B.P. As for the stability of the frequencies, the fundamental periods (around 40, 23, and 19 kyr) do not deteriorate with time over the last 5 Myr, but their relative importance for each insolation and each astronomical parameter is a function of the period considered.

Spectral analysis of paleoclimatic records has provided substantial evidence that, at least near the obliquity and precession frequencies, a considerable fraction of the climatic variance is driven in some way by insolation changes forced by changes in the Earth's orbit. Not only are the fundamental astronomical and climatic frequencies alike, but also the climatic series are phase‐locked and strongly coherent with orbital variations. Provided that monthly insolation (i.e., a detailed seasonal cycle) is considered for the different latitudes, their long‐term deviations can be as large as 13% of the long‐term average, and sometimes considerable changes between extreme values can occur in less than 10,000 years.

Models of different categories of complexity, from conceptual ones to three‐dimensional atmospheric general circulation models and two‐dimensional time‐dependent models of the whole climate system, have now been astronomically forced in order to test the physical reality of the astronomical theory. The output of most recent modeling efforts compares favorably with data of the past 400,000 years. Accordingly, the model predictions for the next 100,000 years are used as a basis for forecasting how climate would evolve when forced by orbital variations in the absence of anthropogenic disturbance. The long‐term cooling trend which began some 6,000 years ago will continue for the next 5,000 years; this first temperature minimum will be followed by an amelioration at around 15 kyr A.P. (after present), by a cold interval centered at 23 kyr A.P., and by a major glaciation at around 60 kyr A.P.

1989
----

*Kolmogorov: Life and Creative Activities*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://projecteuclid.org/euclid.aop/1176991251[+https://projecteuclid.org/euclid.aop/1176991251+]

1995
----

*Dynamics of Jovian Atmospheres* - T. E. Dowling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev.fl.27.010195.001453[+https://www.annualreviews.org/doi/abs/10.1146/annurev.fl.27.010195.001453+]

2000
----

*Volcanic eruptions and climate*

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1998RG000054[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1998RG000054+]

Volcanic eruptions are an important natural cause of climate change on many timescales. A new capability to predict the climatic response to a large tropical eruption for the succeeding 2 years will prove valuable to society. In addition, to detect and attribute anthropogenic influences on climate, including effects of greenhouse gases, aerosols, and ozone‐depleting chemicals, it is crucial to quantify the natural fluctuations so as to separate them from anthropogenic fluctuations in the climate record. Studying the responses of climate to volcanic eruptions also helps us to better understand important radiative and dynamical processes that respond in the climate system to both natural and anthropogenic forcings. Furthermore, modeling the effects of volcanic eruptions helps us to improve climate models that are needed to study anthropogenic effects. 

Large volcanic eruptions inject sulfur gases into the stratosphere, which convert to sulfate aerosols with an e‐folding residence time of about 1 year. Large ash particles fall out much quicker. The radiative and chemical effects of this aerosol cloud produce responses in the climate system. By scattering some solar radiation back to space, the aerosols cool the surface, but by absorbing both solar and terrestrial radiation, the aerosol layer heats the stratosphere. For a tropical eruption this heating is larger in the tropics than in the high latitudes, producing an enhanced pole‐to‐equator temperature gradient, especially in winter. In the Northern Hemisphere winter this enhanced gradient produces a stronger polar vortex, and this stronger jet stream produces a characteristic stationary wave pattern of tropospheric circulation, resulting in winter warming of Northern Hemisphere continents. This indirect advective effect on temperature is stronger than the radiative cooling effect that dominates at lower latitudes and in the summer. 

The volcanic aerosols also serve as surfaces for heterogeneous chemical reactions that destroy stratospheric ozone, which lowers ultraviolet absorption and reduces the radiative heating in the lower stratosphere, but the net effect is still heating. Because this chemical effect depends on the presence of anthropogenic chlorine, it has only become important in recent decades. For a few days after an eruption the amplitude of the diurnal cycle of surface air temperature is reduced under the cloud. On a much longer timescale, volcanic effects played a large role in interdecadal climate change of the Little Ice Age. There is no perfect index of past volcanism, but more ice cores from Greenland and Antarctica will improve the record. There is no evidence that volcanic eruptions produce El Niño events, but the climatic effects of El Niño and volcanic eruptions must be separated to understand the climatic response to each.

2001
----

*Hydrodynamical Modeling Of Oceanic Vortices* - X. Carlton
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023/A%3A1013779219578[+https://link.springer.com/article/10.1023/A%3A1013779219578+]

Mesoscale coherent vortices are numerous in the ocean.Though they possess various structures in temperature and salinity,they are all long-lived, fairly intense and mostly circular. Thephysical variable which best describes the rotation and the density anomaly associated with coherent vortices is potential vorticity. It is diagnostically related to velocity and pressure, when the vortex is stationary. Stationary vortices can be monopolar (circular or elliptical) or multipolar; their stability analysis shows thattransitions between the various stationary shapes are possible when they become unstable. But stable vortices can also undergo unsteady evolutions when perturbed by environmental effects, likelarge-scale shear or strain fields, β-effect or topography. Changes in vortex shapes can also result from vortex interactions. such as the pairing, merger or vertical alignment of two vortices, which depend on their relative polarities and depths. Such interactions transfer energy and enstrophy between scales, and are essential in two-dimensional and in geostrophic turbulence. Finally, in relation with the observations, we describe a few mechanisms of vortex generation.

*Arnol'd Nonlinear Stability Theorems and their Application to the Atmosphere and Oceans*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023/A%3A1014229917728[+https://link.springer.com/article/10.1023/A%3A1014229917728+]

Within the context of atmospheric and oceanic fluid dynamicsthe problems of nonlinear stability and instability, particularlythe Arnol'd second type nonlinear stability, are surveyed.The stability criteria obtained by means of the energy-Casimirand energy-Lagrange methods are presented for a varietyof models, the estimates for various generalized perturbationenergy and enstrophy are given. Potential applications of thesecriteria are shown in the estimation of bounds on the perturbationenergy and enstrophy, in the diagnostic study of the persistence orbreakdown of jet flows in the middle and high latitudes, and in theverification of the validity of the tangent linear model in bothatmospheric dynamics and oceanography.Some further research results are also highlighted.

*Ice Sheet And Satellite Altimetry*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023/A%3A1010765923021[+https://link.springer.com/article/10.1023/A%3A1010765923021+]

Since 1991, the altimeters of the ERS European Satellites allow the observation of 80% of the Antarctica ice sheet and the whole Greenland ice sheet: They thus offer for the first time a unique vision of polar ice caps. Indeed, surface topography is an essential data thanks to its capacity to highlight the physical processes which control the surface shape, or to test models. Moreover, the altimeter is also a radar which makes it possible to estimate the snow surface or subsurface characteristics, such as surface roughness induced by the strong katabatic wind or ice grain size. The polar ice caps may not be in a stationary state, they continue to respond to the climatic warming of the beginning of the Holocene, that is 18000 years ago, and possibly start to react to present climatic warming: the altimeter offers the unique means of estimating the variations of volume and thus the contribution of polar ice caps to present sea level change.

*North Atlantic oscillation - Concepts and studies*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023%2FA%3A1014217317898[+https://link.springer.com/article/10.1023%2FA%3A1014217317898+]

This paper aims to provide a comprehensive review of previous studies and concepts concerning the North Atlantic Oscillation. The North Atlantic Oscillation (NAO) and its recent homologue, the Arctic Oscillation/Northern Hemisphere annular mode (AO/NAM), are the most prominent modes of variability in the Northern Hemisphere winter climate. The NAO teleconnection is characterised by a meridional displacement of atmospheric mass over the North Atlantic area. Its state is usually expressed by the standardised air pressure difference between the Azores High and the Iceland Low. ThisNAO index is a measure of the strength of the westerly flow (positive with strong westerlies, and vice versa). Together with the El Niño/Southern Oscillation (ENSO) phenomenon, the NAO is a major source of seasonal to interdecadal variability in the global atmosphere. On interannual and shorter time scales, the NAO dynamics can be explained as a purely internal mode of variability of the atmospheric circulation. Interdecadal variability maybe influenced, however, by ocean and sea-ice processes.

*The dynamics of ocean heat transport variability*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000084[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000084+]

The north‐south heat transport is the prime manifestation of the ocean's role in global climate, but understanding of its variability has been fragmentary owing to uncertainties in observational analyses, limitations in models, and the lack of a convincing mechanism. We review the dynamics of global ocean heat transport variability, with an emphasis on timescales from monthly to interannual. We synthesize relatively simple dynamical ideas and show that together they explain heat transport variability in a state‐of‐the‐art, high‐resolution ocean general circulation model. Globally, the cross‐equatorial seasonal heat transport fluctuations are close to ±3 × 1015 W, the same amplitude as the cross‐equatorial seasonal atmospheric energy transport. The variability is concentrated within 20° of the equator and dominated by the annual cycle.

The majority of the variability is due to wind‐induced current fluctuations in which the time‐varying wind drives Ekman layer mass transports that are compensated by depth‐independent return flows. The temperature difference between the mass transports gives rise to the time‐dependent heat transport. It is found that in the heat budget the divergence of the time‐varying heat transport is largely balanced by changes in heat storage. Despite the Ekman transport's strong impact on the time‐dependent heat transport, the largely depth‐independent character of its associated meridional overturning stream function means that it does not affect estimates of the time‐mean heat transport made by one‐time hydrographic surveys. Away from the tropics the heat transport variability associated with the depth‐independent gyre and depth‐dependent circulations is much weaker than the Ekman variability. The non‐Ekman contributions can amount to a 0.2–0.4 × 1015 W standard deviation in the heat transport estimated from a one‐time hydrographic survey.

*Glacial cycles: Toward a new paradigm*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000091[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000091+]

The largest environmental changes in the recent geological history of the Earth are undoubtedly the successions of glacial and interglacial times. It has been clearly demonstrated that changes in the orbital parameters of our planet have a crucial role in these cycles. Nevertheless, several problems in classical astronomical theory of paleoclimate have indeed been identified: (1) The main cyclicity in the paleoclimatic record is close to 100,000 years, but there is no significant orbitally induced changes in the radiative forcing of the Earth in this frequency range (the “100‐kyr problem”); (2) the most prominent glacial‐interglacial transition occurs at a time of minimal orbital variations (the “stage 11 problem); and (3) at ∼0.8 Ma a change from a 41‐kyr dominant periodicity to a 100‐kyr periodicity occurred without major changes in orbital forcing or in the Earth's configuration (the “late Pleistocene transition problem”).

Additionally, the traditional view states that the climate system changes slowly and continuously together with the slow evolution of the large continental ice sheets, whereas recent high‐resolution data from ice and marine sediment cores do not support such a gradual scenario. Most of the temperature rise at the last termination occurred over a few decades in the Northern Hemisphere, indicating a major and abrupt reorganization of the ocean‐atmosphere system. Similarly, huge iceberg discharges during glacial times, known as Heinrich events, clearly demonstrate that ice sheet changes may also be sometimes quite abrupt. In light of these recent paleoclimatic data the Earth climate system appears much more unstable and seems to jump abruptly between different quasi steady states. Using the concept of thresholds, this new paradigm can be easily integrated into classical astronomical theory and compared with recent observational evidence. If the ice sheet changes are, by definition, the central phenomenon of glacial‐interglacial cycles, other components of the climate system (atmospheric CO2 concentration, Southern Ocean productivity, or global deep‐ocean circulation) may play an even more fundamental role in these climatic cycles.

*The quasi‐biennial oscillation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1999RG000073[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/1999RG000073+]

The quasi‐biennial oscillation (QBO) dominates the variability of the equatorial stratosphere (∼16–50 km) and is easily seen as downward propagating easterly and westerly wind regimes, with a variable period averaging approximately 28 months. From a fluid dynamical perspective, the QBO is a fascinating example of a coherent, oscillating mean flow that is driven by propagating waves with periods unrelated to that of the resulting oscillation. Although the QBO is a tropical phenomenon, it affects the stratospheric flow from pole to pole by modulating the effects of extratropical waves. Indeed, study of the QBO is inseparable from the study of atmospheric wave motions that drive it and are modulated by it. The QBO affects variability in the mesosphere near 85 km by selectively filtering waves that propagate upward through the equatorial stratosphere, and may also affect the strength of Atlantic hurricanes. 

The effects of the QBO are not confined to atmospheric dynamics. Chemical constituents, such as ozone, water vapor, and methane, are affected by circulation changes induced by the QBO. There are also substantial QBO signals in many of the shorter‐lived chemical constituents. Through modulation of extratropical wave propagation, the QBO has an effect on the breakdown of the wintertime stratospheric polar vortices and the severity of high‐latitude ozone depletion. The polar vortex in the stratosphere affects surface weather patterns, providing a mechanism for the QBO to have an effect at the Earth's surface. As more data sources (e.g., wind and temperature measurements from both ground‐based systems and satellites) become available, the effects of the QBO can be more precisely assessed. 

This review covers the current state of knowledge of the tropical QBO, its extratropical dynamical effects, chemical constituent transport, and effects of the QBO in the troposphere (∼0–16 km) and mesosphere (∼50–100 km). It is intended to provide a broad overview of the QBO and its effects to researchers outside the field, as well as a source of information and references for specialists. The history of research on the QBO is discussed only briefly, and the reader is referred to several historical review papers. The basic theory of the QBO is summarized, and tutorial references are provided.

*Origin and Evolution of the Great Lakes*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S038013300170665X[+https://www.sciencedirect.com/science/article/pii/S038013300170665X+]

This paper presents a synthesis of traditional and recently published work regarding the origin and evolution of the Great Lakes. It differs from previously published reviews by focusing on three topics critical to the development of the Great Lakes: the glaciation of the Great Lakes watershed during the late Cenozoic, the evolution of the Great Lakes since the last glacial maximum, and the record of lake levels and coastal erosion in modern times.

The Great Lakes are a product of glacial scour and were partially or totally covered by glacier ice at least six times since 0.78 Ma. During retreat of the last ice sheet large proglacial lakes developed in the Great Lakes watershed. Their levels and areas varied considerably as the oscillating ice margin opened and closed outlets at differing elevations and locations; they were also significantly affected by channel downcutting, crustal rebound, and catastrophic inflows from other large glacial lakes.

Today, lake level changes of about a 1/3 m annually, and up to 2 m over 10 to 20 year time periods, are mainly climatically-driven. Various engineering works provide small control on lake levels for some but not all the Great Lakes. Although not as pronounced as former changes, these subtle variations in lake level have had a significant effect on shoreline erosion, which is often a major concern of coastal residents.

2002
----

*Monte Carlo Methods in Geophysical Inverse Problems*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000089[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000089+]

Monte Carlo inversion techniques were first used by Earth scientists more than 30 years ago. Since that time they have been applied to a wide range of problems, from the inversion of free oscillation data for whole Earth seismic structure to studies at the meter‐scale lengths encountered in exploration seismology. This paper traces the development and application of Monte Carlo methods for inverse problems in the Earth sciences and in particular geophysics. The major developments in theory and application are traced from the earliest work of the Russian school and the pioneering studies in the west by Press [1968] to modern importance sampling and ensemble inference methods. The paper is divided into two parts. The first is a literature review, and the second is a summary of Monte Carlo techniques that are currently popular in geophysics. These include simulated annealing, genetic algorithms, and other importance sampling approaches. The objective is to act as both an introduction for newcomers to the field and a comprehensive reference source for researchers already familiar with Monte Carlo inversion. It is our hope that the paper will serve as a timely summary of an expanding and versatile methodology and also encourage applications to new areas of the Earth sciences.

*Array Seismology: Methods and Applications*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000100[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000100+]

Since their development in the 1960s, seismic arrays have given a new impulse to seismology. Recordings from many uniform seismometers in a well‐defined, closely spaced configuration produce high‐quality and homogeneous data sets, which can be used to study the Earth's structure in great detail. Apart from an improvement of the signal‐to‐noise ratio due to the simple summation of the individual array recordings, seismological arrays can be used in many different ways to study the fine‐scale structure of the Earth's interior. They have helped to study such different structures as the interior of volcanos, continental crust and lithosphere, global variations of seismic velocities in the mantle, the core‐mantle boundary and the structure of the inner core. For this purpose many different, specialized array techniques have been developed and applied to an increasing number of high‐quality array data sets. 

Most array methods use the ability of seismic arrays to measure the vector velocity of an incident wave front, i.e., slowness and back azimuth. This information can be used to distinguish between different seismic phases, separate waves from different seismic events and improve the signal‐to‐noise ratio by stacking with respect to the varying slowness of different phases. The vector velocity information of scattered or reflected phases can be used to determine the region of the Earth from whence the seismic energy comes and with what structures it interacted. Therefore seismic arrays are perfectly suited to study the small‐scale structure and variations of the material properties of the Earth.

In this review we will give an introduction to various array techniques which have been developed since the 1960s. For each of these array techniques we give the basic mathematical equations and show examples of applications. The advantages and disadvantages and the appropriate applications and restrictions of the techniques will also be discussed. The main methods discussed are the beam‐forming method, which forms the basis for several other methods, different slant stacking techniques, and frequency–wave number analysis. Finally, some methods used in exploration geophysics that have been adopted for global seismology are introduced. This is followed by a description of temporary and permanent arrays installed in the past, as well as existing arrays and seismic networks. We highlight their purposes and discuss briefly the advantages and disadvantages of different array configurations.

*A Millennium of Geomagnetism*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000097[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000097+]

The history of geomagnetism began around the year 1000 with the discovery in China of the magnetic compass. Methodical studies of the Earth's field started in 1600 with William Gilbert's De Magnete [Gilbert, 1600] and continued with the work of (among others) Edmond Halley, Charles Augustin de Coulomb, Carl Friedrich Gauss, and Edward Sabine. The discovery of electromagnetism by Hans Christian Oersted and André‐Marie Ampére led Michael Faraday to the notion of fluid dynamos, and the observation of sunspot magnetism by George Ellery Hale led Sir Joseph Larmor in 1919 to the idea that such dynamos could sustain themselves naturally in convecting conducting fluids. From that came modern dynamo theory, of both the solar and terrestrial magnetic fields. Paleomagnetic studies revealed that the Earth's dipole had undergone reversals in the distant past, and these became the critical evidence in establishing plate tectonics. Finally, the recent availability of scientific spacecraft has demonstrated the intricacy of the Earth's distant magnetic field, as well as the existence of magnetic fields associated with other planets and with satellites in our solar system.

*Advanced Spectral Methods for Climatic Time Series*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000092[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2000RG000092+]

The analysis of univariate or multivariate time series provides crucial information to describe, understand, and predict climatic variability. The discovery and implementation of a number of novel methods for extracting useful information from time series has recently revitalized this classical field of study. Considerable progress has also been made in interpreting the information so obtained in terms of dynamical systems theory. In this review we describe the connections between time series analysis and nonlinear dynamics, discuss signal‐to‐noise enhancement, and present some of the novel methods for spectral analysis. The various steps, as well as the advantages and disadvantages of these methods, are illustrated by their application to an important climatic time series, the Southern Oscillation Index. This index captures major features of interannual climate variability and is used extensively in its prediction. Regional and global sea surface temperature data sets are used to illustrate multivariate spectral methods. Open questions and further prospects conclude the review.

*Evolution of networks*
~~~~~~~~~~~~~~~~~~~~~~~

https://www.tandfonline.com/doi/abs/10.1080/00018730110112519[+https://www.tandfonline.com/doi/abs/10.1080/00018730110112519+]

We review the recent rapid progress in the statistical physics of evolving networks. Interest has focused mainly on the structural properties of complex networks in communications, biology, social sciences and economics. A number of giant artificial networks of this kind have recently been created, which opens a wide field for the study of their topology, evolution, and the complex processes which occur in them. Such networks possess a rich set of scaling properties. A number of them are scale-free and show striking resilience against random breakdowns. In spite of the large sizes of these networks, the distances between most of their vertices are short - a feature known as the 'small-world' effect. We discuss how growing networks self-organize into scale-free structures, and investigate the role of the mechanism of preferential linking. We consider the topological and structural properties of evolving networks, and percolation and disease spread on these networks. We present a number of models demonstrating the main features of evolving networks and discuss current approaches for their simulation and analytical study. Applications of the general results to particular networks in nature are discussed. We demonstrate the generic connections of the network growth processes with the general problems of non-equilibrium physics, econophysics, evolutionary biology, and so on.

*Statistical mechanics of complex networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.47[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.74.47+]

Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network’s robustness against failures and attacks.

2003
----

*Sea breeze: Structure, forecasting, and impacts* - S. T. K. Miller et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2003RG000124[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2003RG000124+]

The sea breeze system (SBS) occurs at coastal locations throughout the world and consists of many spatially and temporally nested phenomena. Cool marine air propagates inland when a cross‐shore mesoscale (2–2000 km) pressure gradient is created by daytime differential heating. The circulation is also characterized by rising currents at the sea breeze front and diffuse sinking currents well out to sea and is usually closed by seaward flow aloft. Coastal impacts include relief from oppressive hot weather, development of thunderstorms, and changes in air quality. This paper provides a review of SBS research extending back 2500 years but focuses primarily on recent discoveries. We address SBS forcing mechanisms, structure and related phenomena, life cycle, forecasting, and impacts on air quality.

*The Earth's Climate in the Next Hundred Thousand years (100 kyr)* - A. Berger et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023/A%3A1023233702670[+https://link.springer.com/article/10.1023/A%3A1023233702670+]

One of the most striking features of the Quaternary paleoclimate records remains the so-called 100-kyr cycle which is undoubtedly linked to the future of our climate. Such a 100-kyr cycle is indeed characterised by long glacial periods followed by a short-interglacial (∼10–15 kyr long). As we are now in an interglacial, the Holocene, the previous one (the Eemian, which corresponds quite well to Marine Isotope Stage 5e, peaking at ∼125 kyr before present, BP) was assumed to be a good analogue for our present-day climate. In addition, as the Holocene is 10 kyr long, paleoclimatologists were naturally inclined to predict that we are quite close to the next ice age. Simulations using the 2-D climate model of Louvain-la-Neuve show, however, that the current interglacial will most probably last much longer than any previous ones. It is suggested here that this is related to the shape of the Earth's orbit around the Sun, which will be almost circular over the next tens of thousands of years. As this is primarily related to the 400-kyr cycle of eccentricity, the best and closest analogue for such a forcing is definitely Marine Isotopic Stage 11 (MIS-11), some 400 kyr ago, not MIS-5e. Because the CO2 concentration in the atmosphere also plays an important role in shaping long-term climatic variations – especially its phase with respect to insolation – a detailed reconstruction of this previous interglacial from deep sea and ice records is urgently needed. Such a study is particularly important in the context of the already exceptional present-day CO2 concentrations (unprecedented over the past million years) and, even more so, because of even larger values predicted to occur during the 21st century due to human activities.

*The Structure and Function of Complex Networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://epubs.siam.org/doi/10.1137/S003614450342480[+https://epubs.siam.org/doi/10.1137/S003614450342480+]

Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.

*Ice streams as the arteries of an ice sheet: their mechanics, stability and significance*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825202001307[+https://www.sciencedirect.com/science/article/pii/S0012825202001307+]

Ice streams are corridors of fast ice flow (ca. 0.8 km/year) within an ice sheet and are responsible for discharging the majority of the ice and sediment within them. Consequently, like the arteries in our body, their behaviour and stability is essential to the well being of an ice sheet. Ice streams may either be constrained by topography (topographic ice streams) or by areas of slow moving ice (pure ice streams). The latter show spatial and temporal patterns of variability that may indicate a potential for instability and are therefore of particular interest. Today, pure ice streams are largely restricted to the Siple Coast of Antarctica and these ice streams have been extensively investigated over the last 20 years. This paper provides an introduction to this substantial body of research and describes the morphology, dynamics, and temporal behaviour of these contemporary ice streams, before exploring the basal conditions that exist beneath them and the mechanisms that drive the fast flow within them. The paper concludes by reviewing the potential of ice streams as unstable elements within ice sheets that may impact on the Earth's dynamic system.

2004
----

*Polynya Dynamics: a Review of Observations and Modeling* - M. A. Morales Maqueda et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2002RG000116[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2002RG000116+]

Polynyas are nonlinear‐shaped openings within the ice cover, ranging in size from 10 to 105 km2. Polynyas play an important climatic role. First, winter polynyas tend to warm the atmosphere, thus affecting atmospheric mesoscale motions. Second, ocean surface cooling and brine rejection during sea ice growth in polynyas lead to vertical mixing and convection, contributing to the transformation of intermediate and deep waters in the global ocean and the maintenance of the oceanic overturning circulation. Since 1990, there has been an upsurge in polynya observations and theoretical models for polynya formation and their impact on the biogeochemistry of the polar seas. This article reviews polynya research carried out in the last 2 decades, focusing on presenting a state‐of‐the‐art picture of the physical interactions between polynyas and the atmosphere‐sea ice‐ocean system. Observational and modeling studies, the surface heat budget, and water mass transformation within these features are addressed.

*Nonlinear multivariate and time series analysis by neural network methods* - W. Hsieh
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2002RG000112[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2002RG000112+]

Methods in multivariate statistical analysis are essential for working with large amounts of geophysical data, data from observational arrays, from satellites, or from numerical model output. In classical multivariate statistical analysis, there is a hierarchy of methods, starting with linear regression at the base, followed by principal component analysis (PCA) and finally canonical correlation analysis (CCA). A multivariate time series method, the singular spectrum analysis (SSA), has been a fruitful extension of the PCA technique. The common drawback of these classical methods is that only linear structures can be correctly extracted from the data. Since the late 1980s, neural network methods have become popular for performing nonlinear regression and classification. More recently, neural network methods have been extended to perform nonlinear PCA (NLPCA), nonlinear CCA (NLCCA), and nonlinear SSA (NLSSA). This paper presents a unified view of the NLPCA, NLCCA, and NLSSA techniques and their applications to various data sets of the atmosphere and the ocean (especially for the El Niño‐Southern Oscillation and the stratospheric quasi‐biennial oscillation). These data sets reveal that the linear methods are often too simplistic to describe real‐world systems, with a tendency to scatter a single oscillatory phenomenon into numerous unphysical modes or higher harmonics, which can be largely alleviated in the new nonlinear paradigm.

*Nonlinear Shallow Water Theories for Coastal Waves* - E. Barthelemy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-003-1281-7[+https://link.springer.com/article/10.1007/s10712-003-1281-7+]

Ocean waves entering the near-shore zone undergo nonlinear and dispersive processes. This paper reviews nonlinear models, focusing on the so-called Serre equations. Techniques to overcome their limitations with respect to the phase speed are presented. Nonlinear behaviours are compared with theoretical results concerning the properties of Stokes waves. In addition, the models are tested against experiments concerning periodic wave transformation over a bar topography and of the shoaling of solitary waves on a beach.

*Theory of Basin Scale Dynamics of a Stratified Rotating Fluid* - L. R. M. Maas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-004-1277-y[+https://link.springer.com/article/10.1007/s10712-004-1277-y+]

The dynamics of a stratified fluid contained in a rotating rectangular box is described in terms of the evolution of the lowest moments of its density and momentum fields. The first moment of the density field also gives the position of the fluid’s centre-of-mass. The resulting low-order model allows for fast assessment both of adopted parameterisations, as well as of particular values of parameters. In the ideal fluid limit (neglect of viscous and diffusive effects), in the absence of wind, the equations have a Hamiltonian structure that is integrable (non-integrable) in the absence (presence) of differential heating. In a non-rotating convective regime, dynamically rich behaviour and strong dependence on the single (lumped) parameter are established. For small values of this parameter, in a self-similar regime, further reduction to an explicit map is discussed in an Appendix. Introducing rotation in a nearly geostrophic regime leads through a Hopf bifurcation to a limit cycle, and under the influence of wind and salt to multiple equilibria and chaos, respectively.

*From Classical To Statistical Ocean Dynamics* - G. Holloway
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-004-1272-3[+https://link.springer.com/article/10.1007/s10712-004-1272-3+]

Traditional ocean modeling treats fields resolved on the model grid according to the classical dynamics of continua. Variability on smaller scales is included through sundry “eddy viscosities”, “mixing coefficients” and other schemes. In this paper we develop an alternative approach based on statistical dynamics. First, we recognize that we treat probabilities of flows, not the flows themselves. Modeled dependent variables are the moments (expectations) of the probabilities of possible flows. Second, we address the challenge to obtain the equations of motion for the moments of probable flows rather than the (traditional) equations for explicit flows. For linear terms and on larger resolved scales, the statistical equations agree with classical dynamics where those of traditional modeling works well.

Differences arise where traditional modeling would relegate unresolved motion to “eddy viscosity”, etc.. Instead, changes of entropy (<-log P> over the probability distribution of possible flows) with respect to the modeled moments act as forcings upon those moments. In this way we obtain a consistent framework for specifying the terms which, traditionally, represent subgridscale effects. Although these statistical equations are close to the classical equations in many ways, important differences are also evident; here, two phenomena are described where the results differ. We consider eddies interacting with bottom topography. It is seen that traditional “eddy viscosity” and/or “topographic drag”, which would reduce large scale flows toward rest, are wrong. The second law of thermodynamics is violated; the “arrow of time” is running backwards! From statistical dynamics, approximate corrections are obtained, yielding a practical improvement to the fidelity of ocean models. Another phenomenon occurs at much smaller scales in the turbulent mixing of heat and salt. Even when both heat and salt are stably stratifying, their rates of turbulent transfer should differ. This suggests a further model improvement.

*Stochastic Models of Quasigeostrophic Turbulence* - T. Delsole
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1023/B%3AGEOP.0000028160.75549.0d[+https://link.springer.com/article/10.1023/B%3AGEOP.0000028160.75549.0d+]

Atmospheric and oceanic eddies are believed to be manifestations of quasigeostrophic turbulence — turbulence that occurs in rapidly rotating, vertically stratified fluid systems. The heat, momentum, and water transport by these eddies constitute a significant component of the climate balance, without which climate change cannot be understood. A major, unsolved problem is whether the turbulent eddy fluxes can be parameterized in terms of the large-scale, background flow. In the past, stochastic models have been used quite extensively to investigate quasigeostrophic turbulence in the case in which the eddy statistics are isotropic and homogeneous. Unfortunately, these models ignore the background shear which is absolutely essential to maintaining the eddies in the presence of dissipation. Recent attempts to extend stochastic models to shear flows have shown significant skill in predicting the structure of the eddy fluxes in arbitrary, three-dimensionally varying flows. This paper provides an accessible introduction to these models. The topics reviewed include quasigeostrophic turbulence and two-dimensional turbulence, non-modal andoptimal perturbations, mathematical theory of stochastic models, stochastic model simulations with realistic background states, and recent closure theories. A list of unsolved problems concludes this review.

*Heinrich events: Massive late Pleistocene detritus layers of the North Atlantic and their global climate imprint*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003RG000128[+https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003RG000128+]

[1] Millennial climate oscillations of the glacial interval are interrupted by extreme events, the so‐called Heinrich events of the North Atlantic. Their near‐global footprint is a testament to coherent interactions among Earth's atmosphere, oceans, and cryosphere on millennial timescales. Heinrich detritus appears to have been derived from the region around Hudson Strait. It was deposited over approximately 500 ± 250 years. Several mechanisms have been proposed for the origin of the layers: binge‐purge cycle of the Laurentide ice sheet, jökulhlaup activity from a Hudson Bay lake, and an ice shelf buildup/collapse fed by Hudson Strait. To determine the origin of the Heinrich events, I recommend (1) further studies of the timing and duration of the events, (2) further sedimentology study near the Hudson Strait, and (3) greater spatial and temporal resolution studies of the layers as well as their precursory intervals. Studies of previous glacial intervals may also provide important constraints.

2005
----

*Madden‐Julian Oscillation* - C. Zhang
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2004RG000158[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2004RG000158+]

The Madden‐Julian Oscillation (MJO) is the dominant component of the intraseasonal (30–90 days) variability in the tropical atmosphere. It consists of large‐scale coupled patterns in atmospheric circulation and deep convection, with coherent signals in many other variables, all propagating eastward slowly (∼5 m s−1) through the portion of the Indian and Pacific oceans where the sea surface is warm. It constantly interacts with the underlying ocean and influences many weather and climate systems. The past decade has witnessed an expeditious progress in the study of the MJO: Its large‐scale and multiscale structures are better described, its scale interaction is recognized, its broad influences on tropical and extratropical weather and climate are increasingly appreciated, and its mechanisms for disturbing the ocean are further comprehended. Yet we are facing great difficulties in accurately simulating and predicting the MJO using sophisticated global weather forecast and climate models, and we are unable to explain such difficulties based on existing theories of the MJO. It is fair to say that the MJO remains an unmet challenge to our understanding of the tropical atmosphere and to our ability to simulate and predict its variability. This review, motivated by both the acceleration and gaps in our knowledge of the MJO, intends to synthesize what we currently know and what we do not know on selected topics: its observed basic characteristics, mechanisms, numerical modeling, air‐sea interaction, and influences on the El Niño and Southern Oscillation.

2006
----

*Complex networks: Structure and dynamics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S037015730500462X?via%3Dihub[+https://www.sciencedirect.com/science/article/pii/S037015730500462X?via%3Dihub+]

Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks’ dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering.

*Orbital changes and climate*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0277379106002691[+https://www.sciencedirect.com/science/article/pii/S0277379106002691+]

At the 41,000-period of orbital tilt, summer insolation forces a lagged response in northern ice sheets. This delayed ice signal is rapidly transferred to nearby northern oceans and landmasses by atmospheric dynamics. These ice-driven responses lead to late-phased changes in atmospheric CO2 that provide positive feedback to the ice sheets and also project ‘late’ 41-K forcing across the tropics and the Southern Hemisphere. Responses in austral regions are also influenced by a fast response to summer insolation forcing at high southern latitudes.

At the 22,000-year precession period, northern summer insolation again forces a lagged ice-sheet response, but with muted transfers to proximal regions and no subsequent effect on atmospheric CO2. Most 22,000-year greenhouse-gas responses have the ‘early’ phase of July insolation. July forcing of monsoonal and boreal wetlands explains the early CH4 response. The slightly later 22-K CO2 response originates in the southern hemisphere. The early 22-K CH4 and CO2 responses add to insolation forcing of the ice sheets.

The dominant 100,000-year response of ice sheets is not externally forced, nor does it result from internal resonance. Internal forcing appears to play at most a minor role. The origin of this signal lies mainly in internal feedbacks (CO2 and ice albedo) that drive the gradual build-up of large ice sheets and then their rapid destruction. Ice melting during terminations is initiated by uniquely coincident forcing from insolation and greenhouse gases at the periods of tilt and precession.

2007
----

*Heterogeneous Multiscale Methods: A Review* - W. E. B. Engquist et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

http://www.global-sci.com/intro/article_detail.html?journal=cicp&article_id=7911[+http://www.global-sci.com/intro/article_detail.html?journal=cicp&article_id=7911+]

This papergives asystematic introductionto HMM,the heterogeneousmultiscale methods, including the fundamental design principles behind the HMM philosophy and the main obstacles that have to be overcome when using HMM for a particular problem. This is illustrated by examples from several application areas, including complex ﬂuids, micro-ﬂuidics, solids, interface problems, stochastic problems, and statistically self-similar problems. Emphasis is given to the technical tools, such as the various constrained molecular dynamics, that have been developed, in order to apply HMM to these problems. Examples of mathematical results on the error analysis of HMM are presented. The review ends with a discussion on some of the problems that have to be solved in order to make HMM a more powerful tool. 

*On the driving processes of the Atlantic meridional overturning circulation* - A. Griesel et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because of its relevance for the global climate the Atlantic meridional overturning circulation (AMOC) has been a major research focus for many years. Yet the question of which physical mechanisms ultimately drive the AMOC, in the sense of providing its energy supply, remains a matter of controversy. Here we review both observational data and model results concerning the two main candidates: vertical mixing processes in the ocean's interior and wind‐induced Ekman upwelling in the Southern Ocean. In distinction to the energy source we also discuss the role of surface heat and freshwater fluxes, which influence the volume transport of the meridional overturning circulation and shape its spatial circulation pattern without actually supplying energy to the overturning itself in steady state. We conclude that both wind‐driven upwelling and vertical mixing are likely contributing to driving the observed circulation. To quantify their respective contributions, future research needs to address some open questions, which we outline.

*The study of Earth's magnetism (1269–1950): A foundation by Peregrinus and subsequent development of geomagnetism and paleomagnetism* - V. Courtillot
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000198[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000198+]

This paper summarizes the histories of geomagnetism and paleomagnetism (1269–1950). The role of Peregrinus is emphasized. In the sixteenth century a debate on local versus global departures of the field from that of an axial dipole pitted Gilbert against Le Nautonier. Regular measurements were undertaken in the seventeenth century. At the turn of the nineteenth century, de Lamanon, de Rossel, and von Humboldt discovered the decrease of intensity as one approaches the equator. Around 1850, three figures of rock magnetism were Fournet (remanent and induced magnetizations), Delesse (remagnetization in a direction opposite to the original), and Melloni (direction of lava magnetization acquired at time of cooling). Around 1900, Brunhes discovered magnetic reversals. In the 1920s, Chevallier produced the first magnetostratigraphy and hypothesized that poles had undergone enormous displacements. Matuyama showed that the Earth's field had reversed before the Pleistocene. Our review ends in the 1940s, when exponential development of geomagnetism and paleomagnetism starts.

*Neural network emulations for complex multidimensional geophysical mappings*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000200[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000200+]

A group of geophysical applications, which from the mathematical point of view, can be formulated as complex, multidimensional, nonlinear mappings and which in terms of the neural network (NN) technique, utilize a particular type of NN, the multilayer perceptron (MLP), is reviewed in this paper. This type of NN application covers the majority of NN applications developed in geosciences like satellite remote sensing, meteorology, oceanography, numerical weather prediction, and climate studies. The major properties of the mappings and MLP NNs are formulated and discussed. Three particular groups of NN applications are presented in this paper as illustrations: atmospheric and oceanic satellite remote sensing applications, NN emulations of model physics for developing atmospheric and oceanic hybrid numerical models, and NN emulations of the dependencies between model variables for application in data assimilation systems.

*Angular momentum in the global atmospheric circulation* - J. Egger et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000213[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000213+]

Angular momentum is a variable of central importance to the dynamics of the atmosphere both regionally and globally. Moreover, the angular momentum equations yield a precise description of the dynamic interaction of the atmosphere with the oceans and the solid Earth via various torques as exerted by friction, pressure against the mountains and the nonspherical shape of the Earth, and by gravity. This review presents recent work with respect to observations and the theory of atmospheric angular momentum of large‐scale motions. It is mainly the recent availability of consistent global data sets spanning decades that sparked renewed interest in angular momentum. In particular, relatively reliable estimates of the torques are now available. In addition, a fairly wide range of theoretical aspects of the role of angular momentum in atmospheric large‐scale dynamics is covered.

*Predictability: Recent insights from information theory* - T. DelSole
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000202[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000202+]

This paper summarizes a framework for investigating predictability based on information theory. This framework connects and unifies a wide variety of statistical methods traditionally used in predictability analysis, including linear regression, canonical correlation analysis, singular value decomposition, discriminant analysis, and data assimilation. Central to this framework is a procedure called predictable component analysis (PrCA). PrCA optimally decomposes variables by predictability, just as principal component analysis optimally decomposes variables by variance. For normal distributions the same predictable components are obtained whether one optimizes predictive information, the dispersion part of relative entropy, mutual information, Mahalanobis error, average signal to noise ratio, normalized mean square error, or anomaly correlation. For joint normal distributions, PrCA is equivalent to canonical correlation analysis between forecast and observations. The regression operator that maps observations to forecasts plays an important role in this framework, with the left singular vectors of this operator being the predictable components and the singular values being the canonical correlations. This correspondence between predictable components and singular vectors occurs only if the singular vectors are computed using Mahalanobis norms, a result that sheds light on the role of norms in predictability. In linear stochastic models the forcing that minimizes predictability is the one that renders the “whitened” dynamical operator normal. This condition for minimum predictability is invariant to linear transformation and is equivalent to detailed balance. The framework also inspires some new approaches to accounting for deficiencies of forecast models and estimating distributions from finite samples.

*Characterization of complex networks: A survey of measurements*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.tandfonline.com/doi/abs/10.1080/00018730601170527[+https://www.tandfonline.com/doi/abs/10.1080/00018730601170527+]

Each complex network (or class of networks) presents specific topological features which characterize its connectivity and highly influence the dynamics of processes executed on the network. The analysis, discrimination, and synthesis of complex networks therefore rely on the use of measurements capable of expressing the most relevant topological features. This article presents a survey of such measurements. It includes general considerations about complex network characterization, a brief review of the principal models, and the presentation of the main existing measurements. Important related issues covered in this work comprise the representation of the evolution of complex networks in terms of trajectories in several measurement spaces, the analysis of the correlations between some of the most traditional measurements, perturbation analysis, as well as the use of multivariate statistics for feature selection and network classification. Depending on the network and the analysis task one has in mind, a specific set of features may be chosen. It is hoped that the present survey will help the proper application and interpretation of measurements.

*Atmospheric bridge, oceanic tunnel, and global climatic teleconnections*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005RG000172[+https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005RG000172+]

We review teleconnections within the atmosphere and ocean, their dynamics and their role in coupled climate variability. We concentrate on teleconnections in the latitudinal direction, notably tropical‐extratropical and interhemispheric interactions, and discuss the timescales of several teleconnection processes. The tropical impact on extratropical climate is accomplished mainly through the atmosphere. In particular, tropical Pacific sea surface temperature anomalies impact extratropical climate variability through stationary atmospheric waves and their interactions with midlatitude storm tracks. Changes in the extratropics can also impact the tropical climate through upper ocean subtropical cells at decadal and longer timescales. On the global scale the tropics and subtropics interact through the atmospheric Hadley circulation and the oceanic subtropical cell. The thermohaline circulation can provide an effective oceanic teleconnection for interhemispheric climate interactions.

2008
----

*Lectures on Probability, Entropy, and Statistical Physics* - Ariel Caticha
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/0808.0012[+https://arxiv.org/abs/0808.0012+]

These lectures deal with the problem of inductive inference, that is, the problem of reasoning under conditions of incomplete information. Is there a general method for handling uncertainty? Or, at least, are there rules that could in principle be followed by an ideally rational mind when discussing scientific matters? What makes one statement more plausible than another? How much more plausible? And then, when new information is acquired how do we change our minds? Or, to put it differently, are there rules for learning? Are there rules for processing information that are objective and consistent? Are they unique? And, come to think of it, what, after all, is information? It is clear that data contains or conveys information, but what does this precisely mean? Can information be conveyed in other ways? Is information physical? Can we measure amounts of information? Do we need to? Our goal is to develop the main tools for inductive inference--probability and entropy--from a thoroughly Bayesian point of view and to illustrate their use in physics with examples borrowed from the foundations of classical statistical physics. 

*Geophysical and astrophysical fluid dynamics beyond the traditional approximation* - T. Gerkema
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000220[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2006RG000220+]

In studies on geophysical fluid dynamics, it is common practice to take the Coriolis force only partially into account by neglecting the components proportional to the cosine of latitude, the so‐called traditional approximation (TA). This review deals with the consequences of abandoning the TA, based on evidence from numerical and theoretical studies and laboratory and field experiments. The phenomena most affected by the TA include mesoscale flows (Ekman spirals, deep convection, and equatorial jets) and internal waves. Abandoning the TA produces a tilt in convective plumes, produces a dependence on wind direction in Ekman spirals, and gives rise to a plethora of changes in internal wave behavior in weakly stratified layers, such as the existence of trapped short low‐frequency waves, and a poleward extension of their habitat. In the astrophysical context of stars and gas giant planets, the TA affects the rate of tidal dissipation and also the patterns of thermal convection.

*Synchronization in complex networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0370157308003384?via%3Dihub[+https://www.sciencedirect.com/science/article/pii/S0370157308003384?via%3Dihub+]

Synchronization processes in populations of locally interacting elements are the focus of intense research in physical, biological, chemical, technological and social systems. The many efforts devoted to understanding synchronization phenomena in natural systems now take advantage of the recent theory of complex networks. In this review, we report the advances in the comprehension of synchronization phenomena when oscillating elements are constrained to interact in a complex network topology. We also take an overview of the new emergent features coming out from the interplay between the structure and the function of the underlying patterns of connections. Extensive numerical work as well as analytical approaches to the problem are presented. Finally, we review several applications of synchronization in complex networks to different disciplines: biological systems and neuroscience, engineering and computer science, and economy and social sciences.

*Classifications of Atmospheric Circulation Patterns*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1196/annals.1446.019[+https://nyaspubs.onlinelibrary.wiley.com/doi/full/10.1196/annals.1446.019+]

We review recent advances in classifications of circulation patterns as a specific research area within synoptic climatology. The review starts with a general description of goals of classification and the historical development in the field. We put circulation classifications into a broader context within climatology and systematize the varied methodologies and approaches. We characterize three basic groups of classifications: subjective (also called manual), mixed (hybrid), and objective (computer‐assisted, automated). The roles of cluster analysis and principal component analysis in the classification process are clarified. Several recent methodological developments in circulation classifications are identified and briefly described: the introduction of nonlinear methods, objectivization of subjective catalogs, efforts to optimize classifications, the need for intercomparisons of classifications, and the progress toward an optimum, if possible unified, classification method. Among the recent tendencies in the applications of circulation classifications, we mention a more extensive use in climate studies, both of past, present, and future climates, innovative applications in the ensemble forecasting, increasing variety of synoptic–climatological investigations, and steps above from the troposphere. After introducing the international activity within the field of circulation classifications, the COST733 Action, we briefly describe outputs of the inventory of classifications in Europe, which was carried out within the Action. Approaches to the evaluation of classifications and their mutual comparisons are also reviewed. A considerable part of the review is devoted to three examples of applications of circulation classifications: in historical climatology, in analyses of recent climate variations, and in analyses of outputs from global climate models.

*Towards the Probabilistic Earth-System Model*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/0812.1074[+https://arxiv.org/abs/0812.1074+]

Multi-model ensembles provide a pragmatic approach to the representation of model uncertainty in climate prediction. However, such representations are inherently ad hoc, and, as shown, probability distributions of climate variables based on current-generation multi-model ensembles, are not accurate. Results from seasonal re-forecast studies suggest that climate model ensembles based on stochastic-dynamic parametrisation are beginning to outperform multi-model ensembles, and have the potential to become significantly more skilful than multi-model ensembles.
The case is made for stochastic representations of model uncertainty in future-generation climate prediction models. Firstly, a guiding characteristic of the scientific method is an ability to characterise and predict uncertainty; individual climate models are not currently able to do this. Secondly, through the effects of noise-induced rectification, stochastic-dynamic parametrisation may provide a (poor man's) surrogate to high resolution. Thirdly, stochastic-dynamic parametrisations may be able to take advantage of the inherent stochasticity of electron flow through certain types of low-energy computer chips, currently under development.
These arguments have particular resonance for next-generation Earth-System models, which purport to be comprehensive numerical representations of climate, and where integrations at high resolution may be unaffordable. 

2009
----

*Introduction to Bioinformatics* - Sabu M. Thampi
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/0911.4230[+https://arxiv.org/abs/0911.4230+]

Bioinformatics is a new discipline that addresses the need to manage and interpret the data that in the past decade was massively generated by genomic research. This discipline represents the convergence of genomics, biotechnology and information technology, and encompasses analysis and interpretation of data, modeling of biological phenomena, and development of algorithms and statistics. This article presents an introduction to bioinformatics.

*Fast Numerical Methods for Stochastic Computations: A Review* - D. Xiu
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

http://www.global-sci.com/intro/article_detail.html?journal=cicp&article_id=7732[+http://www.global-sci.com/intro/article_detail.html?journal=cicp&article_id=7732+]

This paper presents a review of the current state-of-the-art of numerical methods for stochastic computations. The focus is on efﬁcient high-order methods suitable for practical applications, with a particular emphasis on those based on generalized polynomial chaos (gPC) methodology. The framework of gPC is reviewed, along with its Galerkin and collocation approaches for solving stochastic equations. Properties of these methods are summarized by using results from literature. This paper also attempts to present the gPC based methods in a uniﬁed framework based on an extension of the classical spectral methods into multi-dimensional random spaces.

*The “chessboard” classification scheme of mineral deposits: Mineralogy and geology from aluminum to zirconium*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825209001688[+https://www.sciencedirect.com/science/article/pii/S0012825209001688+]

Economic geology is a mixtum compositum of all geoscientific disciplines focused on one goal, finding new mineral depsosits and enhancing their exploitation. The keystones of this mixtum compositum are geology and mineralogy whose studies are centered around the emplacement of the ore body and the development of its minerals and rocks. In the present study, mineralogy and geology act as x- and y-coordinates of a classification chart of mineral resources called the “chessboard” (or “spreadsheet”) classification scheme. Magmatic and sedimentary lithologies together with tectonic structures (1-D/pipes, 2-D/veins) are plotted along the x-axis in the header of the spreadsheet diagram representing the columns in this chart diagram. 63 commodity groups, encompassing minerals and elements are plotted along the y-axis, forming the lines of the spreadsheet. These commodities are subjected to a tripartite subdivision into ore minerals, industrial minerals/rocks and gemstones/ornamental stones.

Further information on the various types of mineral deposits, as to the major ore and gangue minerals, the current models and the mode of formation or when and in which geodynamic setting these deposits mainly formed throughout the geological past may be obtained from the text by simply using the code of each deposit in the chart. This code can be created by combining the commodity (lines) shown by numbers plus lower caps with the host rocks or structure (columns) given by capital letters.

Each commodity has a small preface on the mineralogy and chemistry and ends up with an outlook into its final use and the supply situation of the raw material on a global basis, which may be updated by the user through a direct link to databases available on the internet. In this case the study has been linked to the commodity database of the US Geological Survey. The internal subdivision of each commodity section corresponds to the common host rock lithologies (magmatic, sedimentary, and metamorphic) and structures. Cross sections and images illustrate the common ore types of each commodity. Ore takes priority over the mineral. The minerals and host rocks are listed by their chemical and mineralogical compositions, respectively, separated from the text but supplemented with cross-references to the columns and lines, where they prevalently occur.

*Statistical mechanics of money, wealth, and income*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.81.1703[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.81.1703+]

This Colloquium reviews statistical models for money, wealth, and income distributions developed in the econophysics literature since the late 1990s. By analogy with the Boltzmann-Gibbs distribution of energy in physics, it is shown that the probability distribution of money is exponential for certain classes of models with interacting economic agents. Alternative scenarios are also reviewed. Data analysis of the empirical distributions of wealth and income reveals a two-class distribution. The majority of the population belongs to the lower class, characterized by the exponential (“thermal”) distribution, whereas a small fraction of the population in the upper class is characterized by the power-law (“superthermal”) distribution. The lower part is very stable, stationary in time, whereas the upper part is highly dynamical and out of equilibrium.

2010
----

*The origin and early radiation of dinosaurs*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825210000401[+https://www.sciencedirect.com/science/article/pii/S0012825210000401+]

Dinosaurs were remarkably successful during the Mesozoic and one subgroup, birds, remain an important component of modern ecosystems. Although the extinction of non-avian dinosaurs at the end of the Cretaceous has been the subject of intense debate, comparatively little attention has been given to the origin and early evolution of dinosaurs during the Late Triassic and Early Jurassic, one of the most important evolutionary radiations in earth history. Our understanding of this keystone event has dramatically changed over the past 25 years, thanks to an influx of new fossil discoveries, reinterpretations of long-ignored specimens, and quantitative macroevolutionary analyses that synthesize anatomical and geological data. Here we provide an overview of the first 50 million years of dinosaur history, with a focus on the large-scale patterns that characterize the ascent of dinosaurs from a small, almost marginal group of reptiles in the Late Triassic to the preeminent terrestrial vertebrates of the Jurassic and Cretaceous.

We provide both a biological and geological background for early dinosaur history. Dinosaurs are deeply nested among the archosaurian reptiles, diagnosed by only a small number of characters, and are subdivided into a number of major lineages. The first unequivocal dinosaurs are known from the late Carnian of South America, but the presence of their sister group in the Middle Triassic implies that dinosaurs possibly originated much earlier. The three major dinosaur lineages, theropods, sauropodomorphs, and ornithischians, are all known from the Triassic, when continents were joined into the supercontinent Pangaea and global climates were hot and arid. Although many researchers have long suggested that dinosaurs outcompeted other reptile groups during the Triassic, we argue that the ascent of dinosaurs was more of a matter of contingency and opportunism. Dinosaurs were overshadowed in most Late Triassic ecosystems by crocodile-line archosaurs and showed no signs of outcompeting their rivals. Instead, the rise of dinosaurs was a two-stage process, as dinosaurs expanded in taxonomic diversity, morphological disparity, and absolute faunal abundance only after the extinction of most crocodile-line reptiles and other groups.

*The principles of cryostratigraphy*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825210000413[+https://www.sciencedirect.com/science/article/pii/S0012825210000413+]

Cryostratigraphy adopts concepts from both Russian geocryology and modern sedimentology. Structures formed by the amount and distribution of ice within sediment and rock are termed cryostructures. Typically, layered cryostructures are indicative of syngenetic permafrost while reticulate and irregular cryostructures are indicative of epigenetic permafrost. ‘Cryofacies’ can be defined according to patterns of sediment characterized by distinct ice lenses and layers, volumetric ice content and ice-crystal size. Cryofacies can be subdivided according to cryostructure. Where a number of cryofacies form a distinctive cryostratigraphic unit, these are termed a ‘cryofacies assemblage’. The recognition, if present, of (i) thaw unconformities, (ii) other ice bodies such as vein ice (ice wedges), aggradational ice and thermokarst-cave (‘pool’) ice, and (iii) ice, sand and gravelly pseudomorphs is also important in determining the nature of the freezing process, the conditions under which frozen sediment accumulates, and the history of permafrost.

*The largest volcanic eruptions on Earth*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825210000814[+https://www.sciencedirect.com/science/article/pii/S0012825210000814+]

Large igneous provinces (LIPs) are sites of the most frequently recurring, largest volume basaltic and silicic eruptions in Earth history. These large-volume (> 1000 km3 dense rock equivalent) and large-magnitude (> M8) eruptions produce areally extensive (104–105 km2) basaltic lava flow fields and silicic ignimbrites that are the main building blocks of LIPs. Available information on the largest eruptive units are primarily from the Columbia River and Deccan provinces for the dimensions of flood basalt eruptions, and the Paraná–Etendeka and Afro-Arabian provinces for the silicic ignimbrite eruptions. In addition, three large-volume (675–2000 km3) silicic lava flows have also been mapped out in the Proterozoic Gawler Range province (Australia), an interpreted LIP remnant. Magma volumes of > 1000 km3 have also been emplaced as high-level basaltic and rhyolitic sills in LIPs. 

The data sets indicate comparable eruption magnitudes between the basaltic and silicic eruptions, but due to considerable volumes residing as co-ignimbrite ash deposits, the current volume constraints for the silicic ignimbrite eruptions may be considerably underestimated. Magma composition thus appears to be no barrier to the volume of magma emitted during an individual eruption. Despite this general similarity in magnitude, flood basaltic and silicic eruptions are very different in terms of eruption style, duration, intensity, vent configuration, and emplacement style. Flood basaltic eruptions are dominantly effusive and Hawaiian–Strombolian in style, with magma discharge rates of ~ 106–108 kg s−1 and eruption durations estimated at years to tens of years that emplace dominantly compound pahoehoe lava flow fields. Effusive and fissural eruptions have also emplaced some large-volume silicic lavas, but discharge rates are unknown, and may be up to an order of magnitude greater than those of flood basalt lava eruptions for emplacement to be on realistic time scales (< 10 years).

Most silicic eruptions, however, are moderately to highly explosive, producing co-current pyroclastic fountains (rarely Plinian) with discharge rates of 109–1011 kg s−1 that emplace welded to rheomorphic ignimbrites. At present, durations for the large-magnitude silicic eruptions are unconstrained; at discharge rates of 109 kg s−1, equivalent to the peak of the 1991 Mt Pinatubo eruption, the largest silicic eruptions would take many months to evacuate > 5000 km3 of magma. The generally simple deposit structure is more suggestive of short-duration (hours to days) and high intensity (~ 1011 kg s−1) eruptions, perhaps with hiatuses in some cases. These extreme discharge rates would be facilitated by multiple point, fissure and/or ring fracture venting of magma. Eruption frequencies are much elevated for large-magnitude eruptions of both magma types during LIP-forming episodes.

However, in basalt-dominated provinces (continental and ocean basin flood basalt provinces, oceanic plateaus, volcanic rifted margins), large magnitude (> M8) basaltic eruptions have much shorter recurrence intervals of 103–104 years, whereas similar magnitude silicic eruptions may have recurrence intervals of up to 105 years. The Paraná–Etendeka province was the site of at least nine > M8 silicic eruptions over an ~ 1 Myr period at ~ 132 Ma; a similar eruption frequency, although with a fewer number of silicic eruptions is also observed for the Afro-Arabian Province. The huge volumes of basaltic and silicic magma erupted in quick succession during LIP events raises several unresolved issues in terms of locus of magma generation and storage (if any) in the crust prior to eruption, and paths and rates of ascent from magma reservoirs to the surface.

Available data indicate four end-member magma petrogenetic pathways in LIPs: 1) flood basalt magmas with primitive, mantle-dominated geochemical signatures (often high-Ti basalt magma types) that were either transferred directly from melting regions in the upper mantle to fissure vents at surface, or resided temporarily in reservoirs in the upper mantle or in mafic underplate thereby preventing extensive crustal contamination or crystallisation; 2) flood basalt magmas (often low-Ti types) that have undergone storage at lower ± upper crustal depths resulting in crustal assimilation, crystallisation, and degassing; 3) generation of high-temperature anhydrous, crystal-poor silicic magmas (e.g., Paraná–Etendeka quartz latites) by large-scale AFC processes involving lower crustal granulite melting and/or basaltic underplate remelting; and 4) rejuvenation of upper-crustal batholiths (mainly near-solidus crystal mush) by shallow intrusion and underplating by mafic magma providing thermal and volatile input to produce large volumes of crystal-rich (30–50%) dacitic to rhyolitic magma and for ignimbrite-producing eruptions, well-defined calderas up to 80 km diameter (e.g., Fish Canyon Tuff model), and which characterise of some silicic eruptions in silicic LIPs.

*Complex networks as a unified framework for descriptive analysis and predictive modeling in climate science*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://onlinelibrary.wiley.com/doi/full/10.1002/sam.10100[+https://onlinelibrary.wiley.com/doi/full/10.1002/sam.10100+]

The analysis of climate data has relied heavily on hypothesis‐driven statistical methods, while projections of future climate are based primarily on physics‐based computational models. However, in recent years a wealth of new datasets has become available. Therefore, we take a more data‐centric approach and propose a unified framework for studying climate, with an aim toward characterizing observed phenomena as well as discovering new knowledge in climate science. Specifically, we posit that complex networks are well suited for both descriptive analysis and predictive modeling tasks. We show that the structural properties of ‘climate networks’ have useful interpretation within the domain. Further, we extract clusters from these networks and demonstrate their predictive power as climate indices. Our experimental results establish that the network clusters are statistically significantly better predictors than clusters derived using a more traditional clustering approach. Using complex networks as data representation thus enables the unique opportunity for descriptive and predictive modeling to inform each other. 

*Track and vertex reconstruction: From classical to adaptive methods*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.82.1419[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.82.1419+]

This paper reviews classical and adaptive methods of track and vertex reconstruction in particle physics experiments. Adaptive methods have been developed to meet the experimental challenges at high-energy colliders, in particular, the CERN Large Hadron Collider. They can be characterized by the obliteration of the traditional boundaries between pattern recognition and statistical estimation, by the competition between different hypotheses about what constitutes a track or a vertex, and by a high level of flexibility and robustness achieved with a minimum of assumptions about the data. The theoretical background of some of the adaptive methods is described, and it is shown that there is a close connection between the two main branches of adaptive methods: neural networks and deformable templates, on the one hand, and robust stochastic filters with annealing, on the other hand. As both classical and adaptive methods of track and vertex reconstruction presuppose precise knowledge of the positions of the sensitive detector elements, the paper includes an overview of detector alignment methods and a survey of the alignment strategies employed by past and current experiments.

2011
----

*Self-oscillation* - Alejandro Jenkins
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1109.6640[+https://arxiv.org/abs/1109.6640+]

Physicists are very familiar with forced and parametric resonance, but usually not with self-oscillation, a property of certain dynamical systems that gives rise to a great variety of vibrations, both useful and destructive. In a self-oscillator, the driving force is controlled by the oscillation itself so that it acts in phase with the velocity, causing a negative damping that feeds energy into the vibration: no external rate needs to be adjusted to the resonant frequency. The famous collapse of the Tacoma Narrows bridge in 1940, often attributed by introductory physics texts to forced resonance, was actually a self-oscillation, as was the swaying of the London Millennium Footbridge in 2000. Clocks are self-oscillators, as are bowed and wind musical instruments. The heart is a "relaxation oscillator," i.e., a non-sinusoidal self-oscillator whose period is determined by sudden, nonlinear switching at thresholds. We review the general criterion that determines whether a linear system can self-oscillate. We then describe the limiting cycles of the simplest nonlinear self-oscillators, as well as the ability of two or more coupled self-oscillators to become spontaneously synchronized ("entrained"). We characterize the operation of motors as self-oscillation and prove a theorem about their limit efficiency, of which Carnot's theorem for heat engines appears as a special case. We briefly discuss how self-oscillation applies to servomechanisms, Cepheid variable stars, lasers, and the macroeconomic business cycle, among other applications. Our emphasis throughout is on the energetics of self-oscillation, often neglected by the literature on nonlinear dynamical systems.

*Supercontinents, mantle dynamics and plate tectonics: A perspective based on conceptual vs. numerical models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825210001753[+https://www.sciencedirect.com/science/article/pii/S0012825210001753+]

The periodic assembly and dispersal of supercontinents through the history of the Earth had considerable impact on mantle dynamics and surface processes. Here we synthesize some of the conceptual models on supercontinent amalgamation and disruption and combine it with recent information from numerical studies to provide a unified approach in understanding Wilson Cycle and supercontinent cycle. Plate tectonic models predict that superdownwelling along multiple subduction zones might provide an effective mechanism to pull together dispersed continental fragments into a closely packed assembly. The recycled subducted material that accumulates at the mantle transition zone and sinks down into the core–mantle boundary (CMB) provides the potential fuel for the generation of plumes and superplumes which ultimately fragment the supercontinent. Geological evidence related to the disruption of two major supercontinents (Columbia and Gondwana) attest to the involvement of plumes.

The re-assembly of dispersed continental fragments after the breakup of a supercontinent occurs through complex processes involving ‘introversion’, ‘extroversion’ or a combination of both, with the closure of the intervening ocean occurring through Pacific-type or Atlantic-type processes. The timescales of the assembly and dispersion of supercontinents have varied through the Earth history, and appear to be closely linked with the processes and duration of superplume genesis. The widely held view that the volume of continental crust has increased over time has been challenged in recent works and current models propose that plate tectonics creates and destroys Earth's continental crust with more crust being destroyed than created.

The creation–destruction balance changes over a supercontinent cycle, with a higher crustal growth through magmatic influx during supercontinent break-up as compared to the tectonic erosion and sediment-trapped subduction in convergent margins associated with supercontinent assembly which erodes the continental crust. Ongoing subduction erosion also occurs at the leading edges of dispersing plates, which also contributes to crustal destruction, although this is only a temporary process. The previous numerical studies of mantle convection suggested that there is a significant feedback between mantle convection and continental drift. The process of assembly of supercontinents induces a temperature increase beneath the supercontinent due to the thermal insulating effect. Such thermal insulation leads to a planetary-scale reorganization of mantle flow and results in longest-wavelength thermal heterogeneity in the mantle, i.e., degree-one convection in three-dimensional spherical geometry.

The formation of degree-one convection seems to be integral to the emergence of periodic supercontinent cycles. The rifting and breakup of supercontinental assemblies may be caused by either tensional stress due to the thermal insulating effect, or large-scale partial melting resulting from the flow reorganization and consequent temperature increase beneath the supercontinent. Supercontinent breakup has also been correlated with the temperature increase due to upwelling plumes originating from the deeper lower mantle or CMB as a return flow of plate subduction occurring at supercontinental margins. The active mantle plumes from the CMB may disrupt the regularity of supercontinent cycles. Two end-member scenarios can be envisaged for the mantle convection cycle. One is that mantle convection with dispersing continental blocks has a short-wavelength structure, or close to degree-two structure as the present Earth, and when a supercontinent forms, mantle convection evolves into degree-one structure. Another is that mantle convection with dispersing continental blocks has a degree-one structure, and when a supercontinent forms, mantle convection evolves into degree-two structure. In the case of the former model, it would take longer time to form a supercontinent, because continental blocks would be trapped by different downwellings thus inhibiting collision. 

Although most of the numerical studies have assumed the continent/supercontinent to be rigid or nondeformable body mainly because of numerical limitations as well as a simplification of models, a more recent numerical study allows the modeling of mobile, deformable continents, including oceanic plates, and successfully reproduces continental drift similar to the processes and timescales envisaged in Wilson Cycle.

*Relative sea-level fall since the last interglacial stage: Are coasts uplifting worldwide?*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825211000705[+https://www.sciencedirect.com/science/article/pii/S0012825211000705+]

The growing interest in quantification of vertical ground motion stems from the need to understand in detail how the Earth's crust behaves, for both scientific and social reasons. However, only recently has the refinement of dating techniques made possible the use of paleoshorelines as reliable tools for tectonic studies. Although there are many local studies of Quaternary vertical motions of coastlines, we know of no comprehensive worldwide synthesis. Here we provide a compilation of 890 records of paleoshoreline sequences, with particular emphasis on the last interglacial stage (Marine Isotopic Stage [MIS] 5e, ~ 122 ka). The quality of dating MIS 5e makes it a reliable marker to evaluate vertical ground motion rates during the late Quaternary on a global scale. 

The results show that most coastal segments have risen relative to sea-level with a mean uplift rate higher than 0.2 mm/yr, i.e. more than four times faster than the estimated eustatic drop in sea level. The results also reveal that the uplift rate is faster on average for active margins than for passive margins. Neither dynamic topography nor glacio-hydro-isostasy may explain sustained uplift of all continental margins, as revealed by the wide distribution of uplifted sequences of paleoshorelines. Instead, we suggest that only plate-tectonic processes reconcile all observations of Quaternary coastal uplift. We propose that long-term continental accretion has led to compression of continental plates and uplift of their margins. Therefore this study concludes that plate-tectonics processes impact all margins and emphasizes the fact that the notion of a stable platform is unrealistic. These results therefore seriously challenge the evaluation of past sea levels from the fossil shoreline record.

*Permafrost*
~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825211000894[+https://www.sciencedirect.com/science/article/pii/S0012825211000894+]

Since its introduction, the definition of permafrost has rarely been discussed or reviewed. Recent decades have brought a series of significant, often interdisciplinary works on a periglacial zone and permafrost as well as their relation with other components of the environment, especially with glaciers. They show that, despite its unequivocal definition, the term has lost its sharpness and explicitness with regard to some aspects of research. The article presents a current state of understanding of permafrost phenomenon, regarding the use of the term permafrost, which means a physical state, not a material thing. Processes which it undergoes, that is exclusively aggradation and degradation, and also the possibility of its occurrence in glacial and periglacial environments of geographical space, where it covers over a quarter of land area on the Earth.

*Intermittent search strategies*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.81[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.81+]

This review examines intermittent target search strategies, which combine phases of slow motion, allowing the searcher to detect the target, and phases of fast motion during which targets cannot be detected. It is first shown that intermittent search strategies are actually widely observed at various scales. At the macroscopic scale, this is, for example, the case of animals looking for food; at the microscopic scale, intermittent transport patterns are involved in a reaction pathway of DNA-binding proteins as well as in intracellular transport. Second, generic stochastic models are introduced, which show that intermittent strategies are efficient strategies that enable the minimization of search time. This suggests that the intrinsic efficiency of intermittent search strategies could justify their frequent observation in nature. Last, beyond these modeling aspects, it is proposed that intermittent strategies could also be used in a broader context to design and accelerate search processes.

*Comparison of astrophysical and terrestrial frequency standards*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.1[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.1+]

The stability of pulse arrival times from pulsars and white dwarfs have been reanalyzed using several analysis tools for measuring the noise characteristics of sampled time and frequency data. The best terrestrial artificial clocks are shown to substantially exceed the performance of astronomical sources as timekeepers in terms of accuracy (as defined by cesium primary frequency standards) and stability. This superiority in stability can be directly demonstrated over time periods up to 2 years, where there is high quality data for both. Beyond 2 years there is a deficiency of data for clock-to-clock comparisons, and both terrestrial and astronomical clocks show equal performance being equally limited by the quality of the reference time scales used to make the comparisons. Nonetheless, the detailed accuracy evaluations of modern terrestrial clocks imply that these new clocks are likely to have a stability better than any astronomical source up to comparison times of at least hundreds of years. This article is intended to provide a correct appreciation of the relative merits of natural and artificial clocks. The use of natural clocks as tests of physics under the most extreme conditions is entirely appropriate; however, the contention that these natural clocks, particularly white dwarfs, can compete as timekeepers against devices constructed by mankind is shown to be doubtful.

*Bayesian inference in physics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.943[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.943+]

Bayesian inference provides a consistent method for the extraction of information from physics experiments even in ill-conditioned circumstances. The approach provides a unified rationale for data analysis, which both justifies many of the commonly used analysis procedures and reveals some of the implicit underlying assumptions. This review summarizes the general ideas of the Bayesian probability theory with emphasis on the application to the evaluation of experimental data. As case studies for Bayesian parameter estimation techniques examples ranging from extra-solar planet detection to the deconvolution of the apparatus functions for improving the energy resolution and change point estimation in time series are discussed. Special attention is paid to the numerical techniques suited for Bayesian analysis, with a focus on recent developments of Markov chain Monte Carlo algorithms for high-dimensional integration problems. Bayesian model comparison, the quantitative ranking of models for the explanation of a given data set, is illustrated with examples collected from cosmology, mass spectroscopy, and surface physics, covering problems such as background subtraction and automated outlier detection. Additionally the Bayesian inference techniques for the design and optimization of future experiments are introduced. Experiments, instead of being merely passive recording devices, can now be designed to adapt to measured data and to change the measurement strategy on the fly to maximize the information of an experiment. The applied key concepts and necessary numerical tools which provide the means of designing such inference chains and the crucial aspects of data fusion are summarized and some of the expected implications are highlighted.

*Physical basis of radiation protection in space travel*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.1245[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.1245+]

The health risks of space radiation are arguably the most serious challenge to space exploration, possibly preventing these missions due to safety concerns or increasing their costs to amounts beyond what would be acceptable. Radiation in space is substantially different from Earth: high-energy (E) and charge (Z) particles (HZE) provide the main contribution to the equivalent dose in deep space, whereas γ rays and low-energy α particles are major contributors on Earth. This difference causes a high uncertainty on the estimated radiation health risk (including cancer and noncancer effects), and makes protection extremely difficult. In fact, shielding is very difficult in space: the very high energy of the cosmic rays and the severe mass constraints in spaceflight represent a serious hindrance to effective shielding. Here the physical basis of space radiation protection is described, including the most recent achievements in space radiation transport codes and shielding approaches. Although deterministic and Monte Carlo transport codes can now describe well the interaction of cosmic rays with matter, more accurate double-differential nuclear cross sections are needed to improve the codes. Energy deposition in biological molecules and related effects should also be developed to achieve accurate risk models for long-term exploratory missions. Passive shielding can be effective for solar particle events; however, it is limited for galactic cosmic rays (GCR). Active shielding would have to overcome challenging technical hurdles to protect against GCR. Thus, improved risk assessment and genetic and biomedical approaches are a more likely solution to GCR radiation protection issues.

2012
----

*Tsunami hazard and exposure on the global scale*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825211001619[+https://www.sciencedirect.com/science/article/pii/S0012825211001619+]

In the aftermath of the 2004 Indian Ocean tsunami, a large increase in the activity of tsunami hazard and risk mapping is observed. Most of these are site-specific studies with detailed modelling of the run-up locally. However, fewer studies exist on the regional and global scale. Therefore, tsunamis have been omitted in previous global studies comparing different natural hazards. Here, we present a first global tsunami hazard and population exposure study. A key topic is the development of a simple and robust method for obtaining reasonable estimates of the maximum water level during tsunami inundation. This method is mainly based on plane wave linear hydrostatic transect simulations, and validation against results from a standard run-up model is given. The global hazard study is scenario based, focusing on tsunamis caused by megathrust earthquakes only, as the largest events will often contribute more to the risk than the smaller events. Tsunamis caused by non-seismic sources are omitted. Hazard maps are implemented by conducting a number of tsunami scenario simulations supplemented with findings from literature. The maps are further used to quantify the number of people exposed to tsunamis using the Landscan population data set. Because of the large geographical extents, quantifying the tsunami hazard assessment is focusing on overall trends.

*Global continental and ocean basin reconstructions since 200 Ma*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825212000311[+https://www.sciencedirect.com/science/article/pii/S0012825212000311+]

Global plate motion models provide a spatial and temporal framework for geological data and have been effective tools for exploring processes occurring at the earth's surface. However, published models either have insufficient temporal coverage or fail to treat tectonic plates in a self-consistent manner. They usually consider the motions of selected features attached to tectonic plates, such as continents, but generally do not explicitly account for the continuous evolution of plate boundaries through time. In order to explore the coupling between the surface and mantle, plate models are required that extend over at least a few hundred million years and treat plates as dynamic features with dynamically evolving plate boundaries. 

We have constructed a new type of global plate motion model consisting of a set of continuously-closing topological plate polygons with associated plate boundaries and plate velocities since the break-up of the supercontinent Pangea. Our model is underpinned by plate motions derived from reconstructing the seafloor-spreading history of the ocean basins and motions of the continents and utilizes a hybrid absolute reference frame, based on a moving hotspot model for the last 100 Ma, and a true-polar wander corrected paleomagnetic model for 200 to 100 Ma. Detailed regional geological and geophysical observations constrain plate boundary inception or cessation, and time-dependent geometry. Although our plate model is primarily designed as a reference model for a new generation of geodynamic studies by providing the surface boundary conditions for the deep earth, it is also useful for studies in disparate fields when a framework is needed for analyzing and interpreting spatio-temporal data.

*Singular vectors in atmospheric sciences: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825212000657[+https://www.sciencedirect.com/science/article/pii/S0012825212000657+]

During the last decade, singular vectors (SVs) have received a lot of attention in the research and operational communities especially due to their use in ensemble forecasting and targeting of observations. SVs represent the orthogonal set of perturbations that, according to linear theory, will grow fastest over a finite‐time interval with respect to a specific metric. Hence, the study of SVs gives information about the dynamics and structure of rapidly growing and finite-time instabilities representing an important step toward a better understanding of perturbations evolution in the atmosphere. This paper reviews the SV formulation and gives a brief overview of their recent applications in atmospheric sciences. A particular attention is accorded to the SV sensitivity to different parameters such as optimization time interval, norm, horizontal resolution and tangent linear model, various choices leading to different initial structures and evolutions.

*Phanerozoic polar wander, palaeogeography and dynamics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825212000797[+https://www.sciencedirect.com/science/article/pii/S0012825212000797+]

A significant number of new palaeomagnetic poles have become available since the last time a compilation was made (assembled in 2005, published in 2008) to indicate to us that a new and significantly expanded set of tables with palaeomagnetic results would be valuable, with results coming from the Gondwana cratonic elements, Laurentia, Baltica/Europe, and Siberia. Following the Silurian Caledonian Orogeny, Laurentia's and Baltica's Apparent Polar Wander Paths (APWPs) can be merged into a Laurussia path, followed in turn by a merger of the Laurussia and Siberia data from latest Permian time onward into a Laurasian combined path. Meanwhile, after about 320 Ma, Gondwana's and Laurussia/Laurasia's path can be combined into what comes steadily closer to the ideal of a Global Apparent Polar Wander Path (GAPWaP) for late Palaeozoic and younger times. Tests for True Polar Wander (TPW) episodes are now feasible since Pangaea fusion and we identify four important episodes of Mesozoic TPW between 250 and 100 Ma. TPW rates are in the order of 0.45–0.8°/M.y. but cumulative TPW is nearly zero since the Late Carboniferous. With the exception of a few intervals where data are truly scarce (e.g., 390–340 Ma), the palaeomagnetic database is robust and allows us to make a series of new palaeogeographic reconstructions from the Late Cambrian to the Palaeogene.

*Storytelling in Earth sciences: The eight basic plots*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825212001249[+https://www.sciencedirect.com/science/article/pii/S0012825212001249+]

Reporting results and promoting ideas in science in general, and Earth science in particular, is treated here as storytelling. Just as in literature and drama, storytelling in Earth science is characterized by a small number of basic plots. Though the list is not exhaustive, and acknowledging that multiple or hybrid plots and subplots are possible in a single piece, eight standard plots are identified, and examples provided: cause-and-effect, genesis, emergence, destruction, metamorphosis, convergence, divergence, and oscillation. The plots of Earth science stories are not those of literary traditions, nor those of persuasion or moral philosophy, and deserve separate consideration. Earth science plots do not conform those of storytelling more generally, implying that Earth scientists may have fundamentally different motivations than other storytellers, and that the basic plots of Earth Science derive from the characteristics and behaviors of Earth systems. In some cases preference or affinity to different plots results in fundamentally different interpretations and conclusions of the same evidence. In other situations exploration of additional plots could help resolve scientific controversies. Thus explicit acknowledgement of plots can yield direct scientific benefits. Consideration of plots and storytelling devices may also assist in the interpretation of published work, and can help scientists improve their own storytelling.

*Overturning in the North Atlantic*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-120710-100740[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-120710-100740+]

The global overturning of ocean waters involves the equatorward transport of cold, deep waters and the poleward transport of warm, near-surface waters. Such movement creates a net poleward transport of heat that, in partnership with the atmosphere, establishes the global and regional climates. Although oceanographers have long assumed that a reduction in deep water formation at high latitudes in the North Atlantic translates into a slowing of the ocean's overturning and hence in Earth's climate, observational and modeling studies over the past decade have called this assumed linkage into question. The observational basis for linking water mass formation with the ocean's meridional overturning is reviewed herein. Understanding this linkage is crucial to efforts aimed at predicting the consequences of the warming and freshening of high-latitude surface waters to the climate system.

*Ice structures, patterns, and processes: A view across the icefields*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.84.885[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.84.885+]

From the frontiers of research on ice dynamics in its broadest sense, this review surveys the structures of ice, the patterns or morphologies it may assume, and the physical and chemical processes in which it is involved. Open questions in the various fields of ice research in nature are highlighted, ranging from terrestrial and oceanic ice on Earth, to ice in the atmosphere, to ice on other Solar System bodies and in interstellar space.

*Statistical physics of fracture, friction, and earthquakes*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.84.839[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.84.839+]

The present status of research and understanding regarding the dynamics and the statistical properties of earthquakes is reviewed, mainly from a statistical physical viewpoint. Emphasis is put both on the physics of friction and fracture, which provides a microscopic basis for our understanding of an earthquake instability, and on the statistical physical modelling of earthquakes, which provides macroscopic aspects of such phenomena. Recent numerical results from several representative models are reviewed, with attention to both their critical and their characteristic properties. Some of the relevant notions and related issues are highlighted, including the origin of power laws often observed in statistical properties of earthquakes, apparently contrasting features of characteristic earthquakes or asperities, the nature of precursory phenomena and nucleation processes, and the origin of slow earthquakes, etc.

*Hamiltonian complexity*
~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/75/2/022001[+https://iopscience.iop.org/article/10.1088/0034-4885/75/2/022001+]

In recent years we have seen the birth of a new field known as Hamiltonian complexity lying at the crossroads between computer science and theoretical physics. Hamiltonian complexity is directly concerned with the question: how hard is it to simulate a physical system? Here I review the foundational results, guiding problems, and future directions of this emergent field.

*The physics of wind-blown sand and dust*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/75/10/106901[+https://iopscience.iop.org/article/10.1088/0034-4885/75/10/106901+]

The transport of sand and dust by wind is a potent erosional force, creates sand dunes and ripples, and loads the atmosphere with suspended dust aerosols. This paper presents an extensive review of the physics of wind-blown sand and dust on Earth and Mars. Specifically, we review the physics of aeolian saltation, the formation and development of sand dunes and ripples, the physics of dust aerosol emission, the weather phenomena that trigger dust storms, and the lifting of dust by dust devils and other small-scale vortices. We also discuss the physics of wind-blown sand and dune formation on Venus and Titan.

2013
----

*A Survey on Array Storage, Query Languages, and Systems* - Florin Rusu et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1302.0103[+https://arxiv.org/abs/1302.0103+]

Since scientific investigation is one of the most important providers of massive amounts of ordered data, there is a renewed interest in array data processing in the context of Big Data. To the best of our knowledge, a unified resource that summarizes and analyzes array processing research over its long existence is currently missing. In this survey, we provide a guide for past, present, and future research in array processing. The survey is organized along three main topics. Array storage discusses all the aspects related to array partitioning into chunks. The identification of a reduced set of array operators to form the foundation for an array query language is analyzed across multiple such proposals. Lastly, we survey real systems for array processing. The result is a thorough survey on array data storage and processing that should be consulted by anyone interested in this research topic, independent of experience level. The survey is not complete though. We greatly appreciate pointers towards any work we might have forgotten to mention.

*Category Theory for Scientists* - David I. Spivak
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1302.6946[+https://arxiv.org/abs/1302.6946+]

There are many books designed to introduce category theory to either a mathematical audience or a computer science audience. In this book, our audience is the broader scientific community. We attempt to show that category theory can be applied throughout the sciences as a framework for modeling phenomena and communicating results. In order to target the scientific audience, this book is example-based rather than proof-based. For example, monoids are framed in terms of agents acting on objects, sheaves are introduced with primary examples coming from geography, and colored operads are discussed in terms of their ability to model self-similarity. A new version with solutions to exercises will be available through MIT Press.

*The volcanic response to deglaciation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825213000664[+https://www.sciencedirect.com/science/article/pii/S0012825213000664+]

Several lines of evidence have previously been used to suggest that ice retreat after the last glacial maximum (LGM) resulted in regionally-increased levels of volcanic activity. It has been proposed that this increase in volcanism was globally significant, forming a substantial component of the post-glacial rise in atmospheric CO2, and thereby contributing to climatic warming. However, as yet there has been no detailed investigation of activity in glaciated volcanic arcs following the LGM. Arc volcanism accounts for 90% of present-day subaerial volcanic eruptions. It is therefore important to constrain the impact of deglaciation on arc volcanoes, to understand fully the nature and magnitude of global-scale relationships between volcanism and glaciation.

The first part of this paper examines the post-glacial explosive eruption history of the Andean southern volcanic zone (SVZ), a typical arc system, with additional data from the Kamchatka and Cascade arcs. In all cases, eruption rates in the early post-glacial period do not exceed those at later times at a statistically significant level. In part, the recognition and quantification of what may be small (i.e. less than a factor of two) increases in eruption rate is hindered by the size of our datasets. These datasets are limited to eruptions larger than 0.1 km3, because deviations from power-law magnitude–frequency relationships indicate strong relative under-sampling at smaller eruption volumes. In the southern SVZ, where ice unloading was greatest, eruption frequency in the early post-glacial period is approximately twice that of the mid post-glacial period (although frequency increases again in the late post-glacial). A comparable pattern occurs in Kamchatka, but is not observed in the Cascade arc. The early post-glacial period also coincides with a small number of very large explosive eruptions from the most active volcanoes in the southern and central SVZ, consistent with enhanced ponding of magma during glaciation and release upon deglaciation.

In comparison to non-arc settings, evidence of post-glacial increases in rates of arc volcanism is weak, and there is no need to invoke significantly increased melt production upon ice unloading, as occurred in areas such as Iceland. Non-arc volcanoes may therefore account for a relatively higher proportion of global volcanic emissions in the early post-glacial period than is suggested by the relative contributions of arc and non-arc settings at the present day.

The second part of this paper critically examines global eruption records, in an effort to constrain global-scale changes in volcanic output since the LGM. Accurate interpretation of these records relies on correcting both temporal and spatial variability in eruption recording. In particular, very low recording rates, which also vary spatially by over two orders of magnitude, prevent precise, and possibly even accurate, quantitative analysis. For example, if we assume record completeness for the past century, the number of known eruptions (volcanic explosivity index ≥ 2) from some low-latitude regions, such as Indonesia, is approximately 1 in 20,000 (0.005%) for the period 5–20 ka. There is a need for more regional-scale studies of past volcanism in such regions, where current data are extremely sparse. We attempt to correct for recording biases, and suggest a maximum two-fold (but potentially much less) increase in global eruption rates, relative to the present day, between 13 and 7 ka. Although volcanism may have been an important source of CO2 in the early Holocene, it is unlikely to have been a dominant control on changes in atmospheric CO2 after the LGM.

*Biostratigraphy: Interpretations of Oppel's zones*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825213001463[+https://www.sciencedirect.com/science/article/pii/S0012825213001463+]

Zones like those of Oppel and Hedberg's Oppel-Zone are commonly interpreted as rock units delimited temporally. A more restricted view is that they are rock units empirically defined by bioevents that occur in the same order in all sections. Methods used by Oppel and definitions proposed by Hedberg are reviewed to assess their adequacy for definition of biostratigraphic units and their ability to support temporal inferences. Although they are usually interpreted as chronostratigraphic units, Oppel defined his zones in stratigraphic space, without temporal reference. In contrast, Hedberg required that bioevents for his Oppel-Zone should be approximately isochronous across their distribution but provided no operational way to identify such bioevents. Neither author clearly indicated how boundaries should be defined. Recourse to a principle of biosynchroneity to support inferences that stratigraphically ordered bioevents are temporal markers conflicts with knowledge of the biogeographies of modern taxa. Evolutionary theory explains why some bioevents occur in the same stratigraphic order but does not support the inference that they are isochronous events. Since its inception biostratigraphy has focused on ordered classifications, like those of Oppel. Stratigraphic codes should allow for a complementary category of biofacies zones that reflect depositional environments and are not constrained to occur in a particular order.

*Virtual Geographic Environments (VGEs): A New Generation of Geographic Analysis Tool*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S001282521300127X#bi0005[+https://www.sciencedirect.com/science/article/pii/S001282521300127X#bi0005+]

Virtual Geographic Environments (VGEs) are proposed as a new generation of geographic analysis tool to contribute to human understanding of the geographic world and assist in solving geographic problems at a deeper level. The development of VGEs is focused on meeting the three scientific requirements of Geographic Information Science (GIScience) — multi-dimensional visualization, dynamic phenomenon simulation, and public participation. To provide a clearer image that improves user understanding of VGEs and to contribute to future scientific development, this article reviews several aspects of VGEs. First, the evolutionary process from maps to previous GISystems and then to VGEs is illustrated, with a particular focus on the reasons VGEs were created. Then, extended from the conceptual framework and the components of a complete VGE, three use cases are identified that together encompass the current state of VGEs at different application levels: 1) a tool for geo-object-based multi-dimensional spatial analysis and multi-channel interaction, 2) a platform for geo-process-based simulation of dynamic geographic phenomena, and 3) a workspace for multi-participant-based collaborative geographic experiments. Based on the above analysis, the differences between VGEs and other similar platforms are discussed to draw their clear boundaries. Finally, a short summary of the limitations of current VGEs is given, and future directions are proposed to facilitate ongoing progress toward forming a comprehensive version of VGEs.

*Gemstones and geosciences in space and time: Digital maps to the “Chessboard classification scheme of mineral deposits”*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825213001220[+https://www.sciencedirect.com/science/article/pii/S0012825213001220+]

The gemstones, covering the spectrum from jeweler's to showcase quality, have been presented in a tripartite subdivision, by country, geology and geomorphology realized in 99 digital maps with more than 2600 mineralized sites. The various maps were designed based on the “Chessboard classification scheme of mineral deposits” proposed by Dill (2010a, 2010b) to reveal the interrelations between gemstone deposits and mineral deposits of other commodities and direct our thoughts to potential new target areas for exploration. A number of 33 categories were used for these digital maps: chromium, nickel, titanium, iron, manganese, copper, tin–tungsten, beryllium, lithium, zinc, calcium, boron, fluorine, strontium, phosphorus, zirconium, silica, feldspar, feldspathoids, zeolite, amphibole (tiger's eye), olivine, pyroxenoid, garnet, epidote, sillimanite–andalusite, corundum–spinel − diaspore, diamond, vermiculite–pagodite, prehnite, sepiolite, jet, and amber. 

Besides the political base map (gems by country) the mineral deposit is drawn on a geological map, illustrating the main lithologies, stratigraphic units and tectonic structure to unravel the evolution of primary gemstone deposits in time and space. The geomorphological map is to show the control of climate and subaerial and submarine hydrography on the deposition of secondary gemstone deposits. The digital maps are designed so as to be plotted as a paper version of different scale and to upgrade them for an interactive use and link them to gemological databases.

*High-Frequency Radar Observations of Ocean Surface Currents*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121211-172315[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121211-172315+]

This article reviews the discovery, development, and use of high-frequency (HF) radio wave backscatter in oceanography. HF radars, as the instruments are commonly called, remotely measure ocean surface currents by exploiting a Bragg resonant backscatter phenomenon. Electromagnetic waves in the HF band (3–30 MHz) have wavelengths that are commensurate with wind-driven gravity waves on the ocean surface; the ocean waves whose wavelengths are exactly half as long as those of the broadcast radio waves are responsible for the resonant backscatter. Networks of HF radar systems are capable of mapping surface currents hourly out to ranges approaching 200 km with a horizontal resolution of a few kilometers. Such information has many uses, including search and rescue support and oil-spill mitigation in real time and larval population connectivity assessment when viewed over many years. Today, HF radar networks form the backbone of many ocean observing systems, and the data are assimilated into ocean circulation models.

*Lunar laser ranging: the millimeter challenge*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/76/7/076901[+https://iopscience.iop.org/article/10.1088/0034-4885/76/7/076901+]

Lunar laser ranging has provided many of the best tests of gravitation since the first Apollo astronauts landed on the Moon. The march to higher precision continues to this day, now entering the millimeter regime, and promising continued improvement in scientific results. This review introduces key aspects of the technique, details the motivations, observables, and results for a variety of science objectives, summarizes the current state of the art, highlights new developments in the field, describes the modeling challenges, and looks to the future of the enterprise.

*Geodetic imaging with airborne LiDAR: the Earth's surface revealed*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/76/8/086801[+https://iopscience.iop.org/article/10.1088/0034-4885/76/8/086801+]

The past decade has seen an explosive increase in the number of peer reviewed papers reporting new scientific findings in geomorphology (including fans, channels, floodplains and landscape evolution), geologic mapping, tectonics and faulting, coastal processes, lava flows, hydrology (especially snow and runoff routing), glaciers and geo-archaeology. A common genesis of such findings is often newly available decimeter resolution 'bare Earth' geodetic images, derived from airborne laser swath mapping, a.k.a. airborne LiDAR, observations. In this paper we trace nearly a half century of advances in geodetic science made possible by space age technology, such as the invention of short-pulse-length high-pulse-rate lasers, solid state inertial measurement units, chip-based high speed electronics and the GPS satellite navigation system, that today make it possible to map hundreds of square kilometers of terrain in hours, even in areas covered with dense vegetation or shallow water. To illustrate the impact of the LiDAR observations we present examples of geodetic images that are not only stunning to the eye, but help researchers to develop quantitative models explaining how terrain evolved to its present form, and how it will likely change with time. Airborne LiDAR technology continues to develop quickly, promising ever more scientific discoveries in the years ahead.

*Synthetic biological networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/76/9/096602[+https://iopscience.iop.org/article/10.1088/0034-4885/76/9/096602+]

Despite their obvious relationship and overlap, the field of physics is blessed with many insightful laws, while such laws are sadly absent in biology. Here we aim to discuss how the rise of a more recent field known as synthetic biology may allow us to more directly test hypotheses regarding the possible design principles of natural biological networks and systems. In particular, this review focuses on synthetic gene regulatory networks engineered to perform specific functions or exhibit particular dynamic behaviors. Advances in synthetic biology may set the stage to uncover the relationship of potential biological principles to those developed in physics.

2014
----

*Control: A Perspective* - Karl J.A˚ström et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0005109813005037[+https://www.sciencedirect.com/science/article/pii/S0005109813005037+]

Feedback is an ancient idea, but feedback control is a young field. Nature long ago discovered feedback since it is essential for homeostasis and life. It was the key for harnessing power in the industrial revolution and is today found everywhere around us. Its development as a field involved contributions from engineers, mathematicians, economists and physicists. It is the first systems discipline; it represented a paradigm shift because it cut across the traditional engineering disciplines of aeronautical, chemical, civil, electrical and mechanical engineering, as well as economics and operations research. The scope of control makes it the quintessential multidisciplinary field. Its complex story of evolution is fascinating, and a perspective on its growth is presented in this paper. The interplay of industry, applications, technology, theory and research is discussed.

*Synchronization in complex networks of phase oscillators: A survey* - Florian Dorfler et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0005109814001423[+https://www.sciencedirect.com/science/article/pii/S0005109814001423+]

The emergence of synchronization in a network of coupled oscillators is a fascinating subject of multidisciplinary research. This survey reviews the vast literature on the theory and the applications of complex oscillator networks. We focus on phase oscillator models that are widespread in real-world synchronization phenomena, that generalize the celebrated Kuramoto model, and that feature a rich phenomenology. We review the history and the countless applications of this model throughout science and engineering. We justify the importance of the widespread coupled oscillator model as a locally canonical model and describe some selected applications relevant to control scientists, including vehicle coordination, electric power networks, and clock synchronization. We introduce the reader to several synchronization notions and performance estimates. We propose analysis approaches to phase and frequency synchronization, phase balancing, pattern formation, and partial synchronization. We present the sharpest known results about synchronization in networks of homogeneous and heterogeneous oscillators, with complete or sparse interconnection topologies, and in finite-dimensional and infinite-dimensional settings. We conclude by summarizing the limitations of existing analysis methods and by highlighting some directions for future research.

*The role of palaeogeography in the Phanerozoic history of atmospheric CO2 and climate*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825213001918[+https://www.sciencedirect.com/science/article/pii/S0012825213001918+]

The role of the palaeogeography on the geological evolution of the global carbon cycle has been suspected since the development of the first global geochemical models in the early 80s. The palaeogeography has been rapidly recognized as a key factor controlling the long-term evolution of the atmospheric CO2 through its capability of modulating the efficiency of the silicate weathering. First the role of the latitudinal position of the continents has been emphasized: an averaged low latitudinal position promotes the CO2 consumption by silicate weathering, and is theoretically associated to low CO2 periods.

With the increase of model complexity and the explicit consideration of the hydrological cycle, the importance of the continentality factor has been recognized: periods of supercontinent assembly coincide with high pCO2 values due to the development of arid conditions which weaken the silicate weathering efficiency. These fundamental feedbacks between climate, carbon cycle and tectonic have been discovered by pioneer modelling studies and opened new views in the understanding of the history of Earth's climate. Today, some of the key features of the Phanerozoic climate can be explained by: (1) continental drift; (2) small continental blocks moving to tropical belts; and (3) modulation of the climate sensitivity to CO2 by palaeogeography changes. Those results emphasize the need for a careful process-based modelling of the water cycle and climate response to the continental drift.

*Arctic Ocean glacial history*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0277379113002989[+https://www.sciencedirect.com/science/article/pii/S0277379113002989+]

While there are numerous hypotheses concerning glacial–interglacial environmental and climatic regime shifts in the Arctic Ocean, a holistic view on the Northern Hemisphere's late Quaternary ice-sheet extent and their impact on ocean and sea-ice dynamics remains to be established. Here we aim to provide a step in this direction by presenting an overview of Arctic Ocean glacial history, based on the present state-of-the-art knowledge gained from field work and chronological studies, and with a specific focus on ice-sheet extent and environmental conditions during the Last Glacial Maximum (LGM).

The maximum Quaternary extension of ice sheets is discussed and compared to LGM. We bring together recent results from the circum-Arctic continental margins and the deep central basin; extent of ice sheets and ice streams bordering the Arctic Ocean as well as evidence for ice shelves extending into the central deep basin. Discrepancies between new results and published LGM ice-sheet reconstructions in the high Arctic are highlighted and outstanding questions are identified. Finally, we address the ability to simulate the Arctic Ocean ice sheet complexes and their dynamics, including ice streams and ice shelves, using presently available ice-sheet models. Our review shows that while we are able to firmly reject some of the earlier hypotheses formulated to describe Arctic Ocean glacial conditions, we still lack information from key areas to compile the holistic Arctic Ocean glacial history.

*Interglacials, Milankovitch Cycles, Solar Activity, and Carbon Dioxide*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.hindawi.com/journals/jcli/2014/345482/[+https://www.hindawi.com/journals/jcli/2014/345482/+]

The existing understanding of interglacial periods is that they are initiated by Milankovitch cycles enhanced by rising atmospheric carbon dioxide concentrations. During interglacials, global temperature is also believed to be primarily controlled by carbon dioxide concentrations, modulated by internal processes such as the Pacific Decadal Oscillation and the North Atlantic Oscillation. Recent work challenges the fundamental basis of these conceptions.

*Fractional calculus view of complexity: A tutorial*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1169[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.86.1169+]

The fractional calculus has been part of the mathematics and science literature for 310 years. However, it is only in the past decade or so that it has drawn the attention of mainstream science as a way to describe the dynamics of complex phenomena with long-term memory, spatial heterogeneity, along with nonstationary and nonergodic statistics. The most recent application encompasses complex networks, which require new ways of thinking about the world. Part of the new cognition is provided by the fractional calculus description of temporal and topological complexity. Consequently, this Colloquium is not so much a tutorial on the mathematics of the fractional calculus as it is an exploration of how complex phenomena in the physical, social, and life sciences that have eluded traditional mathematical modeling become less mysterious when certain historical assumptions such as differentiability are discarded and the ordinary calculus is replaced with the fractional calculus. Exemplars considered include the fractional differential equations describing the dynamics of viscoelastic materials, turbulence, foraging, and phase transitions in complex social networks.

*Noise in biology*
~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/2/026601[+https://iopscience.iop.org/article/10.1088/0034-4885/77/2/026601+]

Noise permeates biology on all levels, from the most basic molecular, sub-cellular processes to the dynamics of tissues, organs, organisms and populations. The functional roles of noise in biological processes can vary greatly. Along with standard, entropy-increasing effects of producing random mutations, diversifying phenotypes in isogenic populations, limiting information capacity of signaling relays, it occasionally plays more surprising constructive roles by accelerating the pace of evolution, providing selective advantage in dynamic environments, enhancing intracellular transport of biomolecules and increasing information capacity of signaling pathways. This short review covers the recent progress in understanding mechanisms and effects of fluctuations in biological systems of different scales and the basic approaches to their mathematical modeling.

*Tensegrity, cellular biophysics, and the mechanics of living systems*

https://iopscience.iop.org/article/10.1088/0034-4885/77/4/046603[+https://iopscience.iop.org/article/10.1088/0034-4885/77/4/046603+]

The recent convergence between physics and biology has led many physicists to enter the fields of cell and developmental biology. One of the most exciting areas of interest has been the emerging field of mechanobiology that centers on how cells control their mechanical properties, and how physical forces regulate cellular biochemical responses, a process that is known as mechanotransduction. In this article, we review the central role that tensegrity (tensional integrity) architecture, which depends on tensile prestress for its mechanical stability, plays in biology. We describe how tensional prestress is a critical governor of cell mechanics and function, and how use of tensegrity by cells contributes to mechanotransduction. Theoretical tensegrity models are also described that predict both quantitative and qualitative behaviors of living cells, and these theoretical descriptions are placed in context of other physical models of the cell. In addition, we describe how tensegrity is used at multiple size scales in the hierarchy of life—from individual molecules to whole living organisms—to both stabilize three-dimensional form and to channel forces from the macroscale to the nanoscale, thereby facilitating mechanochemical conversion at the molecular level.

*How to deal with petabytes of data: the LHC Grid project*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/6/065902[+https://iopscience.iop.org/article/10.1088/0034-4885/77/6/065902+]

We review the Grid computing system developed by the international community to deal with the petabytes of data coming from the Large Hadron Collider at CERN in Geneva with particular emphasis on the ATLAS experiment and the UK Grid project, GridPP. Although these developments were started over a decade ago, this article explains their continued relevance as part of the 'Big Data' problem and how the Grid has been forerunner of today's cloud computing.

*The three-body problem*
~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/6/065901[+https://iopscience.iop.org/article/10.1088/0034-4885/77/6/065901+]

The three-body problem, which describes three masses interacting through Newtonian gravity without any restrictions imposed on the initial positions and velocities of these masses, has attracted the attention of many scientists for more than 300 years. In this paper, we present a review of the three-body problem in the context of both historical and modern developments. We describe the general and restricted (circular and elliptic) three-body problems, different analytical and numerical methods of finding solutions, methods for performing stability analysis and searching for periodic orbits and resonances. We apply the results to some interesting problems of celestial mechanics. We also provide a brief presentation of the general and restricted relativistic three-body problems, and discuss their astronomical applications.

*Physics and financial economics (1776–2014): puzzles, Ising and agent-based models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/6/062001[+https://iopscience.iop.org/article/10.1088/0034-4885/77/6/062001+]

This short review presents a selected history of the mutual fertilization between physics and economics—from Isaac Newton and Adam Smith to the present. The fundamentally different perspectives embraced in theories developed in financial economics compared with physics are dissected with the examples of the volatility smile and of the excess volatility puzzle. The role of the Ising model of phase transitions to model social and financial systems is reviewed, with the concepts of random utilities and the logit model as the analog of the Boltzmann factor in statistical physics. Recent extensions in terms of quantum decision theory are also covered. A wealth of models are discussed briefly that build on the Ising model and generalize it to account for the many stylized facts of financial markets. A summary of the relevance of the Ising model and its extensions is provided to account for financial bubbles and crashes. The review would be incomplete if it did not cover the dynamical field of agent-based models (ABMs), also known as computational economic models, of which the Ising-type models are just special ABM implementations. We formulate the 'Emerging Intelligence Market Hypothesis' to reconcile the pervasive presence of 'noise traders' with the near efficiency of financial markets. Finally, we note that evolutionary biology, more than physics, is now playing a growing role to inspire models of financial markets.

*The physics of anomalous ('rogue') ocean waves*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/10/105901[+https://iopscience.iop.org/article/10.1088/0034-4885/77/10/105901+]

There is much speculation that the largest and steepest waves may need to be modelled with different physics to the majority of the waves on the open ocean. This review examines the various physical mechanisms which may play an important role in the dynamics of extreme waves. We examine the evidence for these mechanisms in numerical and physical wavetanks, and look at the evidence that such mechanisms might also exist in the real ocean.

*Non-equilibrium physics and evolution—adaptation, extinction, and ecology: a Key Issues review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/10/102602[+https://iopscience.iop.org/article/10.1088/0034-4885/77/10/102602+]

Evolutionary dynamics in nature constitute an immensely complex non-equilibrium process. We review the application of physical models of evolution, by focusing on adaptation, extinction, and ecology. In each case, we examine key concepts by working through examples. Adaptation is discussed in the context of bacterial evolution, with a view toward the relationship between growth rates, mutation rates, selection strength, and environmental changes. Extinction dynamics for an isolated population are reviewed, with emphasis on the relation between timescales of extinction, population size, and temporally correlated noise. Ecological models are discussed by focusing on the effect of spatial interspecies interactions on diversity. Connections between physical processes—such as diffusion, turbulence, and localization—and evolutionary phenomena are highlighted.

*Stochastic Climate Theory and Modelling*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1409.0423[+https://arxiv.org/abs/1409.0423+]

Stochastic methods are a crucial area in contemporary climate research and are increasingly being used in comprehensive weather and climate prediction models as well as reduced order climate models. Stochastic methods are used as subgrid-scale parameterizations as well as for model error representation, uncertainty quantification, data assimilation and ensemble prediction. The need to use stochastic approaches in weather and climate models arises because we still cannot resolve all necessary processes and scales in comprehensive numerical weather and climate prediction models. In many practical applications one is mainly interested in the largest and potentially predictable scales and not necessarily in the small and fast scales. For instance, reduced order models can simulate and predict large scale modes. Statistical mechanics and dynamical systems theory suggest that in reduced order models the impact of unresolved degrees of freedom can be represented by suitable combinations of deterministic and stochastic components and non-Markovian (memory) terms. Stochastic approaches in numerical weather and climate prediction models also lead to the reduction of model biases. Hence, there is a clear need for systematic stochastic approaches in weather and climate modelling. In this review we present evidence for stochastic effects in laboratory experiments. Then we provide an overview of stochastic climate theory from an applied mathematics perspectives. We also survey the current use of stochastic methods in comprehensive weather and climate prediction models and show that stochastic parameterizations have the potential to remedy many of the current biases in these comprehensive models.

*Saving Human Lives: What Complexity Science and Information Systems can Contribute*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1402.7011[+https://arxiv.org/abs/1402.7011+]

We discuss models and data of crowd disasters, crime, terrorism, war and disease spreading to show that conventional recipes, such as deterrence strategies, are often not effective and sufficient to contain them. Many common approaches do not provide a good picture of the actual system behavior, because they neglect feedback loops, instabilities and cascade effects. The complex and often counter-intuitive behavior of social systems and their macro-level collective dynamics can be better understood by means of complexity science. We highlight that a suitable system design and management can help to stop undesirable cascade effects and to enable favorable kinds of self-organization in the system. In such a way, complexity science can help to save human lives.

*The Matthew effect in empirical data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1408.5124[+https://arxiv.org/abs/1408.5124+]

"The Matthew effect describes the phenomenon that in societies the rich tend to get richer and the potent even more powerful. It is closely related to the concept of preferential attachment in network science, where the more connected nodes are destined to acquire many more links in the future than the auxiliary nodes. Cumulative advantage and success-breads-success also both describe the fact that advantage tends to beget further advantage. The concept is behind the many power laws and scaling behaviour in empirical data, and it is at the heart of self-organization across social and natural sciences. Here we review the methodology for measuring preferential attachment in empirical data, as well as the observations of the Matthew effect in patterns of scientific collaboration, socio-technical and biological networks, the propagation of citations, the emergence of scientific progress and impact, career longevity, the evolution of common English words and phrases, as well as in education and brain development. We also discuss whether the Matthew effect is due to chance or optimisation, for example related to homophily in social systems or efficacy in technological systems, and we outline possible directions for future research."

2015
----

*Theory of Machines through the 20th century* - J. S. Rao
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0094114X14002006[+https://www.sciencedirect.com/science/article/pii/S0094114X14002006+]

This paper is written in honor of Professor Dr. F. R. Erskine Crossley, Professor Emeritus, Architect and Founder Member of the International Federation for Promotion of Theory of Mechanisms and Machines (IFToMM) and its first Vice President on his achieving 100 years of age on 21st July 2015 with glorious service to Kinematics and Kinetics Community of the world. This paper deals with two parts viz., 1. My association with him in the 1960's during the formation of IFToMM and 2. Kinematics and Kinetics, the way this subject is developed in the second half of the 20th century and practiced today; a subject matter very close to Professor Crossley's heart.

*Statistical inference for dynamical systems: A review* - K. McGoff et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://projecteuclid.org/euclid.ssu/1447165229[+https://projecteuclid.org/euclid.ssu/1447165229+]

The topic of statistical inference for dynamical systems has been studied widely across several fields. In this survey we focus on methods related to parameter estimation for nonlinear dynamical systems. Our objective is to place results across distinct disciplines in a common setting and highlight opportunities for further research.

*The vector algebra war: a historical perspective*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1509.00501[+https://arxiv.org/abs/1509.00501+]

There are a wide variety of different vector formalisms currently utilized in engineering and physics. For example, Gibbs' three-vectors, Minkowski four-vectors, complex spinors in quantum mechanics, quaternions used to describe rigid body rotations and vectors defined in Clifford geometric algebra. With such a range of vector formalisms in use, it thus appears that there is as yet no general agreement on a vector formalism suitable for science as a whole. This is surprising, in that, one of the primary goals of nineteenth century science was to suitably describe vectors in three-dimensional space. This situation has also had the unfortunate consequence of fragmenting knowledge across many disciplines, and requiring a significant amount of time and effort in learning the various formalisms. We thus historically review the development of our various vector systems and conclude that Clifford's multivectors best fulfills the goal of describing vectorial quantities in three dimensions and providing a unified vector system for science.

*Graph theory in the geosciences*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825215000239[+https://www.sciencedirect.com/science/article/pii/S0012825215000239+]

Graph theory has long been used in quantitative geography and landscape ecology and has been applied in Earth and atmospheric sciences for several decades. Recently, however, there have been increased, and more sophisticated, applications of graph theory concepts and methods in geosciences, principally in three areas: spatially explicit modeling, small-world networks, and structural models of Earth surface systems. This paper reviews the contrasting goals and methods inherent in these approaches, but focuses on the common elements, to develop a synthetic view of graph theory in the geosciences.

Techniques applied in geosciences are mainly of three types: connectivity measures of entire networks; metrics of various aspects of the importance or influence of particular nodes, links, or regions of the network; and indicators of system dynamics based on graph adjacency matrices. Geoscience applications of graph theory can be grouped in five general categories: (1) Quantification of complex network properties such as connectivity, centrality, and clustering; (2) Tests for evidence of particular types of structures that have implications for system behavior, such as small-world or scale-free networks; (3) Testing dynamical system properties, e.g., complexity, coherence, stability, synchronization, and vulnerability; (4) Identification of dynamics from historical records or time series; and (5) spatial analysis. Recent and future expansion of graph theory in geosciences is related to general growth of network-based approaches. However, several factors make graph theory especially well suited to the geosciences: Inherent complexity, exploration of very large data sets, focus on spatial fluxes and interactions, and increasing attention to state transitions are all amenable to analysis using graph theory approaches.

*Ice streams in the Laurentide Ice Sheet*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825215000203[+https://www.sciencedirect.com/science/article/pii/S0012825215000203+]

This paper presents a comprehensive review and synthesis of ice streams in the Laurentide Ice Sheet (LIS) based on a new mapping inventory that includes previously hypothesised ice streams and includes a concerted effort to search for others from across the entire ice sheet bed. The inventory includes 117 ice streams, which have been identified based on a variety of evidence including their bedform imprint, large-scale geomorphology/topography, till properties, and ice rafted debris in ocean sediment records. Despite uncertainty in identifying ice streams in hard bedrock areas, it is unlikely that any major ice streams have been missed. During the Last Glacial Maximum, Laurentide ice streams formed a drainage pattern that bears close resemblance to the present day velocity patterns in modern ice sheets. Large ice streams had extensive onset zones and were fed by multiple tributaries and, where ice drained through regions of high relief, the spacing of ice streams shows a degree of spatial self-organisation which has hitherto not been recognised. Topography exerted a primary control on the location of ice streams, but there were large areas along the western and southern margin of the ice sheet where the bed was composed of weaker sedimentary bedrock, and where networks of ice streams switched direction repeatedly and probably over short time scales.

As the ice sheet retreated onto its low relief interior, several ice streams show no correspondence with topography or underlying geology, perhaps facilitated by localised build-up of pressurised subglacial meltwater. They differed from most other ice stream tracks in having much lower length-to-width ratios and have no modern analogues. There have been very few attempts to date the initiation and cessation of ice streams, but it is clear that ice streams switched on and off during deglaciation, rather than maintaining the same trajectory as the ice margin retreated. We provide a first order estimate of changes in ice stream activity during deglaciation and show that around 30% of the margin was drained by ice streams at the LGM (similar to that for present day Antarctic ice sheets), but this decreases to 15% and 12% at 12 cal ka BP and 10 cal ka BP, respectively. The extent to which these changes in the ice stream drainage network represent a simple and predictable readjustment to a changing mass balance driven by climate, or internal ice dynamical feedbacks unrelated to climate (or both) is largely unknown and represents a key area for future work to address.

*A review of non-stationarities in climate variability of the last century*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825215000756[+https://www.sciencedirect.com/science/article/pii/S0012825215000756+]

Non-stationarities are inherent characteristics of the climate system and can be observed on different temporal and spatial scales. Non-stationarities in large-scale modes of climate variability of the Northern Hemisphere and the associated changes in the surface climate like modifications of the temperature and precipitation patterns are analysed. As major modes of climate variability the El Niño–Southern Oscillation phenomenon, the Pacific–North American pattern and the North Atlantic Oscillation are selected. The North Atlantic–European area is taken as an example to highlight non-stationarities in the atmospheric circulation and their impact on regional climate. The mechanisms and consequences of circulation-climate non-stationarities are discussed.

*Theoretical foundation of cyclostationary EOF analysis for geophysical and climatic variables*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825215300040[+https://www.sciencedirect.com/science/article/pii/S0012825215300040+]

Natural variability is an essential component of observations of all geophysical and climate variables. In principal component analysis (PCA), also called empirical orthogonal function (EOF) analysis, a set of orthogonal eigenfunctions is found from a spatial covariance function. These empirical basis functions often lend useful insights into physical processes in the data and serve as a useful tool for developing statistical methods. The underlying assumption in PCA is the stationarity of the data analyzed; that is, the covariance function does not depend on the origin of time. The stationarity assumption is often not justifiable for geophysical and climate variables even after removing such cyclic components as the diurnal cycle or the annual cycle. As a result, physical and statistical inferences based on EOFs can be misleading.

Some geophysical and climatic variables exhibit periodically time-dependent covariance statistics. Such a dataset is said to be periodically correlated or cyclostationary. A proper recognition of the time-dependent response characteristics is vital in accurately extracting physically meaningful modes and their space–time evolutions from data. This also has important implications in finding physically consistent evolutions and teleconnection patterns and in spectral analysis of variability—important goals in many climate and geophysical studies. In this study, the conceptual foundation of cyclostationary EOF (CSEOF) analysis is examined as an alternative to regular EOF analysis or other eigenanalysis techniques based on the stationarity assumption. Comparative examples and illustrations are given to elucidate the conceptual difference between the CSEOF technique and other techniques and the entailing ramification in physical and statistical inferences based on computational eigenfunctions.

*GRACE, time-varying gravity, Earth system dynamics and climate change*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/77/11/116801[+https://iopscience.iop.org/article/10.1088/0034-4885/77/11/116801+]

Continuous observations of temporal variations in the Earth's gravity field have recently become available at an unprecedented resolution of a few hundreds of kilometers. The gravity field is a product of the Earth's mass distribution, and these data—provided by the satellites of the Gravity Recovery And Climate Experiment (GRACE)—can be used to study the exchange of mass both within the Earth and at its surface. Since the launch of the mission in 2002, GRACE data has evolved from being an experimental measurement needing validation from ground truth, to a respected tool for Earth scientists representing a fixed bound on the total change and is now an important tool to help unravel the complex dynamics of the Earth system and climate change. In this review, we present the mission concept and its theoretical background, discuss the data and give an overview of the major advances GRACE has provided in Earth science, with a focus on hydrology, solid Earth sciences, glaciology and oceanography.

*Greenland ice sheet mass balance: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/78/4/046801[+https://iopscience.iop.org/article/10.1088/0034-4885/78/4/046801+]

Over the past quarter of a century the Arctic has warmed more than any other region on Earth, causing a profound impact on the Greenland ice sheet (GrIS) and its contribution to the rise in global sea level. The loss of ice can be partitioned into processes related to surface mass balance and to ice discharge, which are forced by internal or external (atmospheric/oceanic/basal) fluctuations. Regardless of the measurement method, observations over the last two decades show an increase in ice loss rate, associated with speeding up of glaciers and enhanced melting. However, both ice discharge and melt-induced mass losses exhibit rapid short-term fluctuations that, when extrapolated into the future, could yield erroneous long-term trends. In this paper we review the GrIS mass loss over more than a century by combining satellite altimetry, airborne altimetry, interferometry, aerial photographs and gravimetry data sets together with modelling studies. We revisit the mass loss of different sectors and show that they manifest quite different sensitivities to atmospheric and oceanic forcing. In addition, we discuss recent progress in constructing coupled ice-ocean-atmosphere models required to project realistic future sea-level changes.

*The physics of Martian weather and climate: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/78/12/125901[+https://iopscience.iop.org/article/10.1088/0034-4885/78/12/125901+]

The planet Mars hosts an atmosphere that is perhaps the closest in terms of its meteorology and climate to that of the Earth. But Mars differs from Earth in its greater distance from the Sun, its smaller size, its lack of liquid oceans and its thinner atmosphere, composed mainly of CO2. These factors give Mars a rather different climate to that of the Earth. In this article we review various aspects of the martian climate system from a physicist's viewpoint, focusing on the processes that control the martian environment and comparing these with corresponding processes on Earth. These include the radiative and thermodynamical processes that determine the surface temperature and vertical structure of the atmosphere, the fluid dynamics of its atmospheric motions, and the key cycles of mineral dust and volatile transport. In many ways, the climate of Mars is as complicated and diverse as that of the Earth, with complex nonlinear feedbacks that affect its response to variations in external forcing. Recent work has shown that the martian climate is anything but static, but is almost certainly in a continual state of transient response to slowly varying insolation associated with cyclic variations in its orbit and rotation. We conclude with a discussion of the physical processes underlying these long- term climate variations on Mars, and an overview of some of the most intriguing outstanding problems that should be a focus for future observational and theoretical studies.

*Soft matter food physics—the physics of food and cooking*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/78/12/124602[+https://iopscience.iop.org/article/10.1088/0034-4885/78/12/124602+]

This review discusses the (soft matter) physics of food. Although food is generally not considered as a typical model system for fundamental (soft matter) physics, a number of basic principles can be found in the interplay between the basic components of foods, water, oil/fat, proteins and carbohydrates. The review starts with the introduction and behavior of food-relevant molecules and discusses food-relevant properties and applications from their fundamental (multiscale) behavior. Typical food aspects from 'hard matter systems', such as chocolates or crystalline fats, to 'soft matter' in emulsions, dough, pasta and meat are covered and can be explained on a molecular basis. An important conclusion is the point that the macroscopic properties and the perception are defined by the molecular interplay on all length and time scales.

*On Holo-Hilbert spectral analysis: a full informational spectral representation for nonlinear and non-stationary data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0206[+https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0206+]

The Holo-Hilbert spectral analysis (HHSA) method is introduced to cure the deficiencies of traditional spectral analysis and to give a full informational representation of nonlinear and non-stationary data. It uses a nested empirical mode decomposition and Hilbert–Huang transform (HHT) approach to identify intrinsic amplitude and frequency modulations often present in nonlinear systems. Comparisons are first made with traditional spectrum analysis, which usually achieved its results through convolutional integral transforms based on additive expansions of an a priori determined basis, mostly under linear and stationary assumptions. Thus, for non-stationary processes, the best one could do historically was to use the time–frequency representations, in which the amplitude (or energy density) variation is still represented in terms of time. For nonlinear processes, the data can have both amplitude and frequency modulations (intra-mode and inter-mode) generated by two different mechanisms: linear additive or nonlinear multiplicative processes. As all existing spectral analysis methods are based on additive expansions, either a priori or adaptive, none of them could possibly represent the multiplicative processes. While the earlier adaptive HHT spectral analysis approach could accommodate the intra-wave nonlinearity quite remarkably, it remained that any inter-wave nonlinear multiplicative mechanisms that include cross-scale coupling and phase-lock modulations were left untreated. To resolve the multiplicative processes issue, additional dimensions in the spectrum result are needed to account for the variations in both the amplitude and frequency modulations simultaneously. HHSA accommodates all the processes: additive and multiplicative, intra-mode and inter-mode, stationary and non-stationary, linear and nonlinear interactions. The Holo prefix in HHSA denotes a multiple dimensional representation with both additive and multiplicative capabilities.

*Principal component analysis: a review and recent developments*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202[+https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202+]

Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori, hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.

*Understanding deep convolutional networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0203[+https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0203+]

Deep convolutional networks provide state-of-the-art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and nonlinearities. A mathematical framework is introduced to analyse their properties. Computations of invariants involve multiscale contractions with wavelets, the linearization of hierarchical symmetries and sparse separations. Applications are discussed.

*Computational sciences in the upstream oil and gas industry*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0429[+https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0429+]

The predominant technical challenge of the upstream oil and gas industry has always been the fundamental uncertainty of the subsurface from which it produces hydrocarbon fluids. The subsurface can be detected remotely by, for example, seismic waves, or it can be penetrated and studied in the extremely limited vicinity of wells. Inevitably, a great deal of uncertainty remains. Computational sciences have been a key avenue to reduce and manage this uncertainty. In this review, we discuss at a relatively non-technical level the current state of three applications of computational sciences in the industry. The first of these is seismic imaging, which is currently being revolutionized by the emergence of full wavefield inversion, enabled by algorithmic advances and petascale computing. The second is reservoir simulation, also being advanced through the use of modern highly parallel computing architectures. Finally, we comment on the role of data analytics in the upstream industry.

*Probabilistic Integration: A Role in Statistical Computation?*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1512.00933[+https://arxiv.org/abs/1512.00933+]

https://arxiv.org/abs/1811.10275[+https://arxiv.org/abs/1811.10275+]

A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.

*The Hydrogen Atom: a Review on the Birth of Modern Quantum Mechanics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1501.05894[+https://arxiv.org/abs/1501.05894+]

The purpose of this work is to retrace the steps that were made by scientists of XX century, like Bohr, Schrodinger, Heisenberg, Pauli, Dirac, for the formulation of what today represents the modern quantum mechanics and that, within two decades, put in question the classical physics. In this context, the study of the electronic structure of hydrogen atom has been the main starting point for the formulation of the theory and, till now, remains the only real case for which the quantum equation of motion can be solved exactly. The results obtained by each theory will be discussed critically, highlighting limits and potentials that allowed the further development of the quantum theory.

*Energy Harvesting Wireless Communications: A Review of Recent Advances*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1501.06026[+https://arxiv.org/abs/1501.06026+]

This article summarizes recent contributions in the broad area of energy harvesting wireless communications. In particular, we provide the current state of the art for wireless networks composed of energy harvesting nodes, starting from the information-theoretic performance limits to transmission scheduling policies and resource allocation, medium access and networking issues. The emerging related area of energy transfer for self-sustaining energy harvesting wireless networks is considered in detail covering both energy cooperation aspects and simultaneous energy and information transfer. Various potential models with energy harvesting nodes at different network scales are reviewed as well as models for energy consumption at the nodes.

*A comparative review of generalizations of the Gumbel extreme value distribution with an application to wind speed data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1502.02708[+https://arxiv.org/abs/1502.02708+]

https://en.wikipedia.org/wiki/Gumbel_distribution[+https://en.wikipedia.org/wiki/Gumbel_distribution+]

https://arxiv.org/abs/1902.07963[+https://arxiv.org/abs/1902.07963+]

The generalized extreme value distribution and its particular case, the Gumbel extreme value distribution, are widely applied for extreme value analysis. The Gumbel distribution has certain drawbacks because it is a non-heavy-tailed distribution and is characterized by constant skewness and kurtosis. The generalized extreme value distribution is frequently used in this context because it encompasses the three possible limiting distributions for a normalized maximum of infinite samples of independent and identically distributed observations. However, the generalized extreme value distribution might not be a suitable model when each observed maximum does not come from a large number of observations. Hence, other forms of generalizations of the Gumbel distribution might be preferable. Our goal is to collect in the present literature the distributions that contain the Gumbel distribution embedded in them and to identify those that have flexible skewness and kurtosis, are heavy-tailed and could be competitive with the generalized extreme value distribution. The generalizations of the Gumbel distribution are described and compared using an application to a wind speed data set and Monte Carlo simulations. We show that some distributions suffer from overparameterization and coincide with other generalized Gumbel distributions with a smaller number of parameters, i.e., are non-identifiable. Our study suggests that the generalized extreme value distribution and a mixture of two extreme value distributions should be considered in practical applications.


*Review of Some Promising Fractional Physical Models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1502.07681[+https://arxiv.org/abs/1502.07681+]

Fractional dynamics is a field of study in physics and mechanics investigating the behavior of objects and systems that are characterized by power-law non-locality, power-law long-term memory or fractal properties by using integrations and differentiation of non-integer orders, i.e., by methods of the fractional calculus. This paper is a review of physical models that look very promising for future development of fractional dynamics. We suggest a short introduction to fractional calculus as a theory of integration and differentiation of non-integer order. Some applications of integro-differentiations of fractional orders in physics are discussed. Models of discrete systems with memory, lattice with long-range inter-particle interaction, dynamics of fractal media are presented. Quantum analogs of fractional derivatives and model of open nano-system systems with memory are also discussed.

*Moment Closure - A Brief Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1505.02190[+https://arxiv.org/abs/1505.02190+]

Moment closure methods appear in myriad scientific disciplines in the modelling of complex systems. The goal is to achieve a closed form of a large, usually even infinite, set of coupled differential (or difference) equations. Each equation describes the evolution of one "moment", a suitable coarse-grained quantity computable from the full state space. If the system is too large for analytical and/or numerical methods, then one aims to reduce it by finding a moment closure relation expressing "higher-order moments" in terms of "lower-order moments". In this brief review, we focus on highlighting how moment closure methods occur in different contexts. We also conjecture via a geometric explanation why it has been difficult to rigorously justify many moment closure approximations although they work very well in practice.

*Natural Data Storage: A Review on sending Information from now to then via Nature*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1505.04890[+https://arxiv.org/abs/1505.04890+]

Digital data explosion drives a demand for robust and reliable data storage medium. Development of better digital storage device to accumulate Zetta bytes (1 ZB = 1021 bytes ) of data that will be generated in near future is a big challenge. Looking at limitations of present day digital storage devices, it will soon be a big challenge for data scientists to provide reliable. affordable and dense storage medium. As an alternative, researcher used natural medium of storage like DNA, bacteria and protein as information storage systems. This article discuss DNA based information storage system in detail along with an overview about bacterial and protein data storage systems.

*Soft Computing Techniques for Change Detection in remotely sensed images : A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.00768[+https://arxiv.org/abs/1506.00768+]

With the advent of remote sensing satellites, a huge repository of remotely sensed images is available. Change detection in remotely sensed images has been an active research area as it helps us understand the transitions that are taking place on the Earths surface. This paper discusses the methods and their classifications proposed by various researchers for change detection. Since use of soft computing based techniques are now very popular among research community, this paper also presents a classification based on learning techniques used in soft-computing methods for change detection.

*Reviewing Data Visualization: an Analytical Taxonomical Study*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.02976[+https://arxiv.org/abs/1506.02976+]

This paper presents an analytical taxonomy that can suitably describe, rather than simply classify, techniques for data presentation. Unlike previous works, we do not consider particular aspects of visualization techniques, but their mechanisms and foundational vision perception. Instead of just adjusting visualization research to a classification system, our aim is to better understand its process. For doing so, we depart from elementary concepts to reach a model that can describe how visualization techniques work and how they convey meaning.

*Specialty Fibers for Terahertz Generation and Transmission: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.03175[+https://arxiv.org/abs/1506.03175+]

Terahertz (THz) frequency range, lying between the optical and microwave range covers a significant portion of the electro-magnetic spectrum. Though its initial usage started in the 1960s, active research in the THz field started only in the 1990s by researchers from both optics and microwaves disciplines. The use of optical fibers for THz application has attracted considerable attention in recent years. In this article, we review the progress and current status of optical fiber-based techniques for THz generation and transmission. The first part of this review focuses on THz sources. After a review on various types of THz sources, we discuss how specialty optical fibers can be used for THz generation. The second part of this review focuses on the guided wave propagation of THz waves for their transmission. After discussing various wave guiding schemes, we consider new fiber designs for THz transmission.

*A review on memristor applications*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.06899[+https://arxiv.org/abs/1506.06899+]

This article presents a review on the main applications of the fourth fundamental circuit element, named "memristor", which had been proposed for the first time by Leon Chua and has recently been developed by a team at HP Laboratories led by Stanley Williams. In particular, after a brief analysis of memristor theory with a description of the first memristor, manufactured at HP Laboratories, we present its main applications in the circuit design and computer technology, together with future developments.

*A review of some recent advances in causal inference*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.07669[+https://arxiv.org/abs/1506.07669+]

We give a selective review of some recent developments in causal inference, intended for researchers who are not familiar with graphical models and causality, and with a focus on methods that are applicable to large data sets. We mainly address the problem of estimating causal effects from observational data. For example, one can think of estimating the effect of single or multiple gene knockouts from wild-type gene expression data, that is, from gene expression measurements that were obtained without doing any gene knockout experiments.
We assume that the observational data are generated from a causal structure that can be represented by a directed acyclic graph (DAG). First, we discuss estimation of causal effects when the underlying causal DAG is known. In large-scale networks, however, the causal DAG is often unknown. Next, we therefore discuss causal structure learning, that is, learning information about the causal structure from observational data. We then combine these two parts and discuss methods to estimate (bounds on) causal effects from observational data when the causal structure is unknown. We also illustrate this method on a yeast gene expression data set. We close by mentioning several extensions of the discussed work.

*Big Data Technology Literature Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1506.08978[+https://arxiv.org/abs/1506.08978+]

A short overview of various algorithms and technologies that are helpful for big data storage and manipulation. Includes pointers to papers for further reading, and, where applicable, pointers to open source projects implementing a described storage type.

*Modeling the Mind: A brief review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1507.01122[+https://arxiv.org/abs/1507.01122+]

The brain is a powerful tool used to achieve amazing feats. There have been several significant advances in neuroscience and artificial brain research in the past two decades. This article is a review of such advances, ranging from the concepts of connectionism, to neural network architectures and high-dimensional representations. There have also been advances in biologically inspired cognitive architectures of which we will cite a few. We will be positioning relatively specific models in a much broader perspective, while comparing and contrasting their advantages and weaknesses. The projects presented are targeted to model the brain at different levels, utilizing different methodologies.

*A review of hydrodynamic investigations into arrays of ocean wave energy converters*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1508.00866[+https://arxiv.org/abs/1508.00866+]

Theoretical, numerical and experimental studies on arrays of ocean wave energy converter are reviewed. The importance of extracting wave power via an array as opposed to individual wave-power machines has long been established. There is ongoing interest in implementing key technologies at commercial scale owing to the recent acceleration in demand for renewable energy. To date, several reviews have been published on the science and technology of harnessing ocean-wave power. However, there have been few reviews of the extensive literature on ocean wave-power arrays. Research into the hydrodynamic modelling of ocean wave-power arrays is analysed. Where ever possible, comparisons are drawn with physical scaled experiments. Some critical knowledge gaps have been found. Specific emphasis has been paid on understanding how the modelling and scaled experiments are likely to be complementary to each other.

*Space-time correlations in turbulent flow: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1508.01258[+https://arxiv.org/abs/1508.01258+]

This paper reviews some of the principal uses, over almost seven decades, of correlations, in both Eulerian and Lagrangian frames of reference, of properties of turbulent flows at variable spatial locations and variable time instants. Commonly called space--time correlations, they have been fundamental to theories and models of turbulence as well as for the analyses of experimental and direct numerical simulation turbulence data.

*Genetic Algorithms for multimodal optimization: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1508.05342[+https://arxiv.org/abs/1508.05342+]

In this article we provide a comprehensive review of the different evolutionary algorithm techniques used to address multimodal optimization problems, classifying them according to the nature of their approach. On the one hand there are algorithms that address the issue of the early convergence to a local optimum by differentiating the individuals of the population into groups and limiting their interaction, hence having each group evolve with a high degree of independence. On the other hand other approaches are based on directly addressing the lack of genetic diversity of the population by introducing elements into the evolutionary dynamics that promote new niches of the genotypical space to be explored. Finally, we study multi-objective optimization genetic algorithms, that handle the situations where multiple criteria have to be satisfied with no penalty for any of them. Very rich literature has arised over the years on these topics, and we aim at offering an overview of the most important techniques of each branch of the field.

*Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1511.01245[+https://arxiv.org/abs/1511.01245+]

"Recent research on problem formulations based on decomposition into low-rank plus sparse matrices shows a suitable framework to separate moving objects from the background. The most representative problem formulation is the Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit (PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix. However, similar robust implicit or explicit decompositions can be made in the following problem formulations: Robust Non-negative Matrix Factorization (RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust Subspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal of these similar problem formulations is to obtain explicitly or implicitly a decomposition into low-rank matrix plus additive matrices. In this context, this work aims to initiate a rigorous and comprehensive review of the similar problem formulations in robust subspace learning and tracking based on decomposition into low-rank plus additive matrices for testing and ranking existing algorithms for background/foreground separation. For this, we first provide a preliminary review of the recent developments in the different problem formulations which allows us to define a unified view that we called Decomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine carefully each method in each robust subspace learning/tracking frameworks with their decomposition, their loss functions, their optimization problem and their solvers. Furthermore, we investigate if incremental algorithms and real-time implementations can be achieved for background/foreground separation. Finally, experimental results on a large-scale dataset called Background Models Challenge (BMC 2012) show the comparative performance of 32 different robust subspace learning/tracking methods."

*Geospatial Big Data Handling Theory and Methods: A Review and Research Challenges*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1511.03010[+https://arxiv.org/abs/1511.03010+]

Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.

*Disconnected, fragmented, or united? A trans-disciplinary review of network science*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1511.03981[+https://arxiv.org/abs/1511.03981+]

During decades the study of networks has been divided between the efforts of social scientists and natural scientists, two groups of scholars who often do not see eye to eye. In this review I present an effort to mutually translate the work conducted by scholars from both of these academic fronts hoping to continue to unify what has become a diverging body of literature. I argue that social and natural scientists fail to see eye to eye because they have diverging academic goals. Social scientists focus on explaining how context specific social and economic mechanisms drive the structure of networks and on how networks shape social and economic outcomes. By contrast, natural scientists focus primarily on modeling network characteristics that are independent of context, since their focus is to identify universal characteristics of systems instead of context specific mechanisms. In the following pages I discuss the differences between both of these literatures by summarizing the parallel theories advanced to explain link formation and the applications used by scholars in each field to justify their approach to network science. I conclude by providing an outlook on how these literatures can be further unified.

*A review and evaluation of numerical tools for fractional calculus and fractional order control*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1511.07521[+https://arxiv.org/abs/1511.07521+]

In recent years, as fractional calculus becomes more and more broadly used in research across different academic disciplines, there are increasing demands for the numerical tools for the computation of fractional integration/differentiation, and the simulation of fractional order systems. Time to time, being asked about which tool is suitable for a specific application, the authors decide to carry out this survey to present recapitulative information of the available tools in the literature, in hope of benefiting researchers with different academic backgrounds. With this motivation, the present article collects the scattered tools into a dashboard view, briefly introduces their usage and algorithms, evaluates the accuracy, compares the performance, and provides informative comments for selection.

*Symbolic Calculus in Mathematical Statistics: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1512.08379[+https://arxiv.org/abs/1512.08379+]

In the last ten years, the employment of symbolic methods has substantially extended both the theory and the applications of statistics and probability. This survey reviews the development of a symbolic technique arising from classical umbral calculus, as introduced by Rota and Taylor in 1994. The usefulness of this symbolic technique is twofold. The first is to show how new algebraic identities drive in discovering insights among topics apparently very far from each other and related to probability and statistics. One of the main tools is a formal generalization of the convolution of identical probability distributions, which allows us to employ compound Poisson random variables in various topics that are only somewhat interrelated. Having got a different and deeper viewpoint, the second goal is to show how to set up algorithmic processes performing efficiently algebraic calculations. In particular, the challenge of finding these symbolic procedures should lead to a new method, and it poses new problems involving both computational and conceptual issues. Evidence of efficiency in applying this symbolic method will be shown within statistical inference, parameter estimation, Lévy processes, and, more generally, problems involving multivariate functions. The symbolic representation of Sheffer polynomial sequences allows us to carry out a unifying theory of classical, Boolean and free cumulants. Recent connections within random matrices have extended the applications of the symbolic method.

*Stochastic Parameterization: Towards a new view of Weather and Climate Models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1510.08682[+https://arxiv.org/abs/1510.08682+]

The last decade has seen the success of stochastic parameterizations in short-term, medium-range and seasonal forecasts: operational weather centers now routinely use stochastic parameterization schemes to better represent model inadequacy and improve the quantification of forecast uncertainty. Developed initially for numerical weather prediction, the inclusion of stochastic parameterizations not only provides better estimates of uncertainty, but it is also extremely promising for reducing longstanding climate biases and relevant for determining the climate response to external forcing. This article highlights recent developments from different research groups which show that the stochastic representation of unresolved processes in the atmosphere, oceans, land surface and cryosphere of comprehensive weather and climate models (a) gives rise to more reliable probabilistic forecasts of weather and climate and (b) reduces systematic model bias. We make a case that the use of mathematically stringent methods for the derivation of stochastic dynamic equations will lead to substantial improvements in our ability to accurately simulate weather and climate at all scales. Recent work in mathematics, statistical mechanics and turbulence is reviewed, its relevance for the climate problem demonstrated, and future research directions outlined.

2016
----

*25 Years of Self-organized Criticality: Concepts and Controversies*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-015-0155-x[+https://link.springer.com/article/10.1007/s11214-015-0155-x+]

Introduced by the late Per Bak and his colleagues, self-organized criticality (SOC) has been one of the most stimulating concepts to come out of statistical mechanics and condensed matter theory in the last few decades, and has played a significant role in the development of complexity science. SOC, and more generally fractals and power laws, have attracted much comment, ranging from the very positive to the polemical.  The other papers in this special issue showcase the considerable body of observations in solar, magnetospheric and fusion plasma inspired by the SOC idea, and expose the fertile role the new paradigm has played in approaches to modeling and understanding multiscale plasma instabilities. This very broad impact, and the necessary process of adapting a scientific hypothesis to the conditions of a given physical system, has meant that SOC as studied in these fields has sometimes differed significantly from the definition originally given by its creators. In Bak’s own field of theoretical physics there are significant observational and theoretical open questions, even 25 years on (Pruessner 2012). One aim of the present review is to address the dichotomy between the great reception SOC has received in some areas, and its shortcomings, as they became manifest in the controversies it triggered. Our article tries to clear up what we think are misunderstandings of SOC in fields more remote from its origins in statistical mechanics, condensed matter and dynamical systems by revisiting Bak, Tang and Wiesenfeld’s original papers.

*25 Years of Self-organized Criticality: Numerical Detection Methods* - R. T. J. McAteer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-015-0158-7[+https://link.springer.com/article/10.1007/s11214-015-0158-7+]

The detection and characterization of self-organized criticality (SOC), in both real and simulated data, has undergone many significant revisions over the past 25 years. The explosive advances in the many numerical methods available for detecting, discriminating, and ultimately testing, SOC have played a critical role in developing our understanding of how systems experience and exhibit SOC. In this article, methods of detecting SOC are reviewed; from correlations to complexity to critical quantities. A description of the basic autocorrelation method leads into a detailed analysis of application-oriented methods developed in the last 25 years.

In the second half of this manuscript space-based, time-based and spatial-temporal methods are reviewed and the prevalence of power laws in nature is described, with an emphasis on event detection and characterization. The search for numerical methods to clearly and unambiguously detect SOC in data often leads us outside the comfort zone of our own disciplines—the answers to these questions are often obtained by studying the advances made in other fields of study. In addition, numerical detection methods often provide the optimum link between simulations and experiments in scientific research. We seek to explore this boundary where the rubber meets the road, to review this expanding field of research of numerical detection of SOC systems over the past 25 years, and to iterate forwards so as to provide some foresight and guidance into developing breakthroughs in this subject over the next quarter of a century.

*History and Applications of Dust Devil Studies* - R. D. Lorenz et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-016-0239-2[+https://link.springer.com/article/10.1007/s11214-016-0239-2+]

Studies of dust devils, and their impact on society, are reviewed. Dust devils have been noted since antiquity, and have been documented in many countries, as well as on the planet Mars. As time-variable vortex entities, they have become a cultural motif. Three major stimuli of dust devil research are identified, nuclear testing, terrestrial climate studies, and perhaps most significantly, Mars research. Dust devils present an occasional safety hazard to light structures and have caused several deaths.

*Statistical physics of inference: thresholds and algorithms*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.tandfonline.com/doi/full/10.1080/00018732.2016.1211393[+https://www.tandfonline.com/doi/full/10.1080/00018732.2016.1211393+]

Many questions of fundamental interest in today's science can be formulated as inference problems: some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. The connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which, formulated as an inference problem, we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.

*A global spectral library to characterize the world's soil*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216300113[+https://www.sciencedirect.com/science/article/pii/S0012825216300113+]

Soil provides ecosystem services, supports human health and habitation, stores carbon and regulates emissions of greenhouse gases. Unprecedented pressures on soil from degradation and urbanization are threatening agro-ecological balances and food security. It is important that we learn more about soil to sustainably manage and preserve it for future generations. To this end, we developed and analyzed a global soil visible–near infrared (vis–NIR) spectral library. It is currently the largest and most diverse database of its kind. We show that the information encoded in the spectra can describe soil composition and be associated to land cover and its global geographic distribution, which acts as a surrogate for global climate variability. We also show the usefulness of the global spectra for predicting soil attributes such as soil organic and inorganic carbon, clay, silt, sand and iron contents, cation exchange capacity, and pH. Using wavelets to treat the spectra, which were recorded in different laboratories using different spectrometers and methods, helped to improve the spectroscopic modelling. 

We found that modelling a diverse set of spectra with a machine learning algorithm can find the local relationships in the data to produce accurate predictions of soil properties. The spectroscopic models that we derived are parsimonious and robust, and using them we derived a harmonized global soil attribute dataset, which might serve to facilitate research on soil at the global scale. This spectroscopic approach should help to deal with the shortage of data on soil to better understand it and to meet the growing demand for information to assess and monitor soil at scales ranging from regional to global. New contributions to the library are encouraged so that this work and our collaboration might progress to develop a dynamic and easily updatable database with better global coverage. We hope that this work will reinvigorate our community's discussion towards larger, more coordinated collaborations. We also hope that use of the database will deepen our understanding of soil so that we might sustainably manage it and extend the research outcomes of the soil, earth and environmental sciences towards applications that we have not yet dreamed of.

*Theoretical, contemporary observational and palaeo-perspectives on ice sheet hydrology: Processes and products*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216300095[+https://www.sciencedirect.com/science/article/pii/S0012825216300095+]

Meltwater drainage through ice sheets has recently been a key focus of glaciological research due to its influence on the dynamics of ice sheets in a warming climate. However, the processes, topologies and products of ice sheet hydrology are some of the least understood components of both past and modern ice sheets. This is to some extent a result of a disconnect between the fields of theoretical, contemporary observational and palaeo-glaciology that each approach ice sheet hydrology from a different perspective and with different research objectives. 
With an increasing realisation of the potential of using the past to inform on the future of contemporary ice sheets, bridging the gaps in the understanding of ice sheet hydrology has become paramount. 

Here, we review the current state of knowledge about ice sheet hydrology from the perspectives of theoretical, observational and palaeo-glaciology. We then explore and discuss some of the key questions in understanding and interpretation between these research fields, including: 1) disagreement between the palaeo-record, glaciological theory and contemporary observations in the operational extent of channelised subglacial drainage and the topology of drainage systems; 2) uncertainty over the magnitude and frequency of drainage events associated with geomorphic activity; and 3) contrasts in scale between the three fields of research, both in a spatial and temporal context.

The main concluding points are that modern observations, modelling experiments and inferences from the palaeo-record indicate that drainage topologies may comprise a multiplicity of forms in an amalgam of drainage modes occurring in different contexts and at different scales. Drainage under high pressure appears to dominate at ice sheet scale and might in some cases be considered efficient; the sustainability of a particular drainage mode is governed primarily by the stability of discharge. To gain better understanding of meltwater drainage under thick ice, determining what drainage topologies are reached under high pressure conditions is of primary importance. Our review attests that the interconnectivity between research sub-disciplines in progressing the field is essential, both in interpreting the palaeo-record and in developing physical understanding of glacial hydrological processes and systems.

*ESR in the 21st century: From buried valleys and deserts to the deep ocean and tectonic uplift*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216300010[+https://www.sciencedirect.com/science/article/pii/S0012825216300010+]

Electron spin resonance (ESR) dating can date many materials, including hydroxyapatite in enamel and some fish scales, aragonite and calcite in corals, molluscs, some travertine and calcrete, and quartz from ash, fluvial deposits, and some flint. Dating studies using these materials have numerous potential applications in many varied Quaternary settings. ESR dating uses signals resulting from trapped charges created by radiation in crystalline solids. Ages are calculated by comparing the accumulated radiation dose in the dating sample with the internal and external radiation dose rates produced by natural radiation in and around the sample and produced by cosmic radiation. When compared to other dating techniques, age agreement has been excellent for teeth, corals, molluscs, and quartz. 

Recent improvements have included using a more complex modelling technique to calculate the cosmic dose rates and more detailed modelling techniques for dealing with variable external dose rates. Methods in development include using quartz from buried fluvial valleys to date geomorphic surfaces and using the signals in barnacles and benthic foraminifera for dating fossils or their associated sediment. New chronometer applications recently developed include using coral and mollusc dates to build sealevel curves and to monitor volcanic activity and tectonic uplift, using tooth and mollusc dates to assess water availability in deserts, and using isochron data to assess U uptake processes into teeth. When coupled with other geochemical and geomorphological techniques, ESR can provide the chronometric control to build paleoclimatic and other paleoenvironmental records. Many other applications are possible, from heating studies for artefacts to dating sulphates and other minerals on distant planets.

*Defining tsunamis: Yoda strikes back?*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216301155[+https://www.sciencedirect.com/science/article/pii/S0012825216301155+]

Charting the journey of the term “tsunami” through to its nearly ubiquitous global use today is not simply a case of determining when it was borrowed from Japanese. It represents an almost 1400 year journey from the earliest historical Japanese reference to waves generated by the AD684 Hakuho-Nankai earthquake to a 95.7% usage by international media to describe the 2010 Chilean tsunami. The gradual rise of the term's usage parallels changes in Japanese society from an educated elite in Kyoto writing down oral event descriptions from various prefectures, to the spread of newspapers, an increasing Western influence and a preference within the education system.

The widespread use of the term tsunami throughout Japan was not truly achieved until the same decade that it was adopted by Western scientists to describe the 1946 Alaskan tsunami in Hawaii. The alternative term kaishou most likely became less popular simply because it was more difficult to understand in Japanese. While there has been a rapid uptake in Western science, and ultimately its wider adoption, this has been aided greatly by two major events to affect the Hawaiian Islands in 1946 (Alaskan) and 1960 (Chilean). During this time the meaning of the term has also changed with semantic narrowing focusing in on the definition we know today. Along the way there have been casualties, with terms such as kaishou now rarely, if ever, appearing. Other terms, which speak volumes about not only the richness of the Japanese language, but also of Japanese experience with tsunamis in the past, now offer us the opportunity to use appropriate qualifying terms to describe the nature of an event. A souteigai-tsunami is an unexpectedly large event whereas a yoda is a small one.

*On the origin of the signals observed across the seismic spectrum*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216301684[+https://www.sciencedirect.com/science/article/pii/S0012825216301684+]

The increasing number of broad-band seismic stations recording the full spectrum of the seismic wavefield continuously has boosted interest in background signals recorded in the absence of earthquakes. Different human-made and natural phenomena other than earthquakes result in Earth vibrations that are recorded on seismometers. Those signals have classically been considered as disturbing noise, but in the last decades this view has turned, as it has been shown that seismic data can be used not only to monitor earthquake activity, but also to investigate climatic changes, track hurricanes, monitor river flows, or survey anthropogenic activity, hence making new links between seismology and different research fields. This contribution reviews state-of-the-art knowledge on the sources of seismic energy in different frequency bands using a single, two-weeks-long, seismic data file recorded by a high quality broad-band station located in the Pyrenees. This data allows exploration of the wide spectrum of ground motion, enabling a review of different processes involved in the generation of what seismologists commonly regard as background noise when focusing on ground motion from local and teleseismic earthquakes and explosions recorded in the same time interval.

*Rip current types, circulation and hazard*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216303117[+https://www.sciencedirect.com/science/article/pii/S0012825216303117+]

Rip currents are narrow and concentrated seaward-directed flows that extend from close to the shoreline, through the surf zone, and varying distances beyond. Rip currents are ubiquitous on wave-exposed coasts. Each year they cause hundreds of drowning deaths and tens of thousands of rescues on beaches worldwide and are therefore the leading deadly hazard to recreational beach users. The broad definition above masks considerable natural variability in terms of rip current occurrence in time and space, flow characteristics and behaviour. In particular, surf-zone rip currents have long been perceived as narrow flows extending well beyond the breakers, flushing out the surf zone at a high rate (‘exit flow’ circulation regime), while more recent studies have shown that rip flow patterns can consist of quasi-steady semi-enclosed vortices retaining most of the floating material within the surf zone (‘circulatory flow’ circulation regime).

Building upon a growing body of rip current literature involving numerical modelling and theory together with emergence of dense Lagrangian field measurements, we develop a robust rip current type classification that provides a relevant framework to understand the primary morphological and hydrodynamic parameters controlling surf-zone rip current occurrence and dynamics. Three broad categories of rip current types are described based on the dominant controlling forcing mechanism. Each category is further divided into two types owing to different physical driving mechanisms for a total of six fundamentally different rip current types: hydrodynamically-controlled (1) shear instability rips and (2) flash rips, which are transient in both time and space and occur on alongshore-uniform beaches; bathymetrically-controlled (3) channel rips and (4) focused rips, which occur at relatively fixed locations and are driven by hydrodynamic processes forced by natural alongshore variability of the morphology in both the surf zone and inner shelf zone; and boundary-controlled (5) deflection rips and (6) shadow rips, which flow against rigid lateral boundaries such as natural headlands or anthropogenic structures.

For each rip current type, flow response to changes in hydrodynamic and morphologic forcing magnitude is examined in regard to velocity modulation and changes in circulation regime, providing key force-response relationships of rip currents. We also demonstrate that in the real world, rip currents form through a mixture of driving mechanisms and the discrete rip types defined in fact form key elements in a wide and complex spectrum of rip currents on natural beaches. It is anticipated that this rip current type classification will serve as a resource for coastal scientists and non-specialists with an interest in the rip current hazard, and as a platform for future rip current studies. Finally, we suggest some important future research directions highlighting the need for coastal and beach safety communities to collaborate in order to improve rip current education and awareness.

*Statistical mechanics of ecological systems: Neutral theory and beyond*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.88.035003[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.88.035003+]

The simplest theories often have much merit and many limitations, and, in this vein, the value of neutral theory (NT) of biodiversity has been the subject of much debate over the past 15 years. NT was proposed at the turn of the century by Stephen Hubbell to explain several patterns observed in the organization of ecosystems. Among ecologists, it had a polarizing effect: There were a few ecologists who were enthusiastic, and there were a larger number who firmly opposed it. Physicists and mathematicians, instead, welcomed the theory with excitement. Indeed, NT spawned several theoretical studies that attempted to explain empirical data and predicted trends of quantities that had not yet been studied. While there are a few reviews of NT oriented toward ecologists, the goal here is to review the quantitative aspects of NT and its extensions for physicists who are interested in learning what NT is, what its successes are, and what important problems remain unresolved. Furthermore, this review could also be of interest to theoretical ecologists because many potentially interesting results are buried in the vast NT literature. It is proposed to make these more accessible by extracting them and presenting them in a logical fashion. The focus of this review is broader than NT: new, more recent approaches for studying ecological systems and how one might introduce realistic non-neutral models are also discussed.

*Physical applications of GPS geodesy: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Geodesy, the oldest science, has become an important discipline in the geosciences, in large part by enhancing Global Positioning System (GPS) capabilities over the last 35 years well beyond the satellite constellation's original design. The ability of GPS geodesy to estimate 3D positions with millimeter-level precision with respect to a global terrestrial reference frame has contributed to significant advances in geophysics, seismology, atmospheric science, hydrology, and natural hazard science. Monitoring the changes in the positions or trajectories of GPS instruments on the Earth's land and water surfaces, in the atmosphere, or in space, is important for both theory and applications, from an improved understanding of tectonic and magmatic processes to developing systems for mitigating the impact of natural hazards on society and the environment. Besides accurate positioning, all disturbances in the propagation of the transmitted GPS radio signals from satellite to receiver are mined for information, from troposphere and ionosphere delays for weather, climate, and natural hazard applications, to disturbances in the signals due to multipath reflections from the solid ground, water, and ice for environmental applications. We review the relevant concepts of geodetic theory, data analysis, and physical modeling for a myriad of processes at multiple spatial and temporal scales, and discuss the extensive global infrastructure that has been built to support GPS geodesy consisting of thousands of continuously operating stations. We also discuss the integration of heterogeneous and complementary data sets from geodesy, seismology, and geology, focusing on crustal deformation applications and early warning systems for natural hazards.

*The hidden simplicity of biology*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/79/10/102601[+https://iopscience.iop.org/article/10.1088/0034-4885/79/10/102601+]

Life is so remarkable, and so unlike any other physical system, that it is tempting to attribute special factors to it. Physics is founded on the assumption that universal laws and principles underlie all natural phenomena, but is it far from clear that there are 'laws of life' with serious descriptive or predictive power analogous to the laws of physics. Nor is there (yet) a 'theoretical biology' in the same sense as theoretical physics. Part of the obstacle in developing a universal theory of biological organization concerns the daunting complexity of living organisms. However, many attempts have been made to glimpse simplicity lurking within this complexity, and to capture this simplicity mathematically. In this paper we review a promising new line of inquiry to bring coherence and order to the realm of biology by focusing on 'information' as a unifying concept.

*A review on locomotion robophysics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/0034-4885/79/11/110001[+https://iopscience.iop.org/article/10.1088/0034-4885/79/11/110001+]

Discovery of fundamental principles which govern and limit effective locomotion (self-propulsion) is of intellectual interest and practical importance. Human technology has created robotic moving systems that excel in movement on and within environments of societal interest: paved roads, open air and water. However, such devices cannot yet robustly and efficiently navigate (as animals do) the enormous diversity of natural environments which might be of future interest for autonomous robots; examples include vertical surfaces like trees and cliffs, heterogeneous ground like desert rubble and brush, turbulent flows found near seashores, and deformable/flowable substrates like sand, mud and soil. In this review we argue for the creation of a physics of moving systems—a 'locomotion robophysics'—which we define as the pursuit of principles of self-generated motion. Robophysics can provide an important intellectual complement to the discipline of robotics, largely the domain of researchers from engineering and computer science. The essential idea is that we must complement the study of complex robots in complex situations with systematic study of simplified robotic devices in controlled laboratory settings and in simplified theoretical models. We must thus use the methods of physics to examine both locomotor successes and failures using parameter space exploration, systematic control, and techniques from dynamical systems. Using examples from our and others' research, we will discuss how such robophysical studies have begun to aid engineers in the creation of devices that have begun to achieve life-like locomotor abilities on and within complex environments, have inspired interesting physics questions in low dimensional dynamical systems, geometric mechanics and soft matter physics, and have been useful to develop models for biological locomotion in complex terrain. The rapidly decreasing cost of constructing robot models with easy access to significant computational power bodes well for scientists and engineers to engage in a discipline which can readily integrate experiment, theory and computation.

*Applications of nuclear physics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/80/2/026301[+https://iopscience.iop.org/article/10.1088/1361-6633/80/2/026301+]

Today the applications of nuclear physics span a very broad range of topics and fields. This review discusses a number of aspects of these applications, including selected topics and concepts in nuclear reactor physics, nuclear fusion, nuclear non-proliferation, nuclear-geophysics, and nuclear medicine. The review begins with a historic summary of the early years in applied nuclear physics, with an emphasis on the huge developments that took place around the time of World War II, and that underlie the physics involved in designs of nuclear explosions, controlled nuclear energy, and nuclear fusion. The review then moves to focus on modern applications of these concepts, including the basic concepts and diagnostics developed for the forensics of nuclear explosions, the nuclear diagnostics at the National Ignition Facility, nuclear reactor safeguards, and the detection of nuclear material production and trafficking. The review also summarizes recent developments in nuclear geophysics and nuclear medicine. The nuclear geophysics areas discussed include geo-chronology, nuclear logging for industry, the Oklo reactor, and geo-neutrinos. The section on nuclear medicine summarizes the critical advances in nuclear imaging, including PET and SPECT imaging, targeted radionuclide therapy, and the nuclear physics of medical isotope production. Each subfield discussed requires a review article unto itself, which is not the intention of the current review; rather, the current review is intended for readers who wish to get a broad understanding of applied nuclear physics.

*Milestones of general relativity*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/80/2/026001[+https://iopscience.iop.org/article/10.1088/1361-6633/80/2/026001+]

We present a summary for non-specialists of the special issue of the journal Classical and Quantum Gravity on 'Milestones of general relativity', commemorating the 100th anniversary of the theory.

*Bioarchitecture: bioinspired art and architecture—a perspective*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0192[+https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0192+]

Art and architecture can be an obvious choice to pair with science though historically this has not always been the case. This paper is an attempt to interact across disciplines, define a new genre, bioarchitecture, and present opportunities for further research, collaboration and professional cooperation. Biomimetics, or the copying of living nature, is a field that is highly interdisciplinary, involving the understanding of biological functions, structures and principles of various objects found in nature by scientists. Biomimetics can lead to biologically inspired design, adaptation or derivation from living nature. As applied to engineering, bioinspiration is a more appropriate term, involving interpretation, rather than direct copying. Art involves the creation of discrete visual objects intended by their creators to be appreciated by others. Architecture is a design practice that makes a theoretical argument and contributes to the discourse of the discipline. Bioarchitecture is a blending of art/architecture and biomimetics/bioinspiration, and incorporates a bioinspired design from the outset in all parts of the work at all scales. Herein, we examine various attempts to date of art and architecture to incorporate bioinspired design into their practice, and provide an outlook and provocation to encourage collaboration among scientists and designers, with the aim of achieving bioarchitecture.

*Horizons of cybernetical physics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0223[+https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0223+]

The subject and main areas of a new research field—cybernetical physics—are discussed. A brief history of cybernetical physics is outlined. The main areas of activity in cybernetical physics are briefly surveyed, such as control of oscillatory and chaotic behaviour, control of resonance and synchronization, control in thermodynamics, control of distributed systems and networks, quantum control.

*Fluid dynamic instabilities: theory and application to pattern forming in complex media*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0155[+https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0155+]

In this review article, we exemplify the use of stability analysis tools to rationalize pattern formation in complex media. Specifically, we focus on fluid flows, and show how the destabilization of their interface sets the blueprint of the patterns they eventually form. We review the potential use and limitations of the theoretical methods at the end, in terms of their applications to practical settings, e.g. as guidelines to design and fabricate structures while harnessing instabilities.

*Industry 1.61803: the transition to an industry with reduced material demand fit for a low carbon future*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0361[+https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0361+]

Arising from a discussion meeting in September 2016, this editorial introduces a special issue on the transition to a future industrial system with greatly reduced demand for material production and attempts to synthesize the main findings. The motivation for such a transition is to reduce industrial greenhouse gas emissions, but unlike previous industrial transformations, there are no major stakeholders who will pursue the change for their own immediate benefit. The special issue, therefore, explores the means by which such a transition could be brought about. The editorial presents an overview of the opportunities identified in the papers of the volume, presents examples of actions that can be taken today to begin the process of change and concludes with an agenda for research that might support a rapid acceleration in the rate of change.

*The Art of DNA Strings: Sixteen Years of DNA Coding Theory*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1607.00266[+https://arxiv.org/abs/1607.00266+]

The idea of computing with DNA was given by Tom Head in 1987, however in 1994 in a seminal paper, the actual successful experiment for DNA computing was performed by Adleman. The heart of the DNA computing is the DNA hybridization, however, it is also the source of errors. Thus the success of the DNA computing depends on the error control techniques. The classical coding theory techniques have provided foundation for the current information and communication technology (ICT). Thus it is natural to expect that coding theory will be the foundational subject for the DNA computing paradigm. For the successful experiments with DNA computing usually we design DNA strings which are sufficiently dissimilar. This leads to the construction of a large set of DNA strings which satisfy certain combinatorial and thermodynamic constraints. Over the last 16 years, many approaches such as combinatorial, algebraic, computational have been used to construct such DNA strings. In this work, we survey this interesting area of DNA coding theory by providing key ideas of the area and current known results.

*Variational Inference: A Review for Statisticians*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1601.00670[+https://arxiv.org/abs/1601.00670+]

One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.

*Review and Analysis of Networking Challenges in Cloud Computing*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1601.05329[+https://arxiv.org/abs/1601.05329+]

Cloud Computing offers virtualized computing, storage, and networking resources, over the Internet, to organizations and individual users in a completely dynamic way. These cloud resources are cheaper, easier to manage, and more elastic than sets of local, physical, ones. This encourages customers to outsource their applications and services to the cloud. The migration of both data and applications outside the administrative domain of customers into a shared environment imposes transversal, functional problems across distinct platforms and technologies. This article provides a contemporary discussion of the most relevant functional problems associated with the current evolution of Cloud Computing, mainly from the network perspective. The paper also gives a concise description of Cloud Computing concepts and technologies. It starts with a brief history about cloud computing, tracing its roots. Then, architectural models of cloud services are described, and the most relevant products for Cloud Computing are briefly discussed along with a comprehensive literature review. The paper highlights and analyzes the most pertinent and practical network issues of relevance to the provision of high-assurance cloud services through the Internet, including security. Finally, trends and future research directions are also presented.

*A short review on Noether's theorems, gauge symmetries and boundary terms*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1601.03616[+https://arxiv.org/abs/1601.03616+]

This review is dedicated to some modern applications of the remarkable paper written in 1918 by E. Noether. On a single paper, Noether discovered the crucial relation between symmetries and conserved charges as well as the impact of gauge symmetries on the equations of motion. Almost a century has gone since the publication of this work and its applications have permeated modern physics. Our focus will be on some examples that have appeared recently in the literature. This review is aim at students, not researchers. The main three topics discussed are (i) global symmetries and conserved charges (ii) local symmetries and gauge structure of a theory (iii) boundary conditions and algebra of asymptotic symmetries. All three topics are discussed through examples.

*Large Graph Models: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1601.06444[+https://arxiv.org/abs/1601.06444+]

Large graphs can be found in a wide array of scientific fields ranging from sociology and biology to scientometrics and computer science. Their analysis is by no means a trivial task due to their sheer size and complex structure. Such structure encompasses features so diverse as diameter shrinking, power law degree distribution and self similarity, edge interdependence, and communities. When the adjacency matrix of a graph is considered, then new, spectral properties arise such as primary eigenvalue component decay function, eigenvalue decay function, eigenvalue sign alternation around zero, and spectral gap. Graph mining is the scientific field which attempts to extract information and knowledge from graphs through their structural and spectral properties. Graph modeling is the associated field of generating synthetic graphs with properties similar to those of real graphs in order to simulate the latter. Such simulations may be desirable because of privacy concerns, cost, or lack of access to real data. Pivotal to simulation are low- and high-level software packages offering graph analysis and visualization capabilities. This survey outlines the most important structural and spectral graph properties, a considerable number of graph models, as well the most common graph mining and graph learning tools.

*Ground-based Observations of the Solar Sources of Space Weather: Invited Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1602.02721[+https://arxiv.org/abs/1602.02721+]

Monitoring of the Sun and its activity is a task of growing importance in the frame of space weather research and awareness. Major space weather disturbances at Earth have their origin in energetic outbursts from the Sun: solar flares, coronal mass ejections and associated solar energetic particles. In this review we discuss the importance and complementarity of ground-based and space-based observations for space weather studies. The main focus is drawn on ground-based observations in the visible range of the spectrum, in particular in the diagnostically manifold Hα spectral line, which enables us to detect and study solar flares, filaments, filament eruptions, and Moreton waves. Existing Hα networks such as the GONG and the Global High-Resolution Hα Network are discussed. As an example of solar observations from space weather research to operations, we present the system of real-time detection of Hα flares and filaments established at Kanzelhöhe Observatory (KSO; Austria) in the frame of the ESA Space Situational Awareness programme. During the evaluation period 7/2013 - 11/2015, KSO provided 3020 hours of real-time Hα observations at the SWE portal. In total, 824 Hα flares were detected and classified by the real-time detection system, including 174 events of Hα importance class 1 and larger. For the total sample of events, 95\% of the automatically determined flare peak times lie within ±5 min of the values given in the official optical flares reports (by NOAA and KSO), and 76\% of the start times. The heliographic positions determined are better than ±5∘. The probability of detection of flares of importance 1 or larger is 95\%, with a false alarm rate of 16\%. These numbers confirm the high potential of automatic flare detection and alerting from ground-based observatories.

*Recursive Recovery of Sparse Signal Sequences from Compressive Measurements: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1602.04518[+https://arxiv.org/abs/1602.04518+]

In this article, we review the literature on design and analysis of recursive algorithms for reconstructing a time sequence of sparse signals from compressive measurements. The signals are assumed to be sparse in some transform domain or in some dictionary. Their sparsity patterns can change with time, although, in many practical applications, the changes are gradual. An important class of applications where this problem occurs is dynamic projection imaging, e.g., dynamic magnetic resonance imaging (MRI) for real-time medical applications such as interventional radiology, or dynamic computed tomography.

*A review on locomotion robophysics: the study of movement at the intersection of robotics, soft matter and dynamical systems*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1602.04712[+https://arxiv.org/abs/1602.04712+]

In this review we argue for the creation of a physics of moving systems -- a locomotion "robophysics" -- which we define as the pursuit of the discovery of principles of self generated motion. Robophysics can provide an important intellectual complement to the discipline of robotics, largely the domain of researchers from engineering and computer science. The essential idea is that we must complement study of complex robots in complex situations with systematic study of simplified robophysical devices in controlled laboratory settings and simplified theoretical models. We must thus use the methods of physics to examine successful and failed locomotion in simplified (abstracted) devices using parameter space exploration, systematic control, and techniques from dynamical systems. Using examples from our and other's research, we will discuss how such robophysical studies have begun to aid engineers in the creation of devices that begin to achieve life-like locomotor abilities on and within complex environments, have inspired interesting physics questions in low dimensional dynamical systems, geometric mechanics and soft matter physics, and have been useful to develop models for biological locomotion in complex terrain. The rapidly decreasing cost of constructing sophisticated robot models with easy access to significant computational power bodes well for scientists and engineers to engage in a discipline which can readily integrate experiment, theory and computation.

*Firefly Algorithm for optimization problems with non-continuous variables: A Review and Analysis*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1602.07884[+https://arxiv.org/abs/1602.07884+]

Firefly algorithm is a swarm based metaheuristic algorithm inspired by the flashing behavior of fireflies. It is an effective and an easy to implement algorithm. It has been tested on different problems from different disciplines and found to be effective. Even though the algorithm is proposed for optimization problems with continuous variables, it has been modified and used for problems with non-continuous variables, including binary and integer valued problems. In this paper a detailed review of this modifications of firefly algorithm for problems with non-continuous variables will be discussed. The strength and weakness of the modifications along with possible future works will be presented.

*Bayesian Computing with INLA: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1604.00860[+https://arxiv.org/abs/1604.00860+]

The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre- Simon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.

http://www.r-inla.org/[+http://www.r-inla.org/+]

*Directional Statistics in Machine Learning: a Brief Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.00316[+https://arxiv.org/abs/1605.00316+]

The modern data analyst must cope with data encoded in various forms, vectors, matrices, strings, graphs, or more. Consequently, statistical and machine learning models tailored to different data encodings are important. We focus on data encoded as normalized vectors, so that their "direction" is more important than their magnitude. Specifically, we consider high-dimensional vectors that lie either on the surface of the unit hypersphere or on the real projective plane. For such data, we briefly review common mathematical models prevalent in machine learning, while also outlining some technical aspects, software, applications, and open mathematical challenges.

*Stochastic Climate Theory*
~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1612.07474[+https://arxiv.org/abs/1612.07474+]

In this chapter we review stochastic modelling methods in climate science. First we provide a conceptual framework for stochastic modelling of deterministic dynamical systems based on the Mori-Zwanzig formalism. The Mori-Zwanzig equations contain a Markov term, a memory term and a term suggestive of stochastic noise. Within this framework we express standard model reduction methods such as averaging and homogenization which eliminate the memory term. We further discuss ways to deal with the memory term and how the type of noise depends on the underlying deterministic chaotic system. Secondly, we review current approaches in stochastic data-driven models. We discuss how the drift and diffusion coefficients of models in the form of stochastic differential equations can be estimated from observational data. We pay attention to situations where the data stems from multi scale systems, a relevant topic in the context of data from the climate system. Furthermore, we discuss the use of discrete stochastic processes (Markov chains) for e.g. stochastic subgrid-scale modeling and other topics in climate science.

*Spatial Verification Using Wavelet Transforms: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.03395[+https://arxiv.org/abs/1605.03395+]

Due to the emergence of new high resolution numerical weather prediction (NWP) models and the availability of new or more reliable remote sensing data, the importance of efficient spatial verification techniques is growing. Wavelet transforms offer an effective framework to decompose spatial data into separate (and possibly orthogonal) scales and directions. Most wavelet based spatial verification techniques have been developed or refined in the last decade and concentrate on assessing forecast performance (i.e. forecast skill or forecast error) on distinct physical scales. Particularly during the last five years, a significant growth in meteorological applications could be observed. However, a comparison with other scientific fields such as feature detection, image fusion, texture analysis, or facial and biometric recognition, shows that there is still a considerable, currently unused potential to derive useful diagnostic information. In order to tab the full potential of wavelet analysis, we revise the state-of-the art in one- and two-dimensional wavelet analysis and its application with emphasis on spatial verification. We further use a technique developed for texture analysis in the context of high-resolution quantitative precipitation forecasts, which is able to assess structural characteristics of the precipitation fields and allows efficient clustering of ensemble data.

*Reviewing METI: A Critical Analysis of the Arguments*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.05663[+https://arxiv.org/abs/1605.05663+]

There is an ongoing debate pertaining to the question of whether Earth should initiate intentional and powerful radio transmissions to putative extra-terrestrial (ET) civilizations in the hope of attracting ET's attention. This practice is known as METI (Messaging to ET Intelligence) or Active SETI. The debate has recently taken on a sense of urgency, as additional proponents have announced their intention to commence de novo transmissions as soon as they become funded and acquire the needed time on a powerful transmitter such as Arecibo. Arguments in favor of METI are reviewed. It is concluded that METI is unwise, unscientific, potentially catastrophic, and unethical.

*Recent progress and review of issues related to Physics Dynamics Coupling in geophysical models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.06480[+https://arxiv.org/abs/1605.06480+]

Geophysical models of the atmosphere and ocean invariably involve parameterizations. These represent two distinct areas: Subgrid processes that the model cannot resolve, and diabatic sources in the equations, due to radiation for example. Hence, coupling between these physics parameterizations and the resolved fluid dynamics and also between the dynamics of the air and water, is necessary. In this paper weather and climate models are used to illustrate the problems. Nevertheless the same applies to other geophysical models. This coupling is an important aspect of geophysical models. However, often model development is strictly segregated into either physics or dynamics. As a consequence, this area has many unanswered questions. Recent developments in the design of dynamical cores, extended process physics and predicted future changes of the computational infrastructure are increasing complexity. This paper reviews the state-of-the-art of the physics-dynamics coupling in geophysical models, surveys the analysis techniques, and illustrates open questions in this field. This paper focuses on two objectives: To illustrate the phenomenology of the coupling problem with references to examples in the literature and to show how the problem can be analysed. Proposals are made on how to advance the understanding and upcoming challenges with emerging modeling strategies. This paper is of interest to model developers who aim to improve the models and have to make choices on and test new implementations, to users who have to understand choices presented to them and finally users of outputs, who have to distinguish physical features from numerical problems in the model data.

*Cognitive Dynamic Systems: A Technical Review of Cognitive Radar*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.08150[+https://arxiv.org/abs/1605.08150+]

We start with the history of cognitive radar, where origins of the PAC, Fuster research on cognition and principals of cognition are provided. Fuster describes five cognitive functions: perception, memory, attention, language, and intelligence. We describe the Perception-Action Cyclec as it applies to cognitive radar, and then discuss long-term memory, memory storage, memory retrieval and working memory. A comparison between memory in human cognition and cognitive radar is given as well. Attention is another function described by Fuster, and we have given the comparison of attention in human cognition and cognitive radar. We talk about the four functional blocks from the PAC: Bayesian filter, feedback information, dynamic programming and state-space model for the radar environment. Then, to show that the PAC improves the tracking accuracy of Cognitive Radar over Traditional Active Radar, we have provided simulation results. In the simulation, three nonlinear filters: Cubature Kalman Filter, Unscented Kalman Filter and Extended Kalman Filter are compared. Based on the results, radars implemented with CKF perform better than the radars implemented with UKF or radars implemented with EKF. Further, radar with EKF has the worst accuracy and has the biggest computation load because of derivation and evaluation of Jacobian matrices. We suggest using the concept of risk management to better control parameters and improve performance in cognitive radar. We believe, spectrum sensing can be seen as a potential interest to be used in cognitive radar and we propose a new approach Probabilistic ICA which will presumably reduce noise based on estimation error in cognitive radar. Parallel computing is a concept based on divide and conquers mechanism, and we suggest using the parallel computing approach in cognitive radar by doing complicated calculations or tasks to reduce processing time. 



*Maximum information entropy principle and the interpretation of probabilities in statistical mechanics - a short review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.08703[+https://arxiv.org/abs/1605.08703+]

In this paper an alternative approach to statistical mechanics based on the maximum information entropy principle (MaxEnt) is examined, specifically its close relation with the Gibbs method of ensembles. It is shown that the MaxEnt formalism is the logical extension of the Gibbs formalism of equilibrium statistical mechanics that is entirely independent of the frequentist interpretation of probabilities only as factual (i.e. experimentally verifiable) properties of the real world. Furthermore, we show that, consistently with the law of large numbers, the relative frequencies of the ensemble of systems prepared under identical conditions (i.e. identical constraints) actually correspond to the MaxEnt probabilites in the limit of a large number of systems in the ensemble. This result implies that the probabilities in statistical mechanics can be interpreted, independently of the frequency interpretation, on the basis of the maximum information entropy principle.

*Kernel Mean Embedding of Distributions: A Review and Beyond*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1605.09522[+https://arxiv.org/abs/1605.09522+]

A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.

*Tests for Comparing Weighted Histograms. Review and Improvements*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1606.06591[+https://arxiv.org/abs/1606.06591+]

Histograms with weighted entries are used to estimate probability density functions. Computer simulation is the main application of this type of histograms. A review on chi-square tests for comparing weighted histograms is presented in this paper. Improvements to these tests that have a size closer to its nominal value are proposed. Numerical examples are presented for evaluation and demonstration of various applications of the tests.

*Discovering the Unexpected in Astronomical Survey Data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1611.05570[+https://arxiv.org/abs/1611.05570+]

Most major discoveries in astronomy are unplanned, and result from surveying the Universe in a new way, rather than by testing a hypothesis or conducting an investigation with planned outcomes. For example, of the 10 greatest discoveries made by the Hubble Space Telescope, only one was listed in its key science goals. So a telescope that merely achieves its stated science goals is not achieving its potential scientific productivity. Several next-generation astronomical survey telescopes are currently being designed and constructed that will significantly expand the volume of observational parameter space, and should in principle discover unexpected new phenomena and new types of object. However, the complexity of the telescopes and the large data volumes mean that these discoveries are unlikely to be found by chance. Therefore, it is necessary to plan explicitly for these unexpected discoveries in the design and construction of the telescope. Two types of discovery are recognised: unexpected objects, and unexpected phenomena. This paper argues that next-generation astronomical surveys require an explicit process for detecting the unexpected, and proposes an implementation of this process. This implementation addresses both types of discovery, and relies heavily on machine-learning techniques, and also on theory-based simulations that encapsulate our current understanding of the Universe to compare with the data. 

*Application of the Allan Variance to Time Series Analysis in Astrometry and Geodesy: A Review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1607.04712[+https://arxiv.org/abs/1607.04712+]

The Allan variance (AVAR) was introduced 50 years ago as a statistical tool for assessing of the frequency standards deviations. For the past decades, AVAR has increasingly being used in geodesy and astrometry to assess the noise characteristics in geodetic and astrometric time series. A specific feature of astrometric and geodetic measurements, as compared with the clock measurements, is that they are generally associated with uncertainties; thus, an appropriate weighting should be applied during data analysis. Besides, some physically connected scalar time series naturally form series of multi-dimensional vectors. For example, three station coordinates time series X, Y, and Z can be combined to analyze 3D station position variations. The classical AVAR is not intended for processing unevenly weighted and/or multi-dimensional data. Therefore, AVAR modifications, namely weighted AVAR (WAVAR), multi-dimensional AVAR (MAVAR), and weighted multi-dimensional AVAR (WMAVAR), were introduced to overcome these deficiencies. In this paper, a brief review is given of the experience of using AVAR and its modifications in processing astro-geodetic time series. 

*Australian Aboriginal Astronomy and Navigation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1607.02215[+https://arxiv.org/abs/1607.02215+]

The traditional cultures of Aboriginal Australians include a significant astronomical component, perpetuated through oral tradition, ceremony, and art. This astronomical knowledge includes a deep understanding of the motion of objects in the sky, which was used for practical purposes such as constructing calendars and for navigation. There is also evidence that traditional Aboriginal Australians made careful records and measurements of cyclical phenomena, recorded unexpected phenomena such as eclipses and meteorite impacts, and could determine the cardinal points to an accuracy of a few degrees. Putative explanations of celestial phenomena appear throughout the oral record, suggesting traditional Aborig- inal Australians sought to understand the natural world around them, in the same way as modern scientists, but within their own cultural context. There is also a growing body of evidence for sophisticated navigational skills, including the use of astronomically based songlines. Songlines are effectively oral maps of the landscape, and are an efficient way of transmitting oral navigational skills in cultures that do not have a written language. The study of Aboriginal astronomy has had an impact extending beyond mere academic curiosity, facilitating cross-cultural understanding, demonstrating the intimate links between science and culture, and helping students to engage with science. 

*Analogue to Digital and Digital to Analogue Converters (ADCs and DACs): A Review Update*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is a review paper updated from that presented for CAS 2004. Essentially, since then, commercial components have continued to extend their performance boundaries but the basic building blocks and the techniques for choosing the best device and implementing it in a design have not changed. Analogue to digital and digital to analogue converters are crucial components in the continued drive to replace analogue circuitry with more controllable and less costly digital processing. This paper discusses the technologies available to perform in the likely measurement and control applications that arise within accelerators. It covers much of the terminology and 'specmanship' together with an application-oriented analysis of the realisable performance of the various types. Finally, some hints and warnings on system integration problems are given.

*Energy saving mechanisms, collective behavior and the variation range hypothesis in biological systems: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1606.08969[+https://arxiv.org/abs/1606.08969+]

Energy saving mechanisms are ubiquitous in nature. Aerodynamic and hydrodynamic drafting, vortice uplift, Bernoulli suction, thermoregulatory coupling, path following, physical hooks, synchronization, and cooperation are only some of the better-known examples. While drafting mechanisms also appear in non-biological systems such as sedimentation and particle vortices, the broad spectrum of these mechanisms appears more diversely in biological systems including bacteria, spermatozoa, various aquatic species, birds, land animals, semi-fluid dwellers like turtle hatchlings, as well as human systems. We present the thermodynamic framework for energy saving mechanisms, and we review evidence in favor of the variation range hypothesis. This hypothesis posits that, as an evolutionary process, the variation range between strongest and weakest group members converges on the equivalent energy saving quantity that is generated by the energy saving mechanism. We also review self-organized structures that emerge due to energy saving mechanisms, including convective processes that can be observed in many systems over both short and long time scales, as well as high collective output processes in which a form of collective position locking occurs.

2017
----

*Lectures on Randomized Numerical Linear Algebra* - Petros Drineas et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1712.08880[+https://arxiv.org/abs/1712.08880+]

*Magnetic Coordinate Systems* - K. M. Laundal et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-016-0275-y[+https://link.springer.com/article/10.1007/s11214-016-0275-y+]

Geospace phenomena such as the aurora, plasma motion, ionospheric currents and associated magnetic field disturbances are highly organized by Earth’s main magnetic field. This is due to the fact that the charged particles that comprise space plasma can move almost freely along magnetic field lines, but not across them. For this reason it is sensible to present such phenomena relative to Earth’s magnetic field. A large variety of magnetic coordinate systems exist, designed for different purposes and regions, ranging from the magnetopause to the ionosphere. In this paper we review the most common magnetic coordinate systems and describe how they are defined, where they are used, and how to convert between them. The definitions are presented based on the spherical harmonic expansion coefficients of the International Geomagnetic Reference Field (IGRF) and, in some of the coordinate systems, the position of the Sun which we show how to calculate from the time and date. The most detailed coordinate systems take the full IGRF into account and define magnetic latitude and longitude such that they are constant along field lines. These coordinate systems, which are useful at ionospheric altitudes, are non-orthogonal. We show how to handle vectors and vector calculus in such coordinates, and discuss how systematic errors may appear if this is not done correctly.

*Astrobiology and the Possibility of Life on Earth and Elsewhere* - H. Cottin et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-015-0196-1[+https://link.springer.com/article/10.1007/s11214-015-0196-1+]

Astrobiology is an interdisciplinary scientific field not only focused on the search of extraterrestrial life, but also on deciphering the key environmental parameters that have enabled the emergence of life on Earth. Understanding these physical and chemical parameters is fundamental knowledge necessary not only for discovering life or signs of life on other planets, but also for understanding our own terrestrial environment. Therefore, astrobiology pushes us to combine different perspectives such as the conditions on the primitive Earth, the physicochemical limits of life, exploration of habitable environments in the Solar System, and the search for signatures of life in exoplanets. Chemists, biologists, geologists, planetologists and astrophysicists are contributing extensively to this interdisciplinary research field. 

From 2011 to 2014, the European Space Agency (ESA) had the initiative to gather a Topical Team of interdisciplinary scientists focused on astrobiology to review the profound transformations in the field that have occurred since the beginning of the new century. The present paper is an interdisciplinary review of current research in astrobiology, covering the major advances and main outlooks in the field. The following subjects will be reviewed and most recent discoveries will be highlighted: the new understanding of planetary system formation including the specificity of the Earth among the diversity of planets, the origin of water on Earth and its unique combined properties among solvents for the emergence of life, the idea that the Earth could have been habitable during the Hadean Era, the inventory of endogenous and exogenous sources of organic matter and new concepts about how chemistry could evolve towards biological molecules and biological systems. In addition, many new findings show the remarkable potential life has for adaptation and survival in extreme environments. All those results from different fields of science are guiding our perspectives and strategies to look for life in other Solar System objects as well as beyond, in extrasolar worlds.

*A Hitch-hiker’s Guide to Stochastic Differential Equations* - R. Du Toit Strauss et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-017-0351-y[+https://link.springer.com/article/10.1007/s11214-017-0351-y+]

In this review, an overview of the recent history of stochastic differential equations (SDEs) in application to particle transport problems in space physics and astrophysics is given. The aim is to present a helpful working guide to the literature and at the same time introduce key principles of the SDE approach via “toy models”. Using these examples, we hope to provide an easy way for newcomers to the field to use such methods in their own research. Aspects covered are the solar modulation of cosmic rays, diffusive shock acceleration, galactic cosmic ray propagation and solar energetic particle transport. We believe that the SDE method, due to its simplicity and computational efficiency on modern computer architectures, will be of significant relevance in energetic particle studies in the years to come.

*Greenland and Antarctica Ice Sheet Mass Changes and Effects on Global Sea Level*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-016-9398-7[+https://link.springer.com/article/10.1007/s10712-016-9398-7+]

Thirteen years of GRACE data provide an excellent picture of the current mass changes of Greenland and Antarctica, with mass loss in the GRACE period 2002–2015 amounting to 265 ± 25 GT/year for Greenland (including peripheral ice caps), and 95 ± 50 GT/year for Antarctica, corresponding to 0.72 and 0.26 mm/year average global sea level change. A significant acceleration in mass loss rate is found, especially for Antarctica, while Greenland mass loss, after a corresponding acceleration period, and a record mass loss in the summer of 2012, has seen a slight decrease in short-term mass loss trend. The yearly mass balance estimates, based on point mass inversion methods, have relatively large errors, both due to uncertainties in the glacial isostatic adjustment processes, especially for Antarctica, leakage from unmodelled ocean mass changes, and (for Greenland) difficulties in separating mass signals from the Greenland ice sheet and the adjacent Canadian ice caps. The limited resolution of GRACE affects the uncertainty of total mass loss to a smaller degree; we illustrate the “real” sources of mass changes by including satellite altimetry elevation change results in a joint inversion with GRACE, showing that mass change occurs primarily associated with major outlet glaciers, as well as a narrow coastal band. For Antarctica, the primary changes are associated with the major outlet glaciers in West Antarctica (Pine Island and Thwaites Glacier systems), as well as on the Antarctic Peninsula, where major glacier accelerations have been observed after the 2002 collapse of the Larsen B Ice Shelf.

*Glacial Isostatic Adjustment and Contemporary Sea Level Rise: An Overview*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-016-9379-x[+https://link.springer.com/article/10.1007/s10712-016-9379-x+]

Glacial isostatic adjustment (GIA) encompasses a suite of geophysical phenomena accompanying the waxing and waning of continental-scale ice sheets. These involve the solid Earth, the oceans and the cryosphere both on short (decade to century) and on long (millennia) timescales. In the framework of contemporary sea-level change, the role of GIA is particular. In fact, among the processes significantly contributing to contemporary sea-level change, GIA is the only one for which deformational, gravitational and rotational effects are simultaneously operating, and for which the rheology of the solid Earth is essential. Here, I review the basic elements of the GIA theory, emphasizing the connections with current sea-level changes observed by tide gauges and altimetry. This purpose is met discussing the nature of the “sea-level equation” (SLE), which represents the basis for modeling the sea-level variations of glacial isostatic origin, also giving access to a full set of geodetic variations associated with GIA. Here, the SLE is employed to characterize the remarkable geographical variability of the GIA-induced sea-level variations, which are often expressed in terms of “fingerprints”. Using harmonic analysis, the spatial variability of the GIA fingerprints is compared to that of other components of contemporary sea-level change. In closing, some attention is devoted to the importance of the “GIA corrections” in the context of modern sea-level observations, based on tide gauges or satellite altimeters.

*Introducing Geometric Algebra to Geometric Computing Software Developers: A Computational Thinking Approach*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1705.06668[+https://arxiv.org/abs/1705.06668+]

Designing software systems for Geometric Computing applications can be a challenging task. Software engineers typically use software abstractions to hide and manage the high complexity of such systems. Without the presence of a unifying algebraic system to describe geometric models, the use of software abstractions alone can result in many design and maintenance problems. Geometric Algebra (GA) can be a universal abstract algebraic language for software engineering geometric computing applications. Few sources, however, provide enough information about GA-based software implementations targeting the software engineering community. In particular, successfully introducing GA to software engineers requires quite different approaches from introducing GA to mathematicians or physicists. This article provides a high-level introduction to the abstract concepts and algebraic representations behind the elegant GA mathematical structure. The article focuses on the conceptual and representational abstraction levels behind GA mathematics with sufficient references for more details. In addition, the article strongly recommends applying the methods of Computational Thinking in both introducing GA to software engineers, and in using GA as a mathematical language for developing Geometric Computing software systems.

*Advances in the Application of Surface Drifters*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-010816-060641[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-010816-060641+]

Surface drifting buoys, or drifters, are used in oceanographic and climate research, oil spill tracking, weather forecasting, search and rescue operations, calibration and validation of velocities from high-frequency radar and from altimeters, iceberg tracking, and support of offshore drilling operations. In this review, we present a brief history of drifters, from the message in a bottle to the latest satellite-tracked, multisensor drifters. We discuss the different types of drifters currently used for research and operations as well as drifter designs in development. We conclude with a discussion of the various properties that can be observed with drifters, with heavy emphasis on a critical process that cannot adequately be observed by any other instrument: dispersion in the upper ocean, driven by turbulence at scales from waves through the submesoscale to the large-scale geostrophic eddies.

*Integral formulas for transformation of potential field parameters in geosciences*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216301751[+https://www.sciencedirect.com/science/article/pii/S0012825216301751+]

In this article boundary-value problems of potential theory, that are required for transforming and continuing selected parameters of a potential field, are discussed. By the potential field parameters we understand the potential and its gradients (potential gradients) up to the third order. In particular, integral equations defined for a spherical boundary transforming potential gradients of different orders and continuing potential gradients of the same order through the 3-D space are reviewed and classified. This mathematical apparatus can be used for any harmonic potential, such as electric, magnetic or gravitational, under the assumption of its conservativeness, i.e., neglecting possible temporal variations. Integral transforms are discussed in context of geoscience applications, namely in terms of the Earth's gravitational field; however, the article can serve as a general reference for integral transforms of potential field parameters in any scientific or engineering area where potential fields are used.

*Geobiology and palaeogenomics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216303610[+https://www.sciencedirect.com/science/article/pii/S0012825216303610+]

Geobiology is centered on interactions of the biosphere with the atmosphere, hydrosphere, and lithosphere. In deep time geobiology has focused on critical intervals of change for Earth and its biota. Such geobiological studies include data from palaeobiology, geochemistry and sedimentary geology within an interdisciplinary framework.
Palaeogenomics has a variety of research agendas, and one of them is to understand the genes involved in key changes in the evolutionary history of life and when those genes and their interactions first evolved. The combination of geobiology and palaeogenomic studies is a powerful approach which leads to a more fundamental understanding of how Earth and life have changed through time.

For example, linking palaeogenomic studies of biomineralization genes in modern organisms and when they first evolved in the Cambrian with geobiological studies of the impact this had on the production of carbonate sedimentary facies such as encrinites and the development of the Neritan ocean provides a unique perspective on how genes have shaped formation of the sedimentary record. Another component of the Cambrian explosion, the evolution of vertical bioturbation at the start of the Cambrian, has had significant effects on biogeochemical cycling and sediment production, and provides an inviting target for future genomic studies.

The remaining Phanerozoic includes a broad variety of evolutionary innovations which within a joint geobiological and palaeogenomic context can also be profitably studied. In pelagic environments the genomic changes leading to evolution of the calcium carbonate biomineralized skeletons of coccolithophores contributed to the widespread deposition of carbonate sediment in deep settings and development in the Mesozoic of the Cretan ocean. On land, linking palaeogenomic studies of the genes that led to the Palaeozoic evolution of lignin and their associated geobiological effects, such as deposition of coal, provide another example of the effects of genes upon development of the sedimentary record. Such work will ultimately provide a history of genomic change and the environmental factors that influenced it, as well as how gen


*A mantle convection perspective on global tectonics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216304354[+https://www.sciencedirect.com/science/article/pii/S0012825216304354+]

The concept of interplay between mantle convection and tectonics goes back to about a century ago, with the proposal that convection currents in the Earth's mantle drive continental drift and deformation (Holmes, 1931). Since this time, plate tectonic theory has established itself as the fundamental framework to study surface deformation, with the remarkable ability to encompass geological and geophysical observations. Mantle convection modeling has progressed to the point where connections with plate tectonics can be made, pushing the idea that tectonics is a surface expression of the global dynamics of one single system: the mantle-lithosphere system.

Here, we present our perspective, as modelers, on the dynamics behind global tectonics with a focus on the importance of self-organisation. We first present an overview of the links between mantle convection and tectonics at the present-day, examining observations such as kinematics, stress and deformation. Despite the numerous achievements of geodynamic studies, this section sheds light on the lack of self-organisation of the models used, which precludes investigations of the feedbacks and evolution of the mantle-lithosphere system. Therefore, we review the modeling strategies, often focused on rheology, that aim at taking into account self-organisation. The fundamental objective is that plate-like behaviour emerges self-consistently in convection models.

We then proceed with the presentation of studies of continental drift, seafloor spreading and plate tectonics in convection models allowing for feedbacks between surface tectonics and mantle dynamics. We discuss the approximation of the rheology of the lithosphere used in these models (pseudo-plastic rheology), for which empirical parameters differ from those obtained in experiments. In this section, we analyse in detail a state-of-the-art 3-D spherical convection calculation, which exhibits fundamental tectonic features (continental drift, one-sided subduction, trench and ridge evolution, transform shear zones, small-scale convection, and plume tectonics). This example leads to a discussion where we try to answer the following question: can mantle convection models transcend the limitations of plate tectonic theory?

*Short-term orbital forcing: A quasi-review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216303865[+https://www.sciencedirect.com/science/article/pii/S0012825216303865+]

The aim of this paper is to provide geoscientists with the most accurate set of the Earth's astro-climatic parameters and daily insolation quantities, able to describe the Short-Term Orbital Forcing (STOF) as represented by the ever-changing incoming solar radiation. We provide an updated review and a pragmatic tool/database using the latest astronomical models and orbital ephemeris, for the entire Holocene and 1 kyr into the future. Our results are compared with the most important database produced for studying long-term orbital forcing showing no systematic discrepancies over the full thirteen thousand years period studied. Our detailed analysis of the periods present in STOF, as perturbed by Solar System bodies, yields a very rich dynamical modulation on annual-to-decadal timescales when compared to previous results. In addition, we addressed, for the first time, the error committed considering daily insolation as a continuous function of orbital longitudes with respect to the nominal values, i.e., calculating the corresponding daily insolation with orbital longitudes tabulated at noon. We found important relative differences up to ± 5%, which correspond to errors of 2.5 W m −2 in the daily mean insolation, for exactly the same calendar day and set of astro-climatic parameters. This previously unrecognized error could have a significant impact in both the initial and boundary conditions for any climate modeling experiment.

*The sensitivity of gas hydrate reservoirs to climate change*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216304378[+https://www.sciencedirect.com/science/article/pii/S0012825216304378+]

Gas hydrate reservoirs store large quantities of gas in sediments on continental margins, in deep lakes, and in continental and relic sub-shelf permafrost. The gas hydrate structure is only stable at sufficiently low temperature and high pressure, and may therefore collapse under changing climatic conditions. If a temperature rise or pressure drop (e.g. through falling sea level) is effective enough to dissociate hydrate deposits, methane (the most common gas component in hydrates and a potent greenhouse gas) is released from the hydrate structure and may eventually enter into the atmosphere. This may generate a positive feedback effect, as resulting enhanced greenhouse gas levels would additionally warm the atmosphere and hence maintain or reinforce hydrate dissociation. The significance of this mechanism has been debated over the past decades, often within the framework of geologically rapid Quaternary climatic oscillations and present-day climate warming. An extensive set of studies has addressed the climate-sensitivity of gas hydrate reservoirs in various study areas and geological settings, and by means of various approaches. No real consensus has yet been reached on the matter. In this study, we seek to evaluate the sensitivity of gas hydrate reservoirs to changes in global climate from a more general perspective, by firstly reviewing the available literature, and secondly developing a new numerical model to quantify gas hydrate destabilization under changing environmental conditions. Qualities of the model include the wide applicability to both marine and permafrost-related hydrate reservoirs and the integrative approach, combining existing hydrate formation models with a dissocation model that accounts for the consumption of latent heat during hydrate dissociation. To determine which settings are most vulnerable, and to acquire insight into the extent, fashion and rates of hydrate dissociation, we apply the model to four distinct types of hydrate reservoirs across a hypothetic high-latitude continental margin under two specific cases of climate change: the last deglaciation following the Last Glacial Maximum and present-day climate warming. The simulations indicate that hydrates on the upper continental slope and in association with thin, sub-shelf permafrost are most sensitive to the imposed climatic variations, whereas deepwater and onshore permafrost-related reservoirs react in a more stable way. However, the deep (i.e. at several tens to hundreds of meters subsurface depth) stratigraphic-type hydrates considered in this study constitute by far the largest fraction of the global gas hydrate volume, but dissociate on slow timescales of thousands to hundreds of thousands of years, even in the most sensitive environments. In contrast, shallow (i.e. at, or a few meters below the surface or seafloor) structural-type hydrates are able to respond to climatic variations on sub-millennial timescales, but the volumes of gas they may release are probably insignificant to the global carbon cycle and climate. Quaternary and present-day climate change do affect the stability of gas hydrate reservoirs, but at long timescales where hydrate volumes are large, and on short timescales where hydrate volumes are small. Consequently, gas hydrates dissociate to an extent that is too small or at a pace that is too slow to create a strong positive feedback effect. While the release of methane from the disintegration of gas hydrates is observed on different margins today, it is not likely to have played a leading role in Quaternary climatic variations or to become a significant process in the coming centuries as a result of present-day rising temperatures.

*Review of the impacts of leaking CO2 gas and brine on groundwater quality*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216302082[+https://www.sciencedirect.com/science/article/pii/S0012825216302082+]

This paper provides an overview of the existing data and knowledge presented in recent literature about the potential leaking of CO2 from the deep subsurface storage reservoirs and the effects on groundwater quality. The objectives are to: 1) present data and discuss potential risks associated with the groundwater quality degradation due to CO2 gas and brine exposure; 2) identify the set of geochemical data required to develop models to assess and predict aquifer responses to CO2 and brine leakage; and 3) present a summary of the findings and reveal future trends in this important and expanding research area. The discussion is focused around aquifer responses to CO2 gas and brine exposure and the degree of impact; major hydrogeological and geochemical processes and site-specific properties known to control aquifer quality under CO2 exposure conditions; contributions from the deep reservoirs (plume characteristics and composition); and the possibility of establishment of a new network of reactions and processes affecting or controlling the overall mobility of major, minor, and trace elements and the fate of the elements released from sediments or transported with brine. This paper also includes a discussion on the development of conceptual and reduced order models (ROMs) to describe and predict aquifer responses and whether or not the release of metals following exposure to CO2 is harmful, which are an essential tool for CO2 sequestration related risk assessment. Future research needs in this area are also included at the end of the paper.

*The global monsoon across time scales: Mechanisms and outstanding issues*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825216302070[+https://www.sciencedirect.com/science/article/pii/S0012825216302070+]

The present paper addresses driving mechanisms of global monsoon (GM) variability and outstanding issues in GM science. This is the second synthesis of the PAGES GM Working Group following the first synthesis “The Global Monsoon across Time Scales: coherent variability of regional monsoons” published in 2014 (Climate of the Past, 10, 2007–2052). Here we introduce the GM as a planetary scale circulation system and give a brief accounting of why it exhibits regional structure. The primary driver of the GM is solar insolation, and the specific features in the underlying surface, such as land-sea distribution, topography, and oceanic circulations, are mainly responsible for the differences among regional monsoon systems. We then analyze the monsoon formation mechanisms, together with the major processes that drive monsoon variations at various timescales, including external forcings and internal feedbacks. On long time scales, external forcings often induce variability on a global scale, whereas short-term variability in regional monsoon systems is usually caused by internal feedbacks within the climate system. Finally, a number of debatable issues are discussed, with an emphasis on time scales beyond the instrumental record. These include the dual nature of the monsoon as wind and rain, the meaning of oxygen isotope in hydrological cycle, in particular of speleothem δ18O, the role of ice-sheet in monsoon variations, etc. In general, the GM as a system comprises a hierarchy of regional and local monsoons with various degrees of similarity, though all show coherent variability driven by a common solar forcing. The goal of the GM concept, therefore, is by no means to replace or diminish research on the regional monsoons, but to help dissect the mechanisms and controlling factors of monsoon variability at various temporal-spatial scales.

*Frontiers of chaotic advection*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.89.025007[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.89.025007+]

This work reviews the present position of and surveys future perspectives in the physics of chaotic advection: the field that emerged three decades ago at the intersection of fluid mechanics and nonlinear dynamics, which encompasses a range of applications with length scales ranging from micrometers to hundreds of kilometers, including systems as diverse as mixing and thermal processing of viscous fluids, microfluidics, biological flows, and oceanographic and atmospheric flows.

*Coupling functions: Universal insights into dynamical interaction mechanisms*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.89.045001[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.89.045001+]

The dynamical systems found in nature are rarely isolated. Instead they interact and influence each other. The coupling functions that connect them contain detailed information about the functional mechanisms underlying the interactions and prescribe the physical rule specifying how an interaction occurs. A coherent and comprehensive review is presented encompassing the rapid progress made recently in the analysis, understanding, and applications of coupling functions. The basic concepts and characteristics of coupling functions are presented through demonstrative examples of different domains, revealing the mechanisms and emphasizing their multivariate nature. The theory of coupling functions is discussed through gradually increasing complexity from strong and weak interactions to globally coupled systems and networks. A variety of methods that have been developed for the detection and reconstruction of coupling functions from measured data is described. These methods are based on different statistical techniques for dynamical inference. Stemming from physics, such methods are being applied in diverse areas of science and technology, including chemistry, biology, physiology, neuroscience, social sciences, mechanics, and secure communications. This breadth of application illustrates the universality of coupling functions for studying the interaction mechanisms of coupled dynamical systems.

*Master equations and the theory of stochastic path integrals*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa5ae2[+https://iopscience.iop.org/article/10.1088/1361-6633/aa5ae2+]

This review provides a pedagogic and self-contained introduction to master equations and to their representation by path integrals. Since the 1930s, master equations have served as a fundamental tool to understand the role of fluctuations in complex biological, chemical, and physical systems. Despite their simple appearance, analyses of master equations most often rely on low-noise approximations such as the Kramers–Moyal or the system size expansion, or require ad-hoc closure schemes for the derivation of low-order moment equations. We focus on numerical and analytical methods going beyond the low-noise limit and provide a unified framework for the study of master equations. After deriving the forward and backward master equations from the Chapman–Kolmogorov equation, we show how the two master equations can be cast into either of four linear partial differential equations (PDEs). Three of these PDEs are discussed in detail. The first PDE governs the time evolution of a generalized probability generating function whose basis depends on the stochastic process under consideration. Spectral methods, WKB approximations, and a variational approach have been proposed for the analysis of the PDE. The second PDE is novel and is obeyed by a distribution that is marginalized over an initial state. It proves useful for the computation of mean extinction times. The third PDE describes the time evolution of a 'generating functional', which generalizes the so-called Poisson representation. Subsequently, the solutions of the PDEs are expressed in terms of two path integrals: a 'forward' and a 'backward' path integral. Combined with inverse transformations, one obtains two distinct path integral representations of the conditional probability distribution solving the master equations. We exemplify both path integrals in analysing elementary chemical reactions. Moreover, we show how a well-known path integral representation of averaged observables can be recovered from them. Upon expanding the forward and the backward path integrals around stationary paths, we then discuss and extend a recent method for the computation of rare event probabilities. Besides, we also derive path integral representations for processes with continuous state spaces whose forward and backward master equations admit Kramers–Moyal expansions. A truncation of the backward expansion at the level of a diffusion approximation recovers a classic path integral representation of the (backward) Fokker–Planck equation. One can rewrite this path integral in terms of an Onsager–Machlup function and, for purely diffusive Brownian motion, it simplifies to the path integral of Wiener. To make this review accessible to a broad community, we have used the language of probability theory rather than quantum (field) theory and do not assume any knowledge of the latter. The probabilistic structures underpinning various technical concepts, such as coherent states, the Doi-shift, and normal-ordered observables, are thereby made explicit.

*Models of life: epigenetics, diversity and cycles*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa5aeb[+https://iopscience.iop.org/article/10.1088/1361-6633/aa5aeb+]

This review emphasizes aspects of biology that can be understood through repeated applications of simple causal rules. The selected topics include perspectives on gene regulation, phage lambda development, epigenetics, microbial ecology, as well as model approaches to diversity and to punctuated equilibrium in evolution. Two outstanding features are repeatedly described. One is the minimal number of rules to sustain specific states of complex systems for a long time. The other is the collapse of such states and the subsequent dynamical cycle of situations that restitute the system to a potentially new metastable state.

*Origins of life: a problem for physics, a key issues review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa7804[+https://iopscience.iop.org/article/10.1088/1361-6633/aa7804+]

The origins of life stands among the great open scientific questions of our time. While a number of proposals exist for possible starting points in the pathway from non-living to living matter, these have so far not achieved states of complexity that are anywhere near that of even the simplest living systems. A key challenge is identifying the properties of living matter that might distinguish living and non-living physical systems such that we might build new life in the lab. This review is geared towards covering major viewpoints on the origin of life for those new to the origin of life field, with a forward look towards considering what it might take for a physical theory that universally explains the phenomenon of life to arise from the seemingly disconnected array of ideas proposed thus far. The hope is that a theory akin to our other theories in fundamental physics might one day emerge to explain the phenomenon of life, and in turn finally permit solving its origins.

*Glacial seismology*
~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa8473[+https://iopscience.iop.org/article/10.1088/1361-6633/aa8473+]

Seismic source and wave propagation studies contribute to understanding structure, transport, fracture mechanics, mass balance, and other processes within glaciers and surrounding environments. Glaciogenic seismic waves readily couple with the bulk Earth, and can be recorded by seismographs deployed at local to global ranges. Although the fracturing, ablating, melting, and/or highly irregular environment of active glaciers can be highly unstable and hazardous, informative seismic measurements can commonly be made at stable proximal ice or rock sites. Seismology also contributes more broadly to emerging studies of elastic and gravity wave coupling between the atmosphere, oceans, solid Earth, and cryosphere, and recent scientific and technical advances have produced glaciological/seismological collaborations across a broad range of scales and processes. This importantly includes improved insight into the responses of cryospheric systems to changing climate and other environmental conditions. Here, we review relevant fundamental physics and glaciology, and provide a broad review of the current state of glacial seismology and its rapidly evolving future directions.

*Randomness in quantum mechanics: philosophy, physics and technology*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa8731[+https://iopscience.iop.org/article/10.1088/1361-6633/aa8731+]

This progress report covers recent developments in the area of quantum randomness, which is an extraordinarily interdisciplinary area that belongs not only to physics, but also to philosophy, mathematics, computer science, and technology. For this reason the article contains three parts that will be essentially devoted to different aspects of quantum randomness, and even directed, although not restricted, to various audiences: a philosophical part, a physical part, and a technological part. For these reasons the article is written on an elementary level, combining simple and non-technical descriptions with a concise review of more advanced results. In this way readers of various provenances will be able to gain while reading the article.

*Limits of predictions in thermodynamic systems: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa9101[+https://iopscience.iop.org/article/10.1088/1361-6633/aa9101+]

The past twenty years have seen a resurgence of interest in nonequilibrium thermodynamics, thanks to advances in the theory of stochastic processes and in their thermodynamic interpretation. Fluctuation theorems provide fundamental constraints on the dynamics of systems arbitrarily far from thermal equilibrium. Thermodynamic uncertainty relations bound the dissipative cost of precision in a wide variety of processes. Concepts of excess work and excess heat provide the basis for a complete thermodynamics of nonequilibrium steady states, including generalized Clausius relations and thermodynamic potentials. But these general results carry their own limitations: fluctuation theorems involve exponential averages that can depend sensitively on unobservably rare trajectories; steady-state thermodynamics makes use of a dual dynamics that lacks any direct physical interpretation. This review aims to present these central results of contemporary nonequilibrium thermodynamics in such a way that the power of each claim for making physical predictions can be clearly assessed, using examples from current topics in soft matter and biophysics.

*Time crystals: a review*
~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa8b38[+https://iopscience.iop.org/article/10.1088/1361-6633/aa8b38+]

Time crystals are time-periodic self-organized structures postulated by Frank Wilczek in 2012. While the original concept was strongly criticized, it stimulated at the same time an intensive research leading to propositions and experimental verifications of discrete (or Floquet) time crystals—the structures that appear in the time domain due to spontaneous breaking of discrete time translation symmetry. The struggle to observe discrete time crystals is reviewed here together with propositions that generalize this concept introducing condensed matter like physics in the time domain. We shall also revisit the original Wilczek's idea and review strategies aimed at spontaneous breaking of continuous time translation symmetry.

*Key issues review: evolution on rugged adaptive landscapes*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa94d4[+https://iopscience.iop.org/article/10.1088/1361-6633/aa94d4+]

Adaptive landscapes represent a mapping between genotype and fitness. Rugged adaptive landscapes contain two or more adaptive peaks: allele combinations with higher fitness than any of their neighbors in the genetic space. How do populations evolve on such rugged landscapes? Evolutionary biologists have struggled with this question since it was first introduced in the 1930s by Sewall Wright.

Discoveries in the fields of genetics and biochemistry inspired various mathematical models of adaptive landscapes. The development of landscape models led to numerous theoretical studies analyzing evolution on rugged landscapes under different biological conditions. The large body of theoretical work suggests that adaptive landscapes are major determinants of the progress and outcome of evolutionary processes.

Recent technological advances in molecular biology and microbiology allow experimenters to measure adaptive values of large sets of allele combinations and construct empirical adaptive landscapes for the first time. Such empirical landscapes have already been generated in bacteria, yeast, viruses, and fungi, and are contributing to new insights about evolution on adaptive landscapes.

In this Key Issues Review we will: (i) introduce the concept of adaptive landscapes; (ii) review the major theoretical studies of evolution on rugged landscapes; (iii) review some of the recently obtained empirical adaptive landscapes; (iv) discuss recent mathematical and statistical analyses motivated by empirical adaptive landscapes, as well as provide the reader with instructions and source code to implement simulations of evolution on adaptive landscapes; and (v) discuss possible future directions for this exciting field.

*Perspectives on theory at the interface of physics and biology*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa995b[+https://iopscience.iop.org/article/10.1088/1361-6633/aa995b+]

Theoretical physics is the search for simple and universal mathematical descriptions of the natural world. In contrast, much of modern biology is an exploration of the complexity and diversity of life. For many, this contrast is prima facie evidence that theory, in the sense that physicists use the word, is impossible in a biological context. For others, this contrast serves to highlight a grand challenge. I am an optimist, and believe (along with many colleagues) that the time is ripe for the emergence of a more unified theoretical physics of biological systems, building on successes in thinking about particular phenomena. In this essay I try to explain the reasons for my optimism, through a combination of historical and modern examples.

*Short-pulse lasers for weather control*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aa8488[+https://iopscience.iop.org/article/10.1088/1361-6633/aa8488+]

Filamentation of ultra-short TW-class lasers recently opened new perspectives in atmospheric research. Laser filaments are self-sustained light structures of 0.1–1 mm in diameter, spanning over hundreds of meters in length, and producing a low density plasma (1015–1017 cm−3) along their path. They stem from the dynamic balance between Kerr self-focusing and defocusing by the self-generated plasma and/or non-linear polarization saturation.

While non-linearly propagating in air, these filamentary structures produce a coherent supercontinuum (from 230 nm to 4 µm, for a 800 nm laser wavelength) by self-phase modulation (SPM), which can be used for remote 3D-monitoring of atmospheric components by Lidar (Light Detection and Ranging). However, due to their high intensity (1013–1014 W cm−2), they also modify the chemical composition of the air via photo-ionization and photo-dissociation of the molecules and aerosols present in the laser path. These unique properties were recently exploited for investigating the capability of modulating some key atmospheric processes, like lightning from thunderclouds, water vapor condensation, fog formation and dissipation, and light scattering (albedo) from high altitude clouds for radiative forcing management. Here we review recent spectacular advances in this context, achieved both in the laboratory and in the field, reveal their underlying mechanisms, and discuss the applicability of using these new non-linear photonic catalysts for real scale weather control.

*How to Characterize Habitable Worlds and Signs of Life*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-082214-122238[+https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-082214-122238+]

The detection of exoplanets orbiting other stars has revolutionized our view of the cosmos. First results suggest that it is teeming with a fascinating diversity of rocky planets, including those in the habitable zone. Even our closest star, Proxima Centauri, harbors a small planet in its habitable zone, Proxima b. With the next generation of telescopes, we will be able to peer into the atmospheres of rocky planets and get a glimpse into other worlds. Using our own planet and its wide range of biota as a Rosetta stone, we explore how we could detect habitability and signs of life on exoplanets over interstellar distances. Current telescopes are not yet powerful enough to characterize habitable exoplanets, but the next generation of telescopes that is already being built will have the capabilities to characterize close-by habitable worlds. The discussion on what makes a planet a habitat and how to detect signs of life is lively. This review will show the latest results, the challenges of how to identify and characterize such habitable worlds, and how near-future telescopes will revolutionize the field. For the first time in human history, we have developed the technology to detect potential habitable worlds. Finding thousands of exoplanets has taken the field of comparative planetology beyond the Solar System.

*Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-082214-122339[+https://www.annualreviews.org/doi/abs/10.1146/annurev-astro-082214-122339+]

Markov chain Monte Carlo–based Bayesian data analysis has now become the method of choice for analyzing and interpreting data in almost all disciplines of science. In astronomy, over the past decade, we have also seen a steady increase in the number of papers that employ Monte Carlo–based Bayesian analysis. New, efficient Monte Carlo–based methods are continuously being developed and explored. In this review, we first explain the basics of Bayesian theory and discuss how to set up data analysis problems within this framework. Next, we provide an overview of various Monte Carlo–based methods for performing Bayesian data analysis. Finally, we discuss advanced ideas that enable us to tackle complex problems and thus hold great promise for the future. We also distribute downloadable computer software (https://github.com/sanjibs/bmcmc) Python that implements some of the algorithms and examples discussed here.

*Comparison of variational balance models for the rotating shallow water equations*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1705.05579[+https://arxiv.org/abs/1705.05579+]

We present an extensive numerical comparison of a family of balance models appropriate to the semi-geostrophic limit of the rotating shallow water equations, and derived by variational asymptotics in Oliver (2006) for small Rossby numbers Ro. This family of generalized large-scale semi-geostrophic (GLSG) models contains the L1-model introduced by Salmon (1983) as a special case. We use these models to produce balanced initial states for the full shallow water equations. We then numerically investigate how well these models capture the dynamics of an initially balanced shallow water flow. It is shown that, whereas the L1-member of the GLSG family is able to reproduce the balanced dynamics of the full shallow water equations on time scales of O(1/Ro) very well, all other members develop significant unphysical high wavenumber contributions in the ageostrophic vorticity which spoil the dynamics.

*Extragalactic Radio Continuum Surveys and the Transformation of Radio Astronomy*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1709.05064[+https://arxiv.org/abs/1709.05064+]

Next-generation radio surveys are about to transform radio astronomy by discovering and studying tens of millions of previously unknown radio sources. These surveys will provide new insights to understand the evolution of galaxies, measuring the evolution of the cosmic star formation rate, and rivalling traditional techniques in the measurement of fundamental cosmological parameters. By observing a new volume of observational parameter space, they are also likely to discover unexpected new phenomena. This review traces the evolution of extragalactic radio continuum surveys from the earliest days of radio astronomy to the present, and identifies the challenges that must be overcome to achieve this transformational change. 

*Statistical physics of human cooperation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1705.07161[+https://arxiv.org/abs/1705.07161+]

Extensive cooperation among unrelated individuals is unique to humans, who often sacrifice personal benefits for the common good and work together to achieve what they are unable to execute alone. The evolutionary success of our species is indeed due, to a large degree, to our unparalleled other-regarding abilities. Yet, a comprehensive understanding of human cooperation remains a formidable challenge. Recent research in social science indicates that it is important to focus on the collective behavior that emerges as the result of the interactions among individuals, groups, and even societies. Non-equilibrium statistical physics, in particular Monte Carlo methods and the theory of collective behavior of interacting particles near phase transition points, has proven to be very valuable for understanding counterintuitive evolutionary outcomes. By studying models of human cooperation as classical spin models, a physicist can draw on familiar settings from statistical physics. However, unlike pairwise interactions among particles that typically govern solid-state physics systems, interactions among humans often involve group interactions, and they also involve a larger number of possible states even for the most simplified description of reality. The complexity of solutions therefore often surpasses that observed in physical systems. Here we review experimental and theoretical research that advances our understanding of human cooperation, focusing on spatial pattern formation, on the spatiotemporal dynamics of observed solutions, and on self-organization that may either promote or hinder socially favorable states.

2018
----

*A Review of automatic differentiation and its efficient implementation* - Charles C. Margossian
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1811.05031[+https://arxiv.org/abs/1811.05031+]

Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation. 

*A Tutorial on Bayesian Optimization* - Peter I. Frazier
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1807.02811[+https://arxiv.org/abs/1807.02811+]

Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.

*Computational and applied topology, tutorial* - Paweł Dłotko
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1807.08607[+https://arxiv.org/abs/1807.08607+]

This is a tutorial in applied and computational topology and topological data analysis. It is illustrated with numerous computational examples that utilize Gudhi library. It is under constant development, so please do not consider this version as final.

*Computing with Chemical Reaction Networks: A Tutorial* - Robert Brijder
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1811.10361[+https://arxiv.org/abs/1811.10361+]

Chemical reaction networks (CRNs) model the behavior of chemical reactions in well-mixed solutions and they can be designed to perform computations. In this tutorial we give an overview of various computational models for CRNs. Moreover, we discuss a method to implement arbitrary (abstract) CRNs in a test tube using DNA. Finally, we discuss relationships between CRNs and other models of computation.

*A Tutorial on Formulating and Using QUBO Models* - F. Glover et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1811.11538[+https://arxiv.org/abs/1811.11538+]

The Quadratic Unconstrained Binary Optimization (QUBO) model has gained prominence in recent years with the discovery that it unifies a rich variety of combinatorial optimization problems. By its association with the Ising problem in physics, the QUBO model has emerged as an underpinning of the quantum computing area known as quantum annealing and has become a subject of study in neuromorphic computing. Through these connections, QUBO models lie at the heart of experimentation carried out with quantum computers developed by D-Wave Systems and neuromorphic computers developed by IBM. Computational experience is being amassed by both the classical and the quantum computing communities that highlights not only the potential of the QUBO model but also its effectiveness as an alternative to traditional modeling and solution methodologies. This tutorial discloses the basic features of the QUBO model that give it the power and flexibility to encompass the range of applications that have thrust it onto center stage of the optimization field. We show how many different types of constraining relationships arising in practice can be embodied within the "unconstrained" QUBO formulation in a very natural manner using penalty functions, yielding exact model representations in contrast to the approximate representations produced by customary uses of penalty functions. Each step of generating such models is illustrated in detail by simple numerical examples, to highlight the convenience of using QUBO models in numerous settings. We also describe recent innovations for solving QUBO models that offer a fertile avenue for integrating classical and quantum computing and for applying these models in machine learning.

*Seven Sketches in Compositionality: An Invitation to Applied Category Theory* - Brendan Fong et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1803.05316[+https://arxiv.org/abs/1803.05316+]

This book is an invitation to discover advanced topics in category theory through concrete, real-world examples. It aims to give a tour: a gentle, quick introduction to guide later exploration. The tour takes place over seven sketches, each pairing an evocative application, such as databases, electric circuits, or dynamical systems, with the exploration of a categorical structure, such as adjoint functors, enriched categories, or toposes.
No prior knowledge of category theory is assumed.

*The Importance of Water for Life* - F. Westall et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-018-0476-7[+https://link.springer.com/article/10.1007/s11214-018-0476-7+]

Liquid water is essential for life as we know it, i.e. carbon-based life. Although other compound-solvent pairs that could exist in very specific physical environments could be envisaged, the elements essential to carbon and water-based life are among the most common in the universe. Carbon molecules and liquid water have physical and chemical properties that make them optimised compound-solvent pairs. Liquid water is essential for important prebiotic reactions. But equally important for the emergence of life is the contact of carbon molecules in liquid water with hot rocks and minerals. We here review the environmental conditions of the early Earth, as soon as it had liquid water at its surface and was habitable. Basing our approach to life as a “cosmic phenomenon” (de Duve 1995), i.e. a chemical continuum, we briefly address the various hypotheses for the origin of life, noting their relevance with respect to early environmental conditions. It appears that hydrothermal environments were important in this respect. We continue with the record of early life noting that, by 3.5 Ga, when the sedimentary environment started being well-preserved, anaerobic life forms had colonised all habitable microenvironments from the sea floor to exposed beach environments and, possibly, in the photic planktonic zone of the sea. Life on Earth had also evolved to the relatively sophisticated stage of anoxygenic photosynthesis. We conclude with an evaluation of the potential for habitability and colonisation of other planets and satellites in the Solar System, noting that the most common life forms in the Solar System and probably in the Universe would be similar to terrestrial chemotrophs whose carbon source is either reduced carbon or CO2 dissolved in water and whose energy would be sourced from oxidized carbon, H2, or other transition elements.

*Order out of Randomness: Self-Organization Processes in Astrophysics* - M. J. Aschwanden et al.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-018-0489-2[+https://link.springer.com/article/10.1007/s11214-018-0489-2+]

Self-organization is a property of dissipative nonlinear processes that are governed by a global driving force and a local positive feedback mechanism, which creates regular geometric and/or temporal patterns, and decreases the entropy locally, in contrast to random processes. Here we investigate for the first time a comprehensive number of (17) self-organization processes that operate in planetary physics, solar physics, stellar physics, galactic physics, and cosmology. Self-organizing systems create spontaneous “order out of randomness”, during the evolution from an initially disordered system to an ordered quasi-stationary system, mostly by quasi-periodic limit-cycle dynamics, but also by harmonic (mechanical or gyromagnetic) resonances. The global driving force can be due to gravity, electromagnetic forces, mechanical forces (e.g., rotation or differential rotation), thermal pressure, or acceleration of nonthermal particles, while the positive feedback mechanism is often an instability, such as the magneto-rotational (Balbus-Hawley) instability, the convective (Rayleigh-Bénard) instability, turbulence, vortex attraction, magnetic reconnection, plasma condensation, or a loss-cone instability. Physical models of astrophysical self-organization processes require hydrodynamic, magneto-hydrodynamic (MHD), plasma, or N-body simulations. Analytical formulations of self-organizing systems generally involve coupled differential equations with limit-cycle solutions of the Lotka-Volterra or Hopf-bifurcation type.

*Venus: The Atmosphere, Climate, Surface, Interior and Near-Space Environment of an Earth-Like Planet*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-018-0467-8[+https://link.springer.com/article/10.1007/s11214-018-0467-8+]

This is a review of current knowledge about Earth’s nearest planetary neighbour and near twin, Venus. Such knowledge has recently been extended by the European Venus Express and the Japanese Akatsuki spacecraft in orbit around the planet; these missions and their achievements are concisely described in the first part of the review, along with a summary of previous Venus observations. The scientific discussions which follow are divided into three main sections: on the surface and interior; the atmosphere and climate; and the thermosphere, exosphere and magnetosphere. These reports are intended to provide an overview for the general reader, and also an introduction to the more detailed topical surveys in the following articles in this issue, where full references to original material may be found.

*Astronomical Distance Determination in the Space Age* - B. Czerny et al
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s11214-018-0466-9[+https://link.springer.com/article/10.1007/s11214-018-0466-9+]

The formal division of the distance indicators into primary and secondary leads to difficulties in description of methods which can actually be used in two ways: with, and without the support of the other methods for scaling. Thus instead of concentrating on the scaling requirement we concentrate on all methods of distance determination to extragalactic sources which are designated, at least formally, to use for individual sources. Among those, the Supernovae Ia is clearly the leader due to its enormous success in determination of the expansion rate of the Universe. However, new methods are rapidly developing, and there is also a progress in more traditional methods. We give a general overview of the methods but we mostly concentrate on the most recent developments in each field, and future expectations.

*Lightning Discharges, Cosmic Rays and Climate*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-018-9469-z[+https://link.springer.com/article/10.1007/s10712-018-9469-z+]

The entirety of the Earth’s climate system is continuously bombarded by cosmic rays and exhibits about 2000 thunderstorms active at any time of the day all over the globe. Any linkage among these vast systems should have global consequences. Numerous studies done in the past deal with partial links between some selected aspects of this grand linkage. Results of these studies vary from weakly to strongly significant and are not yet complete enough to justify the physical mechanism proposed to explain such links. This review is aimed at presenting the current understanding, based on the past studies on the link between cosmic ray, lightning and climate. The deficiencies in some proposed links are pointed out. Impacts of cosmic rays on engineering systems and the possible effects of cosmic rays on human health are also briefly discussed. Also enumerated are some problems for future work which may help in developing the grand linkage among these three vast systems.

*The Earth’s Magnetosphere: A Systems Science Overview and Assessment*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-018-9487-x[+https://link.springer.com/article/10.1007/s10712-018-9487-x+]

A systems science examination of the Earth’s fully interconnected dynamic magnetosphere is presented. Here the magnetospheric system (a.k.a. the magnetosphere–ionosphere–thermosphere system) is considered to be comprised of 14 interconnected subsystems, where each subsystem is a characteristic particle population: 12 of those particle populations are plasmas and two (the atmosphere and the hydrogen geocorona) are neutrals. For the magnetospheric system, an assessment is made of the applicability of several system descriptors, such as adaptive, nonlinear, dissipative, interdependent, open, irreversible, and complex. The 14 subsystems of the magnetospheric system are cataloged and described, and the various types of magnetospheric waves that couple the behaviors of the subsystems to each other are explained. This yields a roadmap of the connectivity of the magnetospheric system. Various forms of magnetospheric activity beyond geomagnetic activity are reviewed, and four examples of emergent phenomena in the Earth’s magnetosphere are presented. Prior systems science investigations of the solar-wind-driven magnetospheric system are discussed: up to the present these investigations have not accounted for the full interconnectedness of the system. This overview and assessment of the Earth’s magnetosphere hopes to facilitate (1) future global systems science studies that involve the entire interconnected magnetospheric system with its diverse time and spatial scales and (2) connections of magnetospheric systems science with the broader Earth systems science.

*Comparing Climate Sensitivity, Past and Present*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063242[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063242+]

Climate sensitivity represents the global mean temperature change caused by changes in the radiative balance of climate; it is studied for both present/future (actuo) and past (paleo) climate variations, with the former based on instrumental records and/or various types of model simulations. Paleo-estimates are often considered informative for assessments of actuo-climate change caused by anthropogenic greenhouse forcing, but this utility remains debated because of concerns about the impacts of uncertainties, assumptions, and incomplete knowledge about controlling mechanisms in the dynamic climate system, with its multiple interacting feedbacks and their potential dependence on the climate background state. This is exacerbated by the need to assess actuo- and paleoclimate sensitivity over different timescales, with different drivers, and with different (data and/or model) limitations. Here, we visualize these impacts with idealized representations that graphically illustrate the nature of time-dependent actuo- and paleoclimate sensitivity estimates, evaluating the strengths, weaknesses, agreements, and differences of the two approaches. We also highlight priorities for future research to improve the use of paleo-estimates in evaluations of current climate change.

*The Bottom Boundary Layer*
~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063351[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063351+]

The oceanic bottom boundary layer extracts energy and momentum from the overlying flow, mediates the fate of near-bottom substances, and generates bedforms that retard the flow and affect benthic processes. The bottom boundary layer is forced by winds, waves, tides, and buoyancy and is influenced by surface waves, internal waves, and stratification by heat, salt, and suspended sediments. This review focuses on the coastal ocean. The main points are that (a) classical turbulence concepts and modern turbulence parameterizations provide accurate representations of the structure and turbulent fluxes under conditions in which the underlying assumptions hold, (b) modern sensors and analyses enable high-quality direct or near-direct measurements of the turbulent fluxes and dissipation rates, and (c) the remaining challenges include the interaction of waves and currents with the erodible seabed, the impact of layer-scale two- and three-dimensional instabilities, and the role of the bottom boundary layer in shelf-slope exchange.

*Mixing Efficiency in the Ocean*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063643[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-121916-063643+]

Mixing efficiency is the ratio of the net change in potential energy to the energy expended in producing the mixing. Parameterizations of efficiency and of related mixing coefficients are needed to estimate diapycnal diffusivity from measurements of the turbulent dissipation rate. Comparing diffusivities from microstructure profiling with those inferred from the thickening rate of four simultaneous tracer releases has verified, within observational accuracy, 0.2 as the mixing coefficient over a 30-fold range of diapycnal diffusivities. Although some mixing coefficients can be estimated from pycnocline measurements, at present mixing efficiency must be obtained from channel flows, laboratory experiments, and numerical simulations. Reviewing the different approaches demonstrates that estimates and parameterizations for mixing efficiency and coefficients are not converging beyond the at-sea comparisons with tracer releases, leading to recommendations for a community approach to address this important issue.

*100 Years of the Ocean General Circulation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.ametsoc.org/doi/abs/10.1175/AMSMONOGRAPHS-D-18-0002.1[+https://journals.ametsoc.org/doi/abs/10.1175/AMSMONOGRAPHS-D-18-0002.1+]

The central change in understanding of the ocean circulation during the past 100 years has been its emergence as an intensely time-dependent, effectively turbulent and wave-dominated, flow. Early technologies for making the difficult observations were adequate only to depict large-scale, quasi-steady flows. With the electronic revolution of the past 50+ years, the emergence of geophysical fluid dynamics, the strongly inhomogeneous time-dependent nature of oceanic circulation physics finally emerged. Mesoscale (balanced), submesoscale oceanic eddies at 100-km horizontal scales and shorter, and internal waves are now known to be central to much of the behavior of the system. Ocean circulation is now recognized to involve both eddies and larger-scale flows with dominant elements and their interactions varying among the classical gyres, the boundary current regions, the Southern Ocean, and the tropics.

*Infragravity waves: From driving mechanisms to impacts*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825217303239[+https://www.sciencedirect.com/science/article/pii/S0012825217303239+]

Infragravity (hereafter IG) waves are surface ocean waves with frequencies below those of wind-generated “short waves” (typically below 0.04 Hz). Here we focus on the most common type of IG waves, those induced by the presence of groups in incident short waves. Three related mechanisms explain their generation: (1) the development, shoaling and release of waves bound to the short-wave group envelopes (2) the modulation by these envelopes of the location where short waves break, and (3) the merging of bores (breaking wave front, resembling to a hydraulic jump) inside the surfzone. When reaching shallow water (O(1–10 m)), IG waves can transfer part of their energy back to higher frequencies, a process which is highly dependent on beach slope. On gently sloping beaches, IG waves can dissipate a substantial amount of energy through depth-limited breaking. When the bottom is very rough, such as in coral reef environments, a substantial amount of energy can be dissipated through bottom friction. IG wave energy that is not dissipated is reflected seaward, predominantly for the lowest IG frequencies and on steep bottom slopes. This reflection of the lowest IG frequencies can result in the development of standing (also known as stationary) waves. Reflected IG waves can be refractively trapped so that quasi-periodic along-shore patterns, also referred to as edge waves, can develop. IG waves have a large range of implications in the hydro-sedimentary dynamics of coastal zones. For example, they can modulate current velocities in rip channels and strongly influence cross-shore and longshore mixing. On sandy beaches, IG waves can strongly impact the water table and associated groundwater flows. On gently sloping beaches and especially under storm conditions, IG waves can dominate cross-shore sediment transport, generally promoting offshore transport inside the surfzone. Under storm conditions, IG waves can also induce overwash and eventually promote dune erosion and barrier breaching. In tidal inlets, IG waves can propagate into the back-barrier lagoon during the flood phase and induce large modulations of currents and sediment transport. Their effect appears to be smaller during the ebb phase, due to blocking by countercurrents, particularly in shallow systems. On coral and rocky reefs, IG waves can dominate over short-waves and control the hydro-sedimentary dynamics over the reef flat and in the lagoon. In harbors and semi-enclosed basins, free IG waves can be amplified by resonance and induce large seiches (resonant oscillations). Lastly, free IG waves that are generated in the nearshore can cross oceans and they can also explain the development of the Earth's “hum” (background free oscillations of the solid earth).

*The multidisciplinary origin of soil geography: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S001282521730329X[+https://www.sciencedirect.com/science/article/pii/S001282521730329X+]

Soil geography should be clearly recognized as a sub-discipline of physical geography and soil science, but at various times over the last century it was accepted as a complementary and descriptive sub-discipline of botany, agronomy and geology. In other words, there was not a clear consensus about its definition and origins. The main goal of this paper is to conduct a historical review (s. XX-XXI) of soil geography to clarify its origin, early methods, first authors and the importance of its interdisciplinary perspective within the scientific community. We found that soil geography was considerably advanced by the work of K.D. Glinka (1867–1927), one of Dokuchaev's students, who could be considered as the father of soil geography. Following the scientific line of Glinka, C.F. Marbut (1863–1935) could be considered one of the first world-reknown soil geographers. During the 1900s, this discipline continued to develop with research conducted by scientists including Kellogg, Simonson, Kubiëna, Huguet del Villar, Fitzpatrick, Duchaufour, Stremme, Zinck and entities such as USDA, FAO-UNESCO and CSIRO.

*Assessing structural, functional and effective hydrologic connectivity with brain neuroscience methods*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825217303744[+https://www.sciencedirect.com/science/article/pii/S0012825217303744+]

While the concept of connectivity has gained popularity in fields like hydrology and ecology, little agreement exists on its definition, which hinders its use in both scientific and legal contexts. In contrast, neuroscientists have developed not only strong conceptualizations of connectivity but also tools to quantify it: a clear distinction is made between structural connectivity, which is determined from brain anatomy; functional connectivity, which is estimated based on statistical dependencies between neuronal electric timeseries; and effective connectivity, which infers causal relations from the same timeseries based on the assumption that “true” interactions occur with a certain time delay. The motivation of this review arose from the hypothesis that connectivity-related statistical techniques, which are applied to timeseries of electrical currents measured by placing electrodes on the scalp of the human brain, could also apply to high-frequency hydrological timeseries acquired to characterize catchment response to precipitation. Here we bring together existing conceptualizations of structural, functional and effective connectivity in hydrology and ecology and compare them with those used in brain neuroscience. We then summarize the most important brain connectivity measures and their associated mathematical frameworks before evaluating the potential of those measures to help advance our understanding of hydrologic connectivity properties – in terms of the frequency, magnitude, timing, duration and rate of water movement linking two disparate locations. Lastly, we present a short case study where a selection of brain connectivity measures is applied to 35 groundwater and streamflow timeseries from a Swiss catchment to infer subsurface flow-driven hydrologic connectivity. Our literature review combined with our short case study suggest that an ensemble of functional and effective connectivity measures should be used and constrained not only by structural connectivity measures but also by interpretation thresholds in order to make results parsimonious. We highlight challenges associated with transferring brain connectivity measures to hydrology, especially those related to choosing the appropriate length and sampling frequency of input timeseries when assessing perennial versus ephemeral connectivity, appropriately detecting and differentiating noisy from indirect connections, and interpreting unbounded connectivity measures. We then offer recommendations for future research and propose that hydrologists use a common classification system encompassing all potential connectivity assessment approaches and measures in order to facilitate scientific communication.

*On the rise and fall of oceanic islands: Towards a global theory*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825217305810[+https://www.sciencedirect.com/science/article/pii/S0012825217305810+]

The careers of Charles Darwin (1809–1882) and James Dwight Dana (1813–1895) are intimately linked to circumnavigations of the globe with the British mapping expedition on the H.M.S. Beagle (1831–1836) under Captain Robert FitzRoy and the United States Exploring Expedition (1838–1842) under Lieutenant Charles Wilkes. The former expedition mainly surveyed coastal South America, but also visited many volcanic islands in the Atlantic, Pacific, and Indian oceans. The latter expedition followed a similar path through the Atlantic, but devoted more time to Pacific Ocean islands. Remembered more today for his visit to the Galapagos Islands and its subsequent impact on understanding the mechanisms of biological evolution, Darwin was motivated early on during his stopover in the Cape Verde Islands to compile studies on the geology of volcanic islands. Better known for his theory of atoll development from the subsidence of volcanic islands stimulated by his visit to the Keeling Islands and published in 1842, Darwin also wrote a related volume published in 1844 with an equally strong emphasis on island uplift. Dana was influenced by Darwin's theory of atoll development, and published his own independent observations on coral reefs and island subsidence in 1843, 1849, and 1853. The work of both geologists matured from primary observations using inductive logic during fieldwork (i.g. unconformable position of limestone on and between basalt flows as an indicator of paleo-sea level) to the advancement of broader theories regarding the behavior of the Earth's oceanic crust. Notably, Dana recognized age differences among islands in Pacific archipelagos and was strongly influenced by the orientations of those island groups. The classic Hawaiian model that features a linear string of progressively older and subsiding islands does not apply easily to many other island groups such as the Galapagos, Azores, Canary, and Cape Verde islands. Geologists and coastal geomorphologists increasingly find that the original observations on island uplift covered in Darwin's, 1844 treatment provide an alternative pathway to understanding the complexities of island histories in oceanic settings. Original work by Darwin and Dana also led to ongoing studies on the trans-oceanic migrations of marine organisms, such as barnacles, corals and non-attached coralline red algae represented by rhodoliths. This work gives added importance to oceanic islands as way stations in the dispersal of biotas over time.

*Geospatial sensor web: A cyber-physical infrastructure for geoscience research and application*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825217305044[+https://www.sciencedirect.com/science/article/pii/S0012825217305044+]

In the last half-century, geoscience research has advanced due to multidisciplinary technologies, among which Information and Communication Technology (ICT) has played a vital role. However, scientifically organizing these ICTs toward improving geoscience measurements, data processing, and information services has encountered tremendous challenges. This paper reviews a profound revolution in geoscience that has resulted from the Geospatial Sensor Web (GSW), serving as a new cyber-physical spatio-temporal information infrastructure for geoscience on the World Wide Web (WWW). In contrast to previous experiment-based and sensor-based paradigms, the GSW-based paradigm is able to accomplish the following: (1) achieve integrated and sharable management of diverse sensing resources, (2) obtain real-time or near real-time and spatiotemporal continuous data, (3) conduct interoperable and online geoscience data processing and analysis, and (4) provide focusing services with web-based geoscience information and knowledge. As a benefit of the GSW, increasingly more geoscience disciplines are enjoying the value of real-time data, multi-source monitoring, online processing, and intelligence services. This paper reviews the evolution of geoscience research paradigm to demonstrate the scientific background of GSW. Then, we elaborates on four key methods provided by GSW, namely, integrated management, collaborative observation, scalable processing and fusion, and focusing service web capacity. Furthermore, current GSW prototypes and applications for environmental, hydrological, and natural disaster analysis are also reviewed. Moreover, four challenges to the future GSW in geoscience research are identified and analyzed, including integration with the Model Web initiative for sophisticated geo-processing, integration with humans for pervasive sensing, integration with Internet of Things (IoT) to achieve high-quality performance and data mining, and integration with Artificial Intelligence (AI) to provide smart geoservices. We have concluded that GSW has become an indispensable cyber-physical infrastructure, and will play a greater role in geoscience research and application.

*Global review of human-induced earthquakes*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S001282521730003X[+https://www.sciencedirect.com/science/article/pii/S001282521730003X+]

The Human-induced Earthquake Database, HiQuake, is a comprehensive record of earthquake sequences postulated to be induced by anthropogenic activity. It contains over 700 cases spanning the period 1868–2016. Activities that have been proposed to induce earthquakes include the impoundment of water reservoirs, erecting tall buildings, coastal engineering, quarrying, extraction of groundwater, coal, minerals, gas, oil and geothermal fluids, excavation of tunnels, and adding material to the subsurface by allowing abandoned mines to flood and injecting fluid for waste disposal, enhanced oil recovery, hydrofracturing, gas storage and carbon sequestration. Nuclear explosions induce earthquakes but evidence for chemical explosions doing so is weak. Because it is currently impossible to determine with 100% certainty which earthquakes are induced and which not, HiQuake includes all earthquake sequences proposed on scientific grounds to have been human-induced regardless of credibility. Challenges to constructing HiQuake include under-reporting which is ~ 30% of M ~ 4 events, ~ 60% of M ~ 3 events and ~ 90% of M ~ 2 events. The amount of stress released in an induced earthquake is not necessarily the same as the anthropogenic stress added because pre-existing tectonic stress may also be released. Thus earthquakes disproportionately large compared with the associated industrial activity may be induced. Knowledge of the magnitude of the largest earthquake that might be induced by a project, MMAX, is important for hazard reduction. Observed MMAX correlates positively with the scale of associated industrial projects, fluid injection pressure and rate, and the yield of nuclear devices. It correlates negatively with calculated inducing stress change, likely because the latter correlates inversely with project scale. The largest earthquake reported to date to be induced by fluid injection is the 2016 M 5.8 Pawnee, Oklahoma earthquake, by water-reservoir impoundment the 2008 M ~ 8 Wenchuan, People's Republic of China, earthquake, and by mass removal the 1976 M 7.3 Gazli, Uzbekistan earthquake. The minimum amount of anthropogenic stress needed to induce an earthquake is an unsound concept since earthquakes occur in the absence of industrial activity. The minimum amount of stress observed to modulate earthquake activity is a few hundredths of a megapascal and possibly as little as a few thousandths, equivalent to a few tens of centimeters of water-table depth. Faults near to failure are pervasive in the continental crust and induced earthquakes may thus occur essentially anywhere. In intraplate regions neither infrastructure nor populations may be prepared for earthquakes. Human-induced earthquakes that cause nuisance are rare, but in some cases may be a significant problem, e.g., in the hydrocarbon-producing areas of Oklahoma, USA. As the size of projects and density of populations increase, the potential nuisance of induced earthquakes is also increasing and effective management strategies are needed.

*Criticality and dynamical scaling in living systems*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.90.031001[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.90.031001+]

A celebrated and controversial hypothesis suggests that some biological systems—parts, aspects, or groups of them—may extract important functional benefits from operating at the edge of instability, halfway between order and disorder, i.e., in the vicinity of the critical point of a phase transition. Criticality has been argued to provide biological systems with an optimal balance between robustness against perturbations and flexibility to adapt to changing conditions as well as to confer on them optimal computational capabilities, large dynamical repertoires, unparalleled sensitivity to stimuli, etc. Criticality, with its concomitant scale invariance, can be conjectured to emerge in living systems as the result of adaptive and evolutionary processes that, for reasons to be fully elucidated, select for it as a template upon which further layers of complexity can rest. This hypothesis is suggestive as it proposes that criticality could constitute a general and common organizing strategy in biology stemming from the physics of phase transitions. However, despite its implications, this is still in its infancy state as a well-founded theory and, as such, it has elicited some skepticism. From the experimental side, the advent of high-throughput technologies has created new prospects in the exploration of biological systems, and empirical evidence in favor of criticality has proliferated, with examples ranging from endogenous brain activity and gene-expression patterns to flocks of birds and insect-colony foraging, to name but a few. Some pieces of evidence are quite remarkable, while in some other cases empirical data are limited, incomplete, or not fully convincing. More stringent experimental setups and theoretical analyses are certainly needed to fully clarify the picture. In any case, the time seems ripe for bridging the gap between this theoretical conjecture and its empirical validation. Given the profound implications of shedding light on this issue, it is both pertinent and timely to review the state of the art and to discuss future strategies and perspectives.

*History of dark matter*
~~~~~~~~~~~~~~~~~~~~~~~~

https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.90.045002[+https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.90.045002+]

Although dark matter is a central element of modern cosmology, the history of how it became accepted as part of the dominant paradigm is often ignored or condensed into an anecdotal account focused around the work of a few pioneering scientists. The aim of this review is to provide a broader historical perspective on the observational discoveries and the theoretical arguments that led the scientific community to adopt dark matter as an essential part of the standard cosmological model.

*Simulating biological processes: stochastic physics from whole cells to colonies*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aaae2c[+https://iopscience.iop.org/article/10.1088/1361-6633/aaae2c+]

The last few decades have revealed the living cell to be a crowded spatially heterogeneous space teeming with biomolecules whose concentrations and activities are governed by intrinsically random forces. It is from this randomness, however, that a vast array of precisely timed and intricately coordinated biological functions emerge that give rise to the complex forms and behaviors we see in the biosphere around us. This seemingly paradoxical nature of life has drawn the interest of an increasing number of physicists, and recent years have seen stochastic modeling grow into a major subdiscipline within biological physics. Here we review some of the major advances that have shaped our understanding of stochasticity in biology. We begin with some historical context, outlining a string of important experimental results that motivated the development of stochastic modeling. We then embark upon a fairly rigorous treatment of the simulation methods that are currently available for the treatment of stochastic biological models, with an eye toward comparing and contrasting their realms of applicability, and the care that must be taken when parameterizing them. Following that, we describe how stochasticity impacts several key biological functions, including transcription, translation, ribosome biogenesis, chromosome replication, and metabolism, before considering how the functions may be coupled into a comprehensive model of a 'minimal cell'. Finally, we close with our expectation for the future of the field, focusing on how mesoscopic stochastic methods may be augmented with atomic-scale molecular modeling approaches in order to understand life across a range of length and time scales.

*Atomic clocks for geodesy*
~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aab409[+https://iopscience.iop.org/article/10.1088/1361-6633/aab409+]

We review experimental progress on optical atomic clocks and frequency transfer, and consider the prospects of using these technologies for geodetic measurements. Today, optical atomic frequency standards have reached relative frequency inaccuracies below 10−17, opening new fields of fundamental and applied research. The dependence of atomic frequencies on the gravitational potential makes atomic clocks ideal candidates for the search for deviations in the predictions of Einstein's general relativity, tests of modern unifying theories and the development of new gravity field sensors. In this review, we introduce the concepts of optical atomic clocks and present the status of international clock development and comparison. Besides further improvement in stability and accuracy of today's best clocks, a large effort is put into increasing the reliability and technological readiness for applications outside of specialized laboratories with compact, portable devices. With relative frequency uncertainties of 10−18, comparisons of optical frequency standards are foreseen to contribute together with satellite and terrestrial data to the precise determination of fundamental height reference systems in geodesy with a resolution at the cm-level. The long-term stability of atomic standards will deliver excellent long-term height references for geodetic measurements and for the modelling and understanding of our Earth.

*A trillion frames per second: the techniques and applications of light-in-flight photography*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aacca1[+https://iopscience.iop.org/article/10.1088/1361-6633/aacca1+]

Cameras capable of capturing videos at a trillion frames per second allow to freeze light in motion, a very counterintuitive capability when related to our everyday experience in which light appears to travel instantaneously. By combining this capability with computational imaging techniques, new imaging opportunities emerge such as 3D imaging of scenes that are hidden behind a corner, the study of relativistic distortion effects, imaging through diffusive media and imaging of ultrafast optical processes such as laser ablation, supercontinuum and plasma generation. We provide an overview of the main techniques that have been developed for ultra-high speed photography with a particular focus on 'light-in-flight' imaging, i.e. applications where the key element is the imaging of light itself at frame rates that allow to freeze its motion and therefore extract information that would otherwise be blurred out and lost.

*Bacterial growth: a statistical physicist's guide*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/aae546[+https://iopscience.iop.org/article/10.1088/1361-6633/aae546+]

Bacterial growth presents many beautiful phenomena that pose new theoretical challenges to statistical physicists, and are also amenable to laboratory experimentation. This review provides some of the essential biological background, discusses recent applications of statistical physics in this field, and highlights the potential for future research.

*Stokes drift*
~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0104[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0104+]

During its periodic motion, a particle floating at the free surface of a water wave experiences a net drift velocity in the direction of wave propagation, known as the Stokes drift (Stokes 1847 Trans. Camb. Philos. Soc.8, 441–455). More generally, the Stokes drift velocity is the difference between the average Lagrangian flow velocity of a fluid parcel and the average Eulerian flow velocity of the fluid. This paper reviews progress in fundamental and applied research on the induced mean flow associated with surface gravity waves since the first description of the Stokes drift, now 170 years ago. After briefly reviewing the fundamental physical processes, most of which have been established for decades, the review addresses progress in laboratory and field observations of the Stokes drift. Despite more than a century of experimental studies, laboratory studies of the mean circulation set up by waves in a laboratory flume remain somewhat contentious. In the field, rapid advances are expected due to increasingly small and cheap sensors and transmitters, making widespread use of small surface-following drifters possible. We also discuss remote sensing of the Stokes drift from high-frequency radar. Finally, the paper discusses the three main areas of application of the Stokes drift: in the coastal zone, in Eulerian models of the upper ocean layer and in the modelling of tracer transport, such as oil and plastic pollution. Future climate models will probably involve full coupling of ocean and atmosphere systems, in which the wave model provides consistent forcing on the ocean surface boundary layer. Together with the advent of new space-borne instruments that can measure surface Stokes drift, such models hold the promise of quantifying the impact of wave effects on the global atmosphere–ocean system and hopefully contribute to improved climate projections.

*Application of the ideas and techniques of classical fluid mechanics to some problems in physical oceanography*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/toc/rsta/376/2111[+https://royalsocietypublishing.org/toc/rsta/376/2111+]

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0092[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0092+]

This review makes a case for describing many of the flows observed in our oceans, simply based on the Euler equation, with (piecewise) constant density and with suitable boundary conditions. The analyses start from the Euler and mass conservation equations, expressed in a rotating, spherical coordinate system (but the f-plane and β-plane approximations are also mentioned); five examples are discussed. For three of them, a suitable non-dimensionalization is introduced, and a single small parameter is identified in each case. These three examples lead straightforwardly and directly to new results for: waves on the Pacific Equatorial Undercurrent (EUC) with a thermocline (in the f-plane); a nonlinear, three-dimensional model for EUC-type flows (in the β-plane); and a detailed model for large gyres. The other two examples are exact solutions of the complete system: a flow which corresponds to the underlying structure of the Pacific EUC; and a flow based on the necessary requirement to use a non-conservative body force, which produces the type of flow observed in the Antarctic Circumpolar Current. (All these examples have been discussed in detail in the references cited.) This review concludes with a few comments on how these solutions can be extended and expanded.

*Blessing of dimensionality: mathematical foundations of the statistical physics of data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0237[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0237+]

The concentrations of measure phenomena were discovered as the mathematical background to statistical mechanics at the end of the nineteenth/beginning of the twentieth century and have been explored in mathematics ever since. At the beginning of the twenty-first century, it became clear that the proper utilization of these phenomena in machine learning might transform the curse of dimensionality into the blessing of dimensionality. This paper summarizes recently discovered phenomena of measure concentration which drastically simplify some machine learning problems in high dimension, and allow us to correct legacy artificial intelligence systems. The classical concentration of measure theorems state that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of a sphere, an average or median-level set of energy or another Lipschitz function, etc.). The new stochastic separation theorems describe the thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set, even for exponentially large random sets. The linear functionals for separation of points can be selected in the form of the linear Fisher’s discriminant. All artificial intelligence systems make errors. Non-destructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic separation theorems provide us with such classifiers and determine a non-iterative (one-shot) procedure for their construction.

*Dissipative structures in biological systems: bistability, oscillations, spatial patterns and waves*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0376[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0376+]

The goal of this review article is to assess how relevant is the concept of dissipative structure for understanding the dynamical bases of non-equilibrium self-organization in biological systems, and to see where it has been applied in the five decades since it was initially proposed by Ilya Prigogine. Dissipative structures can be classified into four types, which will be considered, in turn, and illustrated by biological examples: (i) multistability, in the form of bistability and tristability, which involve the coexistence of two or three stable steady states, or in the form of birhythmicity, which involves the coexistence between two stable rhythms; (ii) temporal dissipative structures in the form of sustained oscillations, illustrated by biological rhythms; (iii) spatial dissipative structures, known as Turing patterns; and (iv) spatio-temporal structures in the form of propagating waves. Rhythms occur with widely different periods at all levels of biological organization, from neural, cardiac and metabolic oscillations to circadian clocks and the cell cycle; they play key roles in physiology and in many disorders. New rhythms are being uncovered while artificial ones are produced by synthetic biology. Rhythms provide the richest source of examples of dissipative structures in biological systems. Bistability has been observed experimentally, but has primarily been investigated in theoretical models in an increasingly wide range of biological contexts, from the genetic to the cell and animal population levels, both in physiological conditions and in disease. Bistable transitions have been implicated in the progression between the different phases of the cell cycle and, more generally, in the process of cell fate specification in the developing embryo. Turing patterns are exemplified by the formation of some periodic structures in the course of development and by skin stripe patterns in animals. Spatio-temporal patterns in the form of propagating waves are observed within cells as well as in intercellular communication. This review illustrates how dissipative structures of all sorts abound in biological systems.

*New perspectives for the prediction and statistical quantification of extreme events in high-dimensional dynamical systems*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0133[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0133+]

We discuss extreme events as random occurrences of strongly transient dynamics that lead to nonlinear energy transfers within a chaotic attractor. These transient events are the result of finite-time instabilities and therefore are inherently connected with both statistical and dynamical properties of the system. We consider two classes of problems related to extreme events and nonlinear energy transfers, namely (i) the derivation of precursors for the short-term prediction of extreme events, and (ii) the efficient sampling of random realizations for the fastest convergence of the probability density function in the tail region. We summarize recent methods on these problems that rely on the simultaneous consideration of the statistical and dynamical characteristics of the system. This is achieved by combining available data, in the form of second-order statistics, with dynamical equations that provide information for the transient events that lead to extreme responses. We present these methods through two high-dimensional, prototype systems that exhibit strongly chaotic dynamics and extreme responses due to transient instabilities, the Kolmogorov flow and unidirectional nonlinear water waves.

*The diversity of tectonic modes and thoughts about transitions between them*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0416[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0416+]

Plate tectonics is a particular mode of tectonic activity that characterizes the present-day Earth. It is directly linked to not only tectonic deformation but also magmatic/volcanic activity and all aspects of the rock cycle. Other terrestrial planets in our Solar System do not operate in a plate tectonic mode but do have volcanic constructs and signs of tectonic deformation. This indicates the existence of tectonic modes different from plate tectonics. This article discusses the defining features of plate tectonics and reviews the range of tectonic modes that have been proposed for terrestrial planets to date. A categorization of tectonic modes relates to the issue of when plate tectonics initiated on Earth as it provides insights into possible pre-plate tectonic behaviour. The final focus of this contribution relates to transitions between tectonic modes. Different transition scenarios are discussed. One follows classic ideas of regime transitions in which boundaries between tectonic modes are determined by the physical and chemical properties of a planet. The other considers the potential that variations in temporal evolution can introduce contingencies that have a significant effect on tectonic transitions. The latter scenario allows for the existence of multiple stable tectonic modes under the same physical/chemical conditions. The different transition potentials imply different interpretations regarding the type of variable that the tectonic mode of a planet represents. Under the classic regime transition view, the tectonic mode of a planet is a state variable (akin to temperature). Under the multiple stable modes view, the tectonic mode of a planet is a process variable. That is, something that flows through the system (akin to heat). The different implications that follow are discussed as they relate to the questions of when did plate tectonics initiate on Earth and why does Earth have plate tectonics.

*NASA and the Search for Technosignatures: A Report from the NASA Technosignatures Workshop*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1812.08681[+https://arxiv.org/abs/1812.08681+]

This report is the product of the NASA Technosignatures Workshop held at the Lunar and Planetary Institute in Houston, Texas, in September 2018. This workshop was convened by NASA for the organization to learn more about the current field and state of the art of searches for technosignatures, and what role NASA might play in these searches in the future. The report, written by the workshop participants, summarizes the material presented at the workshop and incorporates additional inputs from the participants. Section 1 explains the scope and purpose of the document, provides general background about the search for technosignatures, and gives context for the rest of the report. Section 2 discusses which experiments have occurred, along with current limits on technosignatures. Section 3 addresses the current state of the technosignature field as well as the state-of-the-art for technosignature detection. Section 4 addresses near-term searches for technosignatures, and Section 5 discusses emerging and future opportunities in technosignature detection.

*Mechanical resonance: 300 years from discovery to the full understanding of its importance*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1811.08353[+https://arxiv.org/abs/1811.08353+]

Starting from the observation that the simplest form of forced mechanical oscillation serves as a standard model for analyzing a broad variety of resonance processes in many fields of physics and engineering, the remarkably slow development leading to this insight is reviewed. Forced oscillations and mechanical resonance were already described by Galileo early in the 17th century, even though he misunderstood them. The phenomenon was then completely ignored by Newton but was partly rediscovered in the 18th century, as a purely mathematical surprise, by Euler. Not earlier than in the 19th century did Thomas Young give the first correct description. Until then, forced oscillations were not investigated for the purpose of understanding the motion of a pendulum, or of a mass on a spring, or the acoustic resonance, but in the context of the ocean tides. Thus, in the field of pure mechanics the results by Young had no echo at all. On the other hand, in the 19th century mechanical resonance disasters were observed ever more frequently, e.g. with suspension bridges and steam engines, but were not recognized as such. The equations governing forced mechanical oscillations were then rediscovered in other fields like acoustics and electrodynamics and were later found to play an important role also in quantum mechanics. Only then, in the early 20th century, the importance of the one-dimensional mechanical resonance as a fundamental model process was recognized in various fields, at last in engineering mechanics. There may be various reasons for the enormous time span between the introduction of this simple mechanical phenomenon into science and its due scientific appreciation. One of them can be traced back to the frequently made neglect of friction in the governing equation.

*Ocean Tide Influences on the Antarctic and Greenland Ice Sheets*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2016RG000546[+https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2016RG000546+]

Ocean tides are the main source of high‐frequency variability in the vertical and horizontal motion of ice sheets near their marine margins. Floating ice shelves, which occupy about three quarters of the perimeter of Antarctica and the termini of four outlet glaciers in northern Greenland, rise and fall in synchrony with the ocean tide. Lateral motion of floating and grounded portions of ice sheets near their marine margins can also include a tidal component. These tide‐induced signals provide insight into the processes by which the oceans can affect ice sheet mass balance and dynamics. In this review, we summarize in situ and satellite‐based measurements of the tidal response of ice shelves and grounded ice, and spatial variability of ocean tide heights and currents around the ice sheets. We review sensitivity of tide heights and currents as ocean geometry responds to variations in sea level, ice shelf thickness, and ice sheet mass and extent. We then describe coupled ice‐ocean models and analytical glacier models that quantify the effect of ocean tides on lower‐frequency ice sheet mass loss and motion. We suggest new observations and model developments to improve the representation of tides in coupled models that are used to predict future ice sheet mass loss and the associated contribution to sea level change. The most critical need is for new data to improve maps of bathymetry, ice shelf draft, spatial variability of the drag coefficient at the ice‐ocean interface, and higher‐resolution models with improved representation of tidal energy sinks.

*An Overview of Interactions and Feedbacks Between Ice Sheets and the Earth System*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018RG000600[+https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2018RG000600+]

Ice sheet response to forced changes—such as that from anthropogenic climate forcing—is closely regulated by two‐way interactions with other components of the Earth system. These interactions encompass the ice sheet response to Earth system forcing, the Earth system response to ice sheet change, and feedbacks resulting from coupled ice sheet/Earth system evolution. Motivated by the impact of Antarctic and Greenland ice sheet change on future sea level rise, here we review the state of knowledge of ice sheet/Earth system interactions and feedbacks. We also describe emerging observation and model‐based methods that can improve understanding of ice sheet/Earth system interactions and feedbacks. We particularly focus on the development of Earth system models that incorporate current understanding of Earth system processes, ice dynamics, and ice sheet/Earth system couplings. Such models will be critical tools for projecting future sea level rise from anthropogenically forced ice sheet mass loss.

*Spatial modelling with R-INLA: A review*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1802.06350[+https://arxiv.org/abs/1802.06350+]

Coming up with Bayesian models for spatial data is easy, but performing inference with them can be challenging. Writing fast inference code for a complex spatial model with realistically-sized datasets from scratch is time-consuming, and if changes are made to the model, there is little guarantee that the code performs well. The key advantages of R-INLA are the ease with which complex models can be created and modified, without the need to write complex code, and the speed at which inference can be done even for spatial problems with hundreds of thousands of observations.

R-INLA handles latent Gaussian models, where fixed effects, structured and unstructured Gaussian random effects are combined linearly in a linear predictor, and the elements of the linear predictor are observed through one or more likelihoods. The structured random effects can be both standard areal model such as the Besag and the BYM models, and geostatistical models from a subset of the Matérn Gaussian random fields. In this review, we discuss the large success of spatial modelling with R-INLA and the types of spatial models that can be fitted, we give an overview of recent developments for areal models, and we give an overview of the stochastic partial differential equation (SPDE) approach and some of the ways it can be extended beyond the assumptions of isotropy and separability. In particular, we describe how slight changes to the SPDE approach leads to straight-forward approaches for non-stationary spatial models and non-separable space-time models. 

http://www.r-inla.org/[+http://www.r-inla.org/+]

*The ECMWF Ensemble Prediction System: Looking Back (more than) 25 Years and Projecting Forward 25 Years*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1803.06940[+https://arxiv.org/abs/1803.06940+]

This paper has been written to mark 25 years of operational medium-range ensemble forecasting. The origins of the ECMWF Ensemble Prediction System are outlined, including the development of the precursor real-time Met Office monthly ensemble forecast system. In particular, the reasons for the development of singular vectors and stochastic physics - particular features of the ECMWF Ensemble Prediction System - are discussed. The author speculates about the development and use of ensemble prediction in the next 25 years.

*ET Probes, Nodes, and Landbases: A Proposed Galactic Communications Architecture and Implied Search Strategies*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1808.07024[+https://arxiv.org/abs/1808.07024+]

Land-based beacons, information laden probes sent into our solar system, and more distal communication nodes have each been proposed as the most likely means by which we might be contacted by ET. Each method, considered in isolation from ET's point of view, has limitations and flaws. An overarching galactic communication architecture that tethers together probes, nodes, and land bases is proposed to be a better overall solution. From this more efficient construct flows several conclusions: (a) Earth has been thoroughly surveilled, (b) Earth will be contacted in due course, (c) SETI beyond half the distance that Earth's EM has reached (~35-50 LY) is futile, and (d) the very quiescence of the galaxy paradoxically implies that that Drake's N = many, and that there is a system of galactic governance. Search strategies are proposed to detect the described probe-node-land base communications pathway. 

*Grand challenges in social physics: In pursuit of moral behavior*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1810.05516[+https://arxiv.org/abs/1810.05516+]

Methods of statistical physics have proven valuable for studying the evolution of cooperation in social dilemma games. However, recent empirical research shows that cooperative behavior in social dilemmas is only one kind of a more general class of behavior, namely moral behavior, which includes reciprocity, respecting others' property, honesty, equity, efficiency, as well as many others. Inspired by these experimental works, we here open up the path towards studying other forms of moral behavior with methods of statistical physics. We argue that this is a far-reaching direction for future research that can help us answer fundamental questions about human sociality. Why did our societies evolve as they did? What moral principles are more likely to emerge? What happens when different moral principles clash? Can we predict the break out of moral conflicts in advance and contribute to their solution? These are amongst the most important questions of our time, and methods of statistical physics could lead to new insights and contribute towards finding answers.

*The Antarctic Slope Current in a Changing Climate*

https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018RG000624[+https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018RG000624+]

The Antarctic Slope Current (ASC) is a coherent circulation feature that rings the Antarctic continental shelf and regulates the flow of water toward the Antarctic coastline. The structure and variability of the ASC influences key processes near the Antarctic coastline that have global implications, such as the melting of Antarctic ice shelves and water mass formation that determines the strength of the global overturning circulation. Recent theoretical, modeling, and observational advances have revealed new dynamical properties of the ASC, making it timely to review. Earlier reviews of the ASC focused largely on local classifications of water properties of the ASC's primary front. Here we instead provide a classification of the current's frontal structure based on the dynamical mechanisms that govern both the along‐slope and cross‐slope circulation; these two modes of circulation are strongly coupled, similar to the Antarctic Circumpolar Current. Highly variable motions, such as dense overflows, tides, and eddies are shown to be critical components of cross‐slope and cross‐shelf exchange, but understanding of how the distribution and intensity of these processes will evolve in a changing climate remains poor due to observational and modeling limitations. Results linking the ASC to larger modes of climate variability, such as El Niño, show that the ASC is an integral part of global climate. An improved dynamical understanding of the ASC is still needed to accurately model and predict future Antarctic sea ice extent, the stability of the Antarctic ice sheets, and the Southern Ocean's contribution to the global carbon cycle.

2019
----

*The Interpolation of Sparse Geophysical Data*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://link.springer.com/article/10.1007/s10712-018-9501-3[+https://link.springer.com/article/10.1007/s10712-018-9501-3+]

Geophysical data interpolation has attracted much attention in the past decades. While a variety of methods are well established for either regularly sampled or irregularly sampled multi-channel data, an effective method for interpolating extremely sparse data samples is still highly demanded. In this paper, we first review the state-of-the-art models for geophysical data interpolation, focusing specifically on the three main types of geophysical interpolation problems, i.e., for irregularly sampled data, regularly sampled data, and sparse geophysical data. We also review the theoretical implications for different interpolation models, i.e., the sparsity-based and the rank-based regularized interpolation approaches.

Then, we address the challenge for interpolating highly incomplete low-dimensional data by developing a novel shaping regularization-based inversion algorithm. The interpolation can be formulated as an inverse problem. Due to the ill-posedness of the inversion problem, an effective regularization approach is very necessary. We develop a structural smoothness constraint for regularizing the inverse problem based on the shaping regularization framework. The shaping regularization framework offers a flexible way for constraining the model behavior. The proposed method can be easily applied to interpolate incomplete reflection seismic data, ground penetrating radar data, and earthquake data with large gaps and also to interpolate sparse well-log data for preparing high-fidelity initial model for subsequent full-waveform inversion.

*The Global Overturning Circulation*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-010318-095241[+https://www.annualreviews.org/doi/abs/10.1146/annurev-marine-010318-095241+]

In this article, I use the Estimating the Circulation and Climate of the Ocean version 4 (ECCO4) reanalysis to estimate the residual meridional overturning circulation, zonally averaged, over the separate Atlantic and Indo-Pacific sectors. The abyssal component of this estimate differs quantitatively from previously published estimates that use comparable observations, indicating that this component is still undersampled. I also review recent conceptual models of the oceanic meridional overturning circulation and of the mid-depth and abyssal stratification. These theories show that dynamics in the Antarctic circumpolar region are essential in determining the deep and abyssal stratification. In addition, they show that a mid-depth cell consistent with observational estimates is powered by the wind stress in the Antarctic circumpolar region, while the abyssal cell relies on interior diapycnal mixing, which is bottom intensified.

*Trend analysis of climate time series: A review of methods*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825218303726[+https://www.sciencedirect.com/science/article/pii/S0012825218303726+]

The increasing trend curve of global surface temperature against time since the 19th century is the icon for the considerable influence humans have on the climate since the industrialization. The discourse about the curve has spread from climate science to the public and political arenas in the 1990s and may be characterized by terms such as “hockey stick” or “global warming hiatus”. Despite its discussion in the public and the searches for the impact of the warming in climate science, it is statistical science that puts numbers to the warming. Statistics has developed methods to quantify the warming trend and detect change points. Statistics serves to place error bars and other measures of uncertainty to the estimated trend parameters. Uncertainties are ubiquitous in all natural and life sciences, and error bars are an indispensable guide for the interpretation of any estimated curve—to assess, for example, whether global temperature really made a pause after the year 1998.

Statistical trend estimation methods are well developed and include not only linear curves, but also change-points, accelerated increases, other nonlinear behavior, and nonparametric descriptions. State-of-the-art, computing-intensive simulation algorithms take into account the peculiar aspects of climate data, namely non-Gaussian distributional shape and autocorrelation. The reliability of such computer age statistical methods has been testified by Monte Carlo simulation methods using artificial data.

The application of the state-of-the-art statistical methods to the GISTEMP time series of global surface temperature reveals an accelerated warming since the year 1974. It shows that a relative peak in warming for the years around World War II may not be a real feature but a product of inferior data quality for that time interval. Statistics also reveals that there is no basis to infer a global warming hiatus after the year 1998. The post-1998 hiatus only seems to exist, hidden behind large error bars, when considering data up to the year 2013. If the fit interval is extended to the year 2017, there is no significant hiatus. The researcher has the power to select the fit interval, which allows her or him to suppress certain fit solutions and favor other solutions. Power necessitates responsibility. The recommendation therefore is that interval selection should be objective and oriented on general principles. The application of statistical methods to data has also a moral aspect.

*Cyclostratigraphy and the problem of astrochronologic testing*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.sciencedirect.com/science/article/pii/S0012825218303404[+https://www.sciencedirect.com/science/article/pii/S0012825218303404+]

When Milankovitch cycles are preserved in the geologic record they provide a direct link between chronometer and climate change, and thus a remarkable opportunity to constrain the evolution of the surficial Earth System. The identification of such cycles has allowed exploration of the geologic record with unprecedented temporal resolution, and has spurred the development of a rich theoretical framework for climatic change. Accompanying these successes, however, has been a persistent skepticism: how does one reliably test for astronomical forcing/pacing in stratigraphic and paleoclimate data, especially when time is poorly constrained? From this perspective, it would seem that the merits and promise of astrochronology – a Phanerozoic time scale measured in 20,000 to 400,000 year increments – also serves as its Achilles heel, if the confirmation of such geologically short temporal rhythms defies rigorous hypothesis testing. The implications are substantial, since much of our understanding of paleoclimate change throughout the Cenozoic (and beyond) is firmly rooted in astrochronologic interpretation.

In this study, a conceptual framework for assessing Earth System response to astronomical-insolation changes, and the propagation of that signal into the geologic record, is used as a guide to understand the nature of the problem of astrochronologic testing. This framework emphasizes three challenges – contamination, stratigraphic distortion, and temporal calibration. A statistical optimization method (TimeOpt; Meyers, 2015) is formulated as a solution to these three challenges, providing an approach for astrochronologic testing that objectively evaluates time scale uncertainty while simultaneously identifying an optimal model for climate and depositional system response to astronomical forcing. New extensions to the technique are presented, allowing explicit reconstruction of distortions to the primary forcing that are known to be omnipresent in the stratigraphic record. To illustrate the utility of this approach, it is applied to five well-studied stratigraphic series throughout the Phanerozoic, supporting their astronomical origin, and yielding constraints on the evolution of the Earth System and the astronomical solutions themselves. Future directions that build on this foundation are discussed, including the utility of process-based null models, approaches for Earth System transfer function reconstruction, and mapping out ancient Solar System behavior and Earth-Moon history using the geologic archive of Milankovitch cycles. The TimeOpt approach recognizes astronomy, geochronology, paleoclimatology and depositional system reconstruction as a unified geoscientific inverse problem.

*A review of non-fullerene polymer solar cells: from device physics to morphology control*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://iopscience.iop.org/article/10.1088/1361-6633/ab0530[+https://iopscience.iop.org/article/10.1088/1361-6633/ab0530+]

The rise in power conversion efficiency of organic photovoltaic (OPV) devices over the last few years has been driven by the emergence of new organic semiconductors and the growing understanding of morphological control at both the molecular and aggregation scales. Non-fullerene OPVs adopting p-type conjugated polymers as the donor and n-type small molecules as the acceptor have exhibited steady progress, outperforming PCBM-based solar cells and reaching efficiencies of over 15% in 2019. This review starts with a refreshed discussion of charge separation, recombination, and V OC loss in non-fullerene OPVs, followed by a review of work undertaken to develop favorable molecular configurations required for high device performance. We summarize several key approaches that have been employed to tune the nanoscale morphology in non-fullerene photovoltaic blends, comparing them (where appropriate) to their PCBM-based counterparts. In particular, we discuss issues ranging from materials chemistry to solution processing and post-treatments, showing how this can lead to enhanced photovoltaic properties. Particular attention is given to the control of molecular configuration through solution processing, which can have a pronounced impact on the structure of the solid-state photoactive layer. Key challenges, including green solvent processing, stability and lifetime, burn-in, and thickness-dependence in non-fullerene OPVs are briefly discussed.

*On Heaviside's contributions to transmission line theory: waves, diffusion and energy flux*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0457[+https://royalsocietypublishing.org/doi/10.1098/rsta.2017.0457+]

This paper surveys some selected contributions of Oliver Heaviside FRS (1850–1925) to classical electromagnetic theory and electrical engineering science. In particular, the paper focuses on his contributions to the development of electrical transmission line theory and his deep insights into the ‘physical’ nature of the phenomena relating to nineteenth century telegraphic problems. Following a brief historical introduction to the life of Heaviside to put his achievements in context, we explore his contributions to the reformulation of Maxwell's equations and the understanding of electromagnetic wave propagation along the external region of transmission lines. This leads naturally to his researches regarding the electromagnetic diffusion process inside the line conductors and his subsequent realization that the circuital parameters, usually assumed constant, are not always so. Finally, taking both these internal and external viewpoints of the conductors, his important work regarding the flow of energy described by his ‘energy current’ concept is presented.

*Muography: overview and future directions*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0049[+https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0049+]

Cosmic-ray muography uses high-energy particles for imaging applications that are produced by cosmic rays in particle showers in the Earth's atmosphere. This technology has developed rapidly over the last 15 years, and it is currently branching out into many different applications and moving from academic research to commercial application. As in any new sub-field of research and technology, the nomenclature of the field itself is still developing and has not settled yet as new aspects of the field are appearing and with them the terms to describe them.

*Lessons from nature for green science and technology: an overview and bioinspired superliquiphobic/philic surfaces*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0274[+https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0274+]

Nature has developed materials, objects and processes that function from the macroscale to the nanoscale. The emerging field of biomimetics allows one to mimic biology or nature to develop nanomaterials, nanodevices and processes which provide desirable properties. The biologically inspired materials and structured surfaces are being explored for various commercial applications. These should have minimum human impact on the environment, leading to eco-friendly or green science and technology. There are a large number of flora and fauna including bacteria, plants, land and aquatic animals, and seashells with properties of commercial interest. The paper presents an overview of the general field of biomimetics followed by a detailed overview of mechanisms, fabrication techniques and characterization of superliquiphobic/philic surfaces and their applications.

*Formation and dynamics of magma reservoirs*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/full/10.1098/rsta.2018.0019[+https://royalsocietypublishing.org/doi/full/10.1098/rsta.2018.0019+]

The emerging concept of a magma reservoir is one in which regions containing melt extend from the source of magma generation to the surface. The reservoir may contain regions of very low fraction intergranular melt, partially molten rock (mush) and melt lenses (or magma chambers) containing high melt fraction eruptible magma, as well as pockets of exsolved magmatic fluids. The various parts of the system may be separated by a sub-solidus rock or be connected and continuous. Magma reservoirs and their wall rocks span a vast array of rheological properties, covering as much as 25 orders of magnitude from high viscosity, sub-solidus crustal rocks to magmatic fluids. Time scales of processes within magma reservoirs range from very slow melt and fluid segregation within mush and magma chambers and deformation of surrounding host rocks to very rapid development of magma and fluid instability, transport and eruption. Developing a comprehensive model of these systems is a grand challenge that will require close collaboration between modellers, geophysicists, geochemists, geologists, volcanologists and petrologists.

*Assessing the scales in numerical weather and climate predictions: will exascale be the rescue?*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0148[+https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0148+]

We discuss scientific features and computational performance of kilometre-scale global weather and climate simulations, considering the Icosahedral Non-hydrostatic (ICON) model and the Integrated Forecast System (IFS). Scalability measurements and a performance modelling approach are used to derive performance estimates for these models on upcoming exascale supercomputers. This is complemented by preliminary analyses of the model data that illustrate the importance of high-resolution models to gain improvements in the accuracy of convective processes, a better understanding of physics dynamics interactions and poorly resolved or parametrized processes, such as gravity waves, convection and boundary layer.

*Mastering the scales: a survey on the benefits of multiscale computing software*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://royalsocietypublishing.org/doi/full/10.1098/rsta.2018.0147[+https://royalsocietypublishing.org/doi/full/10.1098/rsta.2018.0147+]

In the last few decades, multiscale modelling has emerged as one of the dominant modelling paradigms in many areas of science and engineering. Its rise to dominance is primarily driven by advancements in computing power and the need to model systems of increasing complexity. The multiscale modelling paradigm is now accompanied by a vibrant ecosystem of multiscale computing software (MCS) which promises to address many challenges in the development of multiscale applications. In this paper, we define the common steps in the multiscale application development process and investigate to what degree a set of 21 representative MCS tools enhance each development step. We observe several gaps in the features provided by MCS tools, especially for application deployment and the preparation and management of production runs. In addition, we find that many MCS tools are tailored to a particular multiscale computing pattern, even though they are otherwise application agnostic. We conclude that the gaps we identify are characteristic of a field that is still maturing and features that enhance the deployment and production steps of multiscale application development are desirable for the long-term success of MCS in its application fields.

*ICGEM – 15 years of successful collection and distribution of global gravitational models, associated services and future plans*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.earth-syst-sci-data-discuss.net/essd-2019-17/[+https://www.earth-syst-sci-data-discuss.net/essd-2019-17/+]

The International Centre for Global Earth Models (ICGEM, http://icgem.gfz-potsdam.de/) hosted at the GFZ German Research Centre for Geosciences (GFZ) is one of the five Services coordinated by the International Gravity Field Service (IGFS) of the International Association of Geodesy (IAG). The goal of the ICGEM Service is to provide the scientific community with a state of the art archive of static and temporal global gravity field models of the Earth, and develop and operate interactive calculation and visualisation services of gravity field functionals on user defined grids or at a list of particular points via its website. ICGEM offers the largest collection of global gravity field models, including those from the 1960s, as well as the most recent ones that have been developed using data from dedicated gravity missions, advanced processing methodologies and additional data sources such as satellite and terrestrial gravity. The global gravity field models have been collected from different institutions at international level and after a validation process made publicly available in a standardized format with DOI numbers assigned through GFZ Data Services. The development and maintenance of such a unique platform is crucial for the scientific community in geodesy, geophysics, oceanography, and climate research. The services of ICGEM have motivated researchers worldwide to grant access to their gravity field models and also provide them an access to variety of other gravity field models and their products. In this article, we present the development history and future plans of ICGEM and its current products and essential services. We present the Earth’s static, temporal, and topographic gravity field models as well as the gravity field models of other celestial bodies together with examples produced by the ICGEM’s calculation and 3D visualisation services and give an insight how the ICGEM Service can additionally contribute to the needs of research and society.

*A Modern Retrospective on Probabilistic Numerics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1901.04457[+https://arxiv.org/abs/1901.04457+]

This article attempts to cast the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related to modern formal treatments and applications. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.

*An introduction to the classical three-body problem: From periodic solutions to instabilities and chaos*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1901.07289[+https://arxiv.org/abs/1901.07289+]

The classical three-body problem arose in an attempt to understand the effect of the Sun on the Moon's Keplerian orbit around the Earth. It has attracted the attention of some of the best physicists and mathematicians and led to the discovery of chaos. We survey the three-body problem in its historical context and use it to introduce several ideas and techniques that have been developed to understand classical mechanical systems.

*Colloquium: A Century of Noether's Theorem*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1902.01989[+https://arxiv.org/abs/1902.01989+]

In the summer of 1918, Emmy Noether published the theorem that now bears her name, establishing a profound two-way connection between symmetries and conservation laws. The influence of this insight is pervasive in physics; it underlies all of our theories of the fundamental interactions and gives meaning to conservation laws that elevates them beyond useful empirical rules. Noether's papers, lectures, and personal interactions with students and colleagues drove the development of abstract algebra, establishing her in the pantheon of twentieth-century mathematicians. This essay traces her path from Erlangen through Göttingen to a brief but happy exile at Bryn Mawr College in Pennsylvania, illustrating the importance of "Noether's Theorem" for the way we think today. The text draws on a colloquium presented at Fermilab on 15 August 2018.

*A conceptual framework for discrete inverse problems in geophysics*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1901.07937[+https://arxiv.org/abs/1901.07937+]

In geophysics, inverse modelling can be applied to a wide range of goals, including, for instance, mapping the distribution of rock physical parameters in applied geophysics and calibrating models to forecast the behaviour of natural systems in hydrology, meteorology and climatology. A common, thorough conceptual framework to define inverse problems and to discuss their basic properties in a complete way is still lacking. The main goal of this paper is to propose a step forward toward such a framework, focussing on the discrete inverse problems, that are used in practical applications. The relevance of information and measurements (real world data) for the definition of the calibration target and of the objective function is discussed, in particular with reference to the Bayesian approach. Identifiability of model parameters, posedness (uniqueness and stability) and conditioning of the inverse problems are formally defined. The proposed framework is so general as to permit rigorous definitions and treatment of sensitivity analysis, adjoint-state approach, multi-objective optimization.

*Spatial And Temporal Changes Of The Geomagnetic Field: Insights From Forward And Inverse Core Field Models*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1902.08098[+https://arxiv.org/abs/1902.08098+]

Observational constraints on geomagnetic field changes from interannual to millenial periods are reviewed, and the current resolution of field models (covering archeological to satellite eras) is discussed. With the perspective of data assimilation, emphasis is put on uncertainties entaching Gauss coefficients, and on the statistical properties of ground-based records. These latter potentially call for leaving behind the notion of geomagnetic jerks. The accuracy at which we recover interannual changes also requires considering with caution the apparent periodicity seen in the secular acceleration from satellite data. I then address the interpretation of recorded magnetic fluctuations in terms of core dynamics, highlighting the need for models that allow (or pre-suppose) a magnetic energy orders of magnitudes larger than the kinetic energy at large length-scales, a target for future numerical simulations of the geodynamo. I finally recall the first attempts at implementing geomagnetic data assimilation algorithms.

*The Influence of Time Series Distance Functions on Climate Networks*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://arxiv.org/abs/1902.03298[+https://arxiv.org/abs/1902.03298+]

Network theory has established itself as an important tool for the analysis of complex systems such as the climate. In this context, climate networks are constructed using a spatiotemporal climate dataset and a time series distance function. It consists of representing each spatial area by a node and connecting nodes that present similar time series. One fundamental concern when building climate network is the definition of a metric that captures similarity between time series. The majority of papers in the literature use Pearson correlation with or without lag. Here we study the influence of 29 time series distance functions on climate network construction using global temperature data. We observed that the distance functions used in the literature generate similar networks in general while alternative ones generate distinct networks and exhibit different long-distance connection patterns (teleconnections). These patterns are highly important for the study of climate dynamics since they generally represent long-distance transportation of energy and can be used to forecast climatological events. Therefore, we consider that the measures here studied represent an alternative for the analysis of climate systems due to their capability of capturing different underlying dynamics, what may provide a better understanding of global climate.





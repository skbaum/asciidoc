= WRF Benchmark Notes
Steven K. Baum
0.5, Feb. 16, 2022
:doctype: book
:toc:
:icons:
:source-highlighter: highlight.js

:numbered!:

== Overview

The Purdue WRF benchmark requires both the WRF and WPS (WRF Pre-Processing System) packages.
The documentation is minimal, with the most informative file about the details of the simulation
being the `README.namelist` file
in the `daily12Z` directory.

Fortunately, though, when it comes to simply running the benchmark - as opposed to understanding the hundreds of parameters used,
the hundreds of code files in both WRF and WPS, etc. - the `wrf.sub` bash script has proved
sufficient thus far to properly perform the pre-processing for the simulation.

The basic steps for preparing and running the benchmark are:

* install the WRF and WPS modules
* download and install the Purdue benchmark dataset into scratch
* copy the WRF module directory to scratch
* configure the WRF for compilation
* compile the WRF 
* copy the WPS module directory into the scratch WRF directory as a subdirectory
* configure the WPS for compilation
* compile the WPS
* run WPS to preprocess the dataset for the simulation
* run the WRF simulation

== WRF Benchmarking

Useful resources for performing WRF benchmarks.

*WRF 4.2.2 Benchmark Cases*

https://www2.mmm.ucar.edu/wrf/users/benchmark/benchdata_v422.html[`https://www2.mmm.ucar.edu/wrf/users/benchmark/benchdata_v422.html`]

"Files for benchmarking were created with WPSV4.2 and WRFV4.2.2 (but can be used with any V4* WRF code)."

This page contains download links for the CONUS 12-km and CONUS 2.5-km benchmark cases.

*NCAR WRF Benchmarks*

https://akirakyle.github.io/WRF_benchmarks/[`https://akirakyle.github.io/WRF_benchmarks/`]

"This repo contains the benchmark cases and scripts used for assessing the scaling performance of WRF on NCAR's Cheyenne Supercomputer."

*WRFv4-Benchmarking*

https://github.com/federatedcloud/WRFv4-Benchmarking[`https://github.com/federatedcloud/WRFv4-Benchmarking`]

"Implementations of WRF 4.2.2 to run CONUS benchmarks on bare metal HPC, in a Docker container, and a Singularity container."

*OpenBenchmarking.org WRF*

https://openbenchmarking.org/test/pts/wrf[`https://openbenchmarking.org/test/pts/wrf`]

*BenchPro*

https://github.com/TACC/benchpro[`https://github.com/TACC/benchpro`]

"Benchmark Performance & Reproducibility Orchestrator, is a framework to automate and standardize application
compilation, benchmarking and result collection on large scale HPC systems.  This includes WRF 4.2."

*WRF V3 Parallel Benchmark Page*

https://www2.mmm.ucar.edu/wrf/WG2/bench/[`https://www2.mmm.ucar.edu/wrf/WG2/bench/`]

"These cases only work through V3.9. They are no longer applicable beginning with V4.0."

*AMD WRF*

https://developer.amd.com/spack/hpc-applications-wrf/[`https://developer.amd.com/spack/hpc-applications-wrf/`]

*Google Codelabs*

https://codelabs.developers.google.com/codelabs/wrf-on-slurm-gcp#0[`https://codelabs.developers.google.com/codelabs/wrf-on-slurm-gcp#0`]

"In this codelab, you are going to deploy an auto-scaling High Performance Computing (HPC) cluster on Google Cloud with the Slurm job scheduler. You will use an example Terraform deployment that deploys this cluster with WRFÂ® installed via Spack. Then, you will use this infrastructure to run the CONUS 2.5km benchmark or the CONUS 12km benchmark."

*Parallel I/O Benchmarks, Applications, Traces*

https://web.cels.anl.gov/\~thakur/pio-benchmarks.html[`https://web.cels.anl.gov/~thakur/pio-benchmarks.html`]

*Dell Tests WRF on Ice Lake CPUS*

https://infohub.delltechnologies.com/p/wrf-performance-with-3rd-generation-intel-xeon-scalable-processors-on-dell-emc-poweredge-servers/[`https://infohub.delltechnologies.com/p/wrf-performance-with-3rd-generation-intel-xeon-scalable-processors-on-dell-emc-poweredge-servers/`]

=====
WRF was compiled with the "dm + sm" configuration with avx2 instructions and serial netcdf support (io_form* set to 2). All the available cores were subscribed during WRF simulation runs. To optimize performance, we tested different MPI process counts, OpenMP thread count combinations, and tiling schemes (WRF_NUM_TILES).

Depending on the dataset, the 8380 processor model can deliver up to 19 percent better performance compared to the 6338 processor model. Relative to Cascade Lake, the Ice Lake architecture has more memory channels and offers higher aggregate memory bandwidth. WRF, which is typically memory bandwidth bound, can take advantage of the additional memory bandwidth (Table 3) provided by Ice Lake and the results demonstrate up to 65 percent performance improvement over the Cascade Lake counterparts.
=====

== The WRF EB Module

The installed EB module to use is `WRF-4.3-intel-2021a-dmpar.eb`.  The installed directory for this is at:

-----
/sw/eb/sw/WRF/4.3-intel-2021a-dmpar/WRF-4.3/
-----

It is loaded via:

-----
module load intel-compilers/2021.2.0 impi/2021.2.0 WRF/4.3-dmpar
-----

and contains:

-----
dr-xr-sr-x  2 baum staff  4096 Feb 15 17:00 arch
dr-xr-sr-x  3 baum staff 20480 May 10  2021 chem
-r-xr-xr-x  1 baum staff  3510 May 10  2021 clean
-r-xr-xr-x  1 baum staff 21228 Feb 15 17:01 compile
-r-xr-xr-x  1 baum staff 21190 May 10  2021 compile.orig.eb
-r-xr-xr-x  1 baum staff 37487 Feb 15 17:00 configure
-r-xr-xr-x  1 baum staff 37163 May 10  2021 configure.orig
-r--r--r--  1 baum staff 24208 Feb 15 17:01 configure.wrf
-r--r--r--  1 baum staff 24157 Feb 15 17:01 configure.wrf.orig.eb
dr-xr-sr-x  2 baum staff  4096 May 10  2021 doc
dr-xr-sr-x  2 baum staff 12288 Feb 15 17:25 dyn_em
dr-xr-sr-x  2 baum staff  4096 May 10  2021 dyn_nmm
dr-xr-sr-x 16 baum staff  4096 May 10  2021 external
dr-xr-sr-x  2 baum staff 20480 Feb 15 17:08 frame
dr-xr-sr-x 16 baum staff  4096 May 10  2021 hydro
dr-xr-sr-x  2 baum staff 36864 Feb 15 17:02 inc
-r--r--r--  1 baum staff  1027 May 10  2021 LICENSE.txt
dr-xr-sr-x  2 baum staff  4096 Feb 15 17:59 main
-r--r--r--  1 baum staff 61394 Feb 15 17:00 Makefile
-r--r--r--  1 baum staff 61363 May 10  2021 Makefile.orig
dr-xr-sr-x  2 baum staff 73728 Feb 15 17:21 phys
-r--r--r--  1 baum staff 17697 May 10  2021 README
-r--r--r--  1 baum staff  1151 May 10  2021 README.md
dr-xr-sr-x  2 baum staff  4096 Feb 15 17:02 Registry
dr-xr-sr-x  2 baum staff 12288 Feb 15 17:59 run
dr-xr-sr-x  2 baum staff 20480 Feb 15 17:09 share
dr-xr-sr-x 19 baum staff  4096 May 10  2021 test
dr-xr-sr-x  4 baum staff 12288 Feb 15 17:02 tools
dr-xr-sr-x 14 baum staff  4096 May 10  2021 var
dr-xr-sr-x  2 baum staff  4096 May 10  2021 wrftladj
-----

== The WPS EB Module

The WPS module is used in the benchmark job script.
WPS is the WRF Pre-Processing System, a collection
of Fortran and C programs that provides data used as
input to the `real.exe` and `real_nmm.exe` programs. There 
are three main programs and a number of auxiliary 
programs that are part of WPS.  Both the ARW and NMM 
dynamical cores in WRF are supported by WPS.

The three main programs are `geogrid.exe`, `ungridb.exe` and
`metgrid.exe`.  These programs process input from the
namelist file `namelist.wps`.  The functions of these programs are:

* `geogrid` - Defines the model horizontal domain, horizontally interpolates static data
to the model domain, and creates input files conforming to the WRF I/O API.
* `ungrib` - Decodes GRIB data, uses tables to decide which variables to extract, and creates
preliminary files not in WRF I/O API format.
* `metgrid` - Reads static data and raw meteorological fields, horizontally interpolates
meteorological fields to the model domain, and creates input files conforming to the WRF I/O API.

The WPS module is loaded via:

-----
module load intel-compilers/2021.2.0 impi/2021.2.0 WPS/4.3.1-dmpar
-----

and contains the following:

-----
dr-xr-sr-x 2 baum  4096 Feb 15 11:25 arch
-r-xr-xr-x 1 baum  1765 Nov  5 17:22 clean
-r-xr-xr-x 1 baum  4797 Nov  5 17:22 compile
-r-xr-xr-x 1 baum 15068 Feb 15 11:24 configure
-r-xr-xr-x 1 baum 14910 Nov  5 17:22 configure.orig
-r--r--r-- 1 baum  3689 Feb 15 11:25 configure.wps
-r--r--r-- 1 baum  3576 Feb 15 11:25 configure.wps.orig.eb
-r--r--r-- 1 baum   185 Feb 15 11:25 fort_netcdf.f
dr-xr-sr-x 4 baum  4096 Feb 15 11:25 geogrid
lrwxrwxrwx 1 baum    23 Feb 15 11:25 geogrid.exe -> geogrid/src/geogrid.exe
-r-xr-xr-x 1 baum  1331 Nov  5 17:22 link_grib.csh
dr-xr-sr-x 3 baum  4096 Feb 15 11:26 metgrid
lrwxrwxrwx 1 baum    23 Feb 15 11:26 metgrid.exe -> metgrid/src/metgrid.exe
-r--r--r-- 1 baum   705 Nov  5 17:22 namelist.wps
-r--r--r-- 1 baum  2749 Nov  5 17:22 namelist.wps.all_options
-r-xr-xr-x 1 baum  2077 Nov  5 17:22 namelist.wps.fire
-r--r--r-- 1 baum  1637 Nov  5 17:22 namelist.wps.global
-r--r--r-- 1 baum   654 Nov  5 17:22 namelist.wps.nmm
-r--r--r-- 1 baum 61393 Feb 15 11:25 netcdf.inc
-r--r--r-- 1 baum  6320 Nov  5 17:22 README
dr-xr-sr-x 4 baum  4096 Feb 15 11:26 ungrib
lrwxrwxrwx 1 baum    21 Feb 15 11:25 ungrib.exe -> ungrib/src/ungrib.exe
dr-xr-sr-x 3 baum  4096 Feb 15 11:26 util
-----

== Internal WRF Test Cases

=== Available Test Cases

The WRF 4.3 docs describe the available test cases at:

https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/v4.3/users_guide_chap4.html[`https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/v4.3/users_guide_chap4.html`]

=====
The WRF model has two classes of simulations it can generate: those with an ideal initialization and those utilizing real data. Idealized simulations typically manufacture an initial condition file for the WRF model from an existing 1-D or 2-D sounding and assume a simplified analytic orography. Real-data cases usually require pre-processing from the WPS package, which provides each atmospheric and static field with fidelity appropriate to the chosen grid resolution for the model. The WRF model executable itself is not altered by choosing one initialization option over another (idealized vs. real), but the WRF model pre-processors (the real.exe and ideal.exe programs) are specifically built based upon a user's selection. Either real.exe or ideal.exe will be run prior to running the WRF model.
=====

==== Ideal Test Cases

The available ideal test cases are:

* `em_squall2d_x`
** 2D squall line (x,z) using Kessler microphysics and a fixed 300 m^2/s viscosity
** periodicity condition used in y so that 3D model produces 2D simulation
** v velocity should be zero and there should be no variation in y in the results
* `squall2d_y`
** Same as `squall2d_x`, except with (x) rotated to (y)
** u velocity should be zero and there should be no variation in x in the results
* `em_quarter_ss`
** 3-D quarter-circle shear supercell simulation
** Left and right moving supercells are produced
** See the `README.quarter_ss` file in the test directory for more information
* `em_hill2d_x`
** 2-D flow over a bell-shaped hill
** 10 km half-width, 2 km grid-length, 100 m high hill, 10 m/s flow, N=0.01/s, 30 km high domain, 80 levels, open radiative boundaries, absorbing upper boundary.
** Case is in linear hydrostatic regime, so vertical tilted waves with ~6-km vertical wavelength
* `em_b_wave`
** 3-D baroclinic waves
** Baroclinically unstable jet u(y,z) on an f-plane
** Symmetric north and south, periodic east and west boundaries
** 100-km grid size, 16-km top, with 4-km damping layer
** 41x81 points in (x,y), 64 layers
* `em_grav2d_x`
** 2-D gravity current
** Test case is described in Straka et al, INT J NUMER METH FL 17 (1): 1-22 July 15 1993
** See the `README.grav2d_x` file in the test directory
* `em_seabreeze_x`
** 2-D sea breeze
** 2-km grid size, 20-km top, land/water
** Can be run with full physics, radiation, surface, boundary layer, and land options
* `em_les`
** 3-D large eddy simulation (LES)
** 100-m grid size, 2-km top
** Surface layer physics with fluxes
** Doubly periodic
* `em_heldsuarez`
** 3-D Held-Suarez
** global domain, 625 km in x-direction, 556 km in y-direction, 120-km top
** Radiation, polar filter above 45 deg.
** Periodic in x-direction, polar boundary conditions in y-direction
* `em_scm_xy`
** 1-D single column model
** 4-km grid size, 12-km top
** Full physics
** Doubly periodic
* `em_fire`
** 3-D surface fire
** Geoscientific Model Development Discussions (GMDD) 4, 497-545, 2011
** 50-m, 4.5-km top
** 10:1 subgrid ratio, no physics
** Open boundaries
* `em_tropical_cyclone`
** 3-D tropical cyclone
** Test case described in Jordan, J METEOR 15, 91-97, 1958
** 15-km, 25-km top
** f-plane (f=0.5e-5, about 20 N), SST=28 C
** Full physics with a simple radiative cooling, no cumulus
** Doubly periodic
* `em_convrad`
** 3-D convective-radiative equilibrium
** 1 km grid size, 30 km model top
** tropical condition, small f, weak wind, constant SST
** full physics
** doubly periodic

=== WRF 4.3 Test Cases

We'll try some internal WRF test cases first.
The WRF test cases are located in the `test` subdirectory of the main WRF installation directory.
For the `WRF-4.3-intel-2021a-dmpar` EB installed version this is:

-----
/sw/eb/sw/WRF/4.3-intel-2021a-dmpar/WRF-4.3/test
-----

The directions for running test cases are found at:

https://github.com/wrf-model/WRF/blob/master/doc/README.test_cases[`https://github.com/wrf-model/WRF/blob/master/doc/README.test_cases`]

=====
A suite of tests for the WRF model ARW (Advanced Research WRF) core 
can be found in the directory "test".  Each subdirectory in /test 
contains the necessary data (except for the real data case) and 
input files to run the test specific to that directory.  
To run specific test, builld the WRF model
and the necessary initialization routine by typing:

-----
compile test_name
-----

in the top directory.  For example, to build the executables for the 2D (x,z) squall line
example for Eulerian mass coordinate model, you would type the command 

-----
compile em_squall2d_x
-----

After a successful build, go the the specific test directory:

-----
cd test_name
-----

and first run the initialization code:

-----
ideal.exe
-----

and then run the simulation code:

-----
wrf.exe
-----
=====

The WRF 4.3 docs also describe these at:

https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/v4.3/users_guide_chap4.html[`https://www2.mmm.ucar.edu/wrf/users/docs/user_guide_v4/v4.3/users_guide_chap4.html`]

An example of the procedure for the `em_hill2d_x` example is:

-----
cd /scratch/user/baum/WRF-4.3
ulimit -s unlimited
export WRFIO_NCD_NO_LARGE_FILE_SUPPORT=1
export WRF_EM_CORE=1
module load intel-compilers/2021.2.0 impi/2021.2.0 WRF/4.3-dmpar
module load time/1.9
./clean -a
#  This example is configured with SMP for Intel Xeon (19), and for no nesting (0).
cat<<EOF >configure.input
19
0
EOF
fi
./configure < configure.input >& conf.log
./compile em_seabreeze2d_x >& em_seabreeze2d_x.log
-----

A successful build will finish with something like the following.
Note that this beast takes over a half an hour to compile.

-----
...
build started:   Mon Feb 21 10:35:47 CST 2022
build completed: Mon Feb 21 11:09:14 CST 2022
 
--->                  Executables successfully built                  <---
 
-rwxrwxr-x 1 baum staff 44748856 Feb 21 11:09 main/ideal.exe
-rwxrwxr-x 1 baum staff 52927560 Feb 21 11:09 main/wrf.exe
-----

==== `em_seabreeze2d_x`

Now run the test case `em_seabreeze2d_x`:

=====
The case is more set up now to demonstrate how to set all land
variables so that full physics options may be used. Tuning is
needed to produce real sea-breeze simulation at this point.
=====

An example of running `em_seabreeze2_x` with 2 OpenMP threads is:

-----
export OMP_NUM_THREADS=2
cd test/em_seabreeze2d_x
./ideal.exe
time ./wrf.exe
-----

A table showing the effect of varying the number of threads for this example is:

[cols="1,1,1,1"]
|===
| Threads | real | user | sys

| 1 | 1m7.903s | 1m7.594s | 0m0.138s
| 2 | 1m10.758s | 2m20.013s |  0m1.100s
| 4 | 1m0.348s | 3m59.056s | 0m1.793s
| 8 | 1m5.532s | 8m37.889s | 0m5.009s
|===

==== `em_less`

Now try `em_less`:

=====
A large-eddy simulation (LES) of a free convective boundary 
layer (CBL) with 0 environmental wind at the initial time, and
the turbulence of the free CBL driven/maintained by the 
specified surface heat flux.
=====

Varying the number of threads provides the following performance.

[cols="1,1,1,1"]
|===
| Threads | real | user | sys

| 1 | 2m45.243s | 2m44.763s | 0m0.123s
| 2 | 1m30.110s | 2m59.411s | 0m0.374s
| 4 | 0m49.231s | 3m13.247s | 0m0.541s
| 8 | 0m41.643s | 4m17.053s | 0m1.262s
| 16 | 0m40.257s | 6m52.072s | 0m3.090s
|===

==== `em_heldsuarez`

This failed with the error:

-----
-------------- FATAL CALLED ---------------
FATAL CALLED FROM FILE:  <stdin>  LINE:    2632
module_big_step_utilities_em.F: -DOPTIMIZE_CFL_TEST option does not support global domains
-------------------------------------------
-----


== The CONUS Benchmarks

Standard benchmarks for WRF called CONUS-12km and CONUS-2.5km can be found at:

https://www2.mmm.ucar.edu/wrf/users/benchmark/benchdata_v422.html[`https://www2.mmm.ucar.edu/wrf/users/benchmark/benchdata_v422.html`]

The 

=== CONUS-12km

The files in the CONUS-12km download are:

* `namelist.input` - set up to run a 1 hour restart, using the files provided
* `wrfbdy_d01` - boundary conditions created by the real program
* `wrfout_d01_2019-11-27_00:00:00.orig` - final wrf output file from the original 12 hour wrf simulation
* `wrfrst_d01_2019-11-26_23:00:00` - restart file created during the original 12 hour wrf simulation
* `rsl.out.0000` - simulation log file/output from the original 12 hour wrf simulation
* `runwrf.csh` - batch script used to run wrf for the original simulation and the restart simulation
* `*.dat` - data files needed for Thompson microphysics (these are usually generated during wrf, but takes a while to produce)
* `diffwrf.py` - script to compare original wrfout file with new wrfout file that will be generated with a restart simulation
* `*.png` - sample PNG files of acceptable differences
G
The job file `runwrf.csh` is:

-----
#!/bin/csh

### Project name
#PBS -A NMMM0054 
### Job name
#PBS -N wrf 

### Wallclock time
#PBS -l walltime=01:00:00
### Queue
#PBS -q regular 
### Merge output and error files
#PBS -j oe                    
### Select 1 nodes with 36 CPUs, for 36 MPI processes 
#PBS -l select=4:ncpus=36:mpiprocs=36  

rm rsl.*
mpiexec_mpt ./wrf.exe
-----

== The Purdue WRF Benchmark

The Purdue WRF benchmark is described in "Defining Performance of Scientific Application Workloads on the AMD Milan Platform"
which can be found at:

https://dl.acm.org/doi/fullHtml/10.1145/3437359.3465596[`https://dl.acm.org/doi/fullHtml/10.1145/3437359.3465596`]

The Purdue study used WRF 3.9.1 and WPS 3.9.1.  Here we are using WRF 4.3 and WPS 4.3.1.


=== Downloading

The WRF used by Purdue can be found at:

https://github.itap.purdue.edu/wu979/azure_milan_benchmark/tree/master/wrf[`https://github.itap.purdue.edu/wu979/azure_milan_benchmark/tree/master/wrf`]

The dataset associated with the benchmark is too large for Github and was obtained via Globus.

The dataset was downloaded and placed on the `grace` scratch disk at:

-----
/scratch/group/hprc/benchmarks/wrf
-----

and the dataset consists of:

-----
drwxrws---  2 baum        4096 Aug 16  2018 2018081612
drwxr-sr-x  6 baum       24576 Feb 16 15:18 daily12Z
-rw-r--r--  1 baum  9079054048 Feb 16 15:16 daily12Z.tar.gz
-rw-r--r--  1 baum 30740085248 Feb 16 15:15 geog_high_res_mandatory.tar
-rw-r--r--  1 baum  1612697600 Feb 16 15:15 WPS2018081612.tar
drwxr-sr-x 21 baum        4096 Jun 15  2018 WPS_GEOG
-rw-rw-r--  1 baum          13 Feb 16 15:34 wps_geog.size
-----

The interesting bits that aren't just massive datasets waiting to be transmogrified are contained in the `daily12Z` directory:

-----
-rw-r--r-- 1 baum  29772567 Jun 13  2018 aerosol.formatted
-rw-r--r-- 1 baum       736 Jun 13  2018 aerosol_lat.formatted
-rw-r--r-- 1 baum      1152 Jun 13  2018 aerosol_lon.formatted
-rw-r--r-- 1 baum       192 Jun 13  2018 aerosol_plev.formatted
-rw-r--r-- 1 baum      3042 Jun 13  2018 bulkdens.asc_s_0_03_0_9
-rw-r--r-- 1 baum      3042 Jun 13  2018 bulkradii.asc_s_0_03_0_9
-rw-r--r-- 1 baum  20580056 Jun 13  2018 CAM_ABS_DATA
-rw-r--r-- 1 baum     18208 Jun 13  2018 CAM_AEROPT_DATA
-rw-r--r-- 1 baum     42838 Jun 13  2018 CAMtr_volume_mixing_ratio
-rw-r--r-- 1 baum       955 Jun 13  2018 CAMtr_volume_mixing_ratio.A1B
-rw-r--r-- 1 baum       955 Jun 13  2018 CAMtr_volume_mixing_ratio.A2
-rw-r--r-- 1 baum     42838 Jun 13  2018 CAMtr_volume_mixing_ratio.RCP4.5
-rw-r--r-- 1 baum     42838 Jun 13  2018 CAMtr_volume_mixing_ratio.RCP6
-rw-r--r-- 1 baum      3042 Jun 13  2018 capacity.asc
-rw-r--r-- 1 baum     35288 Jun 13  2018 CCN_ACTIVATE.BIN
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_ALB_ICE_DFS_DATA
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_ALB_ICE_DRC_DATA
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_ASM_ICE_DFS_DATA
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_ASM_ICE_DRC_DATA
-rw-r--r-- 1 baum     68448 Jun 13  2018 CLM_DRDSDT0_DATA
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_EXT_ICE_DFS_DATA
-rw-r--r-- 1 baum    148571 Jun 13  2018 CLM_EXT_ICE_DRC_DATA
-rw-r--r-- 1 baum     68448 Jun 13  2018 CLM_KAPPA_DATA
-rw-r--r-- 1 baum     68448 Jun 13  2018 CLM_TAU_DATA
-rw-r--r-- 1 baum    206228 Jun 13  2018 coeff_p.asc
-rw-r--r-- 1 baum      9398 Jun 13  2018 coeff_q.asc
-rw-r--r-- 1 baum      3950 Jun 13  2018 constants.asc
-rw-r--r-- 1 baum     30200 Jun 13  2018 ETAMPNEW_DATA
-rwxr-xr-x 1 baum     43400 Jun 13  2018 ETAMPNEW_DATA.expanded_rain
-rwxr-xr-x 1 baum     26792 Jun 13  2018 examples.namelist
-rw-r--r-- 1 baum  89382216 Aug 15  2018 FILE:2018-08-15_15
-rw-r--r-- 1 baum  89382216 Aug 15  2018 FILE:2018-08-15_21
-rw-r--r-- 1 baum  89382216 Aug 15  2018 FILE:2018-08-16_15
-rw-r--r-- 1 baum  89382216 Aug 15  2018 FILE:2018-08-17_03
-rw-r--r-- 1 baum  89382216 Aug 15  2018 FILE:2018-08-17_18
-rw-r--r-- 1 baum       261 Jun 13  2018 GENPARM.TBL
-rw-r--r-- 1 baum  51799552 Aug 15  2018 geo_em.d01.nc
drwxr-sr-x 2 baum      4096 Jun 13  2018 geogrid
-rwxr-xr-x 1 baum   3716000 Jun 13  2018 geogrid.exe
-rw-r--r-- 1 baum     14251 Jun 13  2018 geogrid.log
-rw-r--r-- 1 baum     11754 Aug 15  2018 geogrid.log.0000
-rw-r--r-- 1 baum     11754 Aug 15  2018 geogrid.log.0001
...
-rw-r--r-- 1 baum     11754 Aug 15  2018 geogrid.log.0098
-rw-r--r-- 1 baum     11754 Aug 15  2018 geogrid.log.0099
-rw-r--r-- 1 baum     30547 Jun 13  2018 grib2map.tbl
-rw-rw---- 1 baum  48292570 Aug 16  2018 GRIBFILE.AAA
-rw-rw---- 1 baum  51405207 Aug 16  2018 GRIBFILE.AAB
-rw-rw---- 1 baum  52793654 Aug 16  2018 GRIBFILE.AAC
-rw-rw---- 1 baum  54024988 Aug 16  2018 GRIBFILE.AAD
-rw-rw---- 1 baum  55214169 Aug 15  2018 GRIBFILE.AAE
-rw-rw---- 1 baum  54527883 Aug 16  2018 GRIBFILE.AAF
-rw-rw---- 1 baum  54716397 Aug 16  2018 GRIBFILE.AAG
-rw-rw---- 1 baum  54928216 Aug 16  2018 GRIBFILE.AAH
-rw-rw---- 1 baum  55331839 Aug 15  2018 GRIBFILE.AAI
-rw-rw---- 1 baum  55626208 Aug 16  2018 GRIBFILE.AAJ
-rw-rw---- 1 baum  55789472 Aug 16  2018 GRIBFILE.AAK
-rw-rw---- 1 baum  56235139 Aug 16  2018 GRIBFILE.AAL
-rw-rw---- 1 baum  56239945 Aug 16  2018 GRIBFILE.AAM
-rw-rw---- 1 baum  55810143 Aug 16  2018 GRIBFILE.AAN
-rw-rw---- 1 baum  55798348 Aug 15  2018 GRIBFILE.AAO
-rw-rw---- 1 baum  56030017 Aug 16  2018 GRIBFILE.AAP
-rw-rw---- 1 baum  56230908 Aug 16  2018 GRIBFILE.AAQ
-rw-rw---- 1 baum  55991985 Aug 15  2018 GRIBFILE.AAR
-rw-rw---- 1 baum  56348551 Aug 15  2018 GRIBFILE.AAS
-rw-rw---- 1 baum  56838269 Aug 16  2018 GRIBFILE.AAT
-rw-rw---- 1 baum  57251144 Aug 16  2018 GRIBFILE.AAU
-rw-rw---- 1 baum  57156371 Aug 16  2018 GRIBFILE.AAV
-rw-rw---- 1 baum  56755667 Aug 16  2018 GRIBFILE.AAW
-rw-rw---- 1 baum  56915011 Aug 16  2018 GRIBFILE.AAX
-rw-r--r-- 1 baum     65700 Jun 13  2018 gribmap.txt
-rw-r--r-- 1 baum    702587 Jun 13  2018 kernels.asc_s_0_03_0_9
-rw-r--r-- 1 baum     43016 Jun 13  2018 kernels_z.asc
-rw-r--r-- 1 baum      1762 Jun 13  2018 landFilenames
-rw-r--r-- 1 baum     29820 Jun 13  2018 LANDUSE.TBL
-rwxr-xr-x 1 baum      1331 Jun 13  2018 link_grib.csh
-rw-r--r-- 1 baum      3042 Jun 13  2018 masses.asc
-rw-r--r-- 1 baum 215340127 Aug 15  2018 met_em.d01.2018-08-15_18:00:00.nc
-rw-r--r-- 1 baum 219661293 Aug 15  2018 met_em.d01.2018-08-17_06:00:00.nc
-rw-r--r-- 1 baum 218723703 Aug 15  2018 met_em.d01.2018-08-17_09:00:00.nc
-rw-r--r-- 1 baum 217792484 Aug 15  2018 met_em.d01.2018-08-17_15:00:00.nc
-rw-r--r-- 1 baum 217823987 Aug 15  2018 met_em.d01.2018-08-17_18:00:00.nc
drwxr-sr-x 2 baum      4096 Jun 13  2018 metgrid
-rwxr-xr-x 1 baum   3646048 Jun 13  2018 metgrid.exe
-rw-r--r-- 1 baum    693801 Aug 15  2018 metgrid.log.0000
-rw-r--r-- 1 baum    693801 Aug 15  2018 metgrid.log.0001
...
-rw-r--r-- 1 baum    693801 Aug 15  2018 metgrid.log.0098
-rw-r--r-- 1 baum    693801 Aug 15  2018 metgrid.log.0099
-rw-r--r-- 1 baum     47072 Jun 13  2018 MPTABLE.TBL
-rw-r--r-- 1 baum      5143 Aug 16  2018 namelist.input
-rw-r--r-- 1 baum     85583 Aug 15  2018 namelist.output
-rw-r--r-- 1 baum       668 Aug 16  2018 namelist.wps
-rw-r--r-- 1 baum      2751 Jun 13  2018 namelist.wps.all_options
-rwxr-xr-x 1 baum      2079 Jun 13  2018 namelist.wps.fire
-rw-r--r-- 1 baum      1639 Jun 13  2018 namelist.wps.global
-rw-r--r-- 1 baum       656 Jun 13  2018 namelist.wps.nmm
-rw-r--r-- 1 baum    543744 Jun 13  2018 ozone.formatted
-rw-r--r-- 1 baum       536 Jun 13  2018 ozone_lat.formatted
-rw-r--r-- 1 baum       708 Jun 13  2018 ozone_plev.formatted
-rw-r--r-- 1 baum   3266015 Jun 13  2018 p3_lookup_table_1.dat-v2.8.2
-rw-r--r-- 1 baum  24000015 Jun 13  2018 p3_lookup_table_2.dat-v2.8.2
-rw-r--r-- 1 baum      4095 Jun 13  2018 README.grid_fdda
-rw-r--r-- 1 baum    142940 Jun 13  2018 README.namelist
-rw-r--r-- 1 baum     12204 Jun 13  2018 README.obs_fdda
-rwxr-xr-x 1 baum  48097304 Jun 13  2018 real.exe
-rw-r--r-- 1 baum    749248 Jun 13  2018 RRTM_DATA
-rw-r--r-- 1 baum    847552 Jun 13  2018 RRTMG_LW_DATA
-rw-r--r-- 1 baum    680368 Jun 13  2018 RRTMG_SW_DATA
-rw-r--r-- 1 baum     10332 Aug 15  2018 rsl.error.0001
-rw-r--r-- 1 baum     10332 Aug 15  2018 rsl.error.0006
...
-rw-r--r-- 1 baum     10333 Aug 15  2018 rsl.error.0090
-rw-r--r-- 1 baum     10333 Aug 15  2018 rsl.error.0095
-rw-r--r-- 1 baum      2304 Aug 15  2018 rsl.out.0003
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0014
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0015
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0032
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0035
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0036
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0044
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0049
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0064
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0072
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0073
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0079
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0080
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0084
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0085
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0086
-rw-r--r-- 1 baum      2305 Aug 15  2018 rsl.out.0095
-rw-r--r-- 1 baum       358 Aug 15  2018 runwrfcron
-rw-r--r-- 1 baum       633 Jun 13  2018 sample.txt
-rw-r--r-- 1 baum      4399 Jun 13  2018 SOILPARM.TBL
-rw-r--r-- 1 baum      3043 Jun 13  2018 termvels.asc
-rw-r--r-- 1 baum    748503 Jun 13  2018 tr49t67
-rw-r--r-- 1 baum    748503 Jun 13  2018 tr49t85
-rw-r--r-- 1 baum    748503 Jun 13  2018 tr67t85
drwxr-sr-x 3 baum      4096 Jun 13  2018 ungrib
-rwxr-xr-x 1 baum   2309152 Jun 13  2018 ungrib.exe
-rw-r--r-- 1 baum    505728 Aug 15  2018 ungrib.log
-rw-r--r-- 1 baum     11188 Jun 13  2018 URBPARM.TBL
-rw-r--r-- 1 baum     22986 Jun 13  2018 VEGPARM.TBL
-rw-r--r-- 1 baum      4957 Jun 13  2018 Vtable
-rw-r--r-- 1 baum       443 Jun 13  2018 wind-turbine-1.tbl
-rw-r-xr-- 1 baum    322747 Jun 21  2018 windturbines.txt
-rw-r--r-- 1 baum 320174813 Aug 15  2018 wrfbdy_d01
-rwxr-xr-x 1 baum  54460368 Jun 13  2018 wrf.exe
-rw-r--r-- 1 baum 437693226 Aug 15  2018 wrfout_d01_2018-08-15_13:00:00
-rw-r--r-- 1 baum 482847271 Aug 15  2018 wrfout_d01_2018-08-15_20:00:00
-rw-r--r-- 1 baum 483773830 Aug 15  2018 wrfout_d01_2018-08-15_23:00:00
-rw-r--r-- 1 baum 480184864 Aug 15  2018 wrfout_d01_2018-08-16_00:00:00
-rw-r--r-- 1 baum 461009971 Aug 15  2018 wrfout_d01_2018-08-16_07:00:00
-rw-r--r-- 1 baum 466583501 Aug 15  2018 wrfout_d01_2018-08-16_11:00:00
-rw-r--r-- 1 baum 471727411 Aug 15  2018 wrfout_d01_2018-08-16_15:00:00
-rw-r--r-- 1 baum 491077191 Aug 15  2018 wrfout_d01_2018-08-16_21:00:00
-rw-r--r-- 1 baum 490125147 Aug 15  2018 wrfout_d01_2018-08-16_22:00:00
-rw-r--r-- 1 baum 469151410 Aug 15  2018 wrfout_d01_2018-08-17_10:00:00
-rw-r--r-- 1 baum 470730629 Aug 15  2018 wrfout_d01_2018-08-17_11:00:00
-rw-r--r-- 1 baum 478674721 Aug 15  2018 wrfout_d01_2018-08-17_20:00:00
-rw-r--r-- 1 baum 473867673 Aug 15  2018 wrfout_d01_2018-08-18_00:00:00
drwxr-sr-x 2 baum      4096 Aug 15  2018 wrfpost
-rw------- 1 baum     10997 Aug 15  2018 wrfruncron.err
-rw------- 1 baum    411278 Aug 15  2018 wrfruncron.out
-----

=== The Original Job Script

The job submission script `wrf.sub` is not contained in the downloaded files and was separately obtain from the Github address above.
This script is rewritten further on for the TAMU HPRC environment.

It contains:

[source, bash]
-----
#PBS -S /bin/bash
#PBS -l walltime=4:00:00
#PBS -l nodes=4:ppn=24
#PBS -q rcac-a
#PBS -N wrf
#PBS -j oe

cd ${PBS_O_WORKDIR}

module purge

module load intel/17.0.1.132
module load impi/2017.1.132
module load netcdf4/4.5.0

## online tutorial
## http://www2.mmm.ucar.edu/wrf/OnLineTutorial/index.htm
# http://www2.mmm.ucar.edu/wrf/users/docs/user_guide_V3/users_guide_chap5.htm#_Installing_WRF

export WRF_EM_CORE=1
export WRFIO_NCD_LARGE_FILE_SUPPORT=1
export HDF5=$HDF5_HOME
export JASPERLIB=/usr/lib64/libjasper.so
export JASPERINC=/usr/include/jasper

now=$(date +"%Y.%m.%d_%H.%M.%S")
jobid=${PBS_JOBID:=now}
BASEDIR=${CLUSTER_SCRATCH}/wrf.${jobid}
DATADIR=/depot/itap/testpbs/wrf_data
SRCDIR=${PWD}/../src
NP=`cat ${PBS_NODEFILE} | wc -l`

if [ ! -d ${BASEDIR} ]; then
        mkdir ${BASEDIR}
fi

## build WRF
cd $BASEDIR
if [ ! -d WRF ]; then
  tar -zxvf $SRCDIR/WRFV4.0.TAR.gz -C .
fi
cd WRF
if [ ! -f configure.input ]; then
echo "create configure.input"
cat<<EOF >configure.input
15
1
EOF
fi

clean -a
(./configure < configure.input) > configure.log 2>&1
sed -i '154s/mpif90/mpiifort/' configure.wrf
sed -i '155s/mpicc/mpiicc/' configure.wrf
./compile -j ${NP} em_real > compile.log 2>&1 

cd $BASEDIR/WRF
cd run
mpirun -np ${NP} real.exe > run_real.log 2>&1

## build WPS
cd $BASEDIR
if [ ! -d WPS ]; then
  tar -zxvf $SRCDIR/WPSV4.0.TAR.gz -C .
fi
cd WPS
if [ ! -f configure.input ]; then
echo "create configure.input"
cat<<EOF >configure.input
19
EOF
fi
clean -a
(./configure < configure.input) > configure.log
sed -i '68s@glade/u/home/wrfhelp/UNGRIB_LIBRARIES/lib@usr/lib64@' configure.wps
sed -i '68s@glade/u/home/wrfhelp/UNGRIB_LIBRARIES@usr@' configure.wps
sed -i '73s/mpif90/mpiifort/' configure.wps
sed -i '74s/mpicc/mpiicc/' configure.wps
./compile > compile.log 2>&1

## run WRF four nodes
cd ${BASEDIR}
mkdir testdaily4
cd testdaily4
cp -rp ../WPS/*.exe ../WPS/link_grib.csh ../WPS/geogrid ../WPS/metgrid ../WPS/ungrib .
cp ${DATADIR}/daily12Z/namelist.wps ${DATADIR}/daily12Z/Vtable .
sed -i "26s@/depot/mebaldwi/data/WPS_GEOG@${DATADIR}/WPS_GEOG/@" namelist.wps
./geogrid.exe > geogrid.log 2>&1
./link_grib.csh "${DATADIR}/2018081612/nam.t12z.awip32"
./ungrib.exe > ungrib.log 2>&1
./metgrid.exe > metgrid.log 2>&1
cp ../WRF/run/* .
cp ${DATADIR}/daily12Z/namelist.input .
(time mpirun -np ${NP} real.exe) > real.log 2>&1
(time mpirun -np ${NP} wrf.exe) > wrf.log 2>&1
-----

=== Pre-Testing Parts of the Modified Job Script

The various bits of the job script will be separately tested first, and then combined into a final
job script modified for the TAMU HPRC environment.

==== Configuring WRF

The WRF distribution contains a `configure` script that must be run before compiling.
Here is the procedure used to do so on FASTER.
This is contained within the job script and we are testing it here.

-----
# Load the appropriate modules.
module load intel-compilers/2021.2.0 impi/2021.2.0 WRF/4.3-dmpar
module load time/1.9
#  The EB directories are read only, so we must copy the WRF directory to scratch.
rsync -avz /sw/eb/sw/WRF/4.3-intel-2021a-dmpar/WRF-4.3 /scratch/usr/baum
cd /scratch/usr/baum/WRF-4.3
#  Enable the directory for writing.
chmod -R u+w .
#  Create an input configure file for Intel Xeon smpar (19) and no nesting (0).
#  This is for OpenMP tests.  Change to 20 or 21 for dmpar or dm+sm, respectively.
cat<<EOF >configure-19-0.input
19
0
EOF
fi
./configure < configure-10-0.input >& configure.log *
-----

The `configure.log` file is:

-----
checking for perl5... no
checking for perl... found /usr/bin/perl (perl)
Will use NETCDF in dir: /sw/eb/sw/netCDF/4.8.0-iimpi-2021a
HDF5 not set in environment. Will configure WRF for use without.
PHDF5 not set in environment. Will configure WRF for use without.
Will use 'time' to report timing information
$JASPERLIB or $JASPERINC not found in environment, configuring to build without grib2 I/O...
------------------------------------------------------------------------
Please select from among the following Linux x86_64 options:

  1. (serial)   2. (smpar)   3. (dmpar)   4. (dm+sm)   PGI (pgf90/gcc)
  5. (serial)   6. (smpar)   7. (dmpar)   8. (dm+sm)   PGI (pgf90/pgcc): SGI MPT
  9. (serial)  10. (smpar)  11. (dmpar)  12. (dm+sm)   PGI (pgf90/gcc): PGI accelerator
 13. (serial)  14. (smpar)  15. (dmpar)  16. (dm+sm)   INTEL (ifort/icc)
                                         17. (dm+sm)   INTEL (ifort/icc): Xeon Phi (MIC architecture)
 18. (serial)  19. (smpar)  20. (dmpar)  21. (dm+sm)   INTEL (ifort/icc): Xeon (SNB with AVX mods)
 22. (serial)  23. (smpar)  24. (dmpar)  25. (dm+sm)   INTEL (ifort/icc): SGI MPT
 26. (serial)  27. (smpar)  28. (dmpar)  29. (dm+sm)   INTEL (ifort/icc): IBM POE
 30. (serial)               31. (dmpar)                PATHSCALE (pathf90/pathcc)
 32. (serial)  33. (smpar)  34. (dmpar)  35. (dm+sm)   GNU (gfortran/gcc)
 36. (serial)  37. (smpar)  38. (dmpar)  39. (dm+sm)   IBM (xlf90_r/cc_r)
 40. (serial)  41. (smpar)  42. (dmpar)  43. (dm+sm)   PGI (ftn/gcc): Cray XC CLE
 44. (serial)  45. (smpar)  46. (dmpar)  47. (dm+sm)   CRAY CCE (ftn $(NOOMP)/cc): Cray XE and XC
 48. (serial)  49. (smpar)  50. (dmpar)  51. (dm+sm)   INTEL (ftn/icc): Cray XC
 52. (serial)  53. (smpar)  54. (dmpar)  55. (dm+sm)   PGI (pgf90/pgcc)
 56. (serial)  57. (smpar)  58. (dmpar)  59. (dm+sm)   PGI (pgf90/gcc): -f90=pgf90
 60. (serial)  61. (smpar)  62. (dmpar)  63. (dm+sm)   PGI (pgf90/pgcc): -f90=pgf90
 64. (serial)  65. (smpar)  66. (dmpar)  67. (dm+sm)   INTEL (ifort/icc): HSW/BDW
 68. (serial)  69. (smpar)  70. (dmpar)  71. (dm+sm)   INTEL (ifort/icc): KNL MIC
 72. (serial)  73. (smpar)  74. (dmpar)  75. (dm+sm)   FUJITSU (frtpx/fccpx): FX10/FX100 SPARC64 IXfx/Xlfx

Enter selection [1-75] : ------------------------------------------------------------------------
Compile for nesting? (0=no nesting, 1=basic, 2=preset moves, 3=vortex following) [default 0]: 
Configuration successful! 
------------------------------------------------------------------------
testing for fseeko and fseeko64
fseeko64 is supported
------------------------------------------------------------------------

# Settings for    Linux x86_64 ppc64le i486 i586 i686, Xeon (SNB with AVX mods) ifort compiler with icc  (smpar)
#

DESCRIPTION     =       INTEL ($SFC/$SCC): Xeon (SNB with AVX mods)
DMPARALLEL      =       # 1
OMPCPP          =        -D_OPENMP
OMP             =        -qopenmp -fpp -auto
OMPCC           =        -qopenmp -fpp -auto
SFC             =       ifort 
SCC             =       icc 
CCOMP           =       icc 
DM_FC           =       mpif90 -f90=$(SFC)
DM_CC           =       mpicc -cc=$(SCC)
FC              =       $(DM_FC)
CC              =       $(DM_CC) -DFSEEKO64_OK
LD              =       $(FC)
RWORDSIZE       =       $(NATIVE_RWORDSIZE)
PROMOTION       =       -real-size `expr 8 \* $(RWORDSIZE)` -i4
ARCH_LOCAL      =       -DNONSTANDARD_SYSTEM_FUNC -DCHUNK=64 -DXEON_OPTIMIZED_WSM5 -DOPTIMIZE_CFL_TEST  -DWRF_USE_CLM $(NETCDF4_IO_OPTS)
OPTNOSIMD       =
OPTAVX          =       -xAVX
CFLAGS_LOCAL    =       -w -O3 $(OPTAVX) # -DRSL0_ONLY
LDFLAGS_LOCAL   =       $(OPTAVX)
CPLUSPLUSLIB    =       
ESMF_LDFLAG     =       $(CPLUSPLUSLIB)
FCOPTIM         =       -O3 $(OPTAVX)
FCREDUCEDOPT    =       $(FCOPTIM)
FCNOOPT         =       -O0 -fno-inline -no-ip
FCDEBUG         =       # -g $(FCNOOPT) -traceback # -fpe0 -check noarg_temp_created,bounds,format,output_conversion,pointers,uninit -ftrapuv -unroll0 -u
FORMAT_FIXED    =       -FI
FORMAT_FREE     =       -FR
FCSUFFIX        =
BYTESWAPIO      =       -convert big_endian
RECORDLENGTH    =       -assume byterecl
FCBASEOPTS_NO_G =       -w $(OMP) -auto -ftz -fno-alias -fp-model fast=1 -no-prec-div -no-prec-sqrt $(FORMAT_FREE) $(BYTESWAPIO) -auto -align array64byte #-vec-report6
FCBASEOPTS      =       $(FCBASEOPTS_NO_G) $(FCDEBUG)
MODULE_SRCH_FLAG =     
TRADFLAG        =      -traditional-cpp $(NETCDF4_IO_OPTS)
CPP             =      /lib/cpp -P -nostdinc
AR              =      ar
ARFLAGS         =      ru
M4              =      m4
RANLIB          =      ranlib
RLFLAGS         =       
CC_TOOLS        =      gcc

###########################################################
######################
# POSTAMBLE

FGREP = fgrep -iq

ARCHFLAGS       =    $(COREDEFS) -DIWORDSIZE=$(IWORDSIZE) -DDWORDSIZE=$(DWORDSIZE) -DRWORDSIZE=$(RWORDSIZE) -DLWORDSIZE=$(LWORDSIZE) \
                     $(ARCH_LOCAL) \
                     $(DA_ARCHFLAGS) \
                       \
                       \
                      -DNETCDF \
                       \
                       \
                       \
                       \
                       \
                       \
                       \
                       \
                       -DLANDREAD_STUB=1 \
                       \
                       \
                      -DUSE_ALLOCATABLES \
                      -Dwrfmodel \
                      -DGRIB1 \
                      -DINTIO \
                      -DKEEP_INT_AROUND \
                      -DLIMIT_ARGS \
                      -DBUILD_RRTMG_FAST=0 \
                      -DBUILD_RRTMK=0 \
                      -DBUILD_SBM_FAST=1 \
                      -DSHOW_ALL_VARS_USED=0 \
                      -DCONFIG_BUF_LEN=$(CONFIG_BUF_LEN) \
                      -DMAX_DOMAINS_F=$(MAX_DOMAINS) \
                      -DMAX_HISTORY=$(MAX_HISTORY) \
                      -DNMM_NEST=$(WRF_NMM_NEST)
CFLAGS          =    $(CFLAGS_LOCAL)   \
                      -DLANDREAD_STUB=1 \
                      -DMAX_HISTORY=$(MAX_HISTORY) -DNMM_CORE=$(WRF_NMM_CORE)
FCFLAGS         =    $(FCOPTIM) $(FCBASEOPTS)
ESMF_LIB_FLAGS  =    
# ESMF 5 -- these are defined in esmf.mk, included above
 ESMF_IO_LIB     =    -L$(WRF_SRC_ROOT_DIR)/external/esmf_time_f90 -lesmf_time
ESMF_IO_LIB_EXT =    -L$(WRF_SRC_ROOT_DIR)/external/esmf_time_f90 -lesmf_time
INCLUDE_MODULES =    $(MODULE_SRCH_FLAG) \
                     $(ESMF_MOD_INC) $(ESMF_LIB_FLAGS) \
                      -I$(WRF_SRC_ROOT_DIR)/main \
                      -I$(WRF_SRC_ROOT_DIR)/external/io_netcdf \
                      -I$(WRF_SRC_ROOT_DIR)/external/io_int \
                      -I$(WRF_SRC_ROOT_DIR)/frame \
                      -I$(WRF_SRC_ROOT_DIR)/share \
                      -I$(WRF_SRC_ROOT_DIR)/phys \
                      -I$(WRF_SRC_ROOT_DIR)/wrftladj \
                      -I$(WRF_SRC_ROOT_DIR)/chem -I$(WRF_SRC_ROOT_DIR)/inc \
                      -I$(NETCDFPATH)/include -I$(NETCDFFPATH)/include \
                      -I$(NETCDFPATH)/include \
                       
REGISTRY        =    Registry
CC_TOOLS_CFLAGS = -DNMM_CORE=$(WRF_NMM_CORE)

LIB             =    $(LIB_BUNDLED) $(LIB_EXTERNAL) $(LIB_LOCAL) $(LIB_WRF_HYDRO)  $(NETCDF4_DEP_LIB)
LDFLAGS         =    $(OMP) $(FCFLAGS) $(LDFLAGS_LOCAL) 
ENVCOMPDEFS     =    
WRF_CHEM        =       0 
CPPFLAGS        =    $(ARCHFLAGS) $(ENVCOMPDEFS) -I$(LIBINCLUDE) $(TRADFLAG) 
NETCDFPATH      =    /sw/eb/sw/netCDF/4.8.0-iimpi-2021a
NETCDFFPATH     =    /sw/eb/sw/netCDF-Fortran/4.5.3-iimpi-2021a
HDF5PATH        =    
WRFPLUSPATH     =    
RTTOVPATH       =    
PNETCDFPATH     =    

bundled:  io_only 
external: io_only gen_comms_serial module_dm_serial $(ESMF_TARGET)
io_only:  esmf_time wrfio_nf   \
          wrf_ioapi_includes wrfio_grib_share wrfio_grib1 wrfio_int fftpack


######################
------------------------------------------------------------------------
Settings listed above are written to configure.wrf.
If you wish to change settings, please edit that file.
If you wish to change the default options, edit the file:
     arch/configure.defaults
NetCDF users note:
 This installation of NetCDF supports large file support.  To DISABLE large file
 support in NetCDF, set the environment variable WRFIO_NCD_NO_LARGE_FILE_SUPPORT
 to 1 and run configure again. Set to any other value to avoid this message.
  

Testing for NetCDF, C and Fortran compiler

This installation of NetCDF is 64-bit
                 C compiler is 64-bit
           Fortran compiler is 64-bit
              It will build in 64-bit

************************** W A R N I N G ************************************
 
The moving nest option is not available due to missing rpc/types.h file.
Copy landread.c.dist to landread.c in share directory to bypass compile error.
 
*****************************************************************************
*****************************************************************************
This build of WRF will use NETCDF4 with HDF5 compression
*****************************************************************************
-----

===== Possible Configuration Issues

Here are the issues brought forth by the configuration script that might (i.e. will) come back to bite us on the arse:

* PHDF5 not set in environment.  Will configure WRF for use without.
* The moving nest option is not available due to missing rpc/types.h file.
Copy landread.c.dist to landread.c in share directory to bypass compile error.
* This installation of NetCDF supports large file support.  To DISABLE large file
 support in NetCDF, set the environment variable WRFIO_NCD_NO_LARGE_FILE_SUPPORT
 to 1 and run configure again. Set to any other value to avoid this message.

==== Fortran Compiler Options

The compiler settings for choice `19` in the `configure` step are for:

-----
Linux x86_64 ppc64le i486 i586 i686, Xeon (SNB with AVX mods) ifort compiler
-----

These are finally collected in the `FCFLAGS` variable - itself a concatentation of `FCOPTIM` and
`FCBASEOPTS` - and are:

* `-O3` - optimize for maximum speed and enable more aggressive optimizations
          that may not improve performance on some programs
* `-xAVX` - may generate Intel(R) Advanced Vector Extensions (Intel(R)
                    AVX), Intel(R) SSE4.2, SSE4.1, SSSE3, SSE3,
                    SSE2, and SSE instructions for Intel(R) processors
* `-w` - disable all warnings
* `-qopenmp -fpp -auto`
* `-ftz` - enable flush denormal results to zero
* `-fno-alias` - assume no aliasing in program
* `-fp-model fast=1` - enable more aggressive floating point optimizations
* `-no-prec-div` - do not improve precision of FP divides
* `-no-prec-sqrt` - do not determine if certain square root optimizations are enabled
* `-FR` - specify sources are in free format
* `-convert big_endian` - specify the format of unformatted files containing numeric data
* `-align array64byte` - specify how data items are aligned

==== Compiler Options Specific to Ice Lake CPUs

The man page for `ifort` version 2021.2.0 contains option choices specific to Ice Lake processors.
These are:

* `-xICELAKE-CLIENT` or `-xICELAKE-SERVER` - no specifics are given for either of these options for specialized code generation
* `-arch ICELAKE-CLIENT` or `-arch ICELAKE-SERVER` - generate specialized code to optimize for a given architecture

The `-xAVX` option is specified by the WRF scripts for Xeon processors, but the `-arch X` option is not used.


==== Compiling WRF

We compile WRF.  This is also done in the job script and we are testing it here.

-----
cd /scratch/user/baum/WRF-4.3
#  Change these because we are using Intel MPI.
sed -i 's/mpif90/mpiifort/' configure.wrf
sed -i 's/mpicc/mpiicc/' configure.wrf
./compile -j 16 em_real &> compile.log &
-----

The abridged `compile.log` file looks like:

-----
None of WRF_EM_CORE, WRF_NMM_CORE,
        specified in shell environment....

==============================================================================================

V4.3
No git found or not a git repository, git commit version not available.

Compiling: WRF_EM_CORE

Linux login4.cluster 4.18.0-305.25.1.el8_4.x86_64 #1 SMP Wed Nov 3 10:29:07 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.2.0 Build 20210228_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.


==============================================================================================

setting parallel make -j 16

==============================================================================================

The following indicate the compilers selected to build the WRF system

Serial Fortran compiler (mostly for tool generation):
which SFC
/sw/eb/sw/intel-compilers/2021.2.0/compiler/2021.2.0/linux/bin/intel64/ifort

Serial C compiler (mostly for tool generation):
which SCC
/sw/eb/sw/intel-compilers/2021.2.0/compiler/2021.2.0/linux/bin/intel64/icc

Fortran compiler for the model source code:
which FC
/sw/eb/sw/impi/2021.2.0-intel-compilers-2021.2.0/mpi/2021.2.0/bin/mpiifort
Will use 'time' to report timing information

C compiler for the model source code:
which CC
/sw/eb/sw/impi/2021.2.0-intel-compilers-2021.2.0/mpi/2021.2.0/bin/mpiicc

...

( cd test/em_real ; /bin/rm -f GENPARM.TBL ; ln -s ../../run/GENPARM.TBL . )
( cd test/em_real ; /bin/rm -f LANDUSE.TBL ; ln -s ../../run/LANDUSE.TBL . )
( cd test/em_real ; /bin/rm -f SOILPARM.TBL ; ln -s ../../run/SOILPARM.TBL . )
( cd test/em_real ; /bin/rm -f URBPARM.TBL ; ln -s ../../run/URBPARM.TBL . )
( cd test/em_real ; /bin/rm -f URBPARM_LCZ.TBL ; ln -s ../../run/URBPARM_LCZ.TBL . )
( cd test/em_real ; /bin/rm -f VEGPARM.TBL ; ln -s ../../run/VEGPARM.TBL . )
( cd test/em_real ; /bin/rm -f MPTABLE.TBL ; ln -s ../../run/MPTABLE.TBL . )
( cd test/em_real ; /bin/rm -f tr49t67 ; ln -s ../../run/tr49t67 . )
( cd test/em_real ; /bin/rm -f tr49t85 ; ln -s ../../run/tr49t85 . )
( cd test/em_real ; /bin/rm -f tr67t85 ; ln -s ../../run/tr67t85 . )
( cd test/em_real ; /bin/rm -f gribmap.txt ; ln -s ../../run/gribmap.txt . )
( cd test/em_real ; /bin/rm -f grib2map.tbl ; ln -s ../../run/grib2map.tbl . )
( cd run ; /bin/rm -f real.exe ; ln -s ../main/real.exe . )
( cd run ; /bin/rm -f tc.exe ; ln -s ../main/tc.exe . )
( cd run ; /bin/rm -f ndown.exe ; ln -s ../main/ndown.exe . )
( cd run ; if test -f namelist.input ; then \
        /bin/cp -f namelist.input namelist.input.backup.`date +%Y-%m-%d_%H_%M_%S` ; fi ; \
        /bin/rm -f namelist.input ; cp ../test/em_real/namelist.input . )

==========================================================================
build started:   Wed Feb 16 12:54:36 CST 2022
build completed: Wed Feb 16 12:54:53 CST 2022

--->                  Executables successfully built                  <---

-rwxr-xr-x 1 baum staff 48690208 Feb 15 17:59 main/ideal.exe
-rwxrwxr-x 1 baum staff 50235880 Feb 16 13:22 main/ndown.exe
-rwxrwxr-x 1 baum staff 50226368 Feb 16 13:22 main/real.exe
-rwxrwxr-x 1 baum staff 49566696 Feb 16 13:22 main/tc.exe
-rwxrwxr-x 1 baum staff 54529552 Feb 16 13:22 main/wrf.exe

==========================================================================
-----

==== Configuring WPS

Here are the details of the configuration step for WPS.

-----
cd /scratch/user/baum
rsync -avz /sw/eb/sw/WPS/4.3.1-intel-2021a-dmpar ./
mv 4.3.1-intel-2021a WRF-4.3/WPS-4.3.1
cd WRF-4.3/WPS-4.3.1
chmod -R u+w .
export WRF_DIR=/scratch/user/baum/WRF-4.3
./configure

Will use NETCDF in dir: /sw/eb/sw/netCDF/4.8.0-iimpi-2021a
Using WRF I/O library in WRF build identified by $WRF_DIR: /scratch/user/baum/WRF-4.3
Found Jasper environment variables for GRIB2 support...
  $JASPERLIB = /sw/eb/sw/JasPer/2.0.28-GCCcore-10.3.0/lib64/libjasper.so
  $JASPERINC = /sw/eb/sw/JasPer/2.0.28-GCCcore-10.3.0/include/jasper
------------------------------------------------------------------------
Please select from among the following supported platforms.

   1.  Linux x86_64, gfortran    (serial)
   2.  Linux x86_64, gfortran    (serial_NO_GRIB2)
   3.  Linux x86_64, gfortran    (dmpar)
   4.  Linux x86_64, gfortran    (dmpar_NO_GRIB2)
   5.  Linux x86_64, PGI compiler   (serial)
   6.  Linux x86_64, PGI compiler   (serial_NO_GRIB2)
   7.  Linux x86_64, PGI compiler   (dmpar)
   8.  Linux x86_64, PGI compiler   (dmpar_NO_GRIB2)
   9.  Linux x86_64, PGI compiler, SGI MPT   (serial)
  10.  Linux x86_64, PGI compiler, SGI MPT   (serial_NO_GRIB2)
  11.  Linux x86_64, PGI compiler, SGI MPT   (dmpar)
  12.  Linux x86_64, PGI compiler, SGI MPT   (dmpar_NO_GRIB2)
  13.  Linux x86_64, IA64 and Opteron    (serial)
  14.  Linux x86_64, IA64 and Opteron    (serial_NO_GRIB2)
  15.  Linux x86_64, IA64 and Opteron    (dmpar)
  16.  Linux x86_64, IA64 and Opteron    (dmpar_NO_GRIB2)
  17.  Linux x86_64, Intel compiler    (serial)
  18.  Linux x86_64, Intel compiler    (serial_NO_GRIB2)
  19.  Linux x86_64, Intel compiler    (dmpar)
  20.  Linux x86_64, Intel compiler    (dmpar_NO_GRIB2)
  21.  Linux x86_64, Intel compiler, SGI MPT    (serial)
  22.  Linux x86_64, Intel compiler, SGI MPT    (serial_NO_GRIB2)
  23.  Linux x86_64, Intel compiler, SGI MPT    (dmpar)
  24.  Linux x86_64, Intel compiler, SGI MPT    (dmpar_NO_GRIB2)
  25.  Linux x86_64, Intel compiler, IBM POE    (serial)
  26.  Linux x86_64, Intel compiler, IBM POE    (serial_NO_GRIB2)
  27.  Linux x86_64, Intel compiler, IBM POE    (dmpar)
  28.  Linux x86_64, Intel compiler, IBM POE    (dmpar_NO_GRIB2)
  29.  Linux x86_64 g95 compiler     (serial)
  30.  Linux x86_64 g95 compiler     (serial_NO_GRIB2)
  31.  Linux x86_64 g95 compiler     (dmpar)
  32.  Linux x86_64 g95 compiler     (dmpar_NO_GRIB2)
  33.  Cray XE/XC CLE/Linux x86_64, Cray compiler   (serial)
  34.  Cray XE/XC CLE/Linux x86_64, Cray compiler   (serial_NO_GRIB2)
  35.  Cray XE/XC CLE/Linux x86_64, Cray compiler   (dmpar)
  36.  Cray XE/XC CLE/Linux x86_64, Cray compiler   (dmpar_NO_GRIB2)
  37.  Cray XC CLE/Linux x86_64, Intel compiler   (serial)
  38.  Cray XC CLE/Linux x86_64, Intel compiler   (serial_NO_GRIB2)
  39.  Cray XC CLE/Linux x86_64, Intel compiler   (dmpar)
  40.  Cray XC CLE/Linux x86_64, Intel compiler   (dmpar_NO_GRIB2)

Enter selection [1-40] : 19
------------------------------------------------------------------------
Configuration successful. To build the WPS, type: compile
------------------------------------------------------------------------

Testing for NetCDF, C and Fortran compiler

This installation NetCDF is 64-bit
C compiler is 64-bit
Fortran compiler is 64-bit


Your versions of Fortran and NETCDF are not consistent.
-----

==== Compiling WPS

Here are the steps for compiling WPS.

-----
cd /scratch/user/baum/WRF-4.3/WPS-4.3.1
sed -i 's/mpif90/mpiifort/' configure.wps
sed -i 's/mpicc/mpiicc/' configure.wps
./compile >& compile.log &
-----

The `compile.log` file contains:

-----
============================================================================================== 
 
Version 4.3.1
 
Linux login4.cluster 4.18.0-305.25.1.el8_4.x86_64 #1 SMP Wed Nov 3 10:29:07 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
 
Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel(R) 64, Version 2021.2.0 Build 20210228_000000
Copyright (C) 1985-2021 Intel Corporation.  All rights reserved.

 
============================================================================================== 
 
 
**** Compiling WPS and all utilities ****
 
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/geogrid/src'
make[1]: 'geogrid.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/geogrid/src'
( cd src/ngl ; make -i -r DEV_TOP="/scratch/user/baum/WRF-4.3/WPS-4.3.1" CC="icc" FC="ifort" RANLIB="echo" all )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
make[1]: Nothing to be done for 'all'.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r ungrib.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="ifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UNGRIB" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
Makefile:90: warning: overriding recipe for target '.F.o'
../../configure.wps:105: warning: ignoring old recipe for target '.F.o'
Makefile:95: warning: overriding recipe for target '.c.o'
../../configure.wps:97: warning: ignoring old recipe for target '.c.o'
make[1]: 'ungrib.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
if [ -h ungrib.exe ] ; then \
        /bin/rm -f ungrib.exe ; \
fi ; \
if [ -h ../ungrib.exe ] ; then \
        /bin/rm -f ../ungrib.exe ; \
fi ; \
if [ -e src/ungrib.exe ] ; then \
        ln -sf src/ungrib.exe . ; \
fi
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/metgrid/src'
make[1]: 'metgrid.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/metgrid/src'
( cd src/ngl ; make -i -r DEV_TOP="/scratch/user/baum/WRF-4.3/WPS-4.3.1" CC="icc" FC="ifort" RANLIB="echo" all )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
make[1]: Nothing to be done for 'all'.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r g1print.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="ifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_GRIBUTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
Makefile:90: warning: overriding recipe for target '.F.o'
../../configure.wps:105: warning: ignoring old recipe for target '.F.o'
Makefile:95: warning: overriding recipe for target '.c.o'
../../configure.wps:97: warning: ignoring old recipe for target '.c.o'
make[1]: 'g1print.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
if [ -h g1print.exe ] ; then \
        /bin/rm -f g1print.exe ; \
fi ; \
if [ -h ../g1print.exe ] ; then \
        /bin/rm -f ../g1print.exe ; \
fi ; \
if [ -e src/g1print.exe ] ; then \
        ln -sf src/g1print.exe . ; \
fi
( cd src/ngl ; make -i -r DEV_TOP="/scratch/user/baum/WRF-4.3/WPS-4.3.1" CC="icc" FC="ifort" RANLIB="echo" all )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
make[1]: Nothing to be done for 'all'.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src/ngl'
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r g2print.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="ifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_GRIBUTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
Makefile:90: warning: overriding recipe for target '.F.o'
../../configure.wps:105: warning: ignoring old recipe for target '.F.o'
Makefile:95: warning: overriding recipe for target '.c.o'
../../configure.wps:97: warning: ignoring old recipe for target '.c.o'
make[1]: 'g2print.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/ungrib/src'
if [ -h g2print.exe ] ; then \
        /bin/rm -f g2print.exe ; \
fi ; \
if [ -h ../g2print.exe ] ; then \
        /bin/rm -f ../g2print.exe ; \
fi ; \
if [ -e src/g2print.exe ] ; then \
        ln -sf src/g2print.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r rd_intermediate.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'rd_intermediate.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h rd_intermediate.exe ] ; then \
        /bin/rm -f rd_intermediate.exe ; \
if [ -h ../rd_intermediate.exe ] ; then \
        /bin/rm -f ../rd_intermediate.exe ; \
fi ; \
if [ -e src/rd_intermediate.exe ] ; then \
        ln -sf src/rd_intermediate.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r mod_levs.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'mod_levs.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h mod_levs.exe ] ; then \
        /bin/rm -f mod_levs.exe ; \
fi ; \
if [ -h ../mod_levs.exe ] ; then \
        /bin/rm -f ../mod_levs.exe ; \
fi ; \
if [ -e src/mod_levs.exe ] ; then \
        ln -sf src/mod_levs.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r avg_tsfc.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'avg_tsfc.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h avg_tsfc.exe ] ; then \
        /bin/rm -f avg_tsfc.exe ; \
fi ; \
if [ -h ../avg_tsfc.exe ] ; then \
        /bin/rm -f ../avg_tsfc.exe ; \
fi ; \
if [ -e src/avg_tsfc.exe ] ; then \
        ln -sf src/avg_tsfc.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r calc_ecmwf_p.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'calc_ecmwf_p.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h calc_ecmwf_p.exe ] ; then \
        /bin/rm -f calc_ecmwf_p.exe ; \
fi ; \
if [ -h ../calc_ecmwf_p.exe ] ; then \
        /bin/rm -f ../calc_ecmwf_p.exe ; \
fi ; \
if [ -e src/calc_ecmwf_p.exe ] ; then \
        ln -sf src/calc_ecmwf_p.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r height_ukmo.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'height_ukmo.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h height_ukmo.exe ] ; then \
        /bin/rm -f height_ukmo.exe ; \
fi ; \
if [ -h ../height_ukmo.exe ] ; then \
        /bin/rm -f ../height_ukmo.exe ; \
fi ; \
if [ -e src/height_ukmo.exe ] ; then \
        ln -sf src/height_ukmo.exe . ; \
fi
( cd src ; \
        if [ "" = yes ] ; then \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        else \
          WRF_DIR2=/scratch/user/baum/WRF-4.3 ; \
        fi ; \
make -i -r int2nc.exe \
        WRF_DIR="$WRF_DIR2" \
        FC="mpiifort" \
        CC="mpiicc" \
        CPP="/lib/cpp -P -traditional" \
        FFLAGS="-FR -convert big_endian" \
        CFLAGS="-w" \
        LDFLAGS="" \
        CPPFLAGS="-D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DIO_BINARY -DIO_GRIB1 -DBIT32 -D_MPI -D_UTIL" )
make[1]: Entering directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
make[1]: 'int2nc.exe' is up to date.
make[1]: Leaving directory '/scratch/user/baum/WRF-4.3/WPS-4.3.1/util/src'
if [ -h int2nc.exe ] ; then \
        /bin/rm -f int2nc.exe ; \
fi ; \
if [ -h ../int2nc.exe ] ; then \
        /bin/rm -f ../int2nc.exe ; \
fi ; \
if [ -e src/int2nc.exe ] ; then \
        ln -sf src/int2nc.exe . ; \
fi
-----

==== Running WPS

-----
cd $BASEDIR
mkdir test
cd test
cp -rp ../WPS-4.3.1/*.exe ../WPS-4.3.1/link_grib.csh ../WPS-4.3.1/geogrid ../WPS-4.3.1/metgrid ../WPS-4.3.1/ungrib .
export DATADIR=/scratch/group/hprc/benchmarks/wrf
cp ${DATADIR}/daily12Z/namelist.wps ${DATADIR}/daily12Z/Vtable .
sed -i "26s@/depot/mebaldwi/data/WPS_GEOG@${DATADIR}/WPS_GEOG/@" namelist.wps
./geogrid.exe > geogrid.log 2>&1
./link_grib.csh "${DATADIR}/2018081612/nam.t12z.awip32"
./ungrib.exe > ungrib.log &
./metgrid.exe >& metgrid.log &
-----

==== Running WFS

-----
cd $BASEDIR/test
cp ../run/* ./
cp ${DATADIR}/daily12Z/namelist.input .
(time mpirun -np ${NP} real.exe) > real.log 2>&1
(time mpirun -np ${NP} wrf.exe) > wrf.log 2>&1
-----

=== The Modified Job Script (Unfinished)

Here is the job script as modified for the TAMU HPRC environment.

[source, bash]
-----
#!/bin/bash
#SBATCH --export=NONE
#SBATCH --get-user-env=L
#
#SBATCH --job-name=wrf_test
#SBATCH --time=4:00:00
#SBATCH --ntasks=48
#SBATCH --ntasks-per-node=1
#SBATCH --mem=32GB
#SBATCH --output=wrf.%j

ulimit -s unlimited
module purge
#  Load the module to obtain the WRF configure program used below.
module load intel-compilers/2021.2.0 impi/2021.2.0 WRF/4.3-dmpar
module load time/1.9

#  Set WRF-specific environment variables.
export WRF_EM_CORE=1
export WRFIO_NCD_LARGE_FILE_SUPPORT=1

#  Set environment variables for libraries.
#  This should theoretically be done by loading appropriate EB modules.
#  NEEDS SCRUTINY
#export HDF5=/sw/eb/sw/HDF5/1.10.7-iimpi-2021a
#export JASPERLIB=/sw/eb/sw/JasPer/2.0.28-GCCcore-10.3.0/lib64/libjasper.so
#export JASPERINC=/sw/eb/sw/JasPer/2.0.28-GCCcore-10.3.0/include/jasper

#now=$(date +"%Y.%m.%d_%H.%M.%S")
#jobid=${SBATCH_JOBID:=now}
#BASEDIR=${WRF_SCRATCH}/wrf.${jobid}
#
#  Directory containing the WRF scripts.
BASEDIR=/scratch/user/baum/WRF-4.3
#  Directory containing the data files.
DATADIR=/scratch/group/hprc/benchmarks/wrf
#  Set how many nodes are to be used.
#NP=`cat ${SBATCH_NODEFILE} | wc -l`
NP=16

if [ ! -d ${BASEDIR} ]; then
        mkdir ${BASEDIR}
fi
cd $BASEDIR
#  Copy the WRF module directory to scratch since it is read only in the EB hierarchy.
rsync -avz /sw/eb/sw/WRF/4.3-intel-2021a-dmpar/WRF-4.3/* ./
#  Enable writing in the scratch WRF directory.
chmod -R u+w .

#  Create the input file for configuring WRF.
#  20 - dmpar for INTEL (ifort/icc): Xeon (SNB with AVX mods)
#       dmpar = distributed memory parallelism (i.e. MPI)
#   1 - basic nesting
if [ ! -f configure.input ]; then
echo "create configure.input"
cat<<EOF >configure.input
20
1
EOF
fi
$BASEDIR/clean -a
#  Configure WRF.
($BASEDIR/configure < configure.input) > configure.log 2>&1
#  Change mpif90/mpicc to mpiifort/mpiicc because of using Intel MPI rather than OpenMPI.
sed -i 's/mpif90/mpiifort/' configure.wrf
sed -i 's/mpicc/mpiicc/' configure.wrf

# Compile WRF.
$BASEDIR/compile -j ${NP} em_real > compile.log 2>&1

#  Run WRF.
#  Not sure why we're running this now since the preprocessing steps are below.
#  NEEDS SCRUTINY
cd $BASEDIR
cd run
mpirun -np ${NP} real.exe > run_real.log 2>&1

# Copy the WPS directory over as a subdirectory of the WRF directory.
cd $BASEDIR
rsync -avz /sw/eb/sw/WPS/4.3.1-intel-2021a-dmpar ./
mv 4.3.1-intel-2021a-dmpar WPS-4.3.1
cd WPS-4.3.1
export WRF_DIR=/scratch/user/baum/WRF-4.3
#  Create WPS configuration file.
if [ ! -f configure.input ]; then
echo "create configure.input"
cat<<EOF >configure.input
19
EOF
fi
clean -a
#  Configure WPS.
(./configure < configure.input) > configure.log
#sed -i '68s@glade/u/home/wrfhelp/UNGRIB_LIBRARIES/lib@usr/lib64@' configure.wps
#sed -i '68s@glade/u/home/wrfhelp/UNGRIB_LIBRARIES@usr@' configure.wps
sed -i '73s/mpif90/mpiifort/' configure.wps
sed -i '74s/mpicc/mpiicc/' configure.wps
#  Compile WPS.
./compile > compile.log 2>&1

## run WRF four nodes
cd ${BASEDIR}
#  Create a working directory in the base directory.
mkdir bench
cd bench
#  Copy various required files to the working directory.
cp -rp ../WPS-4.3.1/*.exe ../WPS-4.3.1/link_grib.csh ../WPS-4.3.1/geogrid ../WPS-4.3.1/metgrid ../WPS-4.3.1/ungrib .
cp ${DATADIR}/daily12Z/namelist.wps ${DATADIR}/daily12Z/Vtable .
sed -i "26s@/depot/mebaldwi/data/WPS_GEOG@${DATADIR}/WPS_GEOG/@" namelist.wps
#  Run geogrid to horizontally interpolate static data.
echo " Running geogrid..."
./geogrid.exe > geogrid.log 2>&1
#  Soft link files from the data directory to the working directory.
./link_grib.csh "${DATADIR}/2018081612/nam.t12z.awip32"
#  Run ungrib to extract data from GRIB files.
echo " Running ungrib..."
./ungrib.exe > ungrib.log 2>&1
#  Run metgrib to horizontally interpolate meteorological data.
echo " Running metgrid..."
./metgrid.exe > metgrid.log 2>&1
cp ../run/* .
cp ${DATADIR}/daily12Z/namelist.input .
echo " Running real..."
(time mpirun -np ${NP} real.exe) > real.log 2>&1
(time mpirun -np ${NP} -hostfile /etc/hosts wrf.exe) > wrf.log 2>&1
-----

=== Running an OpenMP Benchmark


==== OMP Environment Variables

https://www.bgu.ac.il/intel_fortran_docs/compiler_f/main_for/mergedProjects/optaps_for/common/optaps_par_var.htm[`https://www.bgu.ac.il/intel_fortran_docs/compiler_f/main_for/mergedProjects/optaps_for/common/optaps_par_var.htm`]

===== OMP_NUM_THREADS

===== OMP_STACKSIZE

https://www.intel.com/content/www/us/en/developer/articles/troubleshooting/openmp-stacksize-common-error.html[`https://www.intel.com/content/www/us/en/developer/articles/troubleshooting/openmp-stacksize-common-error.html`]

=====
The default size is 2m or 4m for IA-32 and IntelÂ® 64 systems. You are able to set it by the byte or you can use shortcuts for instance 1g would set the stack size to 1 gigabyte.
=====

The faster2 node has 256Gb of memory to work with.

Attempts:

-----
    THREADS    OMP_STACKSIZE     VIRT      LoadAverage      Start     End    Time (H:M)

       8          12G          7402340         --           10:45    21:29    10:44 
      16           6G          7200000         16           16:36    23:16     6:40
      32           2G          8187676         32            9:25    13:39     4:14
      64         100M           876072         64           13:55    17:57     4:04
-----

Threads     H:M

 8          10:44
16           6:40
32           4:14
64           4:04

Notes:

* The processes `mpirun', `mpiexec.hydra` and `hydra_pmi_proxy` hung after the simulations completed.  Killing them
won't work unless you also kill the bash process that launched them.  The process ID for this bash process is
one less than that of the `mpirun` process.
* The 32 thread simulation stalled out at the start with an `OMP_STACKSIZE` of 6 GB, which theoretically is
less (6 x 32GB) than the available (256GB) memory.  All 32 `wrf.exe` processes just sat there at 0% CPU usage.
This was killed via the procedure in the previous note.

=== Running the Benchmark

Discover how many nodes are available via:

-----
sinfo -a
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
faster*      up   infinite     17  idle~ faster-hw1-g0s0f0c0i0e0,...,faster-hw1-g16s0f0c0i0e0
-----

Find details about a node via:

-----
scontrol show nodes
NodeName=faster-hw1-g0s0f0c0i0e0 CoresPerSocket=32 
   CPUAlloc=0 CPUTot=32 CPULoad=N/A
   AvailableFeatures=(null)
   ActiveFeatures=(null)
   Gres=(null)
   NodeAddr=faster-hw1-g0s0f0c0i0e0 NodeHostName=faster-hw1-g0s0f0c0i0e0 
   RealMemory=257388 AllocMem=0 FreeMem=N/A Sockets=1 Boards=1
   State=IDLE+CLOUD+POWER ThreadsPerCore=1 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A
   Partitions=faster 
   BootTime=None SlurmdStartTime=None
   CfgTRES=cpu=32,mem=257388M,billing=32
   AllocTRES=
   CapWatts=n/a
   CurrentWatts=0 AveWatts=0
   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s
   Comment=(null)
...
-----

Check the state of the nodes:

-----
sinfo -N
NODELIST                  NODES PARTITION STATE 
faster-hw1-g0s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g1s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g2s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g3s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g4s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g5s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g6s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g7s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g8s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g9s0f0c0i0e0       1   faster* idle~ 
faster-hw1-g10s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g11s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g12s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g13s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g14s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g15s0f0c0i0e0      1   faster* idle~ 
faster-hw1-g16s0f0c0i0e0      1   faster* idle~
-----

Therefore we have 17 nodes available, each of which has a CPU with 32 cores and 256GB of RAM.

==== Running with `mpirun`

We first try to run the benchmark on one node using `mpirun`.

-----
time mpirun -np 1 ./real.exe >& ./real.log &
-----

This obtains the error message:

-----
taskid: 0 hostname: login4.cluster
 module_io_quilt_old.F        2931 T
 Ntasks in X            1 , ntasks in Y            1
  Domain # 1: dx =  3000.000 m
REAL_EM V4.3 PREPROCESSOR
No git found or not a git repository, git commit version not available.
 *************************************
 Parent domain
 ids,ide,jds,jde            1         607           1         537
 ims,ime,jms,jme           -4         612          -4         542
 ips,ipe,jps,jpe            1         607           1         537
 *************************************
DYNAMICS OPTION: Eulerian Mass Coordinate
   alloc_space_field: domain            1 ,            12164371856  bytes allocated
d01 2018-08-16_12:00:00  Yes, this special data is acceptable to use: OUTPUT FROM METGRID V4.3.1
d01 2018-08-16_12:00:00  Input data is acceptable to use: met_em.d01.2018-08-16_12:00:00.nc
 metgrid input_wrf.F first_date_input = 2018-08-16_12:00:00
 metgrid input_wrf.F first_date_nml = 2018-08-16_12:00:00
d01 2018-08-16_12:00:00 Timing for input          2 s.
d01 2018-08-16_12:00:00          flag_soil_layers read from met_em file is  1
Max map factor in domain 1 =  1.00. Scale the dt in the model accordingly.
Using sfcprs3 to compute psfc
[login4:512072:0:512072] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x7fff98a216c8)
==== backtrace (tid: 512072) ====
 0 0x000000000002117e ucs_debug_print_backtrace()  /tmp/baum/easybuild/UCX/1.10.0/GCCcore-10.3.0/ucx-1.10.0/src/ucs/debug/debug.c:656
 1 0x0000000000012b20 .annobin_sigaction.c()  sigaction.c:0
 2 0x000000000044ccf9 module_initialize_real_mp_vert_interp_()  ???:0
 3 0x000000000048d0d0 module_initialize_real_mp_init_domain_rk_()  ???:0
 4 0x0000000000497c0d module_initialize_real_mp_init_domain_()  ???:0
 5 0x0000000000416a60 MAIN__()  ???:0
 6 0x00000000004151e2 main()  ???:0
 7 0x0000000000023493 __libc_start_main()  ???:0
 8 0x00000000004150ee _start()  ???:0
=================================
-----

